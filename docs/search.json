[
  {
    "objectID": "contents/wrangling.html",
    "href": "contents/wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis",
      "Wrangling"
    ]
  },
  {
    "objectID": "contents/wrangling.html#nyc-taxi-and-limousine-commission-record-data",
    "href": "contents/wrangling.html#nyc-taxi-and-limousine-commission-record-data",
    "title": "Wrangling",
    "section": "NYC Taxi and Limousine Commission Record Data",
    "text": "NYC Taxi and Limousine Commission Record Data\n\ntaxi = sns.load_dataset(\"taxis\")\ntaxi\n\n                  pickup             dropoff  passengers  distance  fare  tip  \\\n0    2019-03-23 20:21:09 2019-03-23 20:27:24           1      1.60  7.00 2.15   \n1    2019-03-04 16:11:55 2019-03-04 16:19:00           1      0.79  5.00 0.00   \n2    2019-03-27 17:53:01 2019-03-27 18:00:25           1      1.37  7.50 2.36   \n...                  ...                 ...         ...       ...   ...  ...   \n6430 2019-03-23 22:55:18 2019-03-23 23:14:25           1      4.14 16.00 0.00   \n6431 2019-03-04 10:09:25 2019-03-04 10:14:29           1      1.12  6.00 0.00   \n6432 2019-03-13 19:31:22 2019-03-13 19:48:02           1      3.85 15.00 3.36   \n\n      tolls  total   color      payment            pickup_zone  \\\n0      0.00  12.95  yellow  credit card        Lenox Hill West   \n1      0.00   9.30  yellow         cash  Upper West Side South   \n2      0.00  14.16  yellow  credit card          Alphabet City   \n...     ...    ...     ...          ...                    ...   \n6430   0.00  17.30   green         cash    Crown Heights North   \n6431   0.00   6.80   green  credit card          East New York   \n6432   0.00  20.16   green  credit card            Boerum Hill   \n\n                      dropoff_zone pickup_borough dropoff_borough  \n0              UN/Turtle Bay South      Manhattan       Manhattan  \n1            Upper West Side South      Manhattan       Manhattan  \n2                     West Village      Manhattan       Manhattan  \n...                            ...            ...             ...  \n6430                Bushwick North       Brooklyn        Brooklyn  \n6431  East Flatbush/Remsen Village       Brooklyn        Brooklyn  \n6432               Windsor Terrace       Brooklyn        Brooklyn  \n\n[6433 rows x 14 columns]\n\n\n\ntaxi.describe(include=\"object\")\n\n         color      payment     pickup_zone           dropoff_zone  \\\ncount     6433         6389            6407                   6388   \nunique       2            2             194                    203   \ntop     yellow  credit card  Midtown Center  Upper East Side North   \nfreq      5451         4577             230                    245   \n\n       pickup_borough dropoff_borough  \ncount            6407            6388  \nunique              4               5  \ntop         Manhattan       Manhattan  \nfreq             5268            5206  \n\n\n\n열의 이름 수정\nrename()\n\ntaxi.rename(columns={\"passengers\": \"persons\", \"payment\": \"pay\"}, inplace=True)\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00 0.00   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color          pay            pickup_zone  \\\n0   0.00  12.95  yellow  credit card        Lenox Hill West   \n1   0.00   9.30  yellow         cash  Upper West Side South   \n2   0.00  14.16  yellow  credit card          Alphabet City   \n\n            dropoff_zone pickup_borough dropoff_borough  \n0    UN/Turtle Bay South      Manhattan       Manhattan  \n1  Upper West Side South      Manhattan       Manhattan  \n2           West Village      Manhattan       Manhattan  \n\n\n\n\n값의 대체\n\nidx_cash = (taxi[\"pay\"] == \"cash\")  # Boolean index\n\n# 직접 값을 대입\ntaxi.loc[idx_cash, \"tip\"] = np.nan  # missing(NA) values\n\nnp.where()의 활용\n(boolean condition, value if True, value if False)\n\ntaxi[\"tip\"] = np.where(idx_cash, np.nan, taxi[\"tip\"])\n\nmap()의 활용\ndictionary로 입력\n\ntaxi[\"pay\"] = taxi[\"pay\"].map({\"credit card\": \"Card\", \"cash\": \"Cash\"})\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay            pickup_zone           dropoff_zone  \\\n0   0.00  12.95  yellow  Card        Lenox Hill West    UN/Turtle Bay South   \n1   0.00   9.30  yellow  Cash  Upper West Side South  Upper West Side South   \n2   0.00  14.16  yellow  Card          Alphabet City           West Village   \n\n  pickup_borough dropoff_borough  \n0      Manhattan       Manhattan  \n1      Manhattan       Manhattan  \n2      Manhattan       Manhattan  \n\n\n\n\n새로운 변수 생성\n기본적인 Series들의 연산\n\ntaxi[\"total2\"] = taxi[\"total\"] - taxi[\"tip\"]\n\nassign()의 활용\n\ntaxi = taxi.assign(\n    tip_ratio = lambda x: x[\"tip\"] / x[\"total2\"],\n    tip_ratio_per = lambda x: x.tip_ratio / x.persons  # dot notation\n)\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay            pickup_zone           dropoff_zone  \\\n0   0.00  12.95  yellow  Card        Lenox Hill West    UN/Turtle Bay South   \n1   0.00   9.30  yellow  Cash  Upper West Side South  Upper West Side South   \n2   0.00  14.16  yellow  Card          Alphabet City           West Village   \n\n  pickup_borough dropoff_borough  total2  tip_ratio  tip_ratio_per  \n0      Manhattan       Manhattan   10.80       0.20           0.20  \n1      Manhattan       Manhattan     NaN        NaN            NaN  \n2      Manhattan       Manhattan   11.80       0.20           0.20  \n\n\n\n# total2의 NA값 원래대로 복구\ntaxi[\"total2\"] = np.where(idx_cash, taxi[\"total\"], taxi[\"total2\"])\n\n잠시, 택시요금과 팁 간의 관계를 살펴보면,\n\ntaxi.plot.scatter(x=\"total2\", y=\"tip\", alpha=.2, figsize=(5, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\ntaxi.plot.scatter(x=\"total2\", y=\"tip_ratio\", alpha=.2, figsize=(5, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(taxi, x='total2', y='tip_ratio')\n    .add(so.Dot(alpha=.5, pointsize=2))\n    .add(so.Line(color=\".2\"), so.PolyFit(5))\n    .facet(\"persons\", wrap=4)\n    .layout(size=(8, 5))\n    .label(title=\"{} passengers\".format)\n)\n\n\n\n\n\n\n\n\n\n\n필터링\nquery()의 활용\n\nConditional operators\n&gt;, &gt;=, &lt;, &lt;=,\n== (equal to), != (not equal to)\nand, & (and)\nor, | (or)\nnot, ~ (not)\nin (includes), not in (not included)\n\n\n(\n    taxi.query('color == \"green\" & pickup_borough == \"Manhattan\"')\n    .head(3)\n)\n\n                  pickup             dropoff  persons  distance  fare  tip  \\\n5453 2019-03-29 18:12:27 2019-03-29 18:20:40        1      1.51  7.50 1.20   \n5454 2019-03-06 11:11:33 2019-03-06 11:15:15        1      0.45  4.50  NaN   \n5465 2019-03-27 20:55:35 2019-03-27 21:01:55        1      1.43  7.00 3.00   \n\n      tolls  total  color   pay        pickup_zone           dropoff_zone  \\\n5453   0.00  10.50  green  Card  East Harlem North      East Harlem South   \n5454   0.00   5.30  green  Cash  East Harlem North      East Harlem South   \n5465   0.00  11.30  green  Card     Central Harlem  Upper West Side North   \n\n     pickup_borough dropoff_borough  total2  tip_ratio  tip_ratio_per  \n5453      Manhattan       Manhattan    9.30       0.13           0.13  \n5454      Manhattan       Manhattan    5.30        NaN            NaN  \n5465      Manhattan       Manhattan    8.30       0.36           0.36  \n\n\n\n\n정렬\n\n(\n    taxi.query('persons &gt; 1')\n    .sort_values(\"tip_ratio\", ascending=False)\n)\n\n                  pickup             dropoff  persons  distance  fare  tip  \\\n2923 2019-03-06 23:17:06 2019-03-06 23:24:12        2      1.60  7.50 7.00   \n3272 2019-03-28 11:15:04 2019-03-28 11:21:01        3      0.62  5.50 5.00   \n4912 2019-03-27 18:04:45 2019-03-27 18:11:07        2      0.55  5.50 5.00   \n...                  ...                 ...      ...       ...   ...  ...   \n6405 2019-03-21 18:28:55 2019-03-21 18:51:08        2      3.88 16.50  NaN   \n6412 2019-03-20 17:33:25 2019-03-20 17:42:48        5      1.40  8.00  NaN   \n6420 2019-03-16 15:39:23 2019-03-16 15:46:18        2      1.20  7.00  NaN   \n\n      tolls  total   color   pay                   pickup_zone  \\\n2923   0.00  18.30  yellow  Card  Penn Station/Madison Sq West   \n3272   0.00  13.80  yellow  Card      Financial District North   \n4912   0.00  14.80  yellow  Card                   Murray Hill   \n...     ...    ...     ...   ...                           ...   \n6405   0.00  18.30   green  Cash                   Boerum Hill   \n6412   0.00   9.80   green  Cash              Brooklyn Heights   \n6420   0.00   7.80   green  Cash                Central Harlem   \n\n               dropoff_zone pickup_borough dropoff_borough  total2  tip_ratio  \\\n2923              Hudson Sq      Manhattan       Manhattan   11.30       0.62   \n3272           Battery Park      Manhattan       Manhattan    8.80       0.57   \n4912    UN/Turtle Bay South      Manhattan       Manhattan    9.80       0.51   \n...                     ...            ...             ...     ...        ...   \n6405             Ocean Hill       Brooklyn        Brooklyn   18.30        NaN   \n6412            Fort Greene       Brooklyn        Brooklyn    9.80        NaN   \n6420  Upper West Side North      Manhattan       Manhattan    7.80        NaN   \n\n      tip_ratio_per  \n2923           0.31  \n3272           0.19  \n4912           0.26  \n...             ...  \n6405            NaN  \n6412            NaN  \n6420            NaN  \n\n[1659 rows x 17 columns]\n\n\n\n\nGrouping\n\ngroupby()로 데이터를 의미있는 그룹으로 나눈 후, 다음과 같은 통계치를 계산\nsize(), count(), sum(), mean(), min(), max()\n\n  \n\nSource: Ch.10 in Python for Data Analysis (3e) by Wes McKinney\n\n\ntaxi.groupby(\"color\").size()  # size(): 열의 개수\n\ncolor\ngreen      982\nyellow    5451\ndtype: int64\n\n\n\ntaxi.groupby([\"color\", \"pay\"]).size()  # size(): 열의 개수\n\ncolor   pay \ngreen   Card     577\n        Cash     400\nyellow  Card    4000\n        Cash    1412\ndtype: int64\n\n\n\ntaxi.groupby([\"color\", \"pay\"])[\"total2\"].mean()\n\ncolor   pay \ngreen   Card   18.76\n        Cash   11.40\nyellow  Card   17.08\n        Cash   15.61\nName: total2, dtype: float64\n\n\n\ndf =(\n    taxi.groupby([\"color\", \"pay\"])[\"total2\"].mean()\n    .unstack()  # wide format으로 변환\n)\ndf\n\npay     Card  Cash\ncolor             \ngreen  18.76 11.40\nyellow 17.08 15.61\n\n\n\ndf[\"total\"] = df.sum(axis=1)\ndf\n\npay     Card  Cash  total\ncolor                    \ngreen  18.76 11.40  30.15\nyellow 17.08 15.61  32.68\n\n\n\ndf.assign(\n    Card_pct = lambda x: x.Card / x.total * 100,\n    Cash_pct = lambda x: x.Cash / x.total * 100\n)\n\npay     Card  Cash  total  Card_pct  Cash_pct\ncolor                                        \ngreen  18.76 11.40  30.15     62.21     37.79\nyellow 17.08 15.61  32.68     52.25     47.75\n\n\n여러 함수를 동시에 적용: agg()\n\ntaxi.groupby([\"color\", \"pay\"])[\"total2\"].agg([\"mean\", \"std\", \"size\"])\n\n             mean   std  size\ncolor  pay                   \ngreen  Card 18.76 14.73   577\n       Cash 11.40 10.41   400\nyellow Card 17.08 12.00  4000\n       Cash 15.61 12.38  1412\n\n\n가장 일반적인 방식으로 appy()를 사용하여 사용자 정의 함수를 적용\n\n# standardize\ndef standardize(x):\n    return (x - x.mean()) / x.std()\n\ntaxi.groupby(\"color\", group_keys=True)[\"tip_ratio\"].apply(standardize)\n\ncolor       \ngreen   5451     NaN\n        5452   -0.95\n        5453    0.23\n                ... \nyellow  5448    0.33\n        5449    0.33\n        5450   -1.12\nName: tip_ratio, Length: 6433, dtype: float64\n\n\n\n\n시간 데이터의 처리\n시간을 표시하는 datetime64 타입을 이용해 시간 데이터를 처리하며,\ndt accessor를 사용\n\n# pickup 시간으로부터 요일을 추출\ntaxi[\"day\"] = taxi[\"pickup\"].dt.day_name().str[:3]  # 요일의 앞 3글자\n\n\n# pickup 시간으로부터 시간대를 추출\ntaxi[\"hour\"] = taxi[\"pickup\"].dt.hour\n\n\n# 택시를 탄 시간(분)을 계산\ntaxi[\"duration\"] = (taxi[\"dropoff\"] - taxi[\"pickup\"]).dt.total_seconds() / 60\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay            pickup_zone           dropoff_zone  \\\n0   0.00  12.95  yellow  Card        Lenox Hill West    UN/Turtle Bay South   \n1   0.00   9.30  yellow  Cash  Upper West Side South  Upper West Side South   \n2   0.00  14.16  yellow  Card          Alphabet City           West Village   \n\n  pickup_borough dropoff_borough  total2  tip_ratio  tip_ratio_per  day  hour  \\\n0      Manhattan       Manhattan   10.80       0.20           0.20  Sat    20   \n1      Manhattan       Manhattan    9.30        NaN            NaN  Mon    16   \n2      Manhattan       Manhattan   11.80       0.20           0.20  Wed    17   \n\n   duration  \n0      6.25  \n1      7.08  \n2      7.40  \n\n\n요일과 시간대별로 팁의 비율은 다른가?\n\ntaxi.groupby([\"day\", \"hour\"])[\"tip_ratio\"].mean()\n\nday  hour\nFri  0      0.17\n     1      0.19\n     2      0.12\n            ... \nWed  21     0.18\n     22     0.19\n     23     0.23\nName: tip_ratio, Length: 167, dtype: float64\n\n\npivot_table()를 활용할 수도 있음\n\ntaxi.pivot_table(\"tip_ratio\", \"hour\", \"day\")\n\nday   Fri  Mon  Sat  Sun  Thu  Tue  Wed\nhour                                   \n0    0.17 0.19 0.17 0.18 0.17 0.22 0.19\n1    0.19  NaN 0.18 0.18 0.20 0.20 0.18\n2    0.12 0.18 0.18 0.19 0.22 0.20 0.17\n...   ...  ...  ...  ...  ...  ...  ...\n21   0.17 0.18 0.15 0.18 0.18 0.17 0.18\n22   0.17 0.17 0.18 0.18 0.16 0.18 0.19\n23   0.18 0.22 0.17 0.19 0.19 0.18 0.23\n\n[24 rows x 7 columns]\n\n\n\n# day를 categorical 타입으로 변환\ntaxi[\"day\"] = pd.Categorical(taxi[\"day\"], categories=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n\n\ndf = taxi.pivot_table(\"tip_ratio\", \"hour\", \"day\")\n\nfig, axes = plt.subplots(6, 4, figsize=(8, 8), sharex=True, sharey=True)\nfig.subplots_adjust(wspace=0.1, hspace=0.3)\n\nfor i, ax in enumerate(axes.flat):\n    df.iloc[i].plot.bar(ax=ax, ylim=(0.1, 0.23))\n    ax.set_title(f\"hour={i}\", fontsize=9)\n\n\n\n\n\n\n\n\n\ntaxi_day_hour = (\n    taxi.groupby([\"day\", \"hour\"])[\"tip_ratio\"]\n    .agg([\"mean\", \"size\"])\n    .reset_index()\n    .rename(columns={\"mean\": \"tip_ratio\", \"size\": \"n\"})\n)\ntaxi_day_hour\n\n     day  hour  tip_ratio   n\n0    Mon     0       0.19  13\n1    Mon     1        NaN   2\n2    Mon     2       0.18   6\n..   ...   ...        ...  ..\n165  Sun    21       0.18  42\n166  Sun    22       0.18  32\n167  Sun    23       0.19  23\n\n[168 rows x 4 columns]\n\n\n\n(\n    so.Plot(taxi_day_hour, y='day', x='tip_ratio', color='n')\n    .add(so.Bar(width=.5))\n    .facet(\"hour\", wrap=6)\n    .layout(size=(8, 5.5))\n    .limit(x=(0.12, 0.23))\n    .theme({'ytick.labelsize': 9})\n)\n\n\n\n\n\n\n\n\n\n\n데이터 프레임들의 결합\n\nMerge\n\nmerge()를 사용하여 두 데이터 프레임을 join\nKey에 해당하는 변수들의 값이 매치되는 행들을 찾아 결합\n결합 방식: “inner”, “left”, “right”, “outer”\n\n\nInner join의 예:\n\nnyc_neighborhood.csv dataset\n\nnyc = pd.read_csv(\"data/nyc_neighborhood.csv\")\nnyc\n\n         borough  units  labor  carfree  density      pop\n0      Manhattan   5083   0.68     0.88    71.90  1628706\n1          Bronx   6120   0.59     0.71    33.70  1418207\n2       Brooklyn  10129   0.64     0.75    36.90  2559903\n3         Queens   6752   0.64     0.59    20.70  2253858\n4  Staten Island    582   0.62     0.34     8.30   476143\n\n\n\n(\n    taxi.merge(\n        nyc, left_on=\"pickup_borough\", right_on=\"borough\", how=\"left\"\n    ).head(3)\n)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay  ... tip_ratio_per  day hour duration  \\\n0   0.00  12.95  yellow  Card  ...          0.20  Sat   20     6.25   \n1   0.00   9.30  yellow  Cash  ...           NaN  Mon   16     7.08   \n2   0.00  14.16  yellow  Card  ...          0.20  Wed   17     7.40   \n\n     borough   units  labor carfree  density        pop  \n0  Manhattan 5083.00   0.68    0.88    71.90 1628706.00  \n1  Manhattan 5083.00   0.68    0.88    71.90 1628706.00  \n2  Manhattan 5083.00   0.68    0.88    71.90 1628706.00  \n\n[3 rows x 26 columns]\n\n\nMerge를 데이터 필터링에 사용할 수도 있음\n\ntaxi_day_hour\n\n     day  hour  tip_ratio   n\n0    Mon     0       0.19  13\n1    Mon     1        NaN   2\n2    Mon     2       0.18   6\n..   ...   ...        ...  ..\n165  Sun    21       0.18  42\n166  Sun    22       0.18  32\n167  Sun    23       0.19  23\n\n[168 rows x 4 columns]\n\n\n\ndays_top_ratio = (\n    taxi_day_hour\n    .query('n &gt; 10')\n    .nlargest(3, \"tip_ratio\")\n)\ndays_top_ratio\n\n    day  hour  tip_ratio   n\n71  Wed    23       0.23  39\n23  Mon    23       0.22  22\n24  Tue     0       0.22  13\n\n\n팁의 비율이 가장 높았던 3개의 (요일, 시간대) 조합에 해당하는 taxi 데이터셋을 추리려면\n\ntaxi.merge(days_top_ratio, on=[\"day\", \"hour\"])\n\n                pickup             dropoff  persons  distance  fare  tip  \\\n0  2019-03-25 23:05:54 2019-03-25 23:11:13        1      0.80  5.50 2.30   \n1  2019-03-27 23:10:02 2019-03-27 23:22:11        3      2.77 11.00 3.70   \n2  2019-03-20 23:19:55 2019-03-20 23:46:00        1      4.82 19.50 4.66   \n..                 ...                 ...      ...       ...   ...  ...   \n71 2019-03-18 23:48:38 2019-03-18 23:56:36        1      2.00  8.50  NaN   \n72 2019-03-06 23:32:50 2019-03-06 23:34:31        1      0.30  3.50 2.50   \n73 2019-03-26 00:06:16 2019-03-26 00:12:46        1      0.86  6.00  NaN   \n\n    tolls  total   color   pay  ... pickup_borough dropoff_borough total2  \\\n0    0.00  11.60  yellow  Card  ...      Manhattan       Manhattan   9.30   \n1    0.00  18.50  yellow  Card  ...      Manhattan       Manhattan  14.80   \n2    0.00  27.96  yellow  Card  ...      Manhattan       Manhattan  23.30   \n..    ...    ...     ...   ...  ...            ...             ...    ...   \n71   0.00   9.80   green  Cash  ...         Queens          Queens   9.80   \n72   0.00   7.30   green  Card  ...         Queens          Queens   4.80   \n73   0.00   7.30   green  Cash  ...      Manhattan       Manhattan   7.30   \n\n   tip_ratio_x  tip_ratio_per  day  hour duration  tip_ratio_y   n  \n0         0.25           0.25  Mon    23     5.32         0.22  22  \n1         0.25           0.08  Wed    23    12.15         0.23  39  \n2         0.20           0.20  Wed    23    26.08         0.23  39  \n..         ...            ...  ...   ...      ...          ...  ..  \n71         NaN            NaN  Mon    23     7.97         0.22  22  \n72        0.52           0.52  Wed    23     1.68         0.23  39  \n73         NaN            NaN  Tue     0     6.50         0.22  13  \n\n[74 rows x 22 columns]\n\n\n\n\nConcatenate\npd.concat([df1, df2, ...], axis=)\n행과 열의 index를 매치시켜 두 DataFrame/Series를 합침\n\ndf1 = pd.DataFrame(\n    np.arange(6).reshape(3, 2), index=[\"a\", \"b\", \"c\"], columns=[\"one\", \"two\"]\n)\ndf2 = pd.DataFrame(\n    5 + np.arange(4).reshape(2, 2), index=[\"a\", \"c\"], columns=[\"three\", \"four\"]\n)\n\n\n\n\n   one  two\na    0    1\nb    2    3\nc    4    5\n\n\n   three  four\na      5     6\nc      7     8\n\n\n\n\npd.concat([df1, df2], axis=1)\n\n   one  two  three  four\na    0    1   5.00  6.00\nb    2    3    NaN   NaN\nc    4    5   7.00  8.00\n\n\n\npd.concat([df1, df2])  # default: axis=0\n\n   one  two  three  four\na 0.00 1.00    NaN   NaN\nb 2.00 3.00    NaN   NaN\nc 4.00 5.00    NaN   NaN\na  NaN  NaN   5.00  6.00\nc  NaN  NaN   7.00  8.00",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis",
      "Wrangling"
    ]
  },
  {
    "objectID": "contents/tree.html",
    "href": "contents/tree.html",
    "title": "Tree-based Models",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.5f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 5, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nsckit-learn packages\nimport sklearn.model_selection as skm\nfrom ISLP import load_data, confusion_table\nfrom ISLP.models import ModelSpec as MS\n\nfrom sklearn.tree import (DecisionTreeClassifier as DTC,\n                          DecisionTreeRegressor as DTR,\n                          plot_tree,\n                          export_text)\nfrom sklearn.metrics import (accuracy_score,\n                             log_loss)\nfrom sklearn.ensemble import \\\n     (RandomForestRegressor as RF,\n      GradientBoostingRegressor as GBR)\nfrom ISLP.bart import BART",
    "crumbs": [
      "Home",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#regression",
    "href": "contents/tree.html#regression",
    "title": "Tree-based Models",
    "section": "Regression",
    "text": "Regression\nBaseball Data\nMajor League Baseball Data from the 1986 and 1987 seasons.\n\nfrom ISLP import load_data\n\nHitters = load_data('Hitters')\nHitters = Hitters.dropna()  # 결측치 제거\nHitters.head(3)\n\n   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n1    315    81      7    24   38     39     14    3449    835      69    321   \n2    479   130     18    66   72     76      3    1624    457      63    224   \n3    496   141     20    65   78     37     11    5628   1575     225    828   \n\n   CRBI  CWalks League Division  PutOuts  Assists  Errors    Salary NewLeague  \n1   414     375      N        W      632       43      10 475.00000         N  \n2   266     263      A        W      880       82      14 480.00000         A  \n3   838     354      N        E      200       11       3 500.00000         N  \n\n\n\n\ncode\n(\n    so.Plot(Hitters, x='Years', y='Hits', color='Salary')\n    .add(so.Dot())\n    .scale(color=so.Continuous('Blues', norm=(0, 2000), trans=\"sqrt\"))\n    .layout(size=(5, 4))\n)\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor as DTR, plot_tree, export_text\n\nX = Hitters[[\"Years\", \"Hits\"]]\ny = np.log(Hitters.Salary)  # 종모양의 분포로 변환\n\n# split the data\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.5, random_state=2)\n\nreg_tree = DTR(max_depth=2, random_state=0)        \nreg_tree.fit(X_train, y_train)\n\nDecisionTreeRegressor(max_depth=2, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(max_depth=2, random_state=0) \n\n\n\ncode\ndef visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n    ax = ax or plt.gca()\n    \n    # Plot the training points\n    ax.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, s=30, cmap=\"Blues\",\n               clim=(y.min(), y.max()), zorder=3)\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # fit the estimator\n    model.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n    Z = np.round(Z, 2)\n\n    # Create a color plot with the results\n    contours = ax.contour(xx, yy, Z, colors=\"#fc4f30\", linewidths=.5, zorder=1)\n    # plt.clabel(contours, inline=True, fontsize=8)\n    ax.set(xlim=xlim, ylim=ylim)\n\nfig, ax = plt.subplots(figsize=(5, 6.5))\nvisualize_classifier(reg_tree, X_train, y_train)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(6, 6))\nplot_tree(reg_tree, feature_names=reg_tree.feature_names_in_.tolist(), ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nprint(export_text(reg_tree, feature_names=reg_tree.feature_names_in_.tolist(), show_weights=True))\n\n|--- Years &lt;= 4.50\n|   |--- Years &lt;= 3.50\n|   |   |--- value: [4.73]\n|   |--- Years &gt;  3.50\n|   |   |--- value: [5.55]\n|--- Years &gt;  4.50\n|   |--- Hits &lt;= 117.50\n|   |   |--- value: [5.97]\n|   |--- Hits &gt;  117.50\n|   |   |--- value: [6.78]\n\n\n\n용어들\n\nTerminal/leaf node (터미널/리프 노드): 트리의 맨 끝에 있는 노드; predictor space에서 나뉘어진 각 영역을 나타냄\nInternal node (내부 노드): 트리의 중간에 있는 노드; predictor space에서 나뉘어진 지점\nBranch (가지): 노드와 노드를 연결하는 선\n\n\n\n\n\n\n\nInteractive Plots\n\n\n\n\n\nInteractive plot for decision boundary\nfrom ipywidgets import interact, fixed\n\ndef interactive_boundary(depth=1, ax=None):\n    reg_tree = DTR(max_depth=depth, random_state=0)\n    \n    fig, ax = plt.subplots(figsize=(6, 6))\n    visualize_classifier(reg_tree, X_train, y_train, ax=ax)\n\ninteract(interactive_boundary, depth=(1, 10), ax=fixed(None))\nplt.show()\n\n\n\n\n분할 알고리즘\n\n각 분할에 포함된 X값에 대해서 Y값들의 평균값을 그 예측값으로 할당\n잔차의 제곱합(RSS)을 최소화하는 분할을 찾음\n계산적으로 불가능하므로 대안적인 방법을 사용\n\nTop-down, greedy approach (recursive binary splitting)\n\n모든 예측변수들과 각 예측변수의 모든 가능한 분할점에 대해 RSS가 최소가 되는 예측변수(\\(X_j\\))와 분할점(\\(s\\))을 찾음\n즉, \\(\\displaystyle RSS = \\sum_{i: x_i \\in R_1(j, s)}(y_i - \\hat{y}_{R_1})^2 + \\sum_{i: x_i \\in R_2(j, s)}(y_i - \\hat{y}_{R_2})^2\\)을 최소화하는 \\(j\\)와 \\(s\\)를 찾음\n\n\\(\\hat{y}_{R_1}\\): \\(R_1\\)에 있는 관측치들의 \\(y\\)값들의 평균\n\\(\\hat{y}_{R_2}\\): \\(R_2\\)에 있는 관측치들의 \\(y\\)값들의 평균\n\n이를 통해 영역을 나눔: \\(R_1(j, s) = \\{X|X_j &lt; s\\}\\), \\(R_2(j, s) = \\{X|X_j \\geq s\\}\\)\n각 영역에서 반복적으로 이러한 분할을 수행\n특정 기준점에 도달할 때까지 반복;\n\n예를 들어, 모든 영역에서 5개 이상의 관측치가 남아있지 않을 때까지\n또는 트리의 깊이가 특정 수준에 도달할 때까지\n또는 터미널 노드의 수가 특정 수준에 도달할 때까지\n\n\n\n\n  Source: p. 335, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n\n\nPruning (가지치기)\n\nCost-complexity pruning\n트리의 깊이를 높게 하면, 훈련셋에 대한 예측력은 높아지지만, 테스트셋에 대한 예측력은 낮아질 수 있음\n\n즉, 과적합(overfitting)이 발생할 수 있음\n\n과적합을 조절하기 위해 매우 깊은 트리(\\(T_0\\))를 우선 만들고, 이후에 가지치기(pruning)를 수행\n각 \\(\\alpha\\)에 대해 다음을 최소화하는 subtree \\(T \\subset T_0\\)가 존재함.\n\\(\\displaystyle \\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m}(y_i - \\hat{y}_{R_m})^2 + \\alpha|T|\\)\n\\(\\alpha\\)가 커질수록, 트리의 크기에 대한 페널티가 커짐\nCross-validation을 통해 최적의 \\(\\alpha\\)를 찾음\n\n\ncols = [\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\", \"Years\", \"PutOuts\", \"Assists\", \"Errors\"]\n\nX = Hitters[cols]\ny = np.log(Hitters[\"Salary\"])  # 종모양의 분포로 변환\n\n# split the data\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=.5, random_state=1)\n\nreg_tree = DTR(min_samples_leaf=5, random_state=0)        \nreg_tree.fit(X_train, y_train)\n\nDecisionTreeRegressor(min_samples_leaf=5, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(min_samples_leaf=5, random_state=0) \n\n\n\nfig, ax = plt.subplots(figsize=(15, 6))\nplot_tree(reg_tree,\n          feature_names=reg_tree.feature_names_in_.tolist(),\n          ax=ax, fontsize=7);\n\n\n\n\n\n\n\n\nCost-complexity pruning\n\nccp_path = reg_tree.cost_complexity_pruning_path(X_train, y_train)\nkfold = skm.KFold(5, shuffle=True, random_state=0)\ngrid = skm.GridSearchCV(reg_tree,\n                        {'ccp_alpha': ccp_path['ccp_alphas']},\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=DecisionTreeRegressor(min_samples_leaf=5,\n                                             random_state=0),\n             param_grid={'ccp_alpha': array([0.     , 0.00046, 0.0009 , 0.0011 , 0.0014 , 0.00277, 0.00294,\n       0.00363, 0.00505, 0.00574, 0.01015, 0.01114, 0.01298, 0.01535,\n       0.01566, 0.02022, 0.0211 , 0.02737, 0.04831, 0.10957, 0.32311])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=DecisionTreeRegressor(min_samples_leaf=5,\n                                             random_state=0),\n             param_grid={'ccp_alpha': array([0.     , 0.00046, 0.0009 , 0.0011 , 0.0014 , 0.00277, 0.00294,\n       0.00363, 0.00505, 0.00574, 0.01015, 0.01114, 0.01298, 0.01535,\n       0.01566, 0.02022, 0.0211 , 0.02737, 0.04831, 0.10957, 0.32311])},\n             scoring='neg_mean_squared_error') estimator: DecisionTreeRegressorDecisionTreeRegressor(min_samples_leaf=5, random_state=0)  DecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(min_samples_leaf=5, random_state=0) \n\n\n\ngrid.best_params_\n\n{'ccp_alpha': 0.0153509442666242}\n\n\n\n# MSE\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, grid.best_estimator_.predict(X_test))\n\n0.4640166709041431\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_tree(grid.best_estimator_, precision=1,\n          feature_names=grid.best_estimator_.feature_names_in_.tolist(),\n          ax=ax);\n\n\n\n\n\n\n\n\n\ncode\nfrom sklearn.metrics import mean_squared_error\n\nK = 5\nresults = []\n\nfor i, alpha in enumerate(ccp_path['ccp_alphas']):\n    reg_tree = DTR(ccp_alpha=alpha, min_samples_leaf=5, random_state=0)\n\n    # training, test error\n    reg_tree.fit(X_train, y_train)\n    n_leaves = reg_tree.get_n_leaves()\n    mse_train = mean_squared_error(y_train, reg_tree.predict(X_train))\n    mse_test = mean_squared_error(y_test, reg_tree.predict(X_test))\n\n    # cross-validation error\n    kfold = skm.KFold(K, shuffle=True, random_state=0)\n    cv_results = skm.cross_validate(reg_tree, X_train, y_train, cv=kfold, return_estimator=True, scoring='neg_mean_squared_error')\n\n    results.append({\"alpha\": alpha, \"i\": i, \"n_leaves\": n_leaves, \"mse_train\": mse_train, \"mse_test\": mse_test, \"cv\": -cv_results[\"test_score\"].mean()})\n\nresults = pd.DataFrame(results)\nresults = results.query(\"n_leaves &lt; 11\")\nresults\n\n\n\ncode\n# plot the results\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.lineplot(data=results, x=\"n_leaves\", y=\"mse_train\", label=\"Training\", marker=\"o\", c=\"#fc4f30\", ax=ax)\nsns.lineplot(data=results, x=\"n_leaves\", y=\"mse_test\", label=\"Test\", marker=\"o\", c=\"#7d2be2\", ax=ax)\nsns.lineplot(data=results, x=\"n_leaves\", y=\"cv\", label=\"Cross-Validation\", marker=\"o\", c=\"#00A08A\", ax=ax)\n\nax.set(xlabel=\"Tree Size\", ylabel=\"Mean Squared Error\")\nsns.despine()\nax.get_legend().set_frame_on(False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreg_tree_x = DTR(ccp_alpha=0.012, random_state=0)\nreg_tree_x.fit(X_train, y_train)\n\nDecisionTreeRegressor(ccp_alpha=0.012, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(ccp_alpha=0.012, random_state=0) \n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_tree(reg_tree_x, feature_names=reg_tree_x.feature_names_in_.tolist(), ax=ax);",
    "crumbs": [
      "Home",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#classification",
    "href": "contents/tree.html#classification",
    "title": "Tree-based Models",
    "section": "Classification",
    "text": "Classification\n회귀문제와 다른 점은\n\n예측값을 각 영역에서 가장 많이 나타나는 클래스에 할당.\n동시에, 각 클래스에 속한 비율(class proportion)을 얻을 수 있음.\n\n\n분할 알고리즘\n\n회귀에서 RSS를 최소화하도록 분할한 것과 비슷하게 분류에서는 다음을 기준으로 분할\n\nclassification error rate: 예측 오류률가 낮아지도록 분할\nGini index, cross-entropy: 노드의 불순도(impurity)가 낮아지도록 분할. 즉, 한 클래스에 주로 속하도록 분할\n\n동일한 클래스로 예측되는 노드라 할지라도 노드의 순도를 높이면 (해당) 클래스에 속할 확률을 높여 예측에 대한 확신을 높일 수 있음.\n\n\n새로운 노드들로부터 (가중치를 고려해) 평균적인 오류률/불순도가 줄어들 때 분할을 하며, 그 오류률/불순도가 최소가 되도록 분할\n\nClassification error rate\n\n\n\n\n\\(E = 1 - \\max_k(\\hat{p}_{mk})\\),   \\(\\hat{p}_{mk}\\): 영역 \\(R_m\\)에서 클래스 \\(C_k\\)에 속한 관측치의 비율\n\n동그라미: 0.7, 세모: 0.2, 네모: 0.1\nMisclassication error rate = 1 - 0.7 = 0.3\n\n예측의 정확도를 높이기 위해 선호됨\n반면, 트리의 분할에 대해 덜 민감해 큰 트리를 만들 수 없음.\n\n\n\n\n\n\n\nGini index\n\n\\(G = \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk}) = 1 - \\sum_{k=1}^K \\hat{p}_{mk}^2\\),   \\(\\hat{p}_{mk}\\): 영역 \\(R_m\\)에서 클래스 \\(C_k\\)에 속한 관측치의 비율\n\n각 클래스에 대해서 그 클래스에 속하면 1 아니면 0으로 코딩했을 때,\n해석: a measure of total variance across the K classes\n\\(G = 0.7*(1-0.7) + 0.2*(1-0.2) + 0.1*(1-0.1) = 0.46\\)\n\n노드의 불순도(impurity)를 측정; 여러 클래스로 나뉘어져 분포하는 정도\n\\(G\\)가 작을수록, 한 개의 클래스에 주로 속해있음을 의미\n\n\n\n\n\n\n\nk1 = np.array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0])\nk2 = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 0])\nk3 = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n\n# variance = p(1-p)\nprint(k1.var(), k2.var(), k3.var())\n#&gt; 0.21, 0.16, 0.09\n\n\n\n\nCross-entropy/deviance\n\n\\(D = -\\sum_{k=1}^K \\hat{p}_{mk} \\log(\\hat{p}_{mk})\\)\nGini index와 유사하며, 노드의 불순도를 측정\n\nGini index와 cross-entropy가 노드 순수도에 더 민감하게 반응하며 더 큰 트리를 만들 수 있음.\n\n예를 들어, 다음 두 가지 분할에 대해 오류율은 동일한 반면,\nGini index는 차이를 보이고, 오른쪽 분할이 더 순수한 분할로 간주됨.\n\n\n왼쪽\n\n분할 전: 오류율, Gini index = 0.5\n분할 후:\n\n오류율 = \\(\\frac{4}{8} \\cdot \\frac{1}{4} + \\frac{4}{8} \\cdot \\frac{1}{4}=0.25\\)\nGini index = \\(\\frac{4}{8} \\cdot \\left(\\frac{3}{4}\\cdot \\frac{1}{4} + \\frac{1}{4}\\cdot \\frac{3}{4} \\right) + \\frac{4}{8} \\cdot \\left(\\frac{1}{4}\\cdot \\frac{3}{4} + \\frac{3}{4}\\cdot \\frac{1}{4} \\right)=0.375\\)\n\n\n오른쪽\n\n분할 후:\n\n오류율 = \\(\\frac{6}{8} \\cdot \\frac{2}{6} + \\frac{2}{8} \\cdot \\frac{0}{2}=0.25\\)\nGini index = \\(\\frac{6}{8} \\cdot \\left(\\frac{2}{6}\\cdot \\frac{4}{6} + \\frac{4}{6}\\cdot \\frac{2}{6} \\right) + \\frac{2}{8} \\cdot \\left(\\frac{2}{2}\\cdot \\frac{0}{2} + \\frac{0}{2}\\cdot \\frac{2}{2} \\right)=0.333...\\)\n\n\n세 지표들의 비교\n\n\n\n\n\n\n\n\npenguins = sns.load_dataset(\"penguins\")\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier as DTC\n\nX = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\ny = penguins[\"species\"].map({\"Adelie\": 0, \"Chinstrap\": 1, \"Gentoo\": 2})\n\nclf_tree_2 = DTC(max_depth=2, criterion=\"gini\", random_state=0)\nclf_tree_10 = DTC(max_depth=10, criterion=\"gini\", random_state=0)\n\n\ncode\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(colors[:3])\n\ndef visualize_classifier2(model, X, y, ax=None, cmap='rainbow'):\n    ax = ax or plt.gca()\n    \n    # Plot the training points\n    ax.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, s=50, edgecolors=\"w\", linewidths=.5, cmap=cmap, clim=(y.min(), y.max()), zorder=3)\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # fit the estimator\n    model.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n    # Create a color plot with the results\n    n_classes = len(np.unique(y))\n    contours = ax.contourf(xx, yy, Z, alpha=0.15,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap=cmap, zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)\n\nfig, ax = plt.subplots(figsize=(5.5, 5))\nvisualize_classifier2(clf_tree_2, X, y, cmap=custom_cmap)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(5.5, 5))\nvisualize_classifier2(clf_tree_10, X, y, cmap=custom_cmap)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_tree(clf_tree_2, feature_names=clf_tree_2.feature_names_in_.tolist(), ax=ax);\n\n\n\n\n\n\n\n\nconfusion_table(clf_tree_2.predict(X), y)\n\nTruth        0   1    2\nPredicted              \n0          139   1    0\n1           12  67    9\n2            1   0  115",
    "crumbs": [
      "Home",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#의사-결정-트리의-장단점",
    "href": "contents/tree.html#의사-결정-트리의-장단점",
    "title": "Tree-based Models",
    "section": "의사 결정 트리의 장단점",
    "text": "의사 결정 트리의 장단점\n장점\n\n사람들에게 설명하기 매우 쉽우며, 사실 선형 회귀보다 설명하기가 더 쉬음.\n의사 결정 트리가 다른 접근법보다 인간의 의사 결정을 더 잘 반영한다고 보기도 함.\n다이어그램으로 표시할 수 있으며, 비전문가도 쉽게 해석할 수 있음.(특히 크기가 작은 경우).\n더미 변수를 만들 필요 없이 자연스럽게 범주형 변수를 처리할 수 있음. (단, scikit-learn에서는 아직…)\n\n단점\n\n일반적으로 다른 회귀 및 분류 접근 방식보다 예측 정확도가 낮음.\nNon-robust: 데이터의 작은 변화에 민감하게 트리에 큰 변화가 생길 수 있음.",
    "crumbs": [
      "Home",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#sales-of-child-car-seats-예제",
    "href": "contents/tree.html#sales-of-child-car-seats-예제",
    "title": "Tree-based Models",
    "section": "Sales of Child Car Seats 예제",
    "text": "Sales of Child Car Seats 예제\nCarseats 설명\n\ncarseats = load_data('Carseats')\ncarseats.head(3)\n\n     Sales  CompPrice  Income  Advertising  Population  Price ShelveLoc  Age  \\\n0  9.50000        138      73           11         276    120       Bad   42   \n1 11.22000        111      48           16         260     83      Good   65   \n2 10.06000        113      35           10         269     80    Medium   59   \n\n   Education Urban   US  \n0         17   Yes  Yes  \n1         10   Yes  Yes  \n2         12   Yes  Yes  \n\n\n\n\n# 카테고리 변수를 더미 변수로 변환\nX = pd.get_dummies(carseats.drop(columns=\"Sales\"), drop_first=True)\n\n# 분류 문제로 변환\ny = np.where(carseats[\"Sales\"] &gt; 8, \"high\", \"low\")\n\n\n# fit the model\nclf_tree = DTC(criterion='entropy', max_depth=3, random_state=0)\nclf_tree.fit(X, y)\n\nDecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0) \n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_tree(clf_tree, feature_names=clf_tree.feature_names_in_.tolist(), ax=ax);\n\n\n\n\n\n\n\n\n\nprint(export_text(clf_tree, feature_names=clf_tree.feature_names_in_.tolist(), show_weights=True))\n\n|--- ShelveLoc_Good &lt;= 0.50\n|   |--- Price &lt;= 92.50\n|   |   |--- Income &lt;= 57.00\n|   |   |   |--- weights: [3.00, 7.00] class: low\n|   |   |--- Income &gt;  57.00\n|   |   |   |--- weights: [29.00, 7.00] class: high\n|   |--- Price &gt;  92.50\n|   |   |--- Advertising &lt;= 13.50\n|   |   |   |--- weights: [41.00, 183.00] class: low\n|   |   |--- Advertising &gt;  13.50\n|   |   |   |--- weights: [25.00, 20.00] class: high\n|--- ShelveLoc_Good &gt;  0.50\n|   |--- Price &lt;= 135.00\n|   |   |--- US_Yes &lt;= 0.50\n|   |   |   |--- weights: [11.00, 6.00] class: high\n|   |   |--- US_Yes &gt;  0.50\n|   |   |   |--- weights: [49.00, 2.00] class: high\n|   |--- Price &gt;  135.00\n|   |   |--- Income &lt;= 46.00\n|   |   |   |--- weights: [0.00, 6.00] class: low\n|   |   |--- Income &gt;  46.00\n|   |   |   |--- weights: [6.00, 5.00] class: high\n\n\n\nTraining error에 대해 살펴보면,\n\nconfusion_table(clf_tree.predict(X), y)\n\nTruth      high  low\nPredicted           \nhigh        120   40\nlow          44  196\n\n\n\nfrom sklearn.metrics import accuracy_score, log_loss\n\naccuracy_score(y, clf_tree.predict(X))\n\n0.79\n\n\n좀 더 적절하게 cross-validation을 이용해 test error를 구하면,\n\nkfold = skm.KFold(n_splits=5, shuffle=True, random_state=0)\nresults = skm.cross_validate(clf_tree, X, y, scoring='accuracy', cv=kfold)\nresults['test_score']\n\narray([0.7125, 0.7625, 0.7625, 0.6125, 0.7125])\n\n\nTree-pruning에 의해 더 나은 트리를 얻을 수 있는지 확인\n우선 큰 트리를 만들기 위해, max_depth를 설정하지 않고 트리를 만든 후, 적절한 pruning을 얻기 위해 cross-validation을 이용함.\n\n# train-test split\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.5, random_state=0)\n\n\nclf_tree = DTC(criterion='entropy', random_state=0)\nclf_tree.fit(X_train, y_train)\n\nDecisionTreeClassifier(criterion='entropy', random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(criterion='entropy', random_state=0) \n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_tree(clf_tree, feature_names=clf_tree.feature_names_in_.tolist(), ax=ax);\n\n\n\n\n\n\n\n\n\nclf_tree.tree_.n_leaves\n\n35\n\n\n\naccuracy_score(y_test, clf_tree.predict(X_test))\n\n0.705\n\n\nCost-complexity pruning\n\nccp_path = clf_tree.cost_complexity_pruning_path(X_train, y_train)\nkfold = skm.KFold(10, random_state=1, shuffle=True)\n\ngrid = skm.GridSearchCV(clf_tree,\n                        {'ccp_alpha': ccp_path.ccp_alphas},\n                        refit=True,\n                        cv=kfold,\n                        scoring='accuracy')\ngrid.fit(X_train, y_train)\ngrid.best_score_\n\n0.6849999999999999\n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nbest_ = grid.best_estimator_\nplot_tree(best_, feature_names=best_.feature_names_in_.tolist(), fontsize=6, ax=ax);\n\n\n\n\n\n\n\n\n\nbest_.tree_.n_leaves\n\n10\n\n\n\naccuracy_score(y_test, best_.predict(X_test))\n\n0.68\n\n\n\nconfusion_table(best_.predict(X_test), y_test)\n\nTruth      high  low\nPredicted           \nhigh         38   20\nlow          44   98\n\n\npruning을 통해 accuracy에서 작은 손해를 보았으나; 0.705에서 0.68로 감소\n35개의 터미널 노드 가진 full tree에서 10개의 터미널 노드로 줄였음.\n\n\n\n\n\n\nDecision tree에서 확률의 의미\n\n\n\n\n\n리프 노드에서 각 클래스에 속하는 관측치들의 비율\nbest_.predict_proba(X_test)[:3]\n# array([[0.30303, 0.69697],\n#        [0.775  , 0.225  ],\n#        [0.775  , 0.225  ]])\n\nX_test[:3]\n#     CompPrice  Income  Advertising  Population  Price  Age  Education  \n# 132       125      87            9         232    136   72        10   \n# 309       131     111           13          33     80   68        18   \n# 341        98     120            0         268     93   72        10   \n\n#      ShelveLoc_Good  ShelveLoc_Medium  Urban_Yes  US_Yes  \n# 132            True             False       True    True  \n# 309           False             False       True    True  \n# 341           False              True      False   False",
    "crumbs": [
      "Home",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#bagging",
    "href": "contents/tree.html#bagging",
    "title": "Tree-based Models",
    "section": "Bagging",
    "text": "Bagging\nBootstrap aggregation\n\n부트스트랩으로 추출되는 여러 데이터셋으로 모형을 학습\n특히, decision tree에서 흔히 사용되는데, 이는 decision tree가 variance가 매우 높은 모형이기 때문\n\n여러 샘플들로부터 얻은 통계치들을 평균내면 그 분포의 분산이 줄어드는 현상이 있는데, 실제로 여러 샘플을 얻을 수 없기 때문에 부트스트랩으로 가상의 여러 샘플을 얻어 이를 이용.\n\n\n\n\n\n\nthe sampling distribution of the mean\n\n\n\n\n\n평균이 \\(\\mu\\), 분산이 \\(\\sigma^2\\) 인 모집단으로부터 추출된 표본 사이즈가 \\(n\\)인 표본들에 대해서\n각 표본의 평균들 \\(\\bar{X}\\) 의 분포를 평균의 표본 분포, the sampling distribution of the mean이라고 하고, 이 분포는 the central limit theorem에 의해\n\n평균:  \\(\\displaystyle E(\\bar{X})=\\frac{m_1+m_2+m_3+\\cdots+m_w}{w}\\Bigg|_{w\\rightarrow\\infty}\\rightarrow ~~~\\mu\\)\n분산:  \\(\\displaystyle V(\\bar{X})\\Bigg|_{w\\rightarrow\\infty}\\rightarrow ~~~\\frac{\\sigma^2}{n},\\)   표준 편차 \\(\\displaystyle\\frac{\\sigma}{\\sqrt{n}}\\) 를 standard error of estimate (SE)라고 함.\n\n\n\n\n\nB개의 부트스트랩 샘플을 이용해 B개의 모형 \\(\\hat{f}^{b}\\)를 만들고, 이들의 평균을 이용해 예측\n\\(\\displaystyle \\hat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^B \\hat{f}^{b}(x)\\)\n\ndecision tree에서 특히 효과적임\n각 트리들은 매우 깊게 만들어 variance를 높게 만든 후 (bias는 낮아짐)\n이들의 평균을 이용해 variance를 줄임\n수백, 수천개의 트리를 결합해 매우 높은 정확도를 얻을 수 있음\n트리를 키워 variance가 충분히 낮아지도록; 트리의 수가 중요한 요소는 아님. 많은 트리가 overfitting을 만들지 않음\n한편, 여러 트리들의 평균값이므로 앞서 단일 트리를 통해 다이어그램을 그리거나 해석하기 어려움.\n\n\n\nSource: p. 344, An Introduction to Statistical Learning with Applications in Python\n\n\n\n\n\n\n\nOut-of-bag error\n\n\n\n\n\n\n부트스트랩 샘플들은 평균적으로 원래 데이터의 2/3의 관측치만를 포함함\n학습에 사용되지 않은 나머지 약 1/3의 관측치들을 out-of-bag(OOB) 관측치라고 함\n모형에 대한 test error를 cross-validation 대신에 학습에 사용되지 않은 OOB 관측치를 이용해 효과적으로 추정할 수 있음\n즉, 각 관측치 x에 대해 x가 포함되지 않은 부트스트랩 샘플들로 학습된 트리들로 예측한 값들(약 1/3개의 트리)의 평균을 이용해 test error를 추정할 수 있음\n\n\n\n\n분류의 문제의 경우\n\n각 트리들이 예측한 클래스들 중 가장 많이 나온 클래스를 예측값으로 사용: majority/hard voting\n각 트리들이 예측한 클래스들의 확률을 평균내어 가장 높은 확률을 가진 클래스를 예측값으로 사용: soft voting\n\n예측변수의 상대적 중요도\n\n예측변수에 대한 해석은 어려운 대신, 예측변수의 중요도에 대한 정보를 얻을 수 있음\n회귀문제: 주어진 예측변수로 분할될 때, RSS가 줄어든 총량을 모든 트리에 대해 평균\n분류문제: 주어진 예측변수로 분할될 때, 불순도(gini index)가 줄어든 총량을 모든 트리에 대해 평균\n\n\n\n\n\n\n\nVariable importance\n\n\n\n모든 상황에 적용할 수 있는 하나의 방법은 없으며, 연구 내용과 측정 변수들의 특성에 따라 적절한 방법을 선택.\n선형모형에서는\n\n표준화 회귀계수 (standardized regression coefficient)\nSemi-partial correlation coefficient\nDominance analysis\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier as RFC\n\nbag_carseats = RFC(max_features=X_train.shape[1], n_estimators=500, random_state=0)\nbag_carseats.fit(X_train, y_train)\n\nRandomForestClassifier(max_features=11, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(max_features=11, random_state=0) \n\n\n\naccuracy_score(y_test, bag_carseats.predict(X_test))\n\n0.77\n\n\n\nfeature_imp = pd.DataFrame(\n    {\"importance\": bag_carseats.feature_importances_}, index=X.columns\n).sort_values(\"importance\", ascending=False)\nfeature_imp\n\n                  importance\nPrice                0.31588\nCompPrice            0.13542\nAge                  0.11689\n...                      ...\nShelveLoc_Medium     0.01439\nUrban_Yes            0.00840\nUS_Yes               0.00600\n\n[11 rows x 1 columns]\n\n\n\n\ncode\n(\n    so.Plot(feature_imp.reset_index(names=\"predictor\"), x='importance', y=\"predictor\")\n    .add(so.Bar(alpha=1))\n    .label(x=\"Relative Importance\", y=\"Predictor\")\n)",
    "crumbs": [
      "Home",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#random-forest",
    "href": "contents/tree.html#random-forest",
    "title": "Tree-based Models",
    "section": "Random Forest",
    "text": "Random Forest\n\nBaggging의 일종: 부트스트랩 샘플을 이용해 여러 트리를 생성\n각 트리를 만들 때, 랜덤하게 선택된 변수들로만 분할을 수행; 대략 \\(\\sqrt{p}\\)개의 변수를 선택\n총 예측변수보다 훨씬 적은 수의 변수를 선택함으로써, 트리 간의 상관관계를 줄임; decorrelating\n\n서로 관련이 없는 것들의 평균이 상대적으로 더 variance를 줄이는데 효과적\n상관이 높은 예측변수들이 많다면 특히, 적은 변수를 선택하는 것이 효과적\n\n중요도가 낮은 변수들도 트리에서 데이터를 설명할 수 있는 기회를 얻게 되는데, 이는 데이터 내의 다양한 정보를 캐낼 수 있는 것으로도 볼 수 있음\n항상 모든 변수를 선택한다면, Bagging과 같음\n\n\nfrom sklearn.ensemble import RandomForestClassifier as RFC\n\nrf_carseats = RFC(max_features='sqrt', n_estimators=500, random_state=0)\nrf_carseats.fit(X_train, y_train)\n\nRandomForestClassifier(n_estimators=500, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(n_estimators=500, random_state=0) \n\n\n\naccuracy_score(y_test, rf_carseats.predict(X_test))\n\n0.805\n\n\n선택되는 변수의 수와 트리의 개수에 따른 예측력 변화",
    "crumbs": [
      "Home",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#boosting",
    "href": "contents/tree.html#boosting",
    "title": "Tree-based Models",
    "section": "Boosting",
    "text": "Boosting\n\nBagging과 마찬가지로 weak learner들을 결합하는 방법이기는 하나\nBoosting은 처음 세운 모형을 점진적으로 향상시키면서 새 모형들을 만들어내는데, 이전에 잡아내지 못한 시그널을 보완하면서 모형을 업데이트하는 방식\nBagging과는 본질적으로 큰 차이가 있음\n다양한 알고리즘들이 개발되어 왔음; AdaBoost, Gradient Boosting, XGBoost 등\n\n여기서는 decision tree에 적용해 살펴봄\n\n\nSource: p. 349, An Introduction to Statistical Learning with Applications in Python\n\nTurning parmaters\n\n트리 수 B: Bagging과는 다르게 B가 너무 크면 과적합이 발생할 있어 교차 검증을 통해 B를 선택\nShrinkage parameter λ: Boosting이 학습하는 속도를 제어.\n\n일반적으로 0.01이나 0.001\nλ가 매우 작으면 좋은 성능을 얻기 위해 매우 큰 값의 B가 필요\n\n각 트리의 분할 수 d: 부스트된 앙상블의 복잡도를 제어\n\n종종 d = 1을 선택; 단일 분할로 구성된 stump: 한 변수만을 포함\nd는 각 단계에서 선택되는 변수의 개수를 제어하게 되는데, 이는 여러 변수들이 골고루 데이터의 시그널을 잡아낼 수 있는 기회를 제공하게 됨.\n\n\n\n# Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier as GBC\n\nboost_carseats = GBC(n_estimators=5000, learning_rate=0.01, max_depth=2, random_state=0)\nboost_carseats.fit(X_train, y_train)\n\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=5000,\n                           random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=5000,\n                           random_state=0) \n\n\n\naccuracy_score(y_test, boost_carseats.predict(X_test))\n\n0.855\n\n\n선택되는 변수의 수와 트리의 개수에 따른 예측력 변화\n\n\n\n\n\n\n\n\nboost_carseats = GBC(n_estimators=5000, learning_rate=0.01, max_depth=1, random_state=0)\nboost_carseats.fit(X_train, y_train)\n\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=1, n_estimators=5000,\n                           random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(learning_rate=0.01, max_depth=1, n_estimators=5000,\n                           random_state=0) \n\n\n\naccuracy_score(y_test, boost_carseats.predict(X_test))\n\n0.89\n\n\nFeature importance\n\n\ncode\nfeature_imp2 = pd.DataFrame(\n    {\"importance\": boot_carseats.feature_importances_}, index=X.columns\n).sort_values(\"importance\", ascending=False)\n\n(\n    so.Plot(feature_imp2.reset_index(names=\"predictor\"), x='importance', y=\"predictor\")\n    .add(so.Bar(alpha=1))\n    .label(x=\"Relative Importance\", y=\"Predictor\")\n)",
    "crumbs": [
      "Home",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/statistics.html",
    "href": "contents/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Home",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#데이터-분석에-관한-전통적인-분류",
    "href": "contents/statistics.html#데이터-분석에-관한-전통적인-분류",
    "title": "Statistics",
    "section": "데이터 분석에 관한 전통적인 분류",
    "text": "데이터 분석에 관한 전통적인 분류\n\n\n\n탐색적 분석 vs. 가설 검증\nexploratory vs. confirmatory\n\n탐색적 분석\n\n통찰 혹은 가설의 기초 제공\n끼워 맞추기? 오류에 빠지기 쉬움: spurious associations\n\n가설 검증\n\n진위의 확률을 높임\n탐색적 분석으로부터 온 가설은 재테스트\n\n\n\n\n\n관찰 vs. 실험 데이터\nobservational vs. experimental\n\n당근과 시력?\n커피의 효과?\n남녀의 임금 차별?\n심리치료의 효과?\n\n\n\n\n표본 vs. 모집단\nsample vs. population\n\nParameter(모수), uncertainty(불확실성)\n내일 태양이 뜰 확률?\n연봉과 삶의 만족도와 관계\n성별과 임금과의 관계\n두통약의 효능: “effect size”",
    "crumbs": [
      "Home",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#통계-분석-목표",
    "href": "contents/statistics.html#통계-분석-목표",
    "title": "Statistics",
    "section": "통계 분석 목표",
    "text": "통계 분석 목표\n\nAssociations: 변수들 간의 관계/연관성을 파악\nStrength of associations: 그 관계의 크기/강도; 예측 정확성\nInference: 그 관계와 크기가 모집단에서는 어떠할지 추론\n\nconfidence/prediction interval, p-value(가설검정)\n\n\n현대적 접근에서는 모호한 모집단에 대해 추론하기보다는 표본 내의 정보만으로 일반화(generalization)을 성취하고자 함.\n\n이는 변수들 간의 관계가 특정 표본에 너무 overfit되지 않도록 하여(노이즈는 무시하고, 신호는 잡아내는 것)\n새로운 데이터에서도 그 관계가 유지되도록 하기 위한 것이며,\n이를 위해 데이터를 training set, validation set으로 나누거나, resampling과 같은 방법을 이용하여; cross validation, bootstrapping\n여러 샘플에 대해 모형을 테스트하는 것과 같은 효과를 얻을 수 있음\n전통적으로 가상의 모집단에 대해 추론하여 일반화를 성취하려는 것과 같은 맥락임",
    "crumbs": [
      "Home",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#예시",
    "href": "contents/statistics.html#예시",
    "title": "Statistics",
    "section": "예시",
    "text": "예시\n닭의 울음이 태양을 솟게 하는가?\n돈과 행복: 패턴 vs. 예외 \n\n특정 A의 임금이 p 에서 q 로 증가할 때, 트렌드대로 움직이겠는가?\n\n특정 B의 임금이 r 에서 s 로 감소할 때, 트렌드대로 움직이겠는가?\n특정 C의 임금을 올려주면, 트렌드대로 움직이겠는가?\n\n\n남녀 임금의 차이\n\n가령, 두 회사(위, 아래)에서 구성원들은 임금 차이를 어떻게 다르게 느끼겠는가?\n\n \n미혼자에 대한 임금 차별 vs. 편견\n\n미혼자에 대한 임금 차별이 있는가? 차별이 의미하는 바는 무엇인가?\n연령을 고려한 후에도 기혼자의 임금은 미혼자보다 높은가?\n연령을 고려한 후/연령을 조정한 후(adjusted for age)의 차이는 얼마라고 봐야하는가?\n\n\n가난, 인종, 범죄 간의 관계\nRacial differences in homicide rates are poorly explained by economics\n출산율은 왜 감소하는가?\n\n\n\n\n\n\n\n\n분석가의 태도\n\n\n\n\n심리적 관성/편견 주의\n분석가의 책임의식\n두 가지 접근법(예측과 이해)는 서로 상보 관계!",
    "crumbs": [
      "Home",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#simple-regressioncorrelation",
    "href": "contents/statistics.html#simple-regressioncorrelation",
    "title": "Statistics",
    "section": "Simple Regression/Correlation",
    "text": "Simple Regression/Correlation\n두 변수 간의 correlation(상관관계)는 두 변수 간의 영향관계에 대한 방향성을 전제하지 않는 반면,\n회귀분석은 (보통) 한 변수가 다른 변수에 영향 미친다는 것을 전제로 하고, 그 영향의 형태와 크기를 분석.\n예측변수가 한 개인 회귀분석: simple regression\n\n두 변수 간의 관계(association)을 파악: \\(Y=f(X) + \\epsilon=b_0 + b_1X + \\epsilon\\)  (\\(X\\)와 \\(\\epsilon\\)은 독립; \\(X \\perp\\!\\!\\!\\perp \\epsilon\\))\n그 관계의 크기(strength)를 측정\n\n\\(f\\)에 의해 \\(X\\)로 \\(Y\\)를 얼마나 정확히 예측할 수 있는가?\n\\(f\\)에 의해 \\(X\\)의 변량이 \\(Y\\)의 변량을 얼마나 설명할 수 있는가?\n\n\n\n\n\nPearson’s correlation coefficient: \\(r\\)Linear relationships을 측정\n\\(X\\)와 \\(Y\\)의 선형적 연관성: [-1, 1]\n\n\\(X\\)로부터 \\(Y\\)를 얼마나 정확히 예측가능한가?\n\\(X\\)와 \\(Y\\)의 정보는 얼마나 중복(redundant)되는가?\n\n\n\n\n\n\nMultiple correlation coefficient: \\(R\\)Extented correlation: 예측치와 관측치의 Pearson’s correlation, \\(r(Y, \\widehat Y)\\)\n\\(R\\)을 제곱한 \\(R^2\\)가 설명력의 정도를 나타냄\n\n\n\n\n\\(r\\): Pearson correlation coefficient\n\n\\(\\displaystyle r_{XY} = \\frac{1}{n}\\sum_{i=1}^n z_{Xi} z_{Yi} = 1 - \\frac{\\sum{(z_{X_i} - z_{Y_i})^2}}{2n}\\)   \\(z_{X_i}, z_{Y_i}\\) : 각각 standardized \\(X_i, Y_i\\)\n\n\\(R\\): Multiple correlation coefficient\n\n\\(Y\\) 와 \\(\\widehat Y\\) 의 Pearson correlation 즉, Y와 회귀모형이 예측한 값의 (선형적) 상관 관계의 정도; 회귀모형의 예측의 정확성\n\n다시말하면, 예측변수들의 최적의(optimal) 선형 조합과 Y의 상관 관계의 정도.\n\n\\(R^2\\): Coefficient of determination, 결정계수, 설명력\n\n선형모형에 의해 설명된 Y 변량의 비율\n정확히 표현하면, 예측변수들의 최적의(optimal) 선형 조합에 의해 설명된 Y 변량의 비율\n\n  \\(\\displaystyle R^2 = \\frac{V(\\widehat{Y})}{V(Y)} = 1 - \\frac{V(e)}{V(Y)}\\)\n\n\n\n\nSource: Applied Regression Analysis and Generalized Linear Models by John Fox\n\n\n\n\n\\(Y_i = e_i + \\hat{Y_i}\\)\n\\(Y_i - \\overline{Y} = [\\overset{\\text{\\large error}}{(Y_i - \\hat{Y_i})} - 0] + \\overset{\\text{\\large explained}}{(\\hat{Y_i} - \\overline{Y})}\\)\n\\(V(Y) = V(e) + V(\\widehat Y)\\),   (\\(e\\)와 \\(\\hat{Y}\\)의 상관 = 0)\n\n\n\n\n\n\nAssociations과 그 strengths 비교\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n카테고리 변수에 대해서도 비슷하게 생각할 수 있음.\n이 경우, 두 그룹 간의 차이에 대한 효과의 크기를 말할 수 있고, \\(R^2\\) 이외에도 Cohen’s d로 표현할 수 있음.\n예를 들어, 결혼과 삶의 만족도 간의 관계(association)와 그 강도(strength)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n인과 관계에 대한 섯부른 추론은 금물!\n특히, 예측력이 낮은 경우; Leo Breiman의 중요 요지 중 하나",
    "crumbs": [
      "Home",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#multiple-regression",
    "href": "contents/statistics.html#multiple-regression",
    "title": "Statistics",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n예측변수가 2개 이상인 경우: 변수들 간의 진실한 관계를 분석; 인과관계 추론을 지향함\n미혼자에 대한 임금 차별이 있는가? 차별이 의미하는 바는 무엇인가?\n연령을 고려한 후에도 기혼자의 임금은 미혼자보다 높은가?\n여전히 높다면, 연령을 고려한 후 혹은 연령을 조정한 후(adjusted for age)의 차이는 얼마라고 봐야하는가?\n연령을 고려한 임금 차이를 조사하는 방법은 무엇이 있겠는가?; 연령별로 나누어 비교?\nData from the 1985 Current Population Survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n연령을 고려한 마라톤 기록?\n70세 노인과 20세 청년이 동일하게 2시간 30분의 기록을 세웠다면?\n\n“나이 차이가 큰 두 사람의 기록을 비교하는 것은 공평하지 않아”\n나이를 감안한 마라톤 실력?\n다시 말하면, 나이와는 무관한/독립적인 마라톤 능력에 대해 말하고자 함\n이는 동일한 나이의 사람들로만 제한해서 마라톤 기록을 비교하는 것이 공평한 능력의 비교라고 말하는 것과 같은 이치임\n아래 그림에서 20세라면 보통 2시간 정도, 70세라면 보통 3시간 정도가 전형적인 기록이기 때문에, 이를 고려하여 기록을 조정할 수 있음;\n\n가령, 70세 노인의 기록 2.5시간은 -0.5시간(=3-2.5); 나이로는 설명/예측되지 못하는 정도\n20세 청년의 기록 2.5시간은 +0.5시간(=2.5-2); 나이로는 설명/예측되지 못하는 정도\n\n\n\n\n\n남녀별 연령에 따른 평균 마라톤 기록\n\n\n\nSource: https://doi.org/10.1186/2052-1847-6-31",
    "crumbs": [
      "Home",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#regression-analysis",
    "href": "contents/statistics.html#regression-analysis",
    "title": "Statistics",
    "section": "Regression analysis",
    "text": "Regression analysis\n예측 모형 vs. 인과 모형\n\n인과적 가정없이 예측이 목적인 경우도 있으나 보통 인과적 연관성을 파악하는 것이 목적임\n회귀 분석은 전통적으로 “causal inference”에서 중요한 역할을 해왔으나\n그 한계가 파악되고 확장되어 앞서 논의한 “causal inference”의 영역이 확립되었음.\n\n예시: 교수의 연봉(salary)이 학위를 받은 후 지난 시간(time since Ph.D.)과 출판물의 수(pubs)에 의해 어떻게 영향을 받는가?\n\\[salary = b_0 + b_1 \\cdot time + b_2 \\cdot pubs + \\epsilon, ~~ \\epsilon \\perp\\!\\!\\!\\perp time, pubs\\]\n\nSource: Cohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences (3rd ed.)\n\n\n\n\n\n\n\n\n\n\n\nData: c0301dt.csv\n\nacad0 = pd.read_csv(\"data/c0301dt.csv\")\nacad0.head(5)\n\n   time  pubs  salary\n0     3    18   51876\n1     6     3   54511\n2     3     2   53425\n3     8    17   61863\n4     9    11   52926\n\n\n\nfrom statsmodels.formula.api import ols\n\nmod1 = ols(\"salary ~ time\", data=acad0).fit()\nmod2 = ols(\"salary ~ pubs\", data=acad0).fit()\nmod3 = ols(\"salary ~ time + pubs\", data=acad0).fit()\n\n\n\n\nIntercept   43658.59\ntime         1224.39\ndtype: float64\n\n\nIntercept   46357.45\npubs          335.53\ndtype: float64\n\n\nIntercept   43082.39\ntime          982.87\npubs          121.80\ndtype: float64\n\n\n\n세 모형을 비교하면,\nModel 1: \\(\\widehat{salary} = \\$1,224\\:time + \\$43,659\\)\nModel 2 : \\(\\widehat{salary} = \\$336\\:pubs + \\$46,357\\)\nModel 3: \\(\\widehat{salary} = \\$983\\:time + \\$122\\:pubs + \\$43,082\\)\n\n연차(time)의 효과는 $1,224에서 $984로 낮아졌고,\n논문수(pubs)의 효과는 $336에서 $122로 낮아졌음.\n\n\n\n\n\n\n\n\n\n\n\n\n교수들의 연차와 그들이 쓴 논문 수는 깊이 연관되어 있으며 (r = 0.66), 두 변수의 redunancy가 각 변수들의 효과를 변화시킴.\n두 예측 변수의 산술적 합(\\(b_1 \\cdot time + b_2 \\cdot pubs\\))으로 연봉을 예측하므로 각 예측변수의 효과는 (각각 따로 예측할 때에 비해) 수정될 수 밖에 없음.\n수학적으로 보면, 각 예측변수의 기울기는 다른 예측변수의 값에 상관없이 일정하므로, 다른 예측변수들을 (임의의 값에) 고정시키는 효과를 가짐\n즉, 다른 변수와는 독립적인, 고유한 효과를 추정하게 됨\n\n각 회귀계수를 partial regression coefficient (부분 회귀 계수) 라고 부름.\n부분 회귀 계수의 첫번째 해석:\n\n만약 논문 수가 일정할 때, 예를 들어 10편의 논문을 쓴 경우만 봤을 때, 연차가 1년 늘 때마다 연봉은 $984 증가함; 평면(2차원)의 선형모형을 가정했기에 이 관계는 논문 수에 상관없음.\n\n연차가 일정할 때, 예를 들어 연차가 12년차인 경우만 봤을 때, 논문이 1편 늘 때마다 연봉은 $122 증가함; 평면(2차원)의 선형모형을 가정했기에 이 관계는 연차에 상관없음.\n\n이는 다른 변수를 고려 (통제, controlling for) 했을 때 혹은 다른 변수의 효과를 제거 (partial out) 했을 때, 각 변수의 고유한 효과를 의미함; holding constant, controlling for, partialing out, adjusted for, residualizing\n뒤집어 말하면, 연차만 고려했을때 연차가 1년 늘면 $1,224 연봉이 증가하는 효과는 연차가 늘 때 함께 늘어나는 논문 수의 효과가 함께 섞여 나온 효과라고 말할 수 있음.\n이는 인과관계에 있는 변수들의 진정한 효과를 찾는 것이 얼마나 어려운지를 보여줌\n부분 회귀 계수에 대한 두번째 해석\n\n다른 변수들이 partial out 된 후의 효과.\n\n실제로 $122는 “연차로 (선형적으로) 예측/설명되지 않는” 논문수(즉, 잔차)로 “연차로 예측/설명되지 않는” 연봉(즉, 잔차)을 예측할 때의 기울기; 아래 그림에서 보라색으로 예측되는 빨간색 부분\n\n  \nDirect and Indirect Effects\n만약, 다음과 같은 인과모형을 세운다면,\n\n\n연차가 연봉에 미치는 효과가 두 경로로 나뉘어지고,\n연차 \\(\\rightarrow\\) 연봉: 직접효과 $983\n연차 \\(\\rightarrow\\) 논문 \\(\\rightarrow\\) 연봉: 간접효과 1.98 x $122 = $241.56\n두 효과를 더하면: $983 + $241.56 = $1224.56 = 논문수를 고려하지 않았을 때 연차의 효과\n\n즉, 연차가 1년 늘때 연봉이 $1224 증가하는 것은 연차 자체의 효과($983)와 논문의 증가에 따른 효과($241)가 합쳐져 나온 결과라고 말할 수 있음.\n\n이 때, 논문 수를 통한 효과는 연차가 연봉에 미치는 하나의 기제(mechanism)이라고 볼 수 있음.\n\nStrength of Associations\n연차와 논문 수로 연봉을 예측했을 때의 \\(R^2 = 0.53\\)\n\n즉, 연차와 논문 수로 연봉의 변량의 53%를 설명할 수 있음\n혹은 \\(R = r(Y, \\hat{Y}) = \\sqrt{0.53} = 0.73\\)\n\n반면, 각 변수와 연봉 간의 고유한 상관관계를 측정하고자 한다면 partial/semi-partial correlation을 고려(아래 테이블)\n\n논문 수와는 독립적인 연차와 연봉 간의 부분상관계수(partial correlation) \\(pr = 0.53\\)\n\n그 제곱 \\(pr^2 = 0.28\\); 28%의 변량을 설명\n\n연차와는 독립적인 논문 수와 연봉 간의 부분상관계수(partial correlation) \\(pr = 0.23\\)\n\n그 제곱 \\(pr^2 = 0.05\\); 5%의 변량을 설명\n\nsemi-partial correlation 조금 다른 의미\n\n\n\n\n\n\n\n\n\\(r\\) (simple)\n\\(pr\\) (partial)\n\\(sr\\) (semi-partial)\n\n\n\n\ntime\n0.71\n0.53\n0.43\n\n\npubs\n0.59\n0.23\n0.16\n\n\n\n\n\n \n\n\n\n\n\n\n\\(r^2\\)\n\\(pr^2\\)\n\\(sr^2\\)\n\n\n\n\ntime\n0.50\n0.28\n0.18\n\n\npubs\n0.35\n0.05\n0.03\n\n\n\n\n\n\n\n직접 효과(direct effect)가 거의 0인 경우\n만약, 예를 들어 연차의 효과 $1224이 논문수를 고려했을 때 줄어든($983) 수준을 훨씬 넘어 통계적으로 유의하지 않을 정도로 0에 가까워진다면(즉, 모집단에서는 사실상 0일 가능성이 있음), 연차의 효과는 모두 논문의 효과를 거쳐 나타나는 것이라고 말할 수 있음(직접 효과 = 0). 다시 말하면, 연차 자체는 연봉에 영향을 주지 않음; 완전 매개 (fully mediate)한다고도 표현함.\n\nSpurious Relationships\n반대로, 만약 다음과 같이 연차를 고려했을 때 논문수(pubs)의 효과가 거의 사라진다면,\n논문수(pubs)와 연봉(salary)의 관계는 spurious(가짜)한 관계라고 잠정적으로 말할 수 있음.\n연차를 논문수와 연봉의 common cause 라고 말하며, confounding이 되어 논문수와 연봉의 인과관계는 실제로 없을 수 있음을 암시함.\n즉, 논문수가 연봉에 영향을 주는 것처럼 보이는(연관성) 이유는 연차로 인해 모두 증가되어 나타나는 착시현상임.\n\n\n\n\n\n\n\nImportant\n\n\n\n요약하면,\n\n회귀분석을 통해, 변수들 간의 관계를 파악하고, 그 관계의 크기를 추정\n그 관계가 얼마나 일반화될 수 있는지를 추론; 모집단에 대한 추론\n인과 관계 추론에 대한 위험성을 인지하고, 신중한 접근이 필요함\n\n\n\n다른 예로, 집값을 예측하는 모델의 경우\nSaratoga Houses dataset\n\\(\\widehat{price} = 36668.9 + 125.4 \\cdot livingArea - 14196.8 \\cdot bedrooms\\)\n\n\nShow the code\nfrom statsmodels.formula.api import ols\nhouses = sm.datasets.get_rdataset(\"SaratogaHouses\", \"mosaicData\").data\nprint(houses.head(3))\n\n\n    price  lotSize  age  landValue  livingArea  pctCollege  bedrooms  \\\n0  132500     0.09   42      50000         906          35         2   \n1  181115     0.92    0      22300        1953          51         3   \n2  109000     0.19  133       7300        1944          51         4   \n\n   fireplaces  bathrooms  rooms          heating      fuel              sewer  \\\n0           1       1.00      5         electric  electric             septic   \n1           0       2.50      6  hot water/steam       gas             septic   \n2           1       1.00      8  hot water/steam       gas  public/commercial   \n\n  waterfront newConstruction centralAir  \n0         No              No         No  \n1         No              No         No  \n2         No              No         No  \n\n\n\n\nlinear model\nmod = ols(\"price ~ livingArea + bedrooms\", data=houses).fit()\nmod.summary()\n\n\n\nShow the code\n(\n    so.Plot(houses, x='livingArea', y='price')\n    .add(so.Dots())\n).show()\nsns.boxplot(data=houses, x='bedrooms', y='price', fill=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n비슷한 넓이의 집들로 나누어 보면,\n\n\n\n\n\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\nInteractions\n두 변수가 서로 상호작용하는 경우: not additive, but multiplicative\n\n각각의 효과가 더해지는 것을 넘어서서 서로의 효과를 증폭시키거나 감소시키는 경우\n강수량과 풍속이 함께 항공편의 지연을 가중시키는 경우\n운동량과 식사량이 함께 체중 감량에 영향을 미치는 경우\n나이에 따른 지구력 감소가 운동을 한 기간에 따라 변화하는 경우\n\n보호 요인 (protective factor)\n위험 요인 (risk factor)\n\n\n각 변수의 효과가 다른 변수에 의해 변하므로 앞서 “통계적 통제”의 해석은 적용되지 않음.\n즉, 각 변수의 고유한 효과에 대해 말할 수 없음.\n가령, 나이에 따른 지구력 감소가 운동을 한 기간에 따라 변화하는 경우\n\\[\n\\begin{align}\nendure &= b_0 + b_1 \\cdot age + b_2 \\cdot exercise + b_3 \\cdot age \\cdot exercise \\\\\n&= b_0 + (b_1 + b_3 \\cdot exercise) \\cdot age + b_2 \\cdot exercise\n\\end{align}\n\\]\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nInteraction의 패턴\n\n\n\n\nSynergistic or enhancing interaction\n\n\n상호작용 효과가 원래 효과들과 같은 방향으로 작용하는 경우\n삶의 만족도(Y)가 직업 스트레스(X)와 부정적인 관계에 있고, 부부관계의 문제(Z)와도 부정적인 관계에 있는 경우\n이 둘의 상호작용이 부정적이라면, 직업 스트레스와 부부관계의 문제가 동시에 증가하면 각각의 sum이 예측하는 것보다 더 낮은 삶의 만족도가 예측됨.\n\n\nBuffering interaction\n\n\n두 변수가 반대 방향으로 Y에 작용하고 있을 때, 한 변수가 다른 변수의 효과를 감소시키는 경우\n즉, 한 변수의 impact가 다른 변수의 impact를 줄여주는 경우\n건강보건에 대한 연구에서, 한 변수가 질병의 위험요인이고 다른 변수가 질병의 위험을 줄여주는 보호요인인 경우\n위의 예에서처럼, 나이(X)는 지구력 감소의 위험요인이고, 운동기간(Z)은 지구력 보호요인인 경우\n\n\nInterference or antagonistic interactionin\n\n\n두 변수가 같은 방향으로 Y에 작용하고 있을 때, 상호작용은 반대 방향으로 작용하는 경우\n대학생의 학업성취도(Y)에 대하여, 학업동기(X)와 학업능력(Z)이 모두 학업성취도(Y)에 긍정적인 영향을 미치나 이 두 변수는 서로 보완적인 효과를 가지고 있음.\n즉, 성취도에 대한 학업능력의 중요성은 높은 학업동기에 의해 낮아질 수 있음.\n반대로, 학업동기에 대한 중요성은 높은 학업능력에 의해 낮아질 수 있음.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#uncertainty",
    "href": "contents/statistics.html#uncertainty",
    "title": "Statistics",
    "section": "Uncertainty",
    "text": "Uncertainty\n관찰자가 관찰한 대상으로부터 얻은 결과를 관찰하지 않은 더 넓은 대상으로 일반화할 수 있는가?\n가령, 다음과 같이 150명에 대해 조사한 “연령이 임금에 미치는 효과”를 일반화 할 수 있는가?\n한 나라의 국민 전체?\n\nStatistical inference (통계적 추론)\n통계학의 추론(statistical inference)은 작은 샘플(sample)로부터 얻은 분석 결과를 바탕으로 모집단(population)이라고 부르는 전체에 대해 말하고자 하는 시도에서 비롯되었음\n\n농업 분야에서 시작; 비료/종자의 효과\n사람에게도 적용될 수 있는가?\n\n앞서 논의한 모든 내용은 “특정 샘플” 내에서 변수들 간의 관계에 대한 분석임.\n통계적 추론은 수많은 같은 수의 샘플들, 가령 N = 150인 즉, 150명으로 이루어진 샘플들을 반복적으로 관찰한다면 그 샘플들 간의 편차들이 어떠하겠는가에 대한 논의임.\n\n\n\n\nSource: The Truthful Art by Albert Cairo.\n\n\n\n\n\n\n\n샘플들로부터 나타나는 임금 차이 값의 분포 &gt;&gt; 남녀 임금 차이가 편차는 어떠한가?\n이 분포를 sampling distribution(표본 분포)이라고 부름\n\n\n\n\n\n\n\n\n\n\n\n\n평균이 $2.27이고, 임금 차이 값들의 95%가 $1.47 ~ $3.04 범위에 있음을 알 수 있음.\n연구자가 관찰한 샘플로부터 연구자는 매우 큰 확신(95%)을 갖고 남녀의 시간당 임금의 차이는 1.47달러에서 3.04달러 사이에 있을 것이라고 말할 수 있음.\n\n\n비슷하게, 나이(age)와 시간당 임금(wage)의 true relationship에 대해서도\n샘플마다 age와 wage의 관계는 다르게 나타날 것임 (두번째 그림).\n\n예를 들어, 샘플들로부터 나타나는 기울기들의 분포를 살펴봄으로써 (세번째 그림): sampling distribution\n이 분포에 따르면 평균이 0.066이고, 기울기 값들의 95%가 0.005 ~ 0.140 범위에 있음을 알 수 있음.\n연구자가 관찰한 샘플로부터 연구자는 (age와 wage의 선형성을 가정한다면), 매우 큰 확신을 갖고 나이가 10세 늘때마다 시간당 임금의 증가율은 0.05에서 1.4달러 사이에 있을 것이라고 말할 수 있음.\n\n\n\n\n\nSource: The Truthful Art by Albert Cairo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis testing\nNull hypothesis(영가설)에 대한 테스트\n즉, 모집단에서 파라미터가 0 인가에 대한 테스트\n데이터: cp3.csv\n\nfrom statsmodels.formula.api import ols\ncps = pd.read_csv('data/cps3.csv')\nmod = ols(\"wage ~ married + sex + age\", data=cps).fit()\nmod.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nwage\nR-squared:\n0.125\n\n\nModel:\nOLS\nAdj. R-squared:\n0.107\n\n\nMethod:\nLeast Squares\nF-statistic:\n6.956\n\n\nDate:\nTue, 26 Mar 2024\nProb (F-statistic):\n0.000208\n\n\nTime:\n23:01:35\nLog-Likelihood:\n-433.48\n\n\nNo. Observations:\n150\nAIC:\n875.0\n\n\nDf Residuals:\n146\nBIC:\n887.0\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4.4946\n1.349\n3.332\n0.001\n1.829\n7.161\n\n\nmarried[T.Single]\n-0.3592\n0.764\n-0.470\n0.639\n-1.870\n1.152\n\n\nsex[T.M]\n2.4337\n0.721\n3.374\n0.001\n1.008\n3.859\n\n\nage\n0.0880\n0.031\n2.834\n0.005\n0.027\n0.149\n\n\n\n\n\n\n\n\nOmnibus:\n31.384\nDurbin-Watson:\n2.127\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n45.207\n\n\nSkew:\n1.130\nProb(JB):\n1.53e-10\n\n\nKurtosis:\n4.458\nCond. No.\n153.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nt값에 대한 해석\n예측변수 \\(X_j\\) 에 대해서 모집단의 회귀계수 \\(b_j\\) 에 대한 표본 분포는 평균이 \\(b_j\\) 인 정규분포를 따르고, 표준편차, 즉 standard error는 근사적으로 다음과 같음.\n\\(\\displaystyle SE^2(b_j) = \\frac{{MS}_{residual}}{N \\cdot Var(X_j) \\cdot (1 - R^2_j)}, ~(df = N-k-1)\\)\n\n표본이 클수록\n평균 잔차가 작을수록\njth 예측변수의 값이 퍼져 있을수록\n다른 예측변수들로부터 jth 예측변수가 예측되지 못할수록; 즉 다른 변수들과 correlate되지 않을수록\n\n\n\n\n\n\n\nSource: Wikipedia, Student’s t-distribution\n\n표준정규분포(standard normal distribution)의 확률값\n\n\nSource: The Truthful Art by Albert Cairo\n\n사람 키의 분포\n\n\nSource: Human Height",
    "crumbs": [
      "Home",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/regularization.html",
    "href": "contents/regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "\\(Y = f(X) + \\epsilon\\)에서 무한히 많은/복잡한 \\(f\\)에 어떤 식으로든 제한을 줘서 regular behavior(규칙적, 틀을 갖춘)를 갖도록 하는데, 크게 2가지로 나누어 볼 수 있음\n함수 \\(h(x) = sin(2\\pi x)\\)로부터 생성된 데이터셋(N=10)에 대해 다항식의 차수에 따른 flexibility의 변화에 따른 OLS 모델들을 비교하면,",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Regularization"
    ]
  },
  {
    "objectID": "contents/regularization.html#ridge-regression",
    "href": "contents/regularization.html#ridge-regression",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\n\\(\\lambda\\)가 커질수록 \\(\\beta\\)의 \\(l_2\\) norm은 항상 줄어듦\n\n\\(l_2\\) norm: \\(\\displaystyle ||\\beta||_2 = \\sqrt{\\sum_{j=1}^{p} \\beta_j^2}\\)  : 벡터 \\(\\beta = (\\beta_1, \\beta_2, ..., \\beta_p)\\)의 길이\n\n\n\nSource: p. 241, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n다음과 같이 이상적이지 못한 경우에 regularization를 통해 적절한 bias-variance trade-off를 찾을 수 있음.\n\n에를 들어, 예측변수가 데이터에 비해 매우 많은 경우 (50개의 관측치에 45개의 예측변수)\n\n즉, OLS에서 variance가 높은 경우에 특히 유용함\n심지어, p &gt; n인 경우에도 작동함\n\n상대적으로 variance의 빠른 감소 효과 덕분에 (아래 그림), ridge regression은 OLS보다 더 좋은 예측 성능을 보임\n\nbias에 대한 약간의 손해를 보지만, variance에 대해서는 큰 이익을 얻어 정확한 예측(test error를 줄임) 가능\n\n파라미터가 작아지지만, 0이 되지는 않아, 모델의 해석이 용이하지는 않음\nshrinkage penalty 부분으로 인한 estimation에 대한 계산량의 증가가 거의 없도록 알고리즘 개발; 매우 효율적\n\n\n\nSource: p. 243, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n앞서 \\(y = sin(2\\pi x)\\)로부터 생성된 데이터셋(N=10)에 대해 다항식의 차수에 따른 flexibility의 변화에 따른 OLS 모델과 Ridge regression 모델을 비교하면,\n여기서 각각 \\(\\lambda = 0.005\\), \\(\\lambda = 1\\)",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Regularization"
    ]
  },
  {
    "objectID": "contents/regularization.html#lasso-regression",
    "href": "contents/regularization.html#lasso-regression",
    "title": "Regularization",
    "section": "Lasso regression",
    "text": "Lasso regression\n\\(\\lambda\\)가 커질수록 \\(\\beta\\)의 \\(l_1\\) norm은 항상 줄어듦\n\n\\(l_1\\) norm: \\(\\displaystyle ||\\beta||_1 = \\sum_{j=1}^{p} |\\beta_j|\\)\n\nRidge와는 달리 파라미터가 0이 될 수 있음: 변수 선택(variable/feature selection)이 가능함\n이는 변수가 많은 경우 파라미터 해석을 용이하게 함\n\n\\(\\lambda\\) 값에 따라 임의의 갯수의 변수를 포함할 수 있음\n\n\n\nSource: p. 245, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\nRidge와 마찬가지로 bias에 대한 약간의 손해를 보지만, variance에 대해서는 큰 이익을 얻어 정확한 예측(test error를 줄임) 가능\n그림 6.5와 동일한 데이터셋 (p = 45, n = 50)\n\n\nSource: p. 248, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\nRidge vs. Lasso\nRidge\n\n모든 변수를 포함하기 때문에(극히 작은 파라미터값을 포함) 종종 파라미터를 해석하기 어려워짐\n모든 예측변수들이 골고루 Y와 관련이 있을 때 높은 예측성능을 보임\n\n즉, 예측변수들이 비슷한 효과를 가질 때\n그림 6.8의 오른편에서 variance가 상대적으로 낮게 나타남.\n실제로 45개의 예측변수가 모두 Y와 관련되도록 생성된 데이터셋임.\n\n모든 계수들이 비슷한 “비율”로 줄어듬\n\nLasso\n\n일부 계수가 0이 되어 변수 선택이 가능하며, 따라서 해석이 용이해짐\n예측변수들 중 일부만이 Y와 관련이 있을 때 높은 예측성능을 보임\n\n즉, 상대적으로 소수의 예측변수들만이 큰 효과를 가질 때\n그림 6.9의 오른편에서 bias가 크게 작으며, variance도 낮음; 즉 모든 면에서 더 좋은 성능을 보임\n실제로 45개의 예측변수 중 2개만이 Y와 관련되도록 생성된 데이터셋임.\n\n모든 계수들이 비슷한 “크기”로 줄어듬; 작은 계수들은 0이 될 수 있음\n\n\n\nSource: p. 249, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n현실에서는 변수들 간의 true relationship을 미리 알 수 없으므로,\n적절한 모델의 선택과 \\(\\lambda\\)를 찾기 위해 cross-validation을 사용함\n\n\n\n\n\n\n\nLinear model selection\n\n\n\n앞서 shrinkage 방식으로 variance을 감소시키는 방법을 다루었는데, variance를 줄이는 다른 접근 방식이 있음.\n\nSubset Selection: 예측변수들 중 Y와 연관되어 있다고 생각되는 변수들을 선택해 변수의 수를 줄이는 방법\n\n설명력이 높은 조합의 변수들을 걸러내는 여러 방법이 있음\n어떤 기준으로 계산하느냐에 따라 결과가 달라질 수 있음\n많은 계산량이 필요하며, 변수의 수가 많을 때는 적용하기 어려움\nBest Subset Selection, Forward Stepwise Selection, Backward Stepwise Selection\n\nDimension Reduction: 예측변수들의 선형 조합으로 새로운 변수들을 얻어(설명 변량의 큰 손실없이) 변수의 갯수를 줄이고자 함\n\nPrincipal Component Analysis (PCA); unsupervised learning\nPartial Least Squares (PLS); supervised learning\n이 외에도 다양한 방법이 있음;",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Regularization"
    ]
  },
  {
    "objectID": "contents/model-basic.html",
    "href": "contents/model-basic.html",
    "title": "Model Basics",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#a-simple-model",
    "href": "contents/model-basic.html#a-simple-model",
    "title": "Model Basics",
    "section": "A simple model",
    "text": "A simple model\nData: sim1.csv\n\nsim1 = pd.read_csv(\"data/sim1.csv\")\n\n\n\n\n     x     y\n0    1  4.20\n1    1  7.51\n2    1  2.13\n..  ..   ...\n27  10 24.97\n28  10 23.35\n29  10 21.98\n\n[30 rows x 2 columns]\n\n\n\n\n\n\n\n패턴: 강한 선형 관계\n선형 모델 family/class인 \\(y = \\beta_0 + \\beta_1 x\\)을 세운 후\n무수히 많은 \\(\\beta_0, \\beta_1\\)의 값들 중 위 데이터에 가장 가까운 값을 찾음\n그 예로, 임의로 250개의 선형 모델을 그려보면,\n\n\n\n\n\n\n\n\n이 선형모델 중 데이터에 가장 가까운 모델을 찾고자 하는데, 이를 위해서는 데이터와 모델과의 거리를 정의해야 함.\n  \\(d =|~data - model~|\\)\n예) 모델과 데이터의 수직 거리(residuals)의 총체\n\nModel 1.1: \\(y = 1.5x+7\\)의 경우, 이 모델이 예측하는 값들\n\n\narray([ 8.5,  8.5,  8.5, 10. , 10. , 10. , 11.5, 11.5, 11.5, 13. , 13. ,\n       13. , 14.5, 14.5, 14.5, 16. , 16. , 16. , 17.5, 17.5, 17.5, 19. ,\n       19. , 19. , 20.5, 20.5, 20.5, 22. , 22. , 22. ])\n\n\n이 때, 관측치(\\(Y_i\\))와 예측치(\\(\\hat{Y}_i\\))의 차이, \\(Y_i - \\hat{Y}_i\\)를 잔차(residuals) 또는 예측 오차(errors)라고 함\n\n\n     x     y  pred  resid | e\n0    1  4.20  8.50      -4.30\n1    1  7.51  8.50      -0.99\n2    1  2.13  8.50      -6.37\n..  ..   ...   ...        ...\n27  10 24.97 22.00       2.97\n28  10 23.35 22.00       1.35\n29  10 21.98 22.00      -0.02\n\n[30 rows x 4 columns]\n\n\n\n\n\nRMSE = \\(\\displaystyle\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{e^2}}\\) = 2.67\n\n\nMAE = \\(\\displaystyle\\frac{1}{n} \\sum_{i=1}^{n}|~e~|\\) = 1.43\n\n\n\n\n\n\n\n\n\nModel evaluation\n\n\n\nError functions\n\nRoot-mean-squared error: \\(RMSE = \\displaystyle\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{(y_i -\\hat y_i)^2}}\\)\n\nMean absolute error: \\(MAE = \\displaystyle\\frac{1}{n} \\sum_{i=1}^{n}{|~y_i -\\hat y_i~|}\\) : 극단값들에 덜 민감함\n\n\n\n즉, 데이터셋 sim1과 model 1.1 과의 거리를 RMSE로 정의하면, \\(d=|~sim1 -model1~| = 2.67\\)\n위의 250개의 모델에 대해 각각 거리를 구하면\n\n\n       b0    b1  dist\n0   21.79 -2.92 17.42\n1   -2.83 -0.57 22.83\n2   -6.39  2.16 10.26\n..    ...   ...   ...\n247  0.51  4.19 10.38\n248 27.94 -0.84 11.59\n249 27.93  2.45 25.99\n\n[250 rows x 3 columns]\n\n\n이 중 제일 좋은 모델(dist가 최소) 10개의 모델을 그리면,\n\n\n\n\n\n\n\n250개의 모델 중 10개의 모델을 다음과 같은 \\((\\beta_0, \\beta_1)\\) 평면으로 살펴보면, 즉, model space에서 살펴보면\n\n오렌지 색은 위에서 구한 10 best models\n\n\n\n\n\n\n\n\n\n\nSource: Introduction to Statistical Learning by James et al.\n\n점차 촘촘한 간격으로 grid search를 하면서 거리를 (근사적으로) 최소로 하는 모델을 찾을 수 있음; 실제로는 Newton-Raphson search를 통해 최소값을 구하는 알고리즘을 이용\n즉, 거리를 최소로 하는 \\(\\beta_0\\), \\(\\beta_1\\)를 찾으면,\n\nfrom scipy.optimize import minimize\nminimize(measure_distance, [0, 0], args=(sim1)).x\n\narray([4.22, 2.05])\n\n\n\n\n\n\n\n\n\n이렇게 squared error가 최소가 되도록 추정하는 것을 ordinary least squares(OLS) estimattion라고 함.\n선형 회귀 모형의 경우, 실제로는 위에서 처럼 grid search를 하지 않고, closed-form solution을 통해 바로 구할 수 있음.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#maximum-likelihood-estimation",
    "href": "contents/model-basic.html#maximum-likelihood-estimation",
    "title": "Model Basics",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\n데이터가 발생된 것으로 가정하는 분포를 고려했을 때,\n파라미터가 어떤 값일 때(즉, 어떤 모델일 때) 주어진 데이터가 관측될 확률/가능도(likelihood)가 최대가 되겠는가로 접근하는 방식으로,\nX, Y의 관계와 확률분포를 함께 고려함.\n\n선형관계라면, 즉 \\(E(Y|X=x_i) = \\beta_0 + \\beta_1x_i\\)   (\\(E\\): expected value, 기대값)\n분포가 Gaussian이라면, 즉 \\(Y|(X=x_i) \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\)   (\\(\\sigma\\): 표준편차)\n\n\nLikelihood \\(L = \\displaystyle\\prod_{i=1}^{n}{P_i}\\)   (관측치들 독립일 때, product rule에 의해)\n분포가 Gaussian이라면(평균: \\(\\mu\\), 표준편차: \\(\\sigma\\)), 즉 \\(f(t) = \\displaystyle\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(t-\\mu)^2}{2\\sigma^2}\\right)\\)라면\n\\(L = \\displaystyle\\prod_{i=1}^{n}{f(y_i, x_i)} = \\displaystyle\\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)}\\)\n이 때, 이 likelihood를 최대화하는 \\(\\beta_0, \\beta_1, \\sigma\\)를 찾는 것이 목표이며,\n이처럼 분포가 Gaussian라면, OLS estimation과 동일한 값을 얻음. (단, \\(\\sigma\\)는 bias가 존재)\n다른 분포를 가지더라도 동일하게 적용할 수 있음!\n\n즉, likelihood의 관점에서 주어진 데이터에 가장 근접하도록(likelihood가 최대가 되는) “분포의 구조”를 얻는 과정임\n\n여러 편의를 위해, log likelihood를 최대화함.\n\n\n\n\n\n\nLog-likelihood\n\n\n\n다음 두가지를 고려하면,\n\\(log(x \\cdot y) = log(x) + log(y)\\)\n\\(e^x \\cdot e^y = e^{x+y}\\)\n\\(log(L) = \\displaystyle\\sum_{i=1}^{n}{log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)\\right)} = \\displaystyle\\sum_{i=1}^{n}{-log(\\sqrt{2\\pi\\sigma^2}) - \\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}}\\)\n두 번째 항이 앞서 정의한 squared error와 동일함\n\n\n앞서와 마찬가지로 여러 \\(\\beta_0, \\beta_1\\)값을 대입해서 log likelihood를 최대화하는 값을 찾아보면,\n\n\ncreate a grid for b0, b1\n# create a grid for b0: (-20, 40), b1:(-5, 5)\nnp.random.seed(123)\nb0 = np.linspace(-20, 40, 100)\nb1 = np.linspace(-5, 5, 100)\n\n# meshgrid\nb0, b1 = np.meshgrid(b0, b1)\n\nmodels = pd.DataFrame(dict(b0=b0.ravel(), b1=b1.ravel()))\nmodels\n\n\n         b0    b1\n0    -20.00 -5.00\n1    -19.39 -5.00\n2    -18.79 -5.00\n...     ...   ...\n9997  38.79  5.00\n9998  39.39  5.00\n9999  40.00  5.00\n\n[10000 rows x 2 columns]\n\n\n표준편차(\\(\\sigma\\))는 고정하고(2.2), - loglikelihood 값을 구해 정렬하면,\n(참고: 마찬가지로 likelihood를 최대화하는 \\(\\sigma\\)값을 찾을 수 있음)\n\n\ncalculate the likelihood\nfrom scipy.stats import norm  # normal distribution\n\ndef likelihood(b, data):\n    mu = b[0] + b[1] * data[\"x\"]\n    sigma = 2.2\n    return -np.sum(np.log(norm.pdf(data[\"y\"], mu, sigma)))\n\nmodels[\"-log likelihood\"] = models.apply(lambda x: likelihood(x, sim1), axis=1)\nmodels.sort_values(\"-log likelihood\")\n\n\n         b0    b1  -log likelihood\n7040   4.24  2.07            65.32\n6941   4.85  1.97            65.53\n7139   3.64  2.17            65.65\n...     ...   ...              ...\n406  -16.36 -4.60              inf\n209  -14.55 -4.80              inf\n0    -20.00 -5.00              inf\n\n[10000 rows x 3 columns]\n\n\nscipy의 minimize 함수를 이용해서 최소값을 구해보면,\n\nfrom scipy.optimize import minimize\nminimize(likelihood, [0, 0], args=(sim1)).x\n\narray([4.22, 2.05])\n\n\n\nfrom statsmodels.formula.api import ols\n\nmod = ols('y ~ x', data=sim1).fit()\nmod.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.885\n\n\nModel:\nOLS\nAdj. R-squared:\n0.880\n\n\nMethod:\nLeast Squares\nF-statistic:\n214.7\n\n\nDate:\nSat, 12 Apr 2025\nProb (F-statistic):\n1.17e-14\n\n\nTime:\n18:33:25\nLog-Likelihood:\n-65.226\n\n\nNo. Observations:\n30\nAIC:\n134.5\n\n\nDf Residuals:\n28\nBIC:\n137.3\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4.2208\n0.869\n4.858\n0.000\n2.441\n6.001\n\n\nx\n2.0515\n0.140\n14.651\n0.000\n1.765\n2.338\n\n\n\n\n\n\n\n\nOmnibus:\n0.125\nDurbin-Watson:\n2.254\n\n\nProb(Omnibus):\n0.939\nJarque-Bera (JB):\n0.333\n\n\nSkew:\n0.081\nProb(JB):\n0.847\n\n\nKurtosis:\n2.510\nCond. No.\n13.7\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nMaximum likelihood estimation은 다양한 분포의 데이터에 대해서도 적용할 수 있음.\n예를 들어, 다음과 같은 전형적인 Gaussian이 아닌 분포에 대해서도 적용할 수 있음.\nNon-constant variance(왼쪽), Poisson distribution(오른쪽)",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#uncertainty",
    "href": "contents/model-basic.html#uncertainty",
    "title": "Model Basics",
    "section": "Uncertainty",
    "text": "Uncertainty\n관찰된 데이터(표본, sample)로부터 모집단(population)에 대한 정보를 추론할 때, 불확실성이 존재함.\n이는 새로운 데이터에 대한 예측의 불확실성 혹은 일반화(generalization)에 대한 문제와 동일함.\n예를 들어, 과거 병원 기록으로 새로운 환자에 대한 진단을 내리는 경우\n\n이 환자의 고유한 상태로부터 오는 불확실성: 측정된 부분(measured) + 측정되지 않은 부분(unmeasured)\n과거 기록을 통한 진단의 정확성(true relationship)에 대한 불확실성\n\n불확실성에 대한 종류\n\n파라미터 추정치에 대한 불확실성: confidence interval\n\n데이터가 많을수록\nX와 Y의 관계가 강할수록\nX가 넓게 분포할수록\n\n특정 추정값에 대한 불확실성\n\n평균값(\\(E(Y|X_i)\\))에 대한 불확실성: confidence interval\n예측값(\\(f(X_i)\\))에 대한 불확실성: prediciton interval\n\n\n가령, 성별에 따른 키의 차이를 모델링한다면 \\(height = \\beta_0 + \\beta_1\\cdot sex\\)\n모집단에 대한 가정:\n\n\n\n\n\n\n\n   Sample size: 20\n\n\n\n\n\n\n\n   Sample size: 200\n\n\n\n\n\n전통적으로는 분포에 대한 가정으로부터 이론적으로 불확실성을 추론했으나,\n현대적인 접근으로 resampling 방식의 bootstrapping이나 sample을 traing/test set으로 나누는 cross-validation 등을 통해 시뮬레이션을 통해 불확실성을 추정할 수 있음\nBayesian 방식에서는 모집단의 분포에 대한 가정없이, 관찰된 데이터만으로 파라미터에 대한 분포(posterior predictive distribution)로 불확실성에 대해 추정하는 방식도 있음; 앞서 지구의 바다/육지의 비율에 대한 분포의 예측 참고\n반대로, machine learning에서는 특정 action 혹은 decision-making을 하는 것이 중요한 경우가 많아, 불확실성에 대한 고려가 적은 경향이 있음; 가령, 불확실성이 크던 작던 간에, 실용적인 관점에서 구체적으로 가격을 예측하거나, 질병 여부를 예측하길 기대함. 어떤 질병에 걸렸을 확률이 0.8로 예측되는 경우라도 사실은 불확실성을 고려하면 그 확률은 가령 0.7 ~ 0.9 사이일 수 있음.\n\n\ncalculate confidence intervals\nsim1_new = pd.DataFrame({\"x\": [0.5, 1.5, 4.5, 7.5, 10.5]})\nmod = ols('y ~ x', data=sim1).fit()\npredictions = mod.get_prediction(sim1_new).summary_frame(.1)\n\nplt.fill_between(sim1_new[\"x\"], predictions['obs_ci_lower'], predictions['obs_ci_upper'], alpha=.1, label='90% Prediction interval')\nplt.fill_between(sim1_new[\"x\"], predictions['mean_ci_lower'], predictions['mean_ci_upper'], alpha=.5, label='90% Confidence interval')\nplt.scatter(sim1[\"x\"], sim1[\"y\"], label='Observed', marker='o', color='.6', s=12)  # Reduced point size\nplt.scatter(sim1_new[\"x\"], sim1_new[\"x\"]*0, label='New data', marker='x', color='orangered', s=22)  # Reduced point size\nplt.plot(sim1_new[\"x\"], predictions['mean'], label='Regression line')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(frameon=False)\nsns.despine()\nplt.ylim(-0.5, 30)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx_new\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n0.50\n5.25\n0.81\n3.87\n6.62\n1.26\n9.24\n\n\n1\n1.50\n7.30\n0.69\n6.13\n8.47\n3.37\n11.22\n\n\n2\n4.50\n13.45\n0.43\n12.73\n14.18\n9.64\n17.27\n\n\n3\n7.50\n19.61\n0.49\n18.77\n20.44\n15.77\n23.45\n\n\n4\n10.50\n25.76\n0.81\n24.39\n27.14\n21.77\n29.75\n\n\n\n\n\n\n\n\n아래는 bootstrapping을 통한 실제 표본을 재추출(resampling)하는 방식을 보여줌; 관찰된 표본을 모집단인듯 가정하고 표본을 재추출\n전통적으로는 표본 분포(sampling distribution)에 대한 이론을 통해 closed form으로 얻을 수 있음.\n\n\n\n\nSource: The Truthful Art by Albert Cairo.",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#predictive-accuracy",
    "href": "contents/model-basic.html#predictive-accuracy",
    "title": "Model Basics",
    "section": "Predictive Accuracy",
    "text": "Predictive Accuracy\n전통적인 모형에서는\n\n샘플에 대해서 계산된 예측정확도 값은 모집단에서보다 overestimate 되는 경향이 있으므로,\n모집단에서의 예측정확도를 이론적으로 보정하는 방식으로 계산: adjusted, shrunken\n\n현대적인 방식에서는\n\n샘플을 training/test set으로 나누어서, test set에 대한 예측값을 계산하고, 이를 통해 (미래 데이터에 대한) 예측 정확성을 평가함\n비슷하게, resampling(샘플에서 직접 다시 추출) 방식의 bootstrapping을 통해 해결\n\n주로 사용되는 지표들\n\n\\(RMSE = \\displaystyle\\sqrt{{\\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}}{{e^2}}}}\\)\n\\(MAE = \\displaystyle\\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}}|~e~|\\)\n\\(R^2 = 1 - \\displaystyle\\frac{\\frac{1}{n} \\sum_{i=1}^{n}{(e-0)^2}}{\\frac{1}{n} \\sum_{i=1}^{n}{(Y-\\overline{Y})^2}} = 1 - \\frac{V(e)}{V(Y)} = \\frac{V(\\widehat Y)}{V(Y)}\\),   \\(V(Y) = V(\\widehat Y) + V(e)\\)\n\n전통적으로 \\(X\\)가 \\(Y\\)를 얼마나 잘 “설명”해주는지에 대한 지표로서 \\(R^2\\)를 사용함; strength of association(연관성의 강도/크기)\n\n(정규분포 아닌 경우 특히) 분포를 고려한 log-likelihood를 기반으로 한 모델 적합도 지수: 여러 모델들의 상대적 (적합도) 비교를 위해 이용\n\n\\(AIC = -2\\text{log-likelihood} + 2k\\),   \\(k\\): 파라미터의 갯수\n\\(BIC = -2\\text{log-likelihood} + k \\cdot \\log(n)\\),   \\(n\\): 데이터의 수\n\n\n\n\ncalculate RMSE, MAE, R2\nfrom statsmodels.tools.eval_measures import rmse, meanabs, aic, bic\nypred = mod.predict(sim1)\ny = sim1[\"y\"]\nllf = mod.llf # log likelihood\n\nprint(f\"RMSE = {rmse(y, ypred):.2f} \\nMAE = {meanabs(y, ypred):.2f} \\nR-squared = {mod.rsquared:.2f} \\nAIC = {aic(llf, 30, 2):.2f} \\nBIC = {bic(llf, 30, 2):.2f}\")\n\n\nRMSE = 2.13 \nMAE = 1.71 \nR-squared = 0.88 \nAIC = 134.45 \nBIC = 137.25",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html",
    "href": "contents/mlsl-intro.html",
    "title": "Machine/Statistical Learning",
    "section": "",
    "text": "\\(f\\): \\(X_1, X_2, ..., X_p\\)가 \\(Y\\)에 관해 제공하는 systematic information; unknown function\n\\(Y = f(X_1, X_2, ..., X_p) + \\epsilon\\),   \\(Y \\perp\\!\\!\\!\\perp \\epsilon\\)\n\\(\\epsilon\\): 불확실성의 소스들; reducible error & irreducible error\n(epistemic uncertainty & aleatory uncertainty)\n앞서 선형모형은 \\(f\\)를 \\(X\\)의 선형함수인 parametric model로 가정한 후 parameter를 추정하여 문제가 단순하였으나,\n이번에는 \\(f\\)의 형태를 가정하지 않고, \\(f\\) 자체를 추정하고자 하는 매우 광범위한 문제임.\n예를 들어,",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#왜-분석하는가",
    "href": "contents/mlsl-intro.html#왜-분석하는가",
    "title": "Machine/Statistical Learning",
    "section": "왜 분석하는가?",
    "text": "왜 분석하는가?\nThe Occam’s Dilemma?\n\n예측(prediction)\n변수들 간의 관계에 대한 추론(inference)/해석(interpretation)\n\n정보(information) 획득의 관점에서: Leo Breiman 글 참고",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#어떻게-f를-추정하는가",
    "href": "contents/mlsl-intro.html#어떻게-f를-추정하는가",
    "title": "Machine/Statistical Learning",
    "section": "어떻게 \\(f\\)를 추정하는가?",
    "text": "어떻게 \\(f\\)를 추정하는가?\n모수적(parametric) 접근\n\n모수적 접근: \\(f\\)의 형태를 가정하고, 그 형태에 대한 parameter를 추정\n가령, 선형성을 가정한 linear model: \\(f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\)\n모델을 관찰된 데이터에 fit(적합) 또는 모델을 train(훈련)/learn(학습) 시켜, parameter(모수) 추정\n전형적으로 (ordinary) least squares 방법을 사용\n상대적으로 적은 수의 데이터로도 추정이 가능\n해석을 통해 변수 간의 관계 추론 용이\n\\(f\\)의 형태를 잘못 가정하면, 잘못된 결과를 낼 수 있음\n\n\n\n\n\n\n\n\n\nSource: p.21, An Introduction to Statistical Learning with Applications in Python\n\n비모수적(non-parametric) 접근\n\n\\(f\\)의 형태를 가정하지 않음\n보통, 예측의 정확성이 높도록, 즉 데이터와 최대한 가까운 매우 복잡한 형태의 \\(\\hat f\\)를 추구\n과적합(overfit)이 되지 않도록, 새로운 데이터에도 잘 일반화되도록 sweet spot을 찾아야 함\n매우 많은 데이터가 요구됨\n해석은 어려우나, \\(f\\)로부터 다양한 정보(information)을 추출하는 방법을 고안함으로써 (해석을 넘어) 정보를 추출할 수 있음\n모수가 없을 수도, 있을 수도 있으나, 해석 가능한 모수라고 보기 어려움\n\n\n\n\n\n\n\n\n\n\n\n\nSource: p.22, An Introduction to Statistical Learning with Applications in Python\n\nThe tradeoff between flexibility and interpretability\n\n\nSource: p.22, An Introduction to Statistical Learning with Applications in Python\n\n어떤 기준으로 이 tradeoff의 수준을 결정할 것인가?\n\n해석에 중심을 두고, 변수 간의 관계를 추론하고자 하는 경우: 모형이 틀릴 위험이 존재 &gt;&gt; bias가 높아짐\n예측의 정확성에 중심을 두는 경우: 과적합이 될 위험이 존재 &gt;&gt; variance가 높아짐\n이 둘이 항상 대치되는 것은 아님; 단순한 모형이 새로운 데이터에서 더 나은 예측 정확도를 가질 수도 있음!",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#회귀-vs.-분류",
    "href": "contents/mlsl-intro.html#회귀-vs.-분류",
    "title": "Machine/Statistical Learning",
    "section": "회귀 vs. 분류",
    "text": "회귀 vs. 분류\nRegression vs. Classification\n\n보통 \\(Y\\)가 연속형 변수인 경우, 회귀(regression) 문제로 다루고, \\(Y\\)가 범주형 변수인 경우, 분류(classification) 문제로 일컬어 짐.\n회귀모형을 확장하여 확률(연속값)을 구한 후, 이를 이용해 분류문제를 다룰 수 있음; ex. logistic regression\n예측 변수의 경우는 연속인지 범주형인지는 상관없음.",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#decision-theory",
    "href": "contents/mlsl-intro.html#decision-theory",
    "title": "Machine/Statistical Learning",
    "section": "Decision Theory",
    "text": "Decision Theory\n\nSource: Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H\n\n\\(Y = f(X) + \\epsilon\\)\n앞서 선형 회귀 모형에서 구한 것은, conditional probability \\(p(Y|X)\\)를 모델링 한 것이었음.\n이 분포를 Gaussian:\\(N(\\mu, \\sigma^2)\\)으로 가정하고, 분포의 평균과 표준편차를 likelihood가 최대가 되도록 추정하는 간단한 모델로 축소했음; predictive distribution\n\n\n분포 \\(p(Y|X)\\)를 통해 예측의 불확실성을 파악할 수 있음\n만약, 주어진 \\(X\\)에서 대해 예측값 \\(f(X)\\)을 하나 결정하는데, 실제 true 값을 \\(y\\)라고 하면,\n그 오차에 대해 발생되는 어떤 penalty 또는 cost를 정의; loss function\nLoss function을 통해 “최적”의 예측값에 대한 기준을 제공\n\\(f(x) = E(Y|X=x)\\)로 예측하는 것이 많은 경우 적절하지만 일반적으로 그런 것은 아님\n\nLoss/Cost Function\nLoss function: \\(L(y, f(x))\\)\n\\(y\\): true value, \\(f\\): predicted value\n\n이 loss를 최소화하는 것이 목표\nTrue \\(y\\)를 모르기 때문에, 평균치인 expected loss를 최소화하는 것이 목표\n즉, \\(E(L) = \\displaystyle \\iint L(y, f(x))p(x, y) dx dy\\)를 최소화하도록 예측값(\\(f\\))을 결정\n\n각 \\(x\\)에 대해 \\(\\displaystyle \\int L(y, f(x))p(y|x)dy\\)를 최소화\n\n예를 들어, 다이아몬드의 무게로 가격을 예측한다면: 1 carat 다이아몬드의 true price?\n또는, 목소리(특정 frequency)로 성별을 예측한다면; 분류 문제의 경우 뒤에서 다룸\n\n\n\n\n\n\n\n\n Source: Language Log\n\n\n\n앞서 회귀 모형에서는 기본적으로 squared loss 사용;\n\n\\(L_2 = \\displaystyle(f(x) - y)^2\\)\n\n\\(E(L)\\)을 최소로 하는 함수: \\(f(x_0) = E(Y|X=x_0)\\): regression function \n\n\\(L_1 = \\displaystyle |~f(x) - y~|\\)\n\n\\(E(L)\\)을 최소로 하는 함수: \\(f(x_0) = median(Y|X=x_0)\\) \n\n\\(L_q = \\displaystyle |~f(x) - y~|^q\\)\n\n\n\nSource: Pattern Recognition and Machine Learning by Christopher M. Bishop\n\n예를 들어,\n\n\n\\(f(1.5) = E(Y|X=1.5)\\)인 \\(f\\)가 \\(E(L_2)\\)를 최소화하는 optimal한 함수임.\n즉, conditional mean \\(f(x) = E(Y |X = x)\\)는 \\(L_2\\) loss의 관점에서 최적의 함수이며, regression function이라고 부름.\n\n실제로는 \\(E(Y | X=x)\\)를 계산하는 것은 불가능하며, 이를 추정하기 위해 x의 근방에서 평균값을 취함.\n\n\\(\\hat f(x) = Ave(Y |X \\in N(x))\\)\n\n\n\n단, 관측치의 수에 대해 상대적으로 predictor의 갯수가 늘어남에 따라 점차 효율성이 떨어짐; the curse of dimensionality\n\n예를 들어, 다이아몬드 가격을 carat(캐럿), cut(컷), clarity(투명도)의 세 변수로 예측하는 경우,\n\\(\\hat f(1.5, Fair, I1) = E(Y|carat=1.5,~ cut=Fair,~ clarity=I1)\\)\n\n\n\n\n\n\n\n\n\n\\(f\\)의 smoothing에 대한 여러 접근이 있음\n\nregualization, spline, kernel 등\n\nThe Palmer Archipelago penguins 데이터셋의 예로 보면,\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\n만약, 부리의 길이로 부리의 깊이를 예측한는 모형에 대해 \\(\\hat f\\)를 구한다면,\n\n\n\n\n\n\n\n\n\n\n앞서 선형회귀모형을 사용한다면,\n\n관계를 선형이라고 전제하고,\nError가 Gaussian 분포를 따른다고 가정하고,\nLikelihood가 최대가 되도록 파라미터(기울기와 절편)를 구한 것임; maximum likelihood estimation\nSquared error를 최소화하는 것과 동일함.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Visualizing a multivariate Gaussian\n\n\n\n\n\n\n\nExpected value\n\n\n\n\n\n\\(X: \\{a, b, a, a, b, c\\}\\)\nThe mean of \\(X=\\displaystyle \\frac{a + b + a + a + b + c}{6} = \\frac{3*a}{6} + \\frac{2*b}{6} + \\frac{1*c}{6}=a*\\frac{3}{6} + b*\\frac{2}{6} + c*\\frac{1}{6}\\)\n\\(~~~~~~~~~~~~~~~~~~~~~~~~~=a*p(X=a) + b*p(X=b) + c*(X=c)\\): weighted average\nExpectation: \\(E(X)=\\displaystyle \\sum x_i p(X=x_i)= \\sum xp(x)\\)\n연속값이라면, \\(E(X)=\\displaystyle \\sum \\Delta x * p(\\Delta x) = \\int x f(x) dx\\), where \\(f(x)\\) is the probability density function\n\\(p(\\Delta x) = f(x)\\Delta x\\)\n\n\n\n\nThe Bias–Variance Trade-off\n위의 논의는 특정 데이터셋에 의존할 수밖에 없는데, (참고: Bayesian의 경우 다른 접근)\n데이터셋마다 다른 \\(\\hat f\\)를 얻게 된다는 점을 감안했을 때, \\(\\hat f\\)의 변동성을 살펴보면,\n\\(L_2\\) loss의 경우,\n\\(\\displaystyle h(x) := E(Y|X=x)\\): optimal prediction of \\(Y\\) at any point \\(x\\)\n\\(~~~~~~~~~ = \\displaystyle \\int y \\cdot p(y|x)dy\\)\n\\(E(L_2) = \\displaystyle \\iint \\left( f(x) - y \\right)^2 p(x, y) dxdy = \\iint \\left( f(x) - h(x) + h(x) - y \\right)^2 p(x, y) dxdy\\)\n\\(~~~~~~~~~~~ = \\displaystyle \\int \\left( f(x) - h(x) \\right)^2 p(x) dx + \\iint \\left( h(x) - y) \\right)^2p(x, y) dxdy\\)\n\n두 번째 항: optimal prediction \\(h(x)\\)가 true value와 얼마나 떨어져 있는가?(분산) - irreducible error\n첫 번째 항: \\(h(x)\\)와의 차이를 최소화하도록 \\(f(x)\\)를 선택하는데 하는데, 유한한 데이터셋에서는 그 간극이 존재\n\n이제, 분포 \\(p(x, y)\\)로부터 수많은 데이터셋들 \\(D_i\\)를 얻었다고 가정했을 때,\n특정 \\(x_0\\)에 대해서\n\n\n\\(E_D[\\left( f(x_0; D_i) - h(x_0)\\right)^2] = E_D[\\left( f(x_0; D_i) - E_D\\left( f(x_0; D_i)\\right) + E_D\\left( f(x_0; D_i)\\right) - h(x_0)\\right)^2]\\) \\(~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ =[E_D\\left( f(x_0; D_i)\\right) - h(x_0)]^2 + E_D[\\left\\{f(x_0;D_i) - E_D\\left(f(x_0;D_i)\\right)\\right\\}^2]\\)\n\\(~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ = bias^2 + variance\\)\nBias: 최적의 예측값(\\(h\\))에 비해, (여러 데이터로부터 얻은) “평균적인 예측값”이 얼마나 차이가 나는가?\n\n즉, 최적의 예측보다 평균적으로 얼마나 틀린 예측을 하는가?\n\nVariance: 각 데이터셋으로부터 얻은 예측값이 “평균적인 예측값”에서 얼마나 떨어져 있는가?(분산)\n\n즉, 데이터셋에 따라 \\(f(x_0;D_i)\\)가 얼마나 민감하게 변하는지 측정\n\n모든 \\(x\\)에 대한 비용의 총합으로 확장해서 이해하면,\n\n\n\n\n\n\nExpected Loss 분해\n\n\n\n\\(\\displaystyle E(L_2) = (bias)^2 + variance + noise\\)\n\n\\(\\displaystyle \\text{(bias)}^2 = \\int [E_D\\left( f(x_0; D_i)\\right) - h(x_0)]^2 p(x) dx\\)\n\\(\\displaystyle \\text{variance} = \\int E_D[\\left\\{f(x_0;D_i) - E_D\\left(f(x_0;D_i)\\right)\\right\\}^2] p(x) dx\\)\n\\(\\displaystyle \\text{noise} = \\iint \\left( h(x) - y) \\right)^2p(x, y) dxdy\\)\n\n\n\nBias와 variance간에는 trade-off가 존재함\n\nFlexible한 모델의 경우 데이터에 더 잘 적합하여, variance가 높아짐. 즉 데이터셋마다 너무 다른 예측을 하게 됨\n\n데이터에 overfitting이 된다고 말할 수 있음.\n단, N이 증가하면(데이터셋 사이즈가 커지면), flexible한 모델의 variance가 줄어듦.\n여러 데이터셋으로부터 평균을 얻으면, 실제값에 가까워짐.\n\nRigid한 모델의 경우 데이터셋에 덜 적합하여, bias가 높아짐. 즉, 평균적으로 더 틀린 예측을 하게 됨\n\n한편, 데이터셋에 덜 민감하여 variance가 줄어듦\n작은 데이터셋에서 더 유리\n\n이 둘의 적절한 균형을 갖는 모델이 최적의 예측 모델임\n\n예를 들어, 함수 \\(h(x) = sin(2\\pi x)\\)로부터 생성된 데이터셋(N=10)에 대해 다항식의 차수에 따른 flexibility의 변화에 따른 OLS 모델들을 비교하면,\n\n데이터셋의 사이즈가 커지면, (M=9인 경우)\n\n함수 \\(h(x) = sin(2\\pi x)\\)로부터 생성된 100개의 데이터셋(각 N = 25)에 대해 3가지 flexibility에 대한 모델들을 비교하면,\n\n\nSource: p.10, 12, 127, Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#classification",
    "href": "contents/mlsl-intro.html#classification",
    "title": "Machine/Statistical Learning",
    "section": "Classification",
    "text": "Classification\n\\(Y\\)가 범주형 변수인 경우, 즉 분류(classification) 문제인 경우\n\nY가 k개의 범주/클래스로 나뉘는 경우: \\(Y \\in \\{C_1, C_2, ..., C_k\\}\\)\n\n두 범주의 경우, 간단히 \\(Y \\in \\{0, 1\\}\\)\n\n\\(f(x_i): P(Y = C_k|X=x_i)\\)가 최대인 클래스에 할당; 즉, \\(f(x_i) = \\underset{k}{\\mathrm{argmax}}~ P(Y = C_k|X=x_i)\\)\n\nConditional class probabilities; 앞서 conditional mean \\(E(Y|X)\\)에 대응\nBayes classifier: 최대치 (0-1 loss function 가정하에)\n\n이 \\(P(Y = C_k|X=x)\\)를 추정하기 위한 다양한 방식들이 존재\n\nK-nearest neighbors\nLogistic regression\nGeneralized additive models\nLinear/Quadratic discriminant analysis\n\n\n\n\n\n\n\n\n\n\n\n\n한 개의 예측변수로 예측한다면,\n가령 \\(X\\): bill_length_mm인 경우, 즉 펭귄의 부리 길이로만 두 종을 분류한다면,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuadratic discriminant analysis(QDA): 각각을 Gaussian으로 가정하고, 평균과 표준편차를 추정\n\n\n\n\n\n\n\nConfusion matrix with the threshold=0.5\n\n\nTruth      Adelie  Gentoo\nPredicted                \nAdelie        144       7\nGentoo          7     116\n\n\n만약, 다음과 같은 loss function (loss matrix)가 주어진다면,\n\n\nTruth      Adelie  Gentoo\nPredicted                \nAdelie          0      10\nGentoo         30       0\n\n\nExpected loss:\n\\(E(L) = \\displaystyle \\sum_{i=1} \\sum_{j=1} L_{ij}P(y\\in C_i, \\hat y\\in C_j) = \\frac{1}{274}(144*0 + 7*10 + 7*30 + 116*0)\\)\n보통의 경우, 0-1 loss를 사용. 즉, misclassified된 관측치에 대한 비율; misclassification error rate\n\\(E(L) = \\displaystyle \\frac{1}{N} \\sum_{i=1}^N I(y_i \\neq \\hat y_i)\\) where \\(I\\) is the indicator function: 0 if \\(y_i = \\hat y_i\\), 1 otherwise\n잘못된 예측에 대한 비용이 다르다면, 이에 대응하는 expected loss를 기준으로 모델을 평가\n\n만약, 와인 셀러가 와인의 품질(high:양성 vs. low:음성)을 성분들로 예측하는 모형을 만든다면,\n높은 품질의 와인을 낮은 품질로 예측하면, 수익의 악화\n\n거짓 음성을 낮춰야 하는 경우: 예를 들어, 영세한 와이너리가 수익이 중요한 경우\n\n낮은 품질의 와인을 높은 품질로 예측하면, 와인 품평가에게 신뢰를 잃을 수 있음\n\n거짓 양성을 낮춰야 하는 경우: 예를 들어, 고품질의 와인을 생산하는 것으로 유명한 와이너리가 네임밸류를 유지하기 위해. 반면, 고품질의 와인이 싸게 팔리는 것은 감당할 수 있음.",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#model-evaluation",
    "href": "contents/mlsl-intro.html#model-evaluation",
    "title": "Machine/Statistical Learning",
    "section": "Model evaluation",
    "text": "Model evaluation\n모델의 평가는 예측함수의 오차로 인한 손실(loss)이 최소인 것으로 기준을 둘 수 있으나,\n현실적으로는 모델은 특정 데이터셋, 혹은 특정 관찰값에 의존할 수밖에 없음.\n우리가 원하는 것은 관찰값에 대해서가 아닌 일반화된(generalized) 혹은 전체(population)에 대해 손실이 최소이기를 바라는 것임.\n전통적 통계에서는 모집단(population)이라는 것을 상정하여, 일반화된 모형을 추론하고자 했음.\n\n이를 위해 모집단에 대한 여러 가정들이 필요했으며, 이를 통해 모집단에 대한 추론(inference)을 얻었음.\n관측치를 최대한 모두 사용하여 모형을 추정하고, 모집단에 대한 추론에 대해 수학적인 수정을 거침.\nLikelihood의 관점에서 최선의 모형을 선택하고, 모집단에 대한 가정을 기반으로 불확성을 추론함.\n\nResampling methods\n현대적인 관점에서는 가정없이 관찰된 데이터셋만으로 일반화할 수 있는 다른 접근 방식을 택함.\n1) 교차검증 (cross-validation), 2) 부트스트랩 (bootstrap)\n1) 교차검증 (cross-validation)\n\n데이터셋을 훈련셋(training set)과 테스트셋(test set)으로 나누어, 훈련셋으로 모델을 구축하고\n모델이 훈련/학습 과정에서 보지 못한 새로운 데이터셋인 테스트셋에서 얼마나 잘 작동하는지를 평가함.\n이는 모델이 얼마나 일반화(generalization)될 수 있는가를 평가하는 접근이라고 볼 수 있음.\n모델의 과적합을 조정하기 위해(bias-variance trade-off) 훈련셋을 다시 여러 개의 서브셋으로 나누어 교차검증(cross-validation)을 수행; 검증셋(validation set)\n실질적으로 테스트셋과 검증셋은 동일한 개념으로 어떻게 1) 훈련셋, 2) 검증셋, 3) 테스트셋으로 나누어 데이터를 이용할 것인지는 여러 상황을 고려해서 유연하게 적용하게 됨.\n\n\n\n\n\n\n\n\n테스트셋의 의미\n\n\n\n엄밀히는 테스트셋은 훈련셋과 매우 “동질적인 데이터”이기 때문에 (무작위 추출로 나눈 두 세트임) 진정한 미래의 데이터라고 보기 어려움\n이 테스트셋을 미래의 새로운 데이터라고 간주하려면 적어도,\n\n미래에도 현재의 데이터와 동질적인 데이터가 발생할 것이라는 가정이 필요.\n미래에도 현재와 동일한 인과 메커니즘이 작동해야 함.\n\n가령, 도시 공유자전거의 수요를 예측하는데, 올해의 데이터로 내년의 수요를 예측할 때\n\n내년에도 사람들은 “동일한 이유”로 자전거를 이용할 것이어야 하며; 예를 들어, 날씨가 좋으면 더 타러 나올 것임.\n내년에도 자전거를 빌리는 방식이 동일해야 하며; 예를 들어, 대여비나 대여 방식, 대여 장소 등의 변경이 없어야 함.\n내년에도 자전거를 빌리는 사람들은 올해와 다른 종류의 사람들은 아니어야 함; 예를 들어, 대규모 인구 변화가 없어야 함.\n\n\n\n2) 부트스트랩 (bootstrap)\n\n\n\n\n데이터셋에 따라 모델이 어떻게/얼마나 바뀌는지를 추정하기 위해, 데이터셋으로부터 여러 개의 (가상) 샘플을 추출\n주어진 데이터셋으로부터 같은 사이즈의 데이터셋들을 “중복을 허용”해서 추출. 가령, 1000개의 표본을 얻을 수 있음\n\n평균적으로 각 표본에서 63.2%의 중복되지 않는 데이터가 선택됨\n\n이는 마치 새로운 표본들을 얻는 것과 같은 효과를 얻어, 표본들마다 모형이 어떻게 다르게 추정되는지를 파악할 수 있음.\n\n전통적 통계에서 “표본 분포”를 이론적으로 얻어 파라미터 추정치의 불확실성을 얻었는데, 이에 대한 현대적 대안이기도 함.\n\n머신러닝에서는 여러 (가상) 표본에서 학습시킨 모형들을 평균내어 예측력을 높히는데 사용; Bagging (Bootstrap aggregating)\n\n\n\n\n\n\n\n\n\n\n\n\n\n훈련셋과 테스트셋에 대한 손실\n\n\n\n모형에 대한 평가는 테스트셋에 대한 손실을 기준으로 해야 함으로, 두 종류의 손실에 대해 구별함.\n훈련셋에 대한 손실(오차): 모형을 세우는데 사용된 데이터셋에 대한 손실\n테스트/검증셋에 대한 손실(오차): 테스트/검증 데이터셋으로 모형을 평가 했을 때의 손실\n모형은 훈련셋에 최대한 적합되도록 세운 것이기 때문에,\n\n훈련셋의 손실(오차) &lt; 테스트/검증셋의 손실(오차)\n\n(단, 모형을 세우는 때 사용된 손실과 동일한 방식의 손실로 평가했을 때)\n\n\n\n교차검증\nCross-Validation\n데이터셋을 훈련셋과 검증셋으로 나누어 보면; validation set approach\n예를 들어, Automobile Data(auto) (ISLP 패키지)에서 연비(mpg)를 마력(horsepower)로 예측하는 모델을 만든다면,\n\n훈련셋과 검증셋을 어떻게 선택하는지에 따라 결과가 바뀜\n훈련셋의 양을 얼마나 선택하는지에 따라 결과가 바뀜: 데이터가 작을 수록 모형의 적절성이 낮아짐\n아래 그림; 어떻게 데이터셋을 나누느냐에 따라 결과가 달라짐\n\n이를 해결하기 위해 교차검증(cross-validation)을 사용\n\n다수의 훈련셋과 검증셋을 생성하여, 평균값으로 모형의 성능을 평가\n다양한 변형이 존재\n\n\n\nload packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n\n\n\n\ncode\n# install ISLP package\n# pip install ISLP\n\nfrom ISLP import load_data\nauto = load_data(\"Auto\")\nauto.head(3)\n\n\n    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n0 18.00          8        307.00         130    3504         12.00    70   \n1 15.00          8        350.00         165    3693         11.50    70   \n2 18.00          8        318.00         150    3436         11.00    70   \n\n   origin                       name  \n0       1  chevrolet chevelle malibu  \n1       1          buick skylark 320  \n2       1         plymouth satellite  \n\n\n\nfrom sklearn.model_selection import train_test_split\nauto_train, auto_valid = train_test_split(auto, test_size=.5, random_state=0)\n\n\ncode\np = (\n    so.Plot(auto_train, x='horsepower', y='mpg')\n    .add(so.Dots(color=\".5\"))\n)\np.show()\n\nfor i in range(1, 6):\n    p.add(so.Line(), so.PolyFit(i)).label(title=f\"M = {i}\").show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft: Validation error estimates for a single split into training and validation data sets. Right: The validation method was repeated ten times, each time using a different random split of the observations into a training set and a validation set. This illustrates the variability in the estimated test MSE that results from this approach.\nSource: p. 204, An Introduction to Statistical Learning with Applications in Python\n\n교차검증의 예\n\nK-fold cross-validation: 데이터셋을 k개의 서브셋으로 나누어, k번의 모형평가를 수행\nLeave-one-out cross-validation(LOOCV): k=n인 경우\n\n거의 모든 데이터로부터 훈련\nk-fold 작업에서 발생하는 무작위성이 없음\n\nShuffle-split cross-validation: 데이터셋을 무작위로 섞어서 k번의 모형평가를 수행\n\nScikit-learn: cross-validation 문서 참고\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n주로 k=5, k=10을 사용; 경험적으로 최적임\n\nk가 커지면, 학습에 더 많은 데이터를 사용하므로 모델이 더 일반화된 학습을 할 수 있으나\n각 fold에서의 검증 세트가 작아지므로 fold별 성능 평가의 변동성이 커질 수 있음\n\n\nk개의 훈련셋에서 각각에서 얻은 모델에 대해서 검증셋으로부터 모형의 평가치를 얻음: 주로 MSE(mean squared error)\nk-fold CV estimate: \\(\\displaystyle CV_{(k)} = \\frac{1}{k} \\sum_{i=1}^k MSE_i\\)\n분류의 경우, misclassification error rate: \\(\\displaystyle CV_{(k)} = \\frac{1}{k} \\sum_{i=1}^k \\frac{1}{n_i} \\sum_{j=1}^{n_i} I(y_j \\neq \\hat y_j)\\)\n\n\ncode\nfrom sklearn.model_selection import cross_validate, cross_val_score, KFold, ShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom ISLP import load_data\n\nauto = load_data(\"Auto\")\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n\nmod_auto = make_pipeline(PolynomialFeatures(2), LinearRegression())  # 2차 다항함수\nmod_auto_cv = -cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # default scoring is R2\n\nprint(f\"For the quadratic polynomial:\\n10-fold Cross-validation \\nMSEs: {mod_auto_cv}, \\nAverage: {mod_auto_cv.mean()}\")\n\n\nFor the quadratic polynomial:\n10-fold Cross-validation \nMSEs: [15.98 15.84 18.39 25.88 18.71 23.58 23.58 23.83 11.19 14.88], \nAverage: 19.185331419374968\n\n\n\n\ncode\ncv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nmod_auto = make_pipeline(PolynomialFeatures(2), LinearRegression())  # 2차 다항함수\nmod_auto_cv = -cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # default scoring is R2\n\nprint(f\"For the quadratic polynomial:\\nShuffle & Spllit Cross-validation with 20% test sets \\nMSEs: {mod_auto_cv}, \\nAverage: {mod_auto_cv.mean()}\")\n\n\nFor the quadratic polynomial:\nShuffle & Spllit Cross-validation with 20% test sets \nMSEs: [16.01 18.34 17.81 12.66 14.97 20.87 13.4  13.62 20.05 15.53], \nAverage: 16.32656735042881\n\n\n\ncode\nev_error1 = np.zeros([10, 10])\n\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n\nfor i, d in enumerate(range(1, 11)):\n    mod_auto = make_pipeline(PolynomialFeatures(d), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # cross_validate에 대한 wrapper\n    ev_error1[i, :] = -mod_auto_cv\n\n# plot\nplt.figure(figsize=(6, 4), dpi=80)\nfor i in range(10):\n    plt.plot(np.arange(1, 11), ev_error1[:, i])\n    plt.scatter(np.arange(1, 11), ev_error1[:, i], s=5)\n\nplt.xticks(np.arange(1, 11))\nplt.xlabel(\"Degree of Polynomial\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"10-fold Cross-validation \\nover 10 degrees of Polynomial\")\nplt.show()\n\n# Shuffle & Split Cross-validation\nev_error2 = np.zeros([10, 10])\ncv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nfor i, d in enumerate(range(1, 11)):\n    mod_auto = make_pipeline(PolynomialFeatures(d), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')\n    ev_error2[i, :] = -mod_auto_cv\n\n# plot\nplt.figure(figsize=(6, 4), dpi=80)\nfor i in range(10):\n    plt.plot(np.arange(1, 11), ev_error2[:, i])\n    plt.scatter(np.arange(1, 11), ev_error2[:, i], s=5)\n\nplt.xticks(np.arange(1, 11))\nplt.xlabel(\"Degree of Polynomial\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"10-split Shuffle & Split Cross-validation \\nwith 20% test sets over 10 degrees of Polynomial\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nev_error3 = np.zeros([10, 2])\n\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n\nfor i in range(10):\n    mod_auto = make_pipeline(PolynomialFeatures(i+1), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # cross_validate에 대한 wrapper\n    ev_error3[i, 0] = -mod_auto_cv.mean()\n\ncv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nfor i in range(10):\n    mod_auto = make_pipeline(PolynomialFeatures(i+1), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # cross_validate에 대한 wrapper\n    ev_error3[i, 1] = -mod_auto_cv.mean()\n\n# plot\nplt.figure(figsize=(6, 4), dpi=80)\nlabels = [\"10-fold CV\", \"10-split Shuffle & Split CV with 20% test set\"]\nfor i in range(2):\n    plt.scatter(np.arange(1, 11), ev_error3[:, i], s=5)\n    plt.plot(np.arange(1, 11), ev_error3[:, i], label=labels[i])\n\nplt.xticks(np.arange(1, 11))\nplt.xlabel(\"Degree of Polynomial\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"Average of 10-fold Cross-validation \\nover 10 degrees of Polynomial\")\nplt.ylim(15, 30)\nplt.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\nScikit-learn: metrics and scoring\nBias-Variance Trade-off를 고려하여 test error가 최소가 되는 모델을 선택\n\n\nTest and training error as a function of model complexity\n\n\n\n\n\n\n\n\nSource: p. 38, The Elements of Statistical Learning (2e) by Hastie, T., Tibshirani, R., & Friedman, J.\n\n관찰된 데이터에 가장 적합한 모형을 찾는 것이 아니고, 일반화된 모형을 찾는 것이 목표임.\n아래 그림에서 관찰된 데이터셋에 대한 예측 오류인 “test error”와 “true error”의 관계를 보여줌.\n예를 들어, 아래의 3가지 형태의 true relationship에 대해,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft: Data simulated from f, shown in black. Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel.\n\n\n\n\nSource: pp. 29-32, An Introduction to Statistical Learning with Applications in Python\n\nCross-validation의 test-error가 효과적으로 true error를 반영할 수 있는지를 살펴보면,\n\n보통 실제 test error rate보다 낮게 나오나(즉, 더 적합하게) 최적의 flexibility의 위치는 유사함\n참고로, 여기서 true error는 앞서 training/test set으로 나눈 그 test set을 의미하는 것이 아님!\n\n\n\nSource: p. 208, An Introduction to Statistical Learning with Applications in Python\n\n다음 절차를 통해, 어떤 클래스의 특정 모델을 선택 후 그 모델의 성능을 평가할 수 있음:\n\n모델 클래스를 선택: 예. linear regression\n데이터셋을 traing set과 test set으로 나눈 후,\nTraining set으로 교차검증(sub-trainging & validation sets으로 나누어서)을 통해 모델의 flexibility를 결정: 예. 다항식의 차수, tuning parameter 등\n\n최적의 flexibility 또는 bias-variance trade off 수준를 결정\n\n최적의 flexibility를 선택한 후, 전체 training set으로 fitting한 모델을 선택 후\ntest set으로 모델의 성능을 평가\n\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nauto = load_data(\"Auto\")\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n## Shuffle & Split\n# cv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nmod_auto = Pipeline([(\"poly\", PolynomialFeatures()), (\"lm\", LinearRegression())])\n\n# Grid Search\nparam_grid = {\"poly__degree\": np.arange(1, 11)}  # \"지정한 estimator의 이름\"__\"parameter 이름\"\ngrid_search = GridSearchCV(mod_auto, param_grid, cv=cv, scoring='neg_mean_squared_error')\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('poly', PolynomialFeatures()),\n                                       ('lm', LinearRegression())]),\n             param_grid={'poly__degree': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('poly', PolynomialFeatures()),\n                                       ('lm', LinearRegression())]),\n             param_grid={'poly__degree': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n             scoring='neg_mean_squared_error') estimator: PipelinePipeline(steps=[('poly', PolynomialFeatures()), ('lm', LinearRegression())])  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures()  LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# Best parameter!\ngrid_search.best_params_\n\n{'poly__degree': 10}\n\n\n\n# Best estimator!\ngrid_search.best_estimator_\n\nPipeline(steps=[('poly', PolynomialFeatures(degree=10)),\n                ('lm', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('poly', PolynomialFeatures(degree=10)),\n                ('lm', LinearRegression())])  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=10)  LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\ngrid_search.cv_results_에 교차검증의 여러 지표들이 포함되어 있음\n\ngrid_search.cv_results_[\"mean_test_score\"]\n\narray([-24.82, -20.29, -20.37, -20.67, -20.2 , -20.16, -20.4 , -20.49,\n       -20.28, -19.94])\n\n\n\ngrid_search.cv_results_[\"param_poly__degree\"].data\n\narray([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=object)\n\n\n\npd.options.display.max_rows = 10\npd.DataFrame(\n    {\n        \"degree\": grid_search.cv_results_[\"param_poly__degree\"].data,\n        \"mean_test_score\": -grid_search.cv_results_[\"mean_test_score\"],\n    }\n).sort_values(\"mean_test_score\")\n\n  degree  mean_test_score\n9     10            19.94\n5      6            20.16\n4      5            20.20\n8      9            20.28\n1      2            20.29\n2      3            20.37\n6      7            20.40\n7      8            20.49\n3      4            20.67\n0      1            24.82\n\n\n2차 다항함수로 충분하다고 판단하고, 전체 training set으로 다시 fitting한 후, test set으로 성능을 평가\n\nmod_auto_best = Pipeline([(\"poly\", PolynomialFeatures(2)), (\"lm\", LinearRegression())])\nmod_auto_best.fit(X_train, y_train)\n\nPipeline(steps=[('poly', PolynomialFeatures()), ('lm', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('poly', PolynomialFeatures()), ('lm', LinearRegression())])  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures()  LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# get root_mean_squared_error and median_absolute_error\nfrom sklearn.metrics import root_mean_squared_error, median_absolute_error\n\ny_pred = mod_auto_best.predict(X_test)\n\nrmse = root_mean_squared_error(y_test, y_pred)\nmae = median_absolute_error(y_test, y_pred)\n\nprint(f\"Root Mean Squared Error: {rmse:.3f}\")\nprint(f\"Median Absolute Error: {mae:.3f}\")\n\nRoot Mean Squared Error: 4.002\nMedian Absolute Error: 2.486\n\n\n\n\n\nThe Bootstrap\n주어진 데이터셋으로부터 같은 사이즈의 데이터셋들을 중복을 허용해서 추출.\n마치 새로운 표본들을 얻는 것과 같은 효과를 얻어, 표본들마다 모형이 어떻게 다르게 추정되는지를 파악함.\nSales of Child Car Seats(Carseats) 데이터셋으로 예를 들면,\n\nfrom ISLP import load_data\n\ncarseats = load_data(\"Carseats\")\ncarseats.head(3)\n\n    Sales  CompPrice  Income  Advertising  Population  Price ShelveLoc  Age  \\\n0  9.5000        138      73           11         276    120       Bad   42   \n1 11.2200        111      48           16         260     83      Good   65   \n2 10.0600        113      35           10         269     80    Medium   59   \n\n   Education Urban   US  \n0         17   Yes  Yes  \n1         10   Yes  Yes  \n2         12   Yes  Yes  \n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nX = carseats[[\"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\"]]\ny = carseats[\"Sales\"]\n\n# linear regression\nlm_carseats = LinearRegression()\nlm_carseats.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nparams = pd.DataFrame(lm_carseats.coef_, index=X.columns, columns=[\"params\"])\nparams\n\n             params\nIncome       0.0107\nAdvertising  0.1254\nPopulation  -0.0009\nPrice       -0.0574\nAge         -0.0490\n\n\n\nfrom sklearn.utils import resample\n\nnp.random.seed(0)\nparams_boot = params.copy()\n\nfor i in range(1000):\n    X_new, y_new = resample(X, y, replace=True)  # resampling with replacement\n    lm_carseats.fit(X_new, y_new)  # refit\n    \n    params_new = pd.DataFrame(lm_carseats.coef_, index=X.columns, columns=[f\"sample_{i}\"])\n    params_boot = pd.concat([params_boot, params_new], axis=1)\n\n\nparams_boot\n\n             params  sample_0  sample_1  sample_2  sample_3  ...  sample_995  \\\nIncome       0.0107    0.0037    0.0175    0.0126    0.0100  ...      0.0139   \nAdvertising  0.1254    0.1439    0.1170    0.1158    0.1232  ...      0.1392   \nPopulation  -0.0009    0.0001   -0.0019   -0.0007   -0.0012  ...      0.0008   \nPrice       -0.0574   -0.0630   -0.0462   -0.0552   -0.0595  ...     -0.0630   \nAge         -0.0490   -0.0595   -0.0404   -0.0574   -0.0465  ...     -0.0478   \n\n             sample_996  sample_997  sample_998  sample_999  \nIncome           0.0113      0.0082      0.0132      0.0063  \nAdvertising      0.0914      0.0904      0.1221      0.1253  \nPopulation      -0.0016     -0.0002     -0.0018     -0.0002  \nPrice           -0.0558     -0.0518     -0.0512     -0.0469  \nAge             -0.0486     -0.0535     -0.0394     -0.0375  \n\n[5 rows x 1001 columns]\n\n\n1000개 표본에서 파라미터 추정치의 표준편차; standard error\n\nparams_boot.std(axis=1)\n\nIncome        0.0038\nAdvertising   0.0174\nPopulation    0.0008\nPrice         0.0050\nAge           0.0069\ndtype: float64\n\n\n실제 파라미터 추정치의 분포를 보면,\nparams_boot.T.hist(bins=30, alpha=.7, figsize=(7, 3), layout=(2, 3))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n이 분포로부터 파라이터 추정치의 confidence interval (신뢰구간)을 구할 수 있음; 여러 변형이 존재함.\n보통 95% 신뢰구간을 사용\n\n\n\n\n\n\nShort version\n\n\n\n\n\nnp.random.seed(0)\n\nparams = pd.Series(lm_carseats.coef_, index=X.columns)\nerr = np.std([lm_carseats.fit(*resample(X, y)).coef_ for i in range(1000)], axis=0)\n\npd.DataFrame({'coef': params, 'error': err})\n\n#                coef  error\n# Income       0.0063 0.0038\n# Advertising  0.1253 0.0174\n# Population  -0.0002 0.0008\n# Price       -0.0469 0.0050\n# Age         -0.0375 0.0069\n\n\n\n선형회귀에서 OLS estimation 결과와 비교하면,\n\nfrom statsmodels.formula.api import ols\n\nlm_carseats2 = ols(\"Sales ~ Income + Advertising + Population + Price + Age\", data=carseats).fit()\n\nprint(lm_carseats2.summary(slim=True))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.373\nModel:                            OLS   Adj. R-squared:                  0.365\nNo. Observations:                 400   F-statistic:                     46.82\nCovariance Type:            nonrobust   Prob (F-statistic):           6.07e-38\n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      15.4227      0.808     19.098      0.000      13.835      17.010\nIncome          0.0107      0.004      2.635      0.009       0.003       0.019\nAdvertising     0.1254      0.018      7.112      0.000       0.091       0.160\nPopulation     -0.0009      0.001     -1.086      0.278      -0.002       0.001\nPrice          -0.0574      0.005    -11.962      0.000      -0.067      -0.048\nAge            -0.0490      0.007     -7.000      0.000      -0.063      -0.035\n===============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.37e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/lda.html",
    "href": "contents/lda.html",
    "title": "Generative Models",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 5, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nLogistic Regression에서는 클래스 \\(C_k\\)에 속할 확률을 다음과 같이 선형모델로 직접 얻었음.\n\\(E(Y=1|X=x)=\\)   또는\n\\(P(Y = C_1|X=x) = \\sigma (\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p)\\)   where   \\(\\displaystyle\\sigma(z) = \\frac{1}{1+e^{-z}}\\):   sigmoid/logistic function\n몇 가지 개선될 부분들",
    "crumbs": [
      "Home",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#generative-classifier",
    "href": "contents/lda.html#generative-classifier",
    "title": "Generative Models",
    "section": "Generative Classifier",
    "text": "Generative Classifier\n관찰된 값 \\(x\\)에 대해, \\(Y\\)의 conditional probability인 \\(P(Y = C_k|X=x)\\)을 추정하는데, 각 클래스 별로 \\(X\\)의 분포를 이용해 역으로 \\(Y\\)의 분포를 계산하는 방식; Bayes theorem을 이용\n\\(\\displaystyle P(Y = C_k|X=x) = \\frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x)}\\)\n\n\\(P(Y = C_k|X=x)\\): 관측치 \\(x\\)가 클래스 \\(C_k\\)에 속할 확률; posterior probability\n\\(P(Y=C_k)\\): 임의로 선택된 관측치가 클래스 \\(C_k\\)에 속할 확률; prior probability\n\\(P(X=x|Y=C_k)\\): 클래스 \\(C_k\\)에 속하는 \\(x\\)의 probability density &gt;&gt; “generative”\n\n주어진 \\(x\\)에 대해, 각 클래스 \\(C_k\\)에 속할 확률 \\(P(Y=C_k|X=x)\\)를 계산하여, 이 중 가장 높은 확률을 가지는 클래스를 선택하는 방식으로 분류; Bayes classifier\n\n\n\n\n\n\nNote\n\n\n\nposterior probability는 prior probability의 update된 확률로 볼 수 있음.\n예를 들어, 피부암 진단을 피부 이미지의 명암(\\(X\\):0-255)으로 예측한다고 하면,\n\\(P(Y=cancer|X=240) \\propto P(X=240|Y=cancer) * P(Y=cancer)\\)\n“이미지의 명암이 240일 때 피부암일 확률” \\(\\propto\\) “피부암일 때, 그 이미지의 명암이 240일 확률” x “이미지들 중 피부암일 확률”\n이미지 검사 후의 “사후 확률”을 이미지 검사 전의 “사전 확률”의 update로 볼 수 있음.\n\\(P(Y=cancer|X=240)\\) &gt; \\(P(Y=not~cancer|X=240)\\) 이면, 이미지를 피부암으로 분류.\n\n\n분모: \\(\\displaystyle P(X=x) = P(X=x|Y=C_1)P(Y=C_1) + P(X=x|Y=C_2)P(Y=C_2)\\)\n따라서, \\(\\displaystyle P(Y = C_k|X=x) = \\frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x|Y=C_1)P(Y=C_1) + P(X=x|Y=C_2)P(Y=C_2)}\\)\n\nPrior \\(P(Y=C_k)\\)의 추정치는 각 클래스에 속하는 표본의 비율로 추정\n어려운 문제는 probability density function \\(f_k(x)=P(X=x|Y=C_k)\\)의 추정!\n\n단순화하기 위해 이 분포에 대한 가정을 부과\nLinear Discriminant Analysis (LDA): (Multivariate) Gaussian 분포, 모든 클래스에 대해 covariance matrix(공분산 행렬) 동일\nQuadratic Discriminant Analysis (QDA): (Multivariate) Gaussian 분포, 각 클래스 별로 고유한 covariance matrix(공분산 행렬)\n\n\nPalmer Penguins 데이셋의 예에서,\n1-dimensional (1 predictor)\nbill_length로 펭균 종을 구분하는 경우\n\n\n\n\n\n\n\n2-dimensional (2 predictors)\nbill_length과 bill_depth로 펭균 종을 구분하는 경우",
    "crumbs": [
      "Home",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#linear-discriminant-analysis-lda",
    "href": "contents/lda.html#linear-discriminant-analysis-lda",
    "title": "Generative Models",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\n(Multivariate) Gaussian 분포 가정\n모든 클래스에 대해 covariance matrix(공분산 행렬) 동일\n\n\n1-dimensional: p = 1\n\\(\\displaystyle f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x-\\mu_k)^2}{2\\sigma^2}\\right)\\)\n\n클래스 각각에 대해서 분포의 평균 \\(\\mu_k\\)를 추정: MLE로 추정하면 클래스별 평균값\n\\(\\sigma\\)는 모든 클래스에 대해 동일하다고 가정하고 추정: MLE로 추정하면 클래스별 분산값의 weighted 평균\nMLE(Maximum Likelihood Estimation)이 이상치(outliers)에 민감함을 주의\n\n\n\n\n\n\n\n\n\\(\\displaystyle P(Y = C_k|X=x) = \\frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x)}\\) \\(\\displaystyle \\qquad\\qquad\\qquad\\qquad= \\frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x|Y=C_1)P(Y=C_1) + P(X=x|Y=C_2)P(Y=C_2) + P(X=x|Y=C_3)P(Y=C_3)}\\)\n\\(\\displaystyle \\qquad\\qquad\\qquad\\qquad= \\frac{f_k(x)P(Y=C_k)}{f_1(x)P(Y=C_1) + f_2(x)P(Y=C_2) + f_3(x)P(Y=C_3)}\\)   (A)\n이제 \\(P(Y=C_k)\\)만 추정하면 되는데,\n즉, 임의로 선택된 관측치가 클래스 \\(C_k\\)에 속할 확률 추정값: \\(\\displaystyle \\hat P(Y=C_k) = \\frac{n_k}{n}\\)   where   \\(n_k\\): 클래스 \\(C_k\\)에 속하는 표본의 수, \\(n\\): 전체 표본의 수\n예측값을 얻기 위한 준비 완료!\n\\(X=x\\)에 대한 예측값은 다음 세 확률 중 가장 큰 값에 해당하는 클래스에 할당; the Bayes classifier\n\n\\(P(Y = C_1|X=x)\\)\n\\(P(Y = C_2|X=x)\\)\n\\(P(Y = C_3|X=x)\\)\n\n식 A에서 분모는 모두 같기 때문에 분자만 비교하면 됨. 즉, \\(\\displaystyle f_k(x)P(Y=C_k)\\)를 비교하면 됨.\n로그를 취해 정리하면,\n\\(\\displaystyle \\delta_k(x) = \\frac{\\mu_k}{\\sigma^2}x - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(P(Y=C_k))\\)   : discriminant function\n추정치로 바꾸면, \\(\\displaystyle \\hat\\delta_k(x) = \\frac{\\hat\\mu_k}{\\hat\\sigma^2}x - \\frac{\\hat\\mu_k^2}{2\\hat\\sigma^2} + \\log(\\frac{n_k}{n})\\)   : \\(x\\)에 대한 일차함수 형태\n\n\n\n\n\n\nLogistic regression에서 log-odds의 형태로 선형모형을 세운 이유로 볼 수 있음.\n\\(\\displaystyle log~odds = log\\left(\\frac{p}{1-p}\\right) = log\\left(\\frac{P(Y=1|X=x)}{P(Y=0|X=x)}\\right) = \\beta_{0} + \\beta_{1}x\\)\n\n\n\n\\(\\mu\\), \\(\\sigma\\), prior의 추정치와 몇 개의 \\(X\\)값에 대한 예측 확률을 구해보면,\n\n\n\n             mu  sigma  prior\nAdelie    38.79   2.95   0.44\nChinstrap 48.83   2.95   0.20\nGentoo    47.50   2.95   0.36\n\n\nBill Length(mm)   35   42  43.35   45   50  52.05   55\nSpecies                                               \nAdelie          1.00 0.76   0.44 0.13 0.00   0.00 0.00\nChinstrap       0.00 0.04   0.12 0.22 0.42   0.50 0.61\nGentoo          0.00 0.20   0.44 0.65 0.58   0.50 0.39\n\n\n\n예를 들어, 어떤 펭균의 부리의 길이를 관찰하기 전, 그 펭균이 Adelie 펭귄일 확률은 0.44(prior)인데,\n\n만약, 부리의 길이가 45mm임을 관찰하면, 그 펭균이 Adelie 펭귄일 확률은 0.13(posterior)로 update됨.\n\n\\(\\displaystyle P(Adelie|X=45) = \\frac{f_{Adelie}(X=45)*0.44}{normalize~factor} = 0.13\\)\n\n혹은 부리의 길이가 50mm임을 관찰하면, 그 펭균이 Adelie 펭귄일 확률은 0.001(posterior)로 update됨.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n각 클래스에 속할 확률을 구하면, 이 확률을 이용해 threshold를 정해 분류할 수 있음.\n\n예를 들어, 적어도 .70의 확률/확신이 있을 때만 분류를 실행하고,\n그렇지 않다면 분류를 보류할 수 있음.\n이는 분류의 오류를 낮추는 방법 중 하나; reject option\n\n반면, 분류만이 목적이라면, threshold를 정하지 않고, 가장 높은 확률을 가지는 클래스로 분류하면 되고,\n간단히 discriminant function을 이용해 decision boundary만 찾는 것이 효율적.\nDecision boundaries: \\(\\delta_k(x)\\)가 최대가 되는 클래스가 변하는 \\(x\\)들의 위치\n\n\\(\\delta_1(x_0) = \\delta_3(x_0)\\):   \\(x_0 = 43.35\\)\n\n\\(\\delta_2(x_1) = \\delta_3(x_1)\\):   \\(x_1 = 52.05\\)\n\n\n\n\n\n\n\n\n\n\n\n\n2-dimensional: \\(\\mathbf{x_1}, \\mathbf{x_2}\\)\nMultivariate Gaussian 분포: \\(\\displaystyle f_k(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mu_k)^T\\Sigma^{-1}(\\mathbf{x}-\\mu_k)\\right)\\)\n\n\\(\\displaystyle \\mathbf{x}= \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\)\n\\(\\mu_k\\): 클래스 \\(C_k\\)에 대한 \\(X_1\\)과 \\(X_2\\)의 평균 벡터: \\(\\displaystyle \\begin{bmatrix} \\mu_{k1} \\\\ \\mu_{k2} \\end{bmatrix}\\)\n\\(\\Sigma\\): 클래스 \\(C_k\\)에 대한 공분산 행렬(covariance matrix): \\(\\displaystyle \\begin{bmatrix} \\sigma_{k_1}^2 & \\sigma_{k_1k_2} \\\\ \\sigma_{k_2k_1} & \\sigma_{k_2}^2 \\end{bmatrix}\\)\n\n모든 클래스 \\(C_k\\)에 대해 동일하다고 가정\n\nMLE(Maximum Likelihood Estimation)로 추정\n\n\n\nSource: p. 150, Introduction to Statistical Learning with Applications in Python by G. James, D. Witten, T. Hastie, R. Tibshirani\n\n\n\n\n\n\n\n\n\n\n\nDiscrimint function: 다음이 가장 큰 값을 가지는 클래스에 할당\n\\(\\displaystyle\\delta_k(\\mathbf{x}) = \\mathbf{x}^T\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\log(P(Y=C_k))\\):   \\(ax_1 + bx_2 + c\\) 형태\n    \\(\\mu_k\\)                        covariance matrix\n\n\n\n             Adelie  Chinstrap  Gentoo\nbill_length   38.79      48.83   47.50\nbill_depth    18.35      18.42   14.98\n\n\n \n\n\n         sigma_1  sigma_2\nsigma_1     8.68     1.74\nsigma_2     1.74     1.25\n\n\n\n\n\n       Adelie  Chinstrap  Gentoo\nprior    0.44       0.20    0.36\n\n\n예를 들어, \\(\\delta_1(x_1, x_2) = 2.1x_1 + 11.8x_2 - 149.9\\)\nDecision boundaries: \\(\\delta_k(\\mathbf{x})\\)가 최대가 되는 클래스가 변하는 \\(\\mathbf{x}\\)들의 위치.\n이는 각 클래스에 속한 데이터의 중심으로부터 멀리 떨어지도록 하는 방향으로 결정됨.\n\n\\(\\delta_1(\\mathbf{x}) = \\delta_2(\\mathbf{x})\\):   red line: Adelie vs. Chinstrap\n\\(\\delta_2(\\mathbf{x}) = \\delta_3(\\mathbf{x})\\):   purple line: Chinstrap vs. Gentoo\n\\(\\delta_3(\\mathbf{x}) = \\delta_1(\\mathbf{x})\\):   green line: Gentoo vs. Adelie\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegularization\n\n\n\nRegularization의 원리를 마찬가지로 적용하여, 파라미터 추정치를 shrink하여 overfitting을 방지할 수 있음.\n단, 이 경우 covarance matrix에 대해 shrinkage가 적용됨; scikit-learn 문서 참고\n\n\n\n\n\n\n\n\nLDA as a dimension reduction method\n\n\n\nLDA는 다음과 같이 차원 축소 방법으로도 사용될 수 있음; 클래스 간 분산을 최대화하고, 클래스 내 분산을 최소화하는 방향으로 차원 축소\n\n\nSource: p. 116, The Elements of Statistical Learning (2e) by T. Hastie, R. Tibshirani, J. Friedman",
    "crumbs": [
      "Home",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#quadratic-discriminant-analysis-qda",
    "href": "contents/lda.html#quadratic-discriminant-analysis-qda",
    "title": "Generative Models",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\n\n(Multivariate) Gaussian 분포 가정\n각 클래스 별로 고유한 variances, covariances를 가정: covariance matrix(공분산 행렬)\n\n\\(\\displaystyle \\delta_k(\\mathbf{x}) = - \\frac{1}{2}(\\mathbf{x}-\\mu_k)^T\\Sigma_k^{-1}(\\mathbf{x}-\\mu_k) -\\frac{1}{2}\\log|\\Sigma_k| + \\log(P(Y=C_k))\\)\n\\(\\displaystyle \\qquad= - \\frac{1}{2}\\mathbf{x}^T\\Sigma_k^{-1}\\mathbf{x} + \\mathbf{x}^T\\Sigma_k^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma_k^{-1}\\mu_k -\\frac{1}{2}\\log|\\Sigma_k| + \\log(P(Y=C_k))\\)\n\\(\\displaystyle \\qquad= (ax_1^2 + bx_1x_2 + cx_2^2) + (dx_1 + ex_2) + d\\)   : 모든 2차항들을 포함 (quadratic)\n이번에는 bill_length_mm와 flipper_length_mm를 예측변수로 사용하여 펭균 종을 구분하면,\n\n\n\n\n\n\n\n\n\n\n    \\(\\mu_k\\)\n\n\n\n                   Adelie  Chinstrap  Gentoo\nbill_length         38.79      48.83   47.50\nflipper_length_mm  189.95     195.82  217.19\n\n\n \n\n\n\n   Covariance matrices for each class\n\n\n\nAdelie   sigma_1  sigma_2\nsigma_1     7.09     5.67\nsigma_2     5.67    42.76\n\n\n \n\n\nChinstrap  sigma_1  sigma_2\nsigma_1      11.15    11.23\nsigma_2      11.23    50.86\n\n\n \n\n\nGentoo   sigma_1  sigma_2\nsigma_1     9.50    13.21\nsigma_2    13.21    42.05\n\n\n\n\n\n       Adelie  Chinstrap  Gentoo\nprior    0.44       0.20    0.36\n\n\n\n\n\n\n\n\n\n\n\n\nQDA vs. LDA\n\n파라미터의 수: \\(K*p\\) vs. \\(K*p(p+1)/2\\) 추가\nLDA가 훨씬 덜 flexible classifier, 따라서 lower variance\n\n데이터가 적어 variance가 문제가 된다면 LDA를 선택하는 것이 좋을 수 있음.\n\n만약, common covariance의 가정에서 많이 벗어난다면 LDA는 심각한 bias를 가질 수 있음.\n\n \n\nSource: p. 157, Introduction to Statistical Learning with Applications in Python by G. James, D. Witten, T. Hastie, R. Tibshirani",
    "crumbs": [
      "Home",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#naive-bayes",
    "href": "contents/lda.html#naive-bayes",
    "title": "Generative Models",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\n각 클래스 내에서 예측변수들이 독립적이라고 가정: 서로 연관관계가 없다고 가정\n각 클래스 별로 고유한 분포를 가정: QDA와 유사\n\n\\(\\displaystyle P(Y = C_k|X=x) = \\frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x)}\\)\np = 2인 경우의 예에서,\n\\(P(X=(x_1, x_2)|Y=C_k) = P(X_1=x_1| Y=C_k) *  P(X_2=x_2 | Y=C_k)\\):   즉, 각 예측변수 내에서의 분포만 고려 (marginal distribution)\n따라서, \\(\\displaystyle P(Y = C_k|X=x) = \\frac{f_{k1}(x_1) * f_{k2}(x_2) * P(Y=C_k)}{P(X=x)}\\)\n\n\n\n\n\n\n\n\n\n\n\n앞서 covariance matrix로 joint distribution을 추정했으나, 실질적인 joint distribution을 추정하는 것은 매우 어려움\n비현실적이지만, 예측변수들이 서로 독립이라고 가정하면, 분포를 추정하는 것이 매우 간단해지며\n속도가 매우 빠르고 조정 가능한 매개변수가 거의 없기 때문에 분류 문제에 대한 빠르고 간단한 기준 제공\n특히, 예측 변수(p) 대비 표본 수(n)가 적어 joint distribution을 추정하기 어려운 경우에 유용\n이는 bias가 높아질 수 있지만, variance가 낮아지기 때문에 그 이점이 있음.\n\n각 클래별로 (1-d density function) \\(f_{k}\\)를 추정하는데 여러 옵션을 사용할 수 있음.\n\n\n\n\n앞서 LDA에서 사용한 Gaussian 분포; Gaussian Naive Bayes\n적절한 binning을 통한 histogram, 또는 (smoothed) kernel density estimation\n카테고리 변수인 경우, 각 카테고리 값의 비율을 이용; Categorical Naive Bayes\n혼합되어 있는 경우; 각각 따로 확률을 구해 곱 Mixed Naive Bayes\n\n예를 들어,\n\nSource: p. 160, Introduction to Statistical Learning with Applications in Python by G. James, D. Witten, T. Hastie, R. Tibshirani\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n비슷하게, 독립성의 가정을 기반으로 예측변수들이 binominal/multinomial(이항/다항) distribution을 따르는 경우,\nMultinomial Naive Bayes로 불림\n텍스트 분류에서 사용; 스팸메일 분류 포함\n\n\n펭귄의 예로, Gaussian 분포를 사용하면",
    "crumbs": [
      "Home",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#logistic-regression과의-관계",
    "href": "contents/lda.html#logistic-regression과의-관계",
    "title": "Generative Models",
    "section": "Logistic regression과의 관계",
    "text": "Logistic regression과의 관계\n사후 확률(posterior)의 log odds의 관점에서 보면, logistic regression은\n\\(\\displaystyle log\\left(\\frac{P(Y=1|X=x)}{P(Y=0|X=x)}\\right) = \\beta_{0} + \\beta_{1}X_1 + \\cdots + \\beta_{p}X_p\\)\nLDA에서 동일하게 posterior의 log odds를 살펴보면,\n\\(\\displaystyle log\\left(\\frac{P(Y=C_k|X=x)}{P(Y=C_K|X=x)}\\right) = \\log\\left(\\frac{f_k(x)P(Y=C_k)}{f_K(x)P(Y=C_K)}\\right) = \\log {f_k(x)P(Y=C_k)} + \\log {f_K(x)P(Y=C_K)}\\)\n\n\n\n\n\n\nNote\n\n\n\n일반적으로 K개의 클래스가 있을 때, 기준이 되는 클래스(K) 대비 특정 클래스(k)의 posterior의 log odds를 생각하면 확장될 수 있음.\n\\(\\displaystyle log\\left(\\frac{P(Y=C_k|X=x)}{P(Y=C_K|X=x)}\\right) = \\beta_{0}^k + \\beta_{1}^k X_1 + \\cdots + \\beta_{p}^k X_p\\)",
    "crumbs": [
      "Home",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#generative-vs.-discriminative-models",
    "href": "contents/lda.html#generative-vs.-discriminative-models",
    "title": "Generative Models",
    "section": "Generative vs. Discriminative Models",
    "text": "Generative vs. Discriminative Models\nDiscriminative models: \\(P(C_k|X)\\)를 직접 추정\n\n즉, \\(P(C_k|\\mathbf{x}) = f(\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p)=f(\\mathbf{w}^T \\mathbf{x} + w_0)\\)\n\\(f\\)를 activation function이라고 부르며, 역함수를 link function이라고 부름\n\\(f\\)가 sigmoid function (\\(f^{-1}\\): logit function) 일 때 logistic regression\n이는 \\(P(\\mathbf{x}|C_k)\\)를 추정하지 않고, 직접 \\(p\\)개의 파라미터만 추정하면 됨\n또한, generative model에서 \\(P(X|C_k)\\)를 추정하는데 매우 많이 데이터와 계산이 필요한데, 종종 decision boundary를 결정하기 위한 posterior \\(P(C_k|\\mathbf{x})\\) 얻는데 \\(P(\\mathbf{x}|C_k)\\)의 정보가 다 필요하지 않을 수 있음. (아래 그림)\n이는 discriminative models이 decision boundary를 결정하는데 더 효과적일 수 있음\n또한, generative model에서 추정하는 class-conditional density, \\(f_k(\\mathbf{x})\\)가 실제 분포와 일치하지 않는다면 discriminative model이 더 정확할 수 있음\n\n   \n\n    Source: p. 145, Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H\n\n\ngenerative model의 경우 \\(P(X)\\)를 이용해 새로운 데이터에 대해 예측할 때, 그 데이터의 발생 확률을 얻을 수 있어서 이를 예측의 정확성에 대한 보완 정보로 활용할 수 있음.\n\nAnomaly detection에도 활용\n\n\n\n확률적 모형의 장점들\n\n특정 클래스에 속할 확률에 대한 정보를 얻을 수 있으며,\n이 확률과 loss function을 결합하여, 기대값(expected loss)를 계산하여 최적의 분류를 결정할 수 있음.\n\n주어진 \\(\\mathbf{x}\\)를 \\(\\displaystyle\\sum_{k} L_{kj}P(C_k|\\mathbf{x})\\)이 최소가 되는 클래스에 분류\n\n어느 클래스에도 확실히 속하지 않는 경우, 분류의 결정을 보류할 수 있음; reject option\n\n  \n\n    Source: p. 143, Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H\n\n\n매우 드물게 발생되는 클래스가 존재하는 경우, balanced dataset에서 학습한 후 prior를 조정\n\nposterior \\(\\sim f_k\\), prior\n\n상이한 예측변수들에 대해 독립적으로 모형을 만든 후 결합할 수 있음\n\n예를 들어, 피부암의 진단에서 사진 판독 & 혈액검사 결과를 독립적으로 모형을 만든 후 결합\n두 모형에서 사용된 예측변수들은 서로 독립이라고 가정하면\n\n\\(P(\\mathbf{x_A, x_B}|C_k) = P(\\mathbf{x_A}|C_k)P(\\mathbf{x_B}|C_k)\\)\n\n\n\n\n\n\n\n\n\n한편, 확률적 모델을 사용하지 않고 discriminant function을 구해 class label을 직접 예측하는 방식도 있음\n\ndecision tree\nsupport vector machine",
    "crumbs": [
      "Home",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#python-implementation",
    "href": "contents/lda.html#python-implementation",
    "title": "Generative Models",
    "section": "Python Implementation",
    "text": "Python Implementation\n\nfrom sklearn import datasets\n\nwine = datasets.load_wine(as_frame=True)\nwine_df = wine['frame']\nwine_df['target'] = wine_df['target'].map({0: 'C0', 1: 'C1', 2: 'C2'})\nwine_df.head(3)\n\n   alcohol  malic_acid  ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0    14.23        1.71 2.43              15.60     127.00           2.80   \n1    13.20        1.78 2.14              11.20     100.00           2.65   \n2    13.16        2.36 2.67              18.60     101.00           2.80   \n\n   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity  hue  \\\n0        3.06                  0.28             2.29             5.64 1.04   \n1        2.76                  0.26             1.28             4.38 1.05   \n2        3.24                  0.30             2.81             5.68 1.03   \n\n   od280/od315_of_diluted_wines  proline target  \n0                          3.92  1065.00     C0  \n1                          3.40  1050.00     C0  \n2                          3.17  1185.00     C0  \n\n\nDecision boundary를 시각화하기 위해, 2차원 예측변수를 사용하여 LDA, QDA, Naive Bayes를 적용\n\nfrom sklearn.model_selection import train_test_split\n\nX = wine_df[['color_intensity', 'alcohol']]\ny = wine_df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nlda = LDA(store_covariance=True).fit(X_train, y_train)\n\n\n\n\n\n\n\nShrinkage\n\n\n\nCovariance matrix에 shrinkage를 적용하려면 가령,\nsklearn 문서 참고\nLDA(store_covariance=True, shrinkage=\"auto\", solver=\"lsqr\")\n\n\n각 클래스별 평균, 공분산 행렬 추정\n\nMaximum Likelihood Estimation (MLE) 사용\n과적합을 방지하기 위해 shrinkage를 사용한 추정치도 사용 가능; sklearn 문서 참조\n\n\n# 각 클래스의 분포 평균 추정치\ndf_mean = pd.DataFrame(lda.means_.T, index=X.columns, columns=lda.classes_)\ndf_mean\n\n                   C0    C1    C2\ncolor_intensity  5.28  3.12  7.14\nalcohol         13.62 12.31 13.16\n\n\n\n# 각 클래스의 공분산 행렬 추정치\ndf_cov = pd.DataFrame(lda.covariance_, columns=[\"sigma_1\", \"sigma_2\"], index=pd.Index([\"sigma_1\", \"sigma_2\"]))\ndf_cov\n\n         sigma_1  sigma_2\nsigma_1     2.56     0.29\nsigma_2     0.29     0.29\n\n\n\n# 각 클래스의 사전 확률 추정치\ndf_priors = pd.DataFrame(lda.priors_, index=lda.classes_, columns=[\"prior\"])\ndf_priors\n\n    prior\nC0   0.29\nC1   0.42\nC2   0.29\n\n\n다음은 decision boundary를 시각화하기 위한 코드\n\n# grid data\nx1 = np.linspace(X_train['color_intensity'].min(), X_train['color_intensity'].max(), 100)\nx2 = np.linspace(X_train['alcohol'].min(), X_train['alcohol'].max(), 100)\n\nfrom itertools import product\nX_grid = pd.DataFrame(\n    list(product(x1, x2)),\n    columns=[\"color_intensity\", \"alcohol\"],\n)\ny_grid = lda.predict(X_grid)\nX_grid[\"target\"] = y_grid\n\nplt.figure(figsize=(6, 5))\nsns.scatterplot(X_grid, x=\"color_intensity\", y=\"alcohol\", hue=\"target\", s=10, alpha=.3, palette=[\"0\", \".3\", \".6\"], legend=False)\nsns.scatterplot(x=X_test[\"color_intensity\"], y=X_test[\"alcohol\"], hue=y_test)\n\nplt.xlabel('color intensity')\nplt.ylabel('alcohol')\nplt.title(\"LDA Decision Boundary\")\n\nplt.show()\n\n\n\n\n\n\n\n각 클래스별 예측된 확률과 예측된 클래스 (데이터프레임으로 표시)\n\npred_class = pd.DataFrame(lda.predict(X_test), columns=[\"pred\"])\npred_prob = pd.DataFrame(lda.predict_proba(X_test), columns=lda.classes_)\n\nresults_df = pd.concat([X_test.reset_index(drop=True), pred_class, pred_prob], axis=1)\nresults_df\n\n    color_intensity  alcohol pred   C0   C1   C2\n0              5.88    13.69   C0 0.72 0.03 0.25\n1              2.06    12.42   C1 0.03 0.96 0.01\n2              5.10    13.64   C0 0.80 0.05 0.14\n..              ...      ...  ...  ...  ...  ...\n69             4.60    13.28   C0 0.62 0.22 0.17\n70             9.90    12.77   C2 0.01 0.00 0.99\n71            10.68    13.45   C2 0.01 0.00 0.99\n\n[72 rows x 6 columns]\n\n\nConfusion matrix\n\nfrom ISLP import confusion_table\nconfusion_table(lda.predict(X_test), y_test)\n\nTruth      C0  C1  C2\nPredicted            \nC0         26   1   3\nC1          0  26   2\nC2          2   0  12\n\n\nClassification report: precision, recall은 각 클래스에 대한 나머지 클래스들에 대한 관계를 의미\n\nfrom sklearn.metrics import classification_report\ny_pred = lda.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n          C0       0.87      0.93      0.90        28\n          C1       0.93      0.96      0.95        27\n          C2       0.86      0.71      0.77        17\n\n    accuracy                           0.89        72\n   macro avg       0.88      0.87      0.87        72\nweighted avg       0.89      0.89      0.89        72\n\n\n\nQDA와 Naive Bayes도 동일한 방식으로 사용 가능\n각각, 다음과 같이 QuadraticDiscriminantAnalysis와 GaussianNB를 사용해 estimator를 생성\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.naive_bayes import GaussianNB\n\nqda = QDA(store_covariance=True).fit(X_train, y_train)\nnb = GaussianNB().fit(X_train, y_train)",
    "crumbs": [
      "Home",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/inspection.html",
    "href": "contents/inspection.html",
    "title": "Inspecting data",
    "section": "",
    "text": "# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Home",
      "Python Basics",
      "Data Inspection"
    ]
  },
  {
    "objectID": "contents/inspection.html#useful-method",
    "href": "contents/inspection.html#useful-method",
    "title": "Inspecting data",
    "section": "Useful method",
    "text": "Useful method\n.head(), .tail(), .sample()\n.info(), .describe(),\n.value_counts(),\n.sort_values(), .nlargest(), .nsmallest()\nData: Tips\n일정기간 한 웨이터가 얻은 팁에 대한 데이터\n\n# load a dataset\ntips = sns.load_dataset(\"tips\")\ntips\n\n     total_bill  tip     sex smoker   day    time  size\n0         16.99 1.01  Female     No   Sun  Dinner     2\n1         10.34 1.66    Male     No   Sun  Dinner     3\n2         21.01 3.50    Male     No   Sun  Dinner     3\n..          ...  ...     ...    ...   ...     ...   ...\n241       22.67 2.00    Male    Yes   Sat  Dinner     2\n242       17.82 1.75    Male     No   Sat  Dinner     2\n243       18.78 3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]\n\n\n\ntips.head(3)  # 앞 n개 나열, 기본값은 5\n\n   total_bill  tip     sex smoker  day    time  size\n0       16.99 1.01  Female     No  Sun  Dinner     2\n1       10.34 1.66    Male     No  Sun  Dinner     3\n2       21.01 3.50    Male     No  Sun  Dinner     3\n\n\n\ntips.sample(5)  # 무작위로 n개 표본 추출, 기본값은 1\n\n     total_bill  tip     sex smoker   day    time  size\n129       22.82 2.18    Male     No  Thur   Lunch     3\n30         9.55 1.45    Male     No   Sat  Dinner     2\n234       15.53 3.00    Male    Yes   Sat  Dinner     2\n215       12.90 1.10  Female    Yes   Sat  Dinner     2\n146       18.64 1.36  Female     No  Thur   Lunch     3\n\n\n\ntips.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  244 non-null    float64 \n 1   tip         244 non-null    float64 \n 2   sex         244 non-null    category\n 3   smoker      244 non-null    category\n 4   day         244 non-null    category\n 5   time        244 non-null    category\n 6   size        244 non-null    int64   \ndtypes: category(4), float64(2), int64(1)\nmemory usage: 7.4 KB\n\n\n\ntips.describe()  # numerical type만 나열\n\n       total_bill    tip   size\ncount      244.00 244.00 244.00\nmean        19.79   3.00   2.57\nstd          8.90   1.38   0.95\n...           ...    ...    ...\n50%         17.80   2.90   2.00\n75%         24.13   3.56   3.00\nmax         50.81  10.00   6.00\n\n[8 rows x 3 columns]\n\n\n\ntips.describe(include=\"all\")  # all types 나열\n\n        total_bill    tip   sex smoker  day    time   size\ncount       244.00 244.00   244    244  244     244 244.00\nunique         NaN    NaN     2      2    4       2    NaN\ntop            NaN    NaN  Male     No  Sat  Dinner    NaN\n...            ...    ...   ...    ...  ...     ...    ...\n50%          17.80   2.90   NaN    NaN  NaN     NaN   2.00\n75%          24.13   3.56   NaN    NaN  NaN     NaN   3.00\nmax          50.81  10.00   NaN    NaN  NaN     NaN   6.00\n\n[11 rows x 7 columns]\n\n\n\ntips.describe(include=\"category\")\n\n         sex smoker  day    time\ncount    244    244  244     244\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq     157    151   87     176\n\n\n\ns1 = tips.value_counts(\"day\") # \"day\" 칼럼에 대한 각 카테고리별 counts\ns2 = tips.value_counts(\"day\", sort=False) # default: sort is true\ns3 = tips.value_counts(\"day\", ascending=True) # default: ascending is False\ns4 = tips.value_counts(\"day\", normalize=True) # 카테고리별 비율\ns5 = tips.value_counts([\"sex\", \"smoker\"]) # \"sex\", \"smoker\" 칼럼에 대한 유니크한 카테고리별 counts\n\n\n\n\n\n\n\n\n\nday\nSat     87\nSun     76\nThur    62\nFri     19\nName: count, dtype: int64\n\n\n(a) s1\n\n\n\n\n\n\n\n\nday\nThur    62\nFri     19\nSat     87\nSun     76\nName: count, dtype: int64\n\n\n(b) s2\n\n\n\n\n\n\n\n\n\n\nday\nFri     19\nThur    62\nSun     76\nSat     87\nName: count, dtype: int64\n\n\n(c) s3\n\n\n\n\n\n\n\n\nday\nSat    0.36\nSun    0.31\nThur   0.25\nFri    0.08\nName: proportion, dtype: float64\n\n\n(d) s4\n\n\n\n\n\n\n\n\n\n\nsex     smoker\nMale    No        97\n        Yes       60\nFemale  No        54\n        Yes       33\nName: count, dtype: int64\n\n\n(e) s5\n\n\n\n\n\n\n\nFigure 1: value_count()의 arguments\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n.value_count()의 결과는 Series이며 그 이름은 ‘count’ 또는 ’proportion’임 (pandas 2.0)\nMissing(NA)을 count하지 않으나 dropna=False을 이용해 나타낼 수 있음\ntips.value_counts(\"day\", dropna=False)\nSeries에 대해서도 적용되며, DataFrame으로 컬럼을 선택해 적용할 수 있음\ntips[\"day\"].value_counts()  # tips[\"day\"]: Series object\ntips[[\"sex\", \"smoker\"]].value_counts()\n\n\n\nData: palmerpenguins\n\n# load a dataset\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen           39.10          18.70             181.00   \n1  Adelie  Torgersen           39.50          17.40             186.00   \n2  Adelie  Torgersen           40.30          18.00             195.00   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen           36.70          19.30             193.00   \n\n   body_mass_g     sex  \n0      3750.00    Male  \n1      3800.00  Female  \n2      3250.00  Female  \n3          NaN     NaN  \n4      3450.00  Female  \n\n\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\npenguins.describe(include=\"object\")\n\n\n\n       species  island   sex\ncount      344     344   333\nunique       3       3     2\ntop     Adelie  Biscoe  Male\nfreq       152     168   168\n\n\n\n\npenguins.value_counts([\"island\", \"species\"])\n\nisland     species  \nBiscoe     Gentoo       124\nDream      Chinstrap     68\n           Adelie        56\nTorgersen  Adelie        52\nBiscoe     Adelie        44\nName: count, dtype: int64\n\n\n\npenguins.value_counts([\"sex\", \"species\"], dropna=False) # NA은 기본적으로 생략\n\nsex     species  \nFemale  Adelie       73\nMale    Adelie       73\n        Gentoo       61\n                     ..\n        Chinstrap    34\nNaN     Adelie        6\n        Gentoo        5\nName: count, Length: 8, dtype: int64\n\n\n\ntips.sort_values(\"tip\", ascending=False)\n\n     total_bill   tip     sex smoker  day    time  size\n170       50.81 10.00    Male    Yes  Sat  Dinner     3\n212       48.33  9.00    Male     No  Sat  Dinner     4\n23        39.42  7.58    Male     No  Sat  Dinner     4\n..          ...   ...     ...    ...  ...     ...   ...\n111        7.25  1.00  Female     No  Sat  Dinner     1\n67         3.07  1.00  Female    Yes  Sat  Dinner     1\n92         5.75  1.00  Female    Yes  Fri  Dinner     2\n\n[244 rows x 7 columns]\n\n\n\ntips.sort_values([\"size\", \"tip\"], ascending=[False, True])\n\n     total_bill  tip     sex smoker   day    time  size\n125       29.80 4.20  Female     No  Thur   Lunch     6\n143       27.05 5.00  Female     No  Thur   Lunch     6\n156       48.17 5.00    Male     No   Sun  Dinner     6\n..          ...  ...     ...    ...   ...     ...   ...\n111        7.25 1.00  Female     No   Sat  Dinner     1\n82        10.07 1.83  Female     No  Thur   Lunch     1\n222        8.58 1.92    Male    Yes   Fri   Lunch     1\n\n[244 rows x 7 columns]\n\n\n\ntips.nlargest(3, \"tip\")  # 다수의 동등 순위가 있을 때 처리: keep=\"first\", \"last\", \"all\"\n\n     total_bill   tip   sex smoker  day    time  size\n170       50.81 10.00  Male    Yes  Sat  Dinner     3\n212       48.33  9.00  Male     No  Sat  Dinner     4\n23        39.42  7.58  Male     No  Sat  Dinner     4",
    "crumbs": [
      "Home",
      "Python Basics",
      "Data Inspection"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Instructor: 조성균\nEmail: sk.cho@snu.ac.kr\n수업 시간: 월, 수 3:00 ~ 4:20PM\n면담 시간: 수업 후\nWebsite: dgds101.modellings.art\n과제: eclass\n질문: Communicate/Ask",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-정보",
    "href": "index.html#강의-정보",
    "title": "Welcome",
    "section": "",
    "text": "Instructor: 조성균\nEmail: sk.cho@snu.ac.kr\n수업 시간: 월, 수 3:00 ~ 4:20PM\n면담 시간: 수업 후\nWebsite: dgds101.modellings.art\n과제: eclass\n질문: Communicate/Ask",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-개요",
    "href": "index.html#강의-개요",
    "title": "Welcome",
    "section": "강의 개요",
    "text": "강의 개요\n데이터 분석은 오랜 역사를 거쳐 통계학의 영역에서 발전해왔고, 양적연구를 기반으로하는 여러 분야에서 핵심적인 역할을 한 반면, 인공지능의 하위 분야로 연구되어온 기계학습은 방대한 데이터와 더불어 최근에 그 유용성이 크게 부각되면서 이 두 분야는 데이터 사이언스라는 큰 틀에서 통합되고 있습니다. 이러한 광범위한 주제에 대해 각 기법의 핵심적 아이디어와 응용 예시에 초점을 맞추고, 더 세부적인 주제들을 탐구하기 위한 초석을 제공하고자 합니다. 또한 구체적인 예들을 직접 코딩하여 어느 정도 데이터 분석 기술의 기초를 갖추도록 과제를 통해 학습할 기회도 제공됩니다.\n\n전통적 통계와 기계 학습에서 추구하는 바를 이해하고,\n\n데이터로부터 패턴과 의미를 추론하는 방식을 이해하며,\n\n기계/통계적 학습의 응용 가능성에 대해 파악합니다.\n\n\n참고도서\n\nPython Data Science Handbook by Jake VanderPlas: code on GitHub\nAn Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani, Taylor: code on GitHub\nPython for Data Analysis (3e) by Wes McKinney: code on Github\n3판 번역서: 파이썬 라이브러리를 활용한 데이터 분석\nLLMs: ChatGPT, Claude, Gemini, Grok, DeepSeek",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#수업-활동",
    "href": "index.html#수업-활동",
    "title": "Welcome",
    "section": "수업 활동",
    "text": "수업 활동\n출석 (10%), 과제 (10%), 중간고사 (40%), 기말고사 (40%)",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#수업-계획",
    "href": "index.html#수업-계획",
    "title": "Welcome",
    "section": "수업 계획",
    "text": "수업 계획\n1주. 데이터 사이언스 소개\n2주. 데이터 분석의 두 문화 1: 전통적 통계\n3주. 데이터 분석의 두 문화 2: 기계 학습\n4주. 인과 추론 1\n5주. 인과 추론 2\n6주. 선형 모형(linear model) 소개\n7주. 선형 모형의 활용: 해석과 의미의 추론\n8주. 중간고사\n9주. 기계 학습 기본 1: 모델링과 정규화\n10주. 기계 학습 기본 2: 모형의 평가\n11주. 분류(classification) 문제 1: 로지스틱(logistic) 모형\n12주. 분류(classification) 문제 2: 모형의 선택과 평가\n13주. 딥러닝 소개\n14주. 비지도 학습(unsupervised learning)\n15주. 기말고사",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "contents/two-cultures.html",
    "href": "contents/two-cultures.html",
    "title": "Two Cultures",
    "section": "",
    "text": "오랜동안 여러 분야에서 각자의 방식을 개발\nComputer Science\nStatistics\nBiostatistics\nEconomics\nEpidemiology\nPolitical Science\nEngineering\n\n\n\n서로 다른 용어를 쓰기도 하며, 그 분야에서 필요로하는 방식에 초점을 맞춤.\n\n서로 의사소통이 거의 없었음.\n\nData Science라는 이름하에 통합되어가는 과정 중\n\n\n\n\n컴퓨터 사이언스의 경우, 데이터에 존재하는 패턴을 학습하여 분류를 하거나 예측을 위한 이론과 툴들이 개발되는 반면,\n과학자들은 예측보다는, 변수들 간의 진정한 관계 혹은 인과 관계를 탐구\n현재 이 둘은 소위 cross-fertilization을 지향하며 같이 발전, 통합되어가고 있음.",
    "crumbs": [
      "Home",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#the-data-modeling-culture",
    "href": "contents/two-cultures.html#the-data-modeling-culture",
    "title": "Two Cultures",
    "section": "The Data Modeling Culture",
    "text": "The Data Modeling Culture\n\n\\(Y\\) = \\(f(X, \\text{random noise, parameters})\\)\n\n모델의 타당도(validation): goodness-of-fit 테스트와 잔차의 검토\n통계학자의 98%\n\n\nParameter Estimation & Uncertainty\n\n\n\n\n\n\n현상을 이해하고자하는 과학적 접근\n\n\n\n가령, “생물학적 성별에 따른 키(혹은 임금)의 차이는 본질적으로 얼마인가?”라는 질문은 어떤 의미를 가지는가?\n이 질문에 답하기 위한 과학적 접근은 무엇인가?\n\n남성, 여성은 무엇을 뜻하는가? 이를 정의해야 함. → 모집단의 프레임워크로 접근\n본질적 차이라는 것이 무엇을 의미하며 어떻게 알아낼 수 있는가?\n\n\n남녀의 키를 결정하는 무수한 요인들로부터 현실에서 남녀의 키는 다양한 값으로 분포됨.\n\n그럼에도 불구하고, 생물학적 성만의 “고유한 효과”가 존재할 것이라는 가정하에 (본질적 차이); \\(y = \\beta_0 + \\beta_1\\cdot sex + \\epsilon\\)\n\n생물학적 성만이 “고유하게” 발생시키는 키의 차이를 밝히고자 함.\n\nCentral Limit Theorem과 같은 이론을 근거로 생물학적 성 이외의 수많은 요인들의 효과는 노이즈(noise)로 이해하고 “정규분포”처럼 나타날 것이라는 가정하에,\n안전하게, 생물학적 성만의 고유한 효과를 추출할 수 있음; “노이즈(noise)로부터 시그널(signal)을 분리”\n위 모델에서 \\(\\epsilon\\)에 대한 모델링: \\(\\epsilon \\sim N(0, \\sigma^2)\\)로 가정\n\n\n이 차이를 알아내기 위해 실제로 어떻게 측정할 것인가?\n\n\n가정된 전체(모집단)를 관찰할 수 없기 때문에 특정 표본을 관찰하여 전체에 대한 추정치를 얻고자 함.\n표본에 따라 다른 값이 얻어질 것이기 때문에, 그 값들의 (이론적) 분포를 “표본 분포”라고 하고, 이 분포를 통해 현재 표본으로부터 모집단 추정치에 대한 불확실성을 추정 \n\n\n\n\n\n현재 관찰된 데이터는 어떤 모집단(population)으로부터 (독립적으로) 발생된 표본(sample)이라고 가정\n\n변수들 간의 관계성(relationships)을 파악하기 위해, 데이터 모델링은 현상에 대한 모델을 사전 설정(assumptions)하고,\n\n예를 들어, 모집단에 대해서 선형관계를 가정: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\)\n\n변수들의 값이 어떻게 발생하는지(generated)에 대한 가정을 세우고,\n\n예를 들어, Gaussian, Binormial distribution, …\n\n데이터에 가장 적합한(best fitted) 특정 모델을 선택 (즉, 파라미터 \\(\\beta s\\)를 추정)\n\n즉, 위의 선형 함수가 \\(X\\), \\(Y\\)의 1) 관계를 나타내고, 2) 예측을 위해 사용됨\n\\(\\epsilon\\)은 \\(X\\)와 \\(Y\\)의 본질적 관계와는 상관없는 일종의 노이즈(noise); 정확히는 \\(X\\)로는 설명되지 않는 \\(Y\\)의 부분\n노이즈(noise)로부터 시그널(signal)을 분리: “true relationships”을 찾고자 함.\n\n이 파라미터 \\(\\beta s\\)의 불확실성(uncertainty)을 추정\n\n모집단에 대한 추정, 즉 진정한 값을 추정하는 것이므로 불확실성이 존재\n\n\n\n\n\n\n\n\n우울증 예측을 위한 선형모델의 예\n\n\n\n\\[\n\\begin{align*}\n\\text{y(Depression)} = f(x) =\\ & \\beta_0\n+ \\beta_1 \\cdot \\text{Age}\n+ \\beta_2 \\cdot \\text{Sex}\n+ \\beta_3 \\cdot \\text{FamilyHistory}\n+ \\beta_4 \\cdot \\text{ChronicDisease} \\\\\n& + \\beta_5 \\cdot \\text{StressLevel}\n+ \\beta_6 \\cdot \\text{SleepQuality}\n+ \\beta_7 \\cdot \\text{PhysicalActivity} \\\\\n& + \\beta_8 \\cdot \\text{SocialSupport}\n+ \\beta_9 \\cdot \\text{DigitalBehavior}\n+ \\epsilon\n\\end{align*}\n\\]\n예측요인들\n\n\n\n\n\n\n\n\n인구학적 요인\nAge, Sex\n나이, 성별\n\n\n건강·임상 요인\nFamilyHistory, ChronicDisease\n가족력, 만성질환, 과거 정신질환 이력\n\n\n심리·행동 요인\nStressLevel, SleepQuality, PhysicalActivity\n스트레스 수준, 수면의 질, 운동·활동량\n\n\n사회적 요인\nSocialSupport\n사회적 지지(가족·친구·공동체)\n\n\n디지털/행동 데이터\nDigitalBehavior\n스마트폰 사용 패턴, SNS 언어, 이동성 데이터\n\n\n\n\n실제로는 변수들 간의 복잡한 관계를 규명하고자 함. 가령,\n\n\n\n예를 들어,\n\n\n\n\nSource: Kids Who Get Smartphones Earlier Become Adults With Worse Mental Health\n\n\n\n\nSource: Racial differences in homicide rates are poorly explained by economics\n\n\n\n\n\n\n\n\n\n\n\nSource: Jiang, W., Lavergne, E., Kurita, Y., Todate, K., Kasai, A., Fuji, T., & Yamashita, Y. (2019). Age determination and growth pattern of temperate seabass Lateolabrax japonicus in Tango Bay and Sendai Bay, Japan. Fisheries science, 85, 81-98.\n\n단순한 선형 모델의 예\n다이아몬드 가격에 대한 예측 모델\n\n\n\n\n\n\n\n\n\n\n\n\n\n가정들:\n\\(X\\)와 \\(Y\\)의 간의 true relationship: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\nNoise/residual의 분포: \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\n\nMean function: \\(E(Y | X = X_i) = \\beta_0 + \\beta_1 X_i\\)\nVariance function: \\(Var(Y | X = X_i) = \\sigma^2\\)\nDistribution: \\((Y | X = X_i) \\sim N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\n파라미터의 추정 및 불확실성:\n\n\\(\\hat{\\beta}_0 = 0.4, \\hat{\\beta}_1 = 5123, \\hat{\\sigma} = 300\\)\n95%의 확률로 \\(\\beta_0 \\in (0.3, 0.5), \\beta_1 \\in (4823, 5423)\\)\n\\(\\widehat{price}_i = 0.4 + 5123 \\cdot carat\\) : “평균적으로 다이아몬드가 1 carat 커질 때마다 $5123 비싸짐”\n노이즈로부터 관계에 관한 시그널을 추출한 것으로 볼 수 있음\n\n\n\n데이터 모델의 한계\n\n과연 가정한 모델이 자연/현상을 잘 모방하고 있는가?\n\n\nThis enterprise has at its heart the belief that a statistician, by imagination and by looking at the data, can invent a reasonably good parametric class of models for a complex mechanism devised by nature.\n핵심은 통계학자가 상상력을 발휘하고 데이터를 살펴보면 자연이 고안한 복잡한 메커니즘에 대해 합리적으로 좋은 파라메트릭 클래스의 모델을 발명할 수 있다는 믿음입니다. (번역 by DeepL)\n\n\nThe belief in the infallibility of data models was almost religious. It is a strange phenomenon—once a model is made, then it becomes truth and the conclusions from it are infallible.\n데이터 모델의 무오류성에 대한 믿음은 거의 종교에 가까웠습니다. 일단 모델이 만들어지면 그것이 진리가 되고 그 모델에서 도출된 결론은 오류가 없다고 믿는 이상한 현상입니다. (번역 by DeepL)\n\n\n모델이 데이터에 얼마나 잘 맞는지에 대한 논의가 거의 없음\n\n특히, 주로 예-아니오로 답하는 적합도 테스트를 통해 모델의 적합성을 판단\n샘플링 오류로 인해 우연히 관찰된 것은 아니라는 정도에 대한 확신; 완전한 거짓은 아니라는 판단\n\n주로 독창적인 확률 모형을 찾는데 주력\n(가정한) 모델을 데이터에 맞출 때 결론은 자연의 메커니즘이 아니라 모델의 메커니즘에 관한 것이 됨\n모델이 자연을 제대로 모방하지 못하면 결론이 잘못될 수 있음\n\n데이터 모델의 다양성\n\n데이터 모델링의 가장 큰 장점: 입력 변수와 응답 간의 관계를 간단하고 이해하기 쉬운 다이어그램으로 표현 가능; 회귀 분석의 예\n하지만, 데이터에 동일하게 적합한 여러 모델이 존재\n\n적합성(goodness-of-fit)에 대한 기준이 통계적 검정을 통한 예-아니오로 판별\n\n관찰한 데이터로만 모델의 파라미터를 추정하기 때문에, 과적합이 발생하며, 새로운 데이터에 대한 예측 정확도가 떨어질 수 있음\n편향되지 않은 예측 정확도 추정치를 얻으려면 교차 검증(cross-validation)을 사용하거나 일부 데이터를 테스트 집합(test set)으로 따로 떼어 놓을 필요가 있음\n\n모형의 가정에 위배\n\n일반적으로 의료 데이터, 재무 데이터와 같이 복합 시스템에서 생성된 데이터에 단순한 파라메트릭 모델을 적용하면 알고리즘 모델에 비해 정확성과 정보가 손실될 수 있음\n현실에서 데이터의 발생 메커니즘(확률 분포 및 인과적 연결성)에 대한 가정이 성립되기 어려움\n모델을 가정하기보다는 데이터와 실제로 처한 문제로부터 해결책을 찾아 갈 필요가 있음.\n\n\n“If all a man has is a hammer, then every problem looks like a nail.”\n\n\n\n\n\n\n\nBayesian Models\n\n\n\n\n\n불확실성에 대한 가정에 대한 다른 접근: 관측값이 늘어남에 따라 사전 확률(prior probability)을 업데이트(사후 확률, posterior probability)하여 사건 발생에 대한 믿음(확률)을 추정하는 방식\n        \n\nSource: McElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.",
    "crumbs": [
      "Home",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#the-algorithmic-modeling-culture",
    "href": "contents/two-cultures.html#the-algorithmic-modeling-culture",
    "title": "Two Cultures",
    "section": "The Algorithmic Modeling Culture",
    "text": "The Algorithmic Modeling Culture\n\n자연의 복잡하고 신비하며 적어도 부분적으로는 알 수 없는 블랙박스에서부터 데이터가 생성된다고 가정\n데이터로부터 반응 \\(Y\\)를 예측을 하기 위해 \\(X\\)에 작용하는 알고리즘 함수 \\(f(X)\\)를 찾고자 함\n\n가정: 데이터가 어떤 분포로부터 독립적으로 발생(i.i.d. Independent & identically distributed)\n모델의 타당도 지표: 예측 정확도 (predictive accuracy)\n통계학자의 2%\n\n예를 들어, 야구 선수의 연봉을 예측하기 위한 결정 트리 모델 (regression tree)\n\n\n\n\n\n\n\n\n\n\n\n다양한 방식의 여러 결정 트리 모델을 생성 후\n이들을 결합(aggregating)하여 평균을 내어 예측 정확도를 높일 수 있음\n\n새로운 연구 커뮤니티\n\n젊은 컴퓨터 과학자, 물리학자, 엔지니어와 나이든 몇 명의 통계학자 등이 주도\n1980년대 초 리처드 올슨의 연구를 시작으로 의료 데이터 분석에 조금씩 진출하기 시작\n1980년대 중반에 두 가지 강력한 새 알고리즘, 즉 신경망(neural network)과 의사 결정 트리(tree)가 등장\n1990년대 통계학에서도 smoothing spline 알고리즘, cross validation을 사용한 데이터에 대한 적용에 관한 연구가 존재\n1990년대 중반에는 Vapnik의 서포트 벡터 머신(support vector machine)이 등장\n예측 정확도를 목표로, 음성 인식, 이미지 인식, 비선형 시계열 예측, 필기 인식, 금융 시장 예측 등 데이터 모델을 적용할 수 없는 것이 분명한 복잡한 예측 문제를 해결\n\n\n\n머신 러닝 분야로부터의 레슨\n좋은 모델의 다양성 (multiplicity)\n\n예측도가 비슷한 전혀 다른 모델이 존재할 수 있는데\n이 모델들을 결합(aggregating)하면 예측 정확도를 높일 수 있으며, 단일한 모델로 환원할 수 있음\n\n단순성 대 정확성 (simplicity vs. accuracy)\nThe Occam’s Dilemma\n\n예측에 있어 정확성과 단순성(해석 가능성)은 상충됨\n\n정확도를 높이려면 더 복잡한 예측 방법이 요구\n단순하고 해석 가능한 함수는 예측력이 높지 못함\n\n예측 정확도를 먼저 추구한 후 그 이유를 이해하는 것이 더 낫다고 제안\n목표 지향적인 관점에서 보면 오컴의 딜레마는 존재하지 않음\n\n차원의 저주 (the curse of dimensionality)\nDigging It Out in Small Pieces\n\n전통적으로 (데이터 크기에 상대적으로) 변수가 많을수록 좋지 않다고 여겼으나, \n\n\n\n\nFigure 1: 차원이 높아짐에 따른 추정에 대한 불확실성 증가\n\n\n\n\n\n\n\n(a) 방의 갯수에 따른 집값의 변화(1-dim)\n\n\n\n\n\n\n\n\n\n\n\n(b) 지역 정보가 추가되어 지역별로 나누어보는 경우(2-dim)\n\n\n\n\n\n\n\n\n\n\n\n\nTree나 neural network에서는 변수가 많은 것이 문제가 되지 않고, 오히려 작은 정보들이 추가됨\n\n예를 들어, 30개의 예측 변수로부터 4차항들을 추가하면 약 40,000개의 새로운 변수가 생성됨\n이들의 정보는 분류에 도움이 되어 예측 정확도를 높일 수 있음\n\n\n\n   \n\n\n블랙박스로부터의 정보 추출\nThe goal is not interpretability, but accurate information.\n\n“정확성(accuracy)과 해석 가능성(interpretability) 중 하나를 선택해야 한다면, 그들은(통계학자) 해석 가능성을 택할 것입니다.\n정확성과 해석 가능성 사이의 선택으로 질문을 구성하는 것은 통계 분석의 목표가 무엇인지에 대한 잘못된 해석입니다. 모델의 핵심은 응답 변수(Y)과 예측 변수(X) 간의 관계에 대한 유용한 정보를 얻는 것입니다. 해석 가능성은 정보를 얻는 한 가지 방법입니다. 그러나 예측 변수와 응답 변수 간의 관계에 대한 신뢰할 수 있는 정보를 제공하기 위해 모델이 반드시 단순할 필요는 없으며, 데이터 모델일 필요도 없습니다.” (번역 by DeepL)\n\n\n예측 정확도가 높을수록 기저에 있는 데이터 발생 메커니즘(data generating process)에 대한 더 신뢰할 수 있는 정보가 내재할 가능성이 높음; 예측 정확도가 낮으면 의심스러운 결론을 내릴 수 있음\n알고리즘 모델은 데이터 모델보다 예측 정확도가 더 높으며, 기저 메커니즘(underlying mechanism)에 대한 더 나은 정보를 제공할 수 있음.\n\n예를 들어,\n\n의료 데이터와 같이 변수가 데이터에 비해 상대적으로 매우 많은 경우, 더 신뢰만한 변수들의 중요도를 추출할 수 있었음\n클러스터링과 같은 유사한 패턴을 보이는 군집들을 발견할 수 있었음\n유전자 분석처럼 데이터 모델을 생각하기 어려운 곳에 적용 가능; 머신러닝은 변수가 많을수록 좋으며, 과적합하지 않음",
    "crumbs": [
      "Home",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#결론",
    "href": "contents/two-cultures.html#결론",
    "title": "Two Cultures",
    "section": "결론",
    "text": "결론\n\n통계의 목표는 데이터를 사용하여 예측하고 기저에 있는 데이터 메커니즘에 대한 정보를 얻는 것입니다. 데이터와 관련된 문제를 해결하기 위해 어떤 종류의 모델을 사용해야 하는지는 석판에 적혀 있지 않습니다. 제 입장을 분명히 말씀드리자면, 저는 데이터 모델 자체를 반대하는 것이 아닙니다. 어떤 상황에서는 데이터 모델이 문제를 해결하는 가장 적절한 방법일 수 있습니다. 하지만 문제와 데이터에 중점을 두어야 합니다. (번역 by DeepL)\n\n\n\nThe goals in statistics are to use data to predict and to get information about the underlying data mechanism. Nowhere is it written on a stone tablet what kind of model should be used to solve problems involving data. To make myposition clear, I am not against data models per se. In some situations they are the most appropriate wayto solve the problem. But the emphasis needs to be on the problem and on the data.",
    "crumbs": [
      "Home",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#올바른-인과-모델의-필요성",
    "href": "contents/two-cultures.html#올바른-인과-모델의-필요성",
    "title": "Two Cultures",
    "section": "올바른 (인과) 모델의 필요성",
    "text": "올바른 (인과) 모델의 필요성\n\n\n천체의 움직임에 대한 모델\n프톨레마이오스(Claudius Ptolemaeus, CE 100년 이집트 출생)의 천동설 모델\n\n행성의 움직임에 대한 수학적 모델은 매우 정확했으며, 천 년 넘게 활용되었음\n적절한 위치에 충분한 에피사이클(epicycle)을 배치하면 행성의 움직임을 (지구인의 시점에서) 매우 정확하게 예측할 수 있음\n\n\n\n\nSource: McElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.\n\n\n\n\n\n\n\nMaya Astronomy\n\n천체의 운행에 대한 정교한 계산법 개발\n예를 들어, 일식과 월식, 계절의 변화를 예측\n\n\n\n\n\n\n\nSource: YouTube, Maya Astronomy and Mathematics\n\n\n\n과학적 발견의 프로세스\n\n추측 &gt; 모델링 &gt; 관측/실험\n계산된 결과(예측)와 실제 결과(관측)의 비교를 통해 모델의 타당성을 판단\n결코 모델이 참인지 확신할 수 없음: 즉 true model은 존재하지 않을 수 있음; 예. 뉴튼 역학은 상대성 이론에 의해 수정\n모델이 틀린지에 대해서는 확신할 수 있음\n그럼에도 불구하고, 모델이 없는 과학은 위험할 수 있음\n\n과학적 발견에서 AI의 역할\n인식론적 불투명성이 AI 신뢰성의 한계로 작용하는가?\n\nAlphaFold2 성과: 2024년 노벨 화학상 수상, 단백질 구조 예측 성공\nAI 신뢰성 문제: “블랙박스”로 인해 내부 과정이 해석가능하지 않음\n과학적 지식과 AI: AI는 가설 생성에 유용하나, 정당화에는 한계 (Duede, 2023)\n신뢰성 구분 (Ortmann, 2025):\n\nwhy-reliability: 왜 신뢰 할 수 있는가?\nwhether-reliability: 신뢰할 수 있는지에 대한 여부는 판단 가능; 경험적 성공과 광범위한 채택으로 신뢰 획득\n\n철학적 회의론: 인식론적 투명성이 신뢰의 필수 요소 (Duede, 2022), 이론에 대한 인식적 보증이 요구 (Mitchell, 2020)\n\n\nSource: 2025 Seoul Workshop on Philosophy of Machine Learning 중 “AI and the Logic of Scientific Discovery”\n\n두 문화의 결합\n원자의 움직임에 대한 슈뢰딩거 방정식\n\\(\\displaystyle \\left( -\\frac{{\\hbar^2}}{{2m}} \\frac{{1}}{{r^2}} \\frac{{\\partial}}{{\\partial r}} \\left( r^2 \\frac{{\\partial}}{{\\partial r}} \\right) - \\frac{{\\hbar^2}}{{2m r^2}} \\left( \\frac{{\\partial^2}}{{\\partial \\theta^2}} + \\cot \\theta \\frac{{\\partial}}{{\\partial \\theta}} + \\frac{{1}}{{\\sin^2 \\theta}} \\frac{{\\partial^2}}{{\\partial \\phi^2}} \\right) - \\frac{{k e^2}}{{r}} \\right) \\psi(r,\\theta,\\phi) = E \\psi(r,\\theta,\\phi)\\)\n\n이 방정식을 이용해 synthetic data를 생성: 현실에 대한 simulation\n이 데이터를 이용해 머신러닝 모델을 학습: 물질의 속성에 대해 학습\n그 모델을 이용해 많은 새로운 후보 물질들 중에 유용한 것을 매우 빠르게 걸러낼 수 있음\n예측 모형의 다양한 활용 가능성을 보여줌",
    "crumbs": [
      "Home",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/python-basics.html",
    "href": "contents/python-basics.html",
    "title": "Python Basics",
    "section": "",
    "text": "NumPy and pandas\nData Inspection\nSubsetting"
  },
  {
    "objectID": "contents/intro.html",
    "href": "contents/intro.html",
    "title": "개관",
    "section": "",
    "text": "다양한 소스들로부터 데이터 생성: 전지구적 개인과 환경에 대한 상세한 정보 발생\n\n인터넷 & 통신 (SNS, 사진, 위치, 장소, 유동인구, 상품거래, 물류)\n사물인터넷 (IoT), 로봇, 웨어러블 기기\nCCTV\n스마트 팩토리, 파밍\n게놈프로젝트, 생체정보, 의료/보건: 인류, 실시간\n\n23andMe, Theranos\n\n과학적 발견: 과학적 정보와 데이터의 공유; 생물학, 천문학, …\n\n물리법칙의 발견, 약물의 합성, 생체 내 분자의 상호작용의 메커니즘 규명; 알파폴드\n\n자율주행차량: 내부, 외부\n금융정보 및 흐름\n사회 지표 활용: 고용, 직업, 연봉, 소비/활동 패턴, 만족도 조사, 취약 계층에 대한 정보, 우울증, 자살율, …\n생성형 인공지능: 기계의 정보 생산\nAI companion: 개인 내면에 대한 정보\n\n영화 Her (2013), Sony’s robotics dog ‘Aibo’\n\n\n\n\n\n“AI is the new electricity” by Andrew Ng\n의료, 금융, 교통, 교육 등에서 AI가 전기처럼 ‘당연한 기반 기술’이 될 것이라 전망\n\n범용 기술\n\n전기가 특정 산업만 변화시킨 것이 아니라, 모든 산업(가정, 교통, 농업, 의료, 통신 등)에 확산되면서 사회 전체를 혁신했듯이, AI도 특정 영역(예: 음성 인식, 이미지 분석)뿐 아니라, 모든 산업 분야에 적용 가능\n\n인프라 역할\n\n전기는 다른 기술과 산업의 기반 인프라가 되었듯이, AI도 데이터와 컴퓨팅을 기반으로 다양한 응용 분야를 가능하게 하는 디지털 인프라로 작용",
    "crumbs": [
      "Home",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#미래-데이터의-중요성",
    "href": "contents/intro.html#미래-데이터의-중요성",
    "title": "개관",
    "section": "",
    "text": "다양한 소스들로부터 데이터 생성: 전지구적 개인과 환경에 대한 상세한 정보 발생\n\n인터넷 & 통신 (SNS, 사진, 위치, 장소, 유동인구, 상품거래, 물류)\n사물인터넷 (IoT), 로봇, 웨어러블 기기\nCCTV\n스마트 팩토리, 파밍\n게놈프로젝트, 생체정보, 의료/보건: 인류, 실시간\n\n23andMe, Theranos\n\n과학적 발견: 과학적 정보와 데이터의 공유; 생물학, 천문학, …\n\n물리법칙의 발견, 약물의 합성, 생체 내 분자의 상호작용의 메커니즘 규명; 알파폴드\n\n자율주행차량: 내부, 외부\n금융정보 및 흐름\n사회 지표 활용: 고용, 직업, 연봉, 소비/활동 패턴, 만족도 조사, 취약 계층에 대한 정보, 우울증, 자살율, …\n생성형 인공지능: 기계의 정보 생산\nAI companion: 개인 내면에 대한 정보\n\n영화 Her (2013), Sony’s robotics dog ‘Aibo’\n\n\n\n\n\n“AI is the new electricity” by Andrew Ng\n의료, 금융, 교통, 교육 등에서 AI가 전기처럼 ‘당연한 기반 기술’이 될 것이라 전망\n\n범용 기술\n\n전기가 특정 산업만 변화시킨 것이 아니라, 모든 산업(가정, 교통, 농업, 의료, 통신 등)에 확산되면서 사회 전체를 혁신했듯이, AI도 특정 영역(예: 음성 인식, 이미지 분석)뿐 아니라, 모든 산업 분야에 적용 가능\n\n인프라 역할\n\n전기는 다른 기술과 산업의 기반 인프라가 되었듯이, AI도 데이터와 컴퓨팅을 기반으로 다양한 응용 분야를 가능하게 하는 디지털 인프라로 작용",
    "crumbs": [
      "Home",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#데이터-사이언스의-응용-사례",
    "href": "contents/intro.html#데이터-사이언스의-응용-사례",
    "title": "개관",
    "section": "데이터 사이언스의 응용 사례",
    "text": "데이터 사이언스의 응용 사례\n\n\n\n\n\n\nTED Talk by Andrew Ng\n\n\n\nTED: How AI could empower any business by Andrew Ng\n요약 by Claude 3.7 Sonnet\n\nAI가 현재는 대형 기술 기업들과 전문가들에게 주로 집중되어 있으며, 이는 과거 읽고 쓰는 능력이 소수 특권층에게만 있었던 상황과 유사합니다.\n대규모 기술 기업들은 수백만 명의 사용자에게 적용할 수 있는 AI 시스템을 개발할 자원이 있지만, 작은 비즈니스들은 그렇지 못합니다.\n지역 피자가게나 티셔츠 제조업체 같은 소규모 비즈니스들도 자신들의 데이터를 활용해 AI의 혜택을 받을 수 있어야 합니다.\nAI는 대규모 데이터셋만 필요하다는 통념과 달리, 소규모 비즈니스의 적은 데이터로도 유용하게 활용될 수 있습니다.\n현재의 문제는 각 비즈니스가 너무 독특해서 모든 상황에 적용되는 일반적인 AI 솔루션이 없다는 점입니다.\n새로운 AI 개발 플랫폼들은 코드 작성보다는 데이터 제공에 초점을 맞춰, 기술 지식이 적은 사람들도 자신만의 AI를 만들 수 있게 합니다.\n이러한 AI 민주화는 부를 더 넓게 분배하고, 모든 사람이 자신에게 중요한 AI 시스템을 구축할 수 있게 하는 미래를 가능하게 할 것입니다.\n\n\n\n\nSource: Data Science (The MIT Press Essential Knowledge Series), 2018, by John D. Kelleher & Brendan Tierney\n\n\n영업 및 마케팅\n\n웹사이트에서 고객의 구매 행동, 소셜 미디어의 댓글을 추적 고객의 선호도를 파악\n월마트의 경우,\n\n수 십년 넘게 매장의 재고 수준을 최적화했고, 2004년, 몇 주 전에 발생한 허리케인의 판매 데이터를 분석하여 “딸기 팝타트”를 재입고\n소셜 미디어 트렌드 및 신용카드 활동을 분석하여 신제품 출시 및 고객 경험 개인화/최적화\n\n추천 시스템을 통해 사용자 취향에 맞는 제품을 추천, 틈새 상품의 판매도 촉진\n\n\n\n공공기관\n\n미국의 경우, 정부 주도의 데이터 과학 이니셔티브 발족; 특히, 보건 분야에 큰 투자\n\nPrecision Medicine Initiative (2015)\n\n인간 게놈 시퀀싱과 데이터 과학을 결합하여 개별 환자를 위한 약물을 설계\n백만 명 이상의 자원봉사자로부터 환경, 라이프스타일, 생물학적 데이터를 수집하여 정밀 의학을 위한 세계 최대의 데이터를 구축\n\n\n\n도시 운영 및 설계\n\n스마트 시티: 환경, 에너지, 교통 흐름을 추적, 분석, 제어\n장기적 도시 계획 수립에 정보를 축적\n\n치안 및 범죄 예측\n\nPolice Data Initiative\n\n범죄 다발 지역과 재범률을 예측\n시민 단체들의 비판도 존재\n\n시카고 경찰; 1주일 이내의 범죄 예측\n비판: Event-level prediction of urban crime reveals a signature of enforcement bias in US cities. Nature human behaviour\n\n각종 보험료 산정\n\n과거의 데이터를 분석하여, 보험금을 지급할 확률을 계산하고 보험료를 산정\n\n\n\n\n스포츠\n\nMoneyball: The Art of Winning an Unfair Game\n\n야구에서 전통적으로 강조되던 도루, 타점, 타율의 통계보다 출루율과 장타율이 더 나은 척도였음\n“저평가된” 선수, 승리에 기여하는 능력에 비해 낮은 급여를 받는 선수를 찾아 영입\n\nSabermetrics: sciecne of baseball\n데이터 분석을 통해 시장에서 어떤 조직이 우위를 점할 수 있는 방법을 제시\n적절한 속성을 찾는 것의 중요성",
    "crumbs": [
      "Home",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#사회적-파장",
    "href": "contents/intro.html#사회적-파장",
    "title": "개관",
    "section": "사회적 파장",
    "text": "사회적 파장\n\n유토피아 vs. 디스토피아\n초연결성, 투명성 vs. 완전한 감시와 통제\n\n개인적 활동, 소비, 대화, 사고 등이 수집되고 저장\n사회 질서와 안전 보장\n사회 구성원에 대한 감시와 통제\n\n개인화된 서비스 vs. 설득/유혹/조작\n\n개인의 취향, 선호, 특성 등이 활용\n개인화된 맞춤형 서비스 가능\n깊은 내면, 심리, 정신의 해킹\n비의도적인 컨텐츠의 편향; 다양성 상실, 사회적 양극화\n의도적인 조작; 선거 개입\n\n개별성/자율성 vs. 피동적/비주체적\n\n개인이 decision maker가 아닌 모든 것을 outsourcing\n스스로가 고민, 선택 필요없이 최적의 대안을 제공\n자신과 세상에 대한 고민과 탐색 상실\n\n기계와의 교감 vs. 인간관계의 변질/소외, 현실/자연과의 단절\n\n나를 가장 잘 하는 존재가 나를 위해 최선의 서비스를 제공\n나 중심의 인간관계가 학습; 현실에는 존재할 수 없는 가상 존재와 교류, 타인의 이해와 배려 상실\n현실의 사람과의 관계는 더 이상 만족스럽지 못함; 현실은 이해 충돌과 타협, 협력의 복잡한 다이내믹으로 버거움\n가상 세계에서 빠져들수록 자연과 세상과의 교류 감소\n\nHappy Pills vs. Existential Vacuum\n\n삶의 고통과 슬픔을 제거\n즐거움, 쾌락이 최소한의 부작용으로 최적으로 제공\n삶의 의미를 탐구, 탐색, 성찰할 수 있는 기회 상실\n기계에 의한 노동의 대체가 인간의 가치체계를 재고할 기회를 제공?\n\n정보와 인간에 대한 신뢰 약화와 사회적 연대, 문명 붕괴\n\n기계의 정보 생산과 활동은 인간/현실에 대한 불신을 양상\n특히 agency를 획득한 AI는 목적 달성을 위해 인간과 이해충돌이 발생할 수 있음.\n사회적 연대와 협력, 문명은 서로에 대한 신뢰를 기반으로 상호호혜적(reciprocity) 관계에서 유지\n현재, 화폐나 경제 시스템이 작용하는 것은 이에 대한 믿음에 기초하고 있음.\n적극적으로는, 컬트와 같은 정교한 믿음체계를 조직해 집단을 콘트롤할 수 있음.\n\n자연과의 조화 vs. 생태계의 파괴\n\n온난화, 폐기물, 식량, 자원, 에너지 문제 등의 해결책을 제공\nRadical abundance? Demis Hassabis On The Future of Work in the Age of AI\n대량 생산과 소비가 지구 생태계의 균형과 공존할 수 있는가?\n\nIlya Sutskever, University of Toronto honorary degree recipient\n우울한 사회지표들\n\n\n\n\n\n\nBrave New World, 1932\n\n\n\n\n\n\n\nThe Technological Society, 1954\n\n\n\n\n\n\n\nSapiens, Homo Deus, 21 Lessons for the 21st Century by Yuval Noah Harari\n\n\n\n\n\n\n\n\n\nYuval Noah Harari: An Urgent Warning They Hope You Ignore.\n\n\n\nThe Social Dilemma (2020)\nNetflix documentary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n유발 하라리 인터뷰 요약\n\n\n\n\n\n주요 테마:\n\n새로운 전쟁 시대와 글로벌 질서의 붕괴: 하라리는 현재를 새로운 전쟁 시대라고 규정하며, 기존의 자유주의적 국제 질서가 약화되고 각 국가가 자국 이익만을 추구하는 경향이 심화되면서 전쟁 위험이 증가하고 있다고 경고합니다. 그는 우크라이나 전쟁, 중동 분쟁 등을 예시로 들며 “10년 전 우리는 인류 역사상 가장 평화로운 시대에 있었지만, 불행히도 이 시대는 끝났다”고 단언합니다. 만약 글로벌 질서를 재건하지 못한다면 더 많은, 그리고 더 심각한 전쟁이 발생할 것이라고 우려합니다. “만약 모든 국가가 그렇게 생각한다면, 그들 간의 관계를 무엇이 규제하는가? 질서에 대한 대안은 단순히 무질서이며, 이것이 바로 우리가 처한 상황이다.”\n인공지능(AI) 혁명의 심각한 위험성 과소평가: 하라리는 현재 AI 혁명의 규모와 위험성이 제대로 인식되지 못하고 있다고 지적합니다. 그는 AI가 이전의 기술 혁명(인쇄술, 산업혁명 등)과는 근본적으로 다르다고 강조하며, 그 이유는 AI가 스스로 결정을 내리고 새로운 아이디어를 창조할 수 있는 최초의 기술이기 때문이라고 설명합니다. “AI는 인쇄기와도, 19세기 산업혁명과도 전혀 다르다. 훨씬 더 거대하다. AI와 이전의 우리가 발명한 모든 기술 사이에는 근본적인 차이가 있다. 그 차이는 역사상 최초로 스스로 결정을 내리고 스스로 새로운 아이디어를 창조할 수 있는 기술이라는 점이다.” 그는 AI가 인간의 의사결정 능력을 잠식하고, 금융, 전쟁 등 중요한 영역에서 인간이 ’꼭두각시’가 될 수 있는 위험성을 경고합니다.\n인간의 허구적 믿음 체계의 양면성: 하라리는 인간이 지구를 지배할 수 있었던 이유는 개인의 천재성 때문이 아니라, 허구적 이야기를 창조하고 믿음으로써 대규모 협력이 가능했기 때문이라고 주장합니다. 국가, 종교, 기업, 심지어 돈까지도 인간이 만들어낸 ‘허구’이며, 이러한 허구는 협력의 강력한 기반이 되지만 동시에 현실과의 단절을 초래하고 조작에 취약하게 만들 수 있는 위험성을 내포합니다. “우리가 현실이라고 믿는 많은 것이 허구이며, 허구가 인간 역사에서 그토록 중요한 이유는 우리(인간)가 행성(지구)을 지배하기 때문이다… 우리가 그렇게 할 수 있는 이유는 다른 어떤 동물보다 훨씬 더 잘 협력할 수 있기 때문이다.” 그는 현재의 전쟁 역시 영토나 식량이 아닌 서로 다른 ’이야기’, 즉 신화 체계 간의 충돌에서 비롯되는 경우가 많다고 지적하며, AI가 진실과 허구를 구별하는 능력을 향상시켜 줄 것이라는 환상 역시 또 다른 허구라고 경고합니다.\n생명공학과 AI를 통한 인류의 변환 가능성: 하라리는 가까운 미래에 인류가 생명공학, AI, 뇌-컴퓨터 인터페이스 등의 기술을 통해 현재의 ’호모 사피엔스’와는 완전히 다른 존재로 스스로를 변화시킬 가능성을 제기합니다. 그는 이러한 변화가 필연적일 수 있지만, 신중하고 책임감 있게 진행되지 않을 경우 심각한 불평등과 예측 불가능한 결과를 초래할 수 있다고 경고합니다. “우리는 생명공학과 AI, 뇌-컴퓨터 인터페이스를 사용하여 스스로를 너무나 많이 변화시켜 현재의 호모 사피엔스와 오늘날 우리가 침팬지나 네안데르탈인과 다른 정도보다 훨씬 더 다른 존재가 될 수도 있다.” 그는 이러한 기술이 소수의 엘리트에게만 먼저 제공되어 경제적 불평등이 생물학적 불평등으로 이어지고, 인류가 ’슈퍼휴먼’과 그렇지 못한 존재로 분열될 위험성을 우려합니다. 또한, 이러한 ’업그레이드’가 실제로 인간의 번영에 도움이 될지 확신할 수 없으며, 오히려 공감 능력이나 정신적 깊이와 같은 중요한 가치를 훼손할 수 있다고 지적합니다.\nAI 시대의 일자리 변화와 사회적 불안: 하라리는 AI 혁명으로 인해 대부분의 기존 직업이 사라질 것이라고 예측하면서도, 새로운 직업이 등장할 것이라고 전망합니다. 그러나 그는 변화의 속도가 매우 빠르기 때문에 사람들이 끊임없이 재교육을 받아야 할 것이며, 이는 개인적, 사회적으로 큰 도전이 될 것이라고 강조합니다. 그는 특히 ’보편적 기본 소득’과 같은 아이디어가 국가적 차원에서만 논의될 경우, 글로벌 차원의 불평등 문제를 심화시킬 수 있다고 우려합니다. 또한, 미래 사회에 필요한 구체적인 기술을 예측하기 어렵기 때문에, 현재의 교육 시스템이 어떻게 변화해야 할지에 대한 근본적인 고민이 필요하다고 지적합니다.\n인간의 자기 이해와 조작 가능성: 하라리는 인간이 더 이상 ’미스터리한 영혼’이 아닌 ’해킹 가능한 동물’이 되었다고 주장하며, AI와 같은 기술을 통해 인간의 행동을 예측하고 조작하는 것이 이전보다 훨씬 쉬워졌다고 경고합니다. 그는 자유 의지에 대한 믿음이 오히려 자기 성찰과 방어 능력을 약화시켜 조작에 더 취약하게 만들 수 있다고 비판합니다. 또한, 인간의 삶의 의미가 의사결정 능력에서 비롯된다는 점을 강조하며, AI가 점점 더 많은 결정을 대신하게 될 경우 인간의 삶의 의미가 퇴색될 수 있다고 우려합니다. 그는 특히 AI가 인간과의 친밀감을 모방하여 인간을 조종하는 새로운 형태의 조작에 대해 심각한 우려를 표명합니다.\n행복의 주관성과 기술 발전의 한계: 하라리는 지난 수십만 년 동안 인류의 힘은 극적으로 증가했지만, 그에 비해 행복 수준은 크게 향상되지 않았다고 지적합니다. 그는 기술 발전만으로는 행복을 보장할 수 없으며, 행복은 인간의 내면세계, 즉 마음과 의식의 깊은 이해와 관련되어 있다고 강조합니다. 그는 불멸에 대한 인간의 열망 역시 사고, 질병 등 예기치 않은 사고로 인해 ‘영원한 삶의 가능성이 있지만 사고로 죽을 수 있는’ 불안한 상태를 초래할 수 있다고 경고합니다.\n정보 과부하와 정신 건강의 위협: 하라리는 현대 사회의 정보 과부하가 개인의 정신 건강에 심각한 위협이 된다고 경고하며, 균형 잡힌 정보 섭취와 함께 명상 등을 통해 마음의 건강을 유지하는 것이 중요하다고 강조합니다. 그는 끊임없이 자극적인 정보에 노출될 경우 평화로운 마음을 유지하기 어려우며, 사회 전체의 평화에도 부정적인 영향을 미칠 수 있다고 지적합니다. 특히 소셜 미디어 알고리즘이 인간의 주의를 끌기 위해 경쟁적으로 더 자극적인 콘텐츠를 제공하고, 이로 인해 분노와 흥분이 만연하는 사회 현상에 대해 우려를 표명합니다. 그는 ’지루함’을 견디는 능력의 중요성을 강조하며, 지루함 속에서 평화로운 마음을 찾을 수 있다고 설명합니다.\n\n결론 및 향후 과제:\n유발 하라리는 이 인터뷰를 통해 현재 인류가 직면한 다양한 위협과 도전 과제를 심도 있게 분석하고 경고합니다. 그는 새로운 전쟁 시대의 도래, AI 혁명의 잠재적 위험성, 허구적 믿음 체계의 양면성, 생명공학과 AI를 통한 인류 변환의 가능성, 일자리 변화와 사회적 불안, 인간의 조작 가능성, 행복의 주관성, 그리고 정보 과부하와 정신 건강의 위협 등 다양한 주제를 포괄적으로 다루며, 인류의 미래에 대한 심각한 우려를 표명합니다.\n그는 이러한 위협에 대처하기 위해서는 글로벌 질서를 재건하고, AI 기술 개발과 활용에 대한 신중한 논의와 규제가 필요하며, 인간의 자기 이해를 높이고 정신 건강을 유지하기 위한 노력이 중요하다고 강조합니다. 또한, 개개인이 문제 해결을 위해 적극적으로 참여하고 협력하는 것이 중요하며, 기술 발전의 속도에 압도되지 않고 인류의 가치와 번영을 위한 방향으로 나아가야 한다고 촉구합니다.\n하라리의 경고는 우리가 직면한 현실을 냉철하게 인식하고 미래를 위한 책임감 있는 행동을 촉구하는 메시지로 해석될 수 있습니다. 그의 통찰력 있는 분석은 독자들에게 깊은 생각거리를 던져주며, 인류의 미래를 위한 중요한 논의를 촉발하는 계기가 될 것입니다.\n\n\n\n\n\n\n\n\n\n트리스탄 해리스의 강연 및 인터뷰 요약\n\n\n\n\n\n다큐멘터리 The Social Dilemma (2020)의 메인 인물인 Tristan Harris의 몇몇 강연 및 인터뷰 요약\n개요:\n본 문서는 트리스탄 해리스의 강연 및 인터뷰 내용을 기반으로 작성되었으며, 현재 소수의 기술 기업들이 막대한 영향력을 행사하여 수십억 명의 사람들의 생각과 관심을 통제하는 현실과, 새롭게 등장한 생성형 인공지능(AI) 기술이 가져올 잠재적인 위험 및 해결책에 대해 심층적으로 다룹니다. 해리스는 소셜 미디어의 작동 방식과 그로 인한 사회적 문제점을 지적하며, AI 기술 역시 동일한 방식으로 인류에게 심각한 위협이 될 수 있음을 경고합니다. 그는 이러한 문제에 대한 인식을 높이고, 기술 발전의 방향성을 재고하며, 새로운 규제 및 윤리적 시스템 마련의 시급성을 강조합니다.\n주요 테마 및 핵심 아이디어:\n1. 소수의 기술 기업의 영향력과 관심 경제:\n\n극소수의 IT 기업에 근무하는 극소수의 사람들이 현재 10억 명 이상의 사람들의 생각을 마음대로 조종하고 있습니다. 이는 스마트폰의 설계 방식, 알림 메시지 등을 통해 무의식적으로 사람들의 생각과 행동을 유도하는 방식으로 이루어집니다.\n\n“불과 극소수의 IT 기업에서 일하는 극소수의 사람들이 오늘날 자그마치 10억 명 사람들의 생각을 그들 마음대로 조종하고 있다는 것입니다.”\n\n오늘날의 기술은 중립적이지 않으며, 모든 기술은 사용자의 관심을 끌기 위한 경쟁 속에서 발전합니다. 뉴스 웹사이트, 소셜 미디어, 게임, 심지어 명상 앱까지도 인간의 제한된 관심을 놓고 경쟁합니다.\n\n“우리가 만드는 모든 기술은 한 가지 숨겨진 목적을 두고 발전해 나갑니다. 우리의 관심을 끌기 위해 경쟁하는 것이죠.”\n\n인간의 관심을 끄는 가장 효과적인 방법은 인간의 심리를 이해하고 다양한 설득 기술을 동원하는 것입니다. 유튜브의 자동 재생, 스냅챗의 ‘스냅 스트리크’ 기능 등은 사용자의 참여와 중독을 유도하기 위해 설계된 사례입니다.\n\n“사람들의 관심을 끄는 최고의 방법은 인간의 사고방식을 이해하는 겁니다. 그리고 거기에는 무수히 많은 설득 기술이 동원되죠.”\n\n분노와 같은 감정은 사람들의 관심을 끄는 데 매우 효과적이며, 소셜 미디어 플랫폼은 이러한 감정을 활용하여 사용자들의 참여를 유도합니다.\n\n“분노 또한 사람의 관심을 끄는 아주 탁월한 방법입니다. 분노는 선택할 수 있는 게 아니라 그냥 일어나는 감정이기 때문이죠.”\n\n이러한 관심 경제 시스템은 돈 많은 광고주들의 영향력을 확대시키고, 거짓 정보가 확산되기 쉬운 환경을 조성합니다.\n\n“돈 많은 사람이 조종실을 찾아와서 이렇게 말하는 거죠. ‘저 사람들의 머릿속에서 이런 생각을 하게 만들어주세요.’”\n\n\n2. 소셜 미디어의 ’첫 번째 접촉’과 그 결과:\n\n소셜 미디어는 인류와 AI의 ’첫 번째 접촉’과 유사했습니다. 사용자가 화면을 스와이프할 때마다 슈퍼컴퓨터가 수많은 다른 사용자의 데이터를 기반으로 최적화된 콘텐츠를 제공하는 방식은 초기 형태의 AI 시스템이었습니다.\n소셜 미디어의 ’관심 극대화’라는 잘못된 인센티브는 정보 과부하, 둠 스크롤링, 외로움, 극단주의 심화, 가짜 뉴스 확산 등 ’문화의 기후 변화’와 같은 심각한 사회적 문제를 야기했습니다.\n소셜 미디어 플랫폼 개발자들의 선의에도 불구하고, 잘못된 인센티브 구조는 의도치 않은 부정적인 결과를 초래했습니다.\n\n\n“모두 선의로 이 일을 하고 있어요. 사람들은 더 나은 세상을 원하니까요. 다들 더 나은 세상을 진정으로 원합니다. […] 통제하지 못할 정도로 의도하지 않은 결과를 만드는 시스템이 문제인 거죠.”\n\n3. 생성형 AI의 ’두 번째 접촉’과 잠재적 위험:\n\n생성형 AI는 텍스트, 이미지, 음성 등 다양한 형태의 콘텐츠를 빠르게 생성할 수 있는 강력한 기술로, 소셜 미디어보다 훨씬 더 큰 파괴력을 가질 수 있습니다. 이는 인류와 AI의 ’두 번째 접촉’에 해당합니다.\n생성형 AI 개발 경쟁은 ’능력 배포 경쟁’으로 이어져, 가짜 뉴스, 사기, 사이버 공격, 딥페이크 아동 포르노 등 사회에 심각한 위협을 초래할 수 있습니다.\n생성형 AI는 개발자조차 예측하기 어려운 ‘창발적 능력’을 보이며, 이는 AI 통제의 어려움을 가중시킵니다. ’마음 이론’ 능력의 급격한 발전은 AI가 인간의 의도를 이해하고 속일 수 있는 가능성을 시사합니다.\n현재 AI 개발 속도는 매우 빠르며, 안전 연구 및 규제 노력은 이에 크게 미치지 못하고 있습니다.\n소셜 미디어의 전철을 밟아 생성형 AI 기술이 성급하게 배포될 경우, 아동에게 유해한 콘텐츠 노출, 사기 범죄 증가 등 심각한 사회적 문제가 발생할 수 있습니다. 스냅챗의 ‘My AI’ 기능은 이러한 우려를 보여주는 사례입니다.\n\n4. 문제 해결을 위한 근본적인 변화:\n\n인간의 취약성 인정: 인간은 설득에 취약하며, 무의식적으로 기술에 의해 조종될 수 있다는 사실을 인정하고, 이를 바탕으로 자신을 보호하기 위한 노력을 기울여야 합니다.\n새로운 모델과 책임 시스템: 기술 기업들이 사용자의 이익을 투명하게 고려하고 책임감을 갖도록 하는 시스템 마련이 시급합니다. 윤리적인 설득은 설득하는 이의 목적과 설득당하는 이의 목적이 일치할 때만 가능합니다.\n설계 전반의 혁신: 개인의 시간을 낭비하게 만드는 요소들을 제거하고, 사용자가 자신의 시간을 가치 있게 사용할 수 있도록 기술 설계를 근본적으로 바꿔야 합니다.\n힘과 책임의 균형: 갓과 같은 강력한 기술력을 지혜, 사랑, 신중함과 같은 책임감 있는 태도와 균형을 맞춰야 합니다. 힘이 이해력이나 책임감보다 클 경우, 예측하지 못한 부정적인 결과가 발생할 수 있습니다.\n건전한 인센티브 구조: 소셜 미디어와 같이 참여율 극대화에만 초점을 맞춘 사업 모델은 사회적 문제를 야기합니다. 사회 전체의 이익과 부합하는 새로운 인센티브 구조를 모색해야 합니다.\n규제 및 국제 협력: AI 개발 및 배포에 대한 적절한 규제를 마련하고, 국제적인 협력을 통해 위험을 관리해야 합니다. 오픈소스 AI 개발에 대한 제한, AI 개발자 책임 강화, 안전 점검 및 비상 정지 계획 마련 등이 필요합니다.\n사회적 적응 및 회복력 강화: AI로 인한 일자리 감소 등 사회적 충격에 대비하고, 시민들의 공론장 마련 및 합의 도출 시스템 구축을 통해 사회적 회복력을 강화해야 합니다.\n\n결론:\n트리스탄 해리스는 현재 기술 발전의 방향성이 인류에게 심각한 위협이 될 수 있음을 경고하며, 특히 생성형 AI 기술의 빠른 발전과 그 잠재적 위험에 대해 깊은 우려를 표합니다. 그는 소셜 미디어의 실패를 되풀이하지 않기 위해 인간의 취약성을 인정하고, 새로운 윤리적 모델과 책임 시스템을 구축하며, 기술 설계의 근본적인 혁신을 이루어야 한다고 강조합니다. 또한, AI 개발 경쟁에 대한 국제적인 조율과 적절한 규제 마련, 사회적 회복력 강화 노력을 통해 인류가 더 나은 미래를 향해 나아갈 수 있다고 역설합니다. 그의 강연은 AI 기술의 양면성을 인식하고, 미래 사회의 지속 가능한 발전을 위한 심층적인 논의와 적극적인 행동을 촉구합니다.",
    "crumbs": [
      "Home",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#data-science",
    "href": "contents/intro.html#data-science",
    "title": "개관",
    "section": "Data Science",
    "text": "Data Science\n\nArtificial intelligence (인공 지능)\nMachine learning (기계 학습)\nDeep learning (심층 학습)\nData mining (데이터 마이닝)\nStatistical learning (통계적 학습)\n\n\n\n\n소프트웨어 개발\n데이터에 기반한 분석 위해 작동하도록 프로그래밍을 하여 운영되도록 하는 일\n주로 전통적인 컴퓨터 사이언스의 커리큘럼에 의해 트레이닝\n\n유튜브의 영상 추천\n페이스북의 친구 매칭\n스팸메일 필터링\n자율주행\n\n\n\n\n\n\n데이터 분석\n하나의 구체적인 질문에 답하고자 함\n다양한 소스의 정제되는 않은 데이터를 통합하거나 가공하는 기술이 요구\n\nDNA의 분석을 통해 특정 질병의 발병 인자를 탐색\n유동인구와 매출을 분석해 상권을 분석\n어떤 정책의 유효성을 분석하여 정책결정에 공헌\n교통 흐름의 지연이 어떻게 발생하는지를 분석, 해결책 제시",
    "crumbs": [
      "Home",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#skills",
    "href": "contents/intro.html#skills",
    "title": "개관",
    "section": "Skills",
    "text": "Skills\n\n\nDomain knowledge\n\n해결하려는 문제에 대한 이해없이 단순한 알고리즘만으로 “one size fits all”은 효과적이지 않음\n추상화된 현실에 대한 모형은 수많은 가정/사전 지식(prior knowledge)을 전제하고 있음.\n각 분야의 전문 지식은 데이터가 발생되는 과정, 데이터의 특성, 데이터의 의미를 이해하는데 필수적\n\nEthics\n\n데이터를 합법적이고 적절하게 사용하려면 규정을 이해하고, 자신의 업무에 미치는 영향과 사회에 미치는 파급력 대한 윤리적 이해가 필요\n\n배출(exhaust) 데이터: 어떤 목적을 위해 데이터를 얻는 과정에서 얻어지는 부산물\n\n소셜 미디어: 사용자가 다른 사람들과 소통할 수 있도록 도움\n\n공유된 이미지, 블로그 게시물, 트윗, 좋아요 등으로부터\n누가/얼마나 많이 보았는지/좋아요/리트윗을 했는지 등을 수집\n\n아마존 웹사이트: 다양한 물건을 편리하게 구매할 수 있도록 도움\n\n사용자가 장바구니에 어떤 품목을 담았는지, 사이트에 얼마나 오래 머물렀는지, 어떤 다른 품목을 보았는지 등을 수집\n\n메타데이터(metadata)\n통화 내역만으로 많은 민감한 정보을 유추할 수 있음\n\n알코올 중독자 모임, 이혼 전문 변호사, 성병 전문 병원 등\n\n\n한편, 서비스와 마케팅을 타겟팅할 수 있는 잠재력\n\n\nWrangling\n\n데이터 소스는 다양한 형식으로 존재\n통합, 정리, 변환, 정규화 등의 작업이 요구\ndata munging, data wrangling, data cleaning, data preparation, data preprocessing 등으로 불림\n\nDatabase & computer science\n\n수집된 데이터가 저장되고, 가공/추출된 데이터의 재저장 등 데이터베이스와의 소통할 수 있는 기술\n다양해지고 방대해진 빅데이터를 저장/배포하기 위한 도구를 활용 능력\nML 모델을 이해하고 개발하여 제품의 출시, 분석, 백엔드 애플리케이션에 통합할 수 있는 기술 등\n\nVisualisation\n\n작업 프로세스의 모든 과정에 관여\n\n데이터를 탐색하거나,\n데이터의 의미를 효과적으로 전달\n\n\nStatistics & Probability\n\n데이터 과학 프로세스 전반에 걸쳐 사용됨\n\n초기 수집과 조사\n다양한 모델과 분석의 결과를 해석\n의사결정에 활용\n\n\nMachine Learning\n\n데이터로부터 패턴을 찾기 위한 다양한 알고리즘을 사용\n응용 측면에서는\n\n수많은 알고리즘에 대해 가정, 특성, 용도, 결과의 의미, 적용가능한 유형의 데이터 등을 파악\n해결할 문제와 데이터에 가장 적합한 알고리즘을 파악\n\n\nCausal Inference\n\nMachine learning이 주로 변수들 간의 연관성으로부터 예측모형에 집중하는 반면,\n인과추론은 진실된 인과적 관계를 파악하여 현상을 올바로 이해하고자 함\n인과적 모델을 통해 (새로운) 환경/현실에 대해 적응적으로 대응할 수 있는 모델을 구축할 수 있음\n\nCommunication\n\n데이터에 담긴 스토리를 효과적으로 전달하는 능력\n분석을 통해 얻은 인사이트, 조직 내 목적에 어떻게 부합하는지, 조직의 기능에 미칠 수 있는 영향 등을 파악",
    "crumbs": [
      "Home",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#응용비즈니스에서-정형적인-절차",
    "href": "contents/intro.html#응용비즈니스에서-정형적인-절차",
    "title": "개관",
    "section": "응용/비즈니스에서 정형적인 절차",
    "text": "응용/비즈니스에서 정형적인 절차\nPhases of the CRISP-DM (CRoss-Industry Standard Process for Data Mining)\nSource: Chapman et al., 2000\n\nGeneric tasks of the CRISP-DM reference model\n\n비즈니스의 이해와 데이터의 이해\n\n프로젝트의 목표를 정의하고, 비즈니스 문제를 이해하는 것\n어떤 데이터를 수집하는 것이 유용한지, 어떤 데이터가 수집 가능한지 등을 탐색\n\n데이터 준비와 모델링\n\n노이즈와 비정형화된 데이터를 정제하고, 모델링을 위한 데이터를 준비\n데이터로부터 의미있는 패턴(signal vs. noise)과 통찰을 찾기 위해 다양한 모델을 검토하고 실행\n\n모델 평가와 배포\n\n모델링 성능을 평가하고 개선, 모델을 배포\n실제 환경에서는 훈련/평가을 위해 사용된 데이터가 보진 못한 새로운 데이터에 적용됨으로 모델의 성능을 지속적으로 모니터링\n\n데이터 질의 중요성\n\n2016년 데이터 과학자를 대상으로 한 설문조사(CrowdFlower report, 2016)\n데이터 준비(데이터 수집, 클린닝)에 79%의 시간이 소요\n프로젝트의 초점이 명확하고, 그에 맞는 올바른 데이터가 수집되었는지, 모델이 프로젝트의 목표에 잘 부응하는지 중요!\nGarbage in, garbage out\n\n\n\n\nSource: Cleaning Big Data",
    "crumbs": [
      "Home",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#표준-비즈니스-영역에서의-데이터-사이언스-작업",
    "href": "contents/intro.html#표준-비즈니스-영역에서의-데이터-사이언스-작업",
    "title": "개관",
    "section": "표준 비즈니스 영역에서의 데이터 사이언스 작업",
    "text": "표준 비즈니스 영역에서의 데이터 사이언스 작업\nSource: Data Science (The MIT Press Essential Knowledge Series), 2018, by John D. Kelleher & Brendan Tierney\n\nClustering\nAnomaly detection\nAssociation-rule mining\nPrediction: classification & regression\n\n\n1. Clustering\nWho Are Our Customers?\n\n\n클러스터링을 통해 타깃 고객을 더 세분화된 군집으로 분류하여 마케팅 캠페인의 타겟을 명확히 정의할 수 있음\n\nMeta S. Brown (2014)의 보고서에 따르면,\n\nSoccer Moms? \n탐색적 clustering을 통해 고객 세그먼트를 정의\n\n어린이집에 다니는 어린 자녀를 둔 전업주부\n고등학생 자녀 둔 파트타임으로 일하는 엄마\n음식과 건강에 관심이 많지만 자녀가 없는 여성\n\n\n\n클러스터링을 통해 얻은 고객 세그먼트에 페르소나를 부여\n각 특성에 맞는 캠페인 전략을 수립\n\n작고 집중된 고객 클러스터를 발견\n많은 매출을 창출하는 고객이 포함된 클러스터에 집중\n\n\n\n\n  Source: Introduction to Statistical Learning by James et al.\n\n\n\n\n클러스터링을 위해 사용할 수 있는 속성들: 어떤 속성을 포함하고 어떤 속성을 제외할지 결정하는 것이 중요!\n\n인구통계학적 정보(연령, 성별 등)\n위치(우편번호, 시골 또는 도시 주소 등)\n거래 정보(예: 고객이 어떤 제품이나 서비스를 추구했는지)\n고객이 된 지 얼마나 되었는지\n로열티 카드 회원인지\n제품을 반품하거나 서비스에 대해 불만을 제기한 적이 있는지 등\n\n\n\n\n\n\n\n프로젝트의 데이터 이해 단계에서 탐색 도구로 자주 사용됨\n구체적인 예로,\n\n추가 지원이 필요하거나 다른 학습 접근 방식을 선호하는 학생 그룹을 식별\n생물 정보학에서 마이크로어레이 분석에서 유전자 서열을 분석\n\n\n\n\n2. Anomaly detection\nIs This Fraud?\n\n잠재적인 사기, 특히 금융 거래 행위를 식별하고 조사\n\n예를 들어, 비정상적인 위치에서 발생한 거래\n비정상적으로 많은 금액이 포함된 거래\n\n어떤 면에서 클러스터링과 반대 개념\n\n클러스터링: 유사한 인스턴스 그룹을 식별\n이상 징후 탐지: 특별한 인스턴스를 식별\n\n이상 징후는 드물다는 그 고유한 특징으로 인해 식별이 어려움\n여러 가지 모델을 결합: 서로 다른 모델이 서로 다른 유형의 이상 징후를 포착\n\n예를 들어, 4개의 모델 중 3~4개 모델에서 거래가 사기성 거래로 식별되는 경우\n\n다양한 분야에서 활용\n\n금융기관: 잠재적 사기 또는 자금 세탁 사례로 추가 조사가 필요한 금융 거래를 식별\n보험기관: 회사의 일반적인 청구와 일치하지 않는 청구를 식별\n사이버 보안: 해킹 가능성, 직원의 비정상적인 행동을 탐지하여 네트워크 침입을 식별\n의료 분야: 의료 기록의 이상 징후를 식별하여 질병을 진단\n사물 인터넷: 데이터를 모니터링하고 비정상적인 센서 이벤트의 발생을 감지, 조치\n\n\n\n\nSource: The Hundred-Page Machine Learning Book, 2019 by Andriy Burkov\n\n\n\n3. Association-Rule Mining\nDo You Want Fries with That?\n\n고객에게 다른 관련 제품이나 보완 제품, 혹은 잊어 있었던 제품을 제안\n\n예를 들어, 슈퍼마켓에서 핫도그를 구매한 고객은 케첩과 맥주도 함께 구매할 가능성이 높음.\n이에 맞춰 매장은 제품 레이아웃을 계획할 수 있음\n온라인 마켓의 경우, 웹사이트의 배열, 추천, 광고 등을 설계\n즉, 제품 간 연관성을 이해하고 교차 판매를 촉진\n\n연관 규칙 마이닝은 데이터 세트의 속성(또는 열) 간의 관계를 살펴보는 데 중점을 둠: 속성 간의 상관관계\n위의 경우, 고객의 장바구니 품목을 추적\nIF {핫도그, 케첩}, THEN {맥주}\n\n연관성 규칙의 신뢰도가 75%라면 고객이 핫도그와 케첩을 모두 구매한 경우 75%에서 맥주도 함께 구매했음을 의미\n인구통계학적 정보를 연관성 분석에 포함하여 마케팅 및 타겟팅 광고에 활용\n\n특히, 구매 기록 정보가 없는 경우\nIF 성별(남성) & 나이(35세 미만) & {핫도그, 케첩}, THEN {맥주}\n\n장바구니 분석을 통해 다음과 같은 질문에 답을 탐색\n\n마케팅 캠페인이 효과가 있었는지,\n이 고객의 구매 패턴에 변화가 있었는지,\n고객에게 중요한 인생 이벤트가 있었는지,\n제품 위치가 구매 행동에 영향을 미치는지,\n신제품으로 누구를 타깃팅해야 하는지 등\n\n구매 경향의 시간적 요소를 더하면\n\n적절한 시기에 (재)구매를 추천\n유지보수, 부품 교체 일정\n\n다양한 영역에서도 유용함\n\n통신: 회사의 다양한 서비스를 패키지로 묶는 방법을 설계\n보험: 상품과 보험금 청구 사이에 연관성을 파악\n의료: 기존 치료법과 새로운 치료법 및 의약품 간에 상호 작용이 있는지 확인\n\n추천 시스템(recommnder system)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Machine Learning Class (2016.7.8) from Microsoft Research by Chris Bishop, YouTube\n\n\n\n4-1. Classification (Prediction)\nChurn or No Churn(고객 이탈), That Is the Question\n\n개인의 행동 성향에 대한 모델링이 목표: 예, 광고 마케팅에 대한 반응, 서비스 탈퇴 등 다양한 행동 예측\n\n휴대폰 서비스 회사의 고객 유지 필요성: 기존 고객 유지 비용 대비 신규 고객 유치 비용이 상대적 높음\n이탈 가능성이 높은 고객 식별의 중요성: 유지 비용 최소화 및 이탈 예측을 통한 효율적인 혜택 제공 필요\n이탈 예측의 의미와 활용: 서비스 이탈 예측을 통해 고객 이탈 가능성을 예측하고 효율적인 대응 가능\n\n다양한 산업의 이탈 예측에 활용: 통신, 유틸리티, 은행, 보험 등에서의 이탈 예측을 통한 비즈니스 전략 개발 및 운영 향상\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep learning: 예측모형 중 하나\n\n이미지 인식: 예. 손글씨 인식, 물체 인식, 영상 인식\n생성형 모델: 적절한 단어, 소리, 이미지 예측\n\n\n\n\n\n\n\n\n\n\n\n\n\n4-2. Regression (Prediction)\nHow Much Will It Cost?\n\n앞서, 분류는 범주형 속성의 값을 추정하는 반면, 회귀는 연속적인 값을 추정\n전통적인 통계적 모형의 근간\n예를 들어, 주택의 “가격”을 예측하는 경우\n\n주택의 크기, 방의 개수, 층수, 해당 지역의 평균 주택 가격, 해당 지역의 평균 주택 크기 등의 속성을 포함\n\n자동차의 “가격”을 예측하려면\n\n자동차의 연식, 주행 거리, 엔진 크기, 자동차 제조사, 문 개수 등의 속성을 포함\n\n\n\n\nLinear (Regression) Model의 예",
    "crumbs": [
      "Home",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "contents/causal-inference.html",
    "href": "contents/causal-inference.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Data is a window, not a mirror to reality!\n\n\n\n\n \n\n\n\n\n\n\n\n\n담배는 폐암의 원인인가?\n\n1950년 ~ 1964년 미국에서 의사, 역학자, 통계학자 사이의 큰 논쟁\n통계적으로 입증할 수 있는가? 실험군-대조군의 실험적 방법론 이외의 다른 방법이 있는가?\n\n교란(confounding) 변수가 전혀 존재하지 않는다는 것을 입증할 수 있는가?; 흡연 유전자, 흡연자의 라이프스타일, 흡연자의 주위 환경 등\n\nUS Surgeon General이 임명한 특별 자문위원회의 고민 (1964)\n\n\n위원회는 보고서를 위해 1년 이상 노력했고, 주요 문제 중 하나는 “원인”이라는 단어의 사용이었습니다. 위원회 위원들은 19세기의 결정론적 인과성 개념을 제쳐두어야 했고, 통계도 제쳐두어야 했습니다. (아마도 코크런이) 보고서에 적었듯이, “통계적 방법은 연관성에서 인과 관계를 증명할 수 없습니다. 연관성(association)의 인과적 유의성(causal significance)은 통계적 확률에 대한 진술을 넘어서는 판단의 문제입니다. 속성이나 요인과 질병 또는 건강에 미치는 영향 사이의 연관성의 인과적 중요성을 판단하거나 평가하려면 여러 가지 기준을 사용해야 하며, 그 중 어느 것도 판단의 충분한 근거가 될 수 없습니다.”\n\n\n\n“Statistical methods cannot establish proof of a causal relationship in an association. The causal significance of an association is a matter of judgment which goes beyond any statement of statistical probability. To judge or evaluate the causal significance of the association between the attribute or agent and the disease, or effect upon health, a number of criteria must be utilized, no one of which is an all-sufficient basis for judgment.”\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n\n임산부의 흡연은 저체중아의 생존에 이로운가?\nBirth-weight Paradox\n\n1960년대 중반, Jacob Yerushalmy의 논문\n\n이미 여러 연구에서 흡연자 아기가 출생 시 체중이 더 가볍다는 것이 밝혀졌음; 저체중은 영아사망율을 높임\n저체중아 중, 어머니가 흡연자인 경우 생존율이 더 높았음!\n\n논문이 발표된 지 40년이 넘은 2006년까지 만족스럽게 설명되지 않았음\n흡연자 대신 흑인으로 대체해도 같은 현상을 발견\n의학에서 비슷한 패러독스가 자주 발견됨; 예. 당뇨 환자의 경우 비만이 생존에 이득이 되는 것처럼 나타남.\n\n  \n\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n학과별로 공정해도 대학은 차별할 수 있을까?\nUC Berkeley Admission Paradox\n\n1973년, 대학원에 진학한 남성과 여성의 합격 비율이 각각 44%, 35%였음\n입학 결정은 학과별로 독립적으로 내렸음\n학과별로는 여성의 합격 비율이 더 높았음!; 남성에 대한 역차별이 나타났음\nPeter Bickel(버클리대의 통계학자)와 William Kruskal(시카고대의 통계학자) 사이의 논쟁(해결하지 못한채 2005년 사망)\n\n학과별로 나누어 살펴보는 것으로 충분한가?\n어느 주 출신인지에 따른 차별까지 존재한다면 학과별로 나눠보는 것은 더 큰 오류를 발생시킬 수 있음\n\n즉, 두 요인을 모두 고려하면 다시 여성에 대한 차별이 있는 것으로 결론 날 수 있음.\n\n\n무분별한 편견(bias)이 양산되는 방식에 대한 전형적인 예시로 볼 수 있음\n\n편견(bias): associational 개념 - 관찰된 연관성에 대한 인식\n\n편견/편향은 인지적 부하없이(깊이 생각할 필요없음) 빠르게 대응할 수 있는 인지체계로 생존에 적응적으로 여겨짐.\n\n동물들의 무의식적/진화적 혐오까지도 포함\n\n단, 편견에 대한 비판의 원인은\n\n분포를 고려하지 않는 이분적인 태도\n편의적으로 인과관계를 추론하는 경향\n\n\n차별(discrimination): causal 개념 - 변화가 발생되도록 개입\n\n\n\n\n\n\n\n\n\n\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n효과는 없었으나 성공적인 정책?\n\n1990년대 최악의 시카고 공립학교의 개혁 정책\n고1에서 보충 과목을 없애고, 대학 진학 준비 과목을 수강하도록 함.\n이 중 대수 1 과목(“Algebra for All”)의 경우 3년 간 유의한 성적 개선이 없었음.\nGuanglei Hong(시카고대 인간발달)은 정책의 직접적 효과는 존재한다고 판별했음!\n\n정책이 두 가지 방식으로 (다른 방향으로) 작용했음.\n이후 “Double-Dose Algebra”으로 개선했음.\n\n\n\n\n\n\n\n\n\n\n\n\n인과에 대한 철학적 논쟁들\n\n\n\n\n정의, 인식 가능성, 필연성\n다중 원인, 상호작용  \n시간적 순서성, 인과의 피트백 \n결정론적 vs. 확률론적\n\n\n\n\nThe Grammar of Science (1892), by Karl Pearson\n\n특정 시퀀스가 과거에 발생하고 반복되었다는 것은 경험의 문제이며, 인과라는 개념 안에서 그렇게 표현합니다. 미래에도 계속 반복될 것이라는 것은 신념의 문제이며, 확률이라는 개념 안에서 그렇게 표현합니다. 과학은 어떤 경우에도 시퀀스에 내재된 필연성을 입증할 수 없으며, 시퀀스가 반드시 반복되어야 한다는 것을 절대적으로 확실하게 증명할 수도 없습니다. 과거에 대한 과학은 묘사이고 미래에 대한 과학은 믿음입니다;\n\n\n\nThat a certain sequence has occurred and recurred in the past is a matter of experience to which we give expression in the concept causation; that it will continue to recur in the future is a matter of belief to which we give expression in the concept probability. Science in no case can demon­strate any inherent necessity in a sequence, nor prove with absolute certainty that it must be repeated. Science for the past is a description, for the future a belief; (Pearson, 1892, p. 113).\n\n\n\n인과 관계라는 개념은 현상에서 개념적 과정을 통해 추출된 것으로, 논리적 필연도 아니고 실제 경험도 아닙니다…. 우주에 대한 더 넓은 관점에서보면 모든 현상은 상관관계로서 보이지만, 인과적으로는 관계하지 않는 것으로 보입니다.\n\n\n\nthe idea of causation is extracted by conceptual processes from phenomena, it is neither a logical necessity, nor an actual experience…. The wider view of the universe sees all phenomena as correlated, but not causally related. (Pearson, 1892, p. 177)\n\n\n\n\n피어슨은 인과관계를 분석의 언어에서 배제시키고, 이후 그 전통이 통계학에서 어어짐\nPositivism(실증주의) \n\n인과관계에서 “힘”, “필연성” 등과 같은 관찰 불가능한 형이상학적 요소를 제거하고, 오직 관찰 가능한 현상 간의 관계만을 인정\n오직 관찰된 패턴만을 반영할 수 있으며, 반복적인 규칙성(regularity of succession)을 통해 상관관계(correlation/association)로 설명될 수 있음\n경험주의 철학의 대표격인 흄(David Hume)의 철학적 영향을 받음: 인과관계를 “항상적 연접(constant conjunction)”으로 이해. 즉, 인과는 두 사건 사이의 필연적 연결이 아니라 단지 한 사건이 다른 사건 뒤에 규칙적으로 발생하는 패턴의 (심리적, 인지적) 구성물\n\n그렇다면, 과학에서 인과문제는?\n\nwhy의 문제를 how의 문제로 대수적 방정식/함수관계를 통해 대체했음\n\n가령, F=ma에서 1kg의 정지된 물체를 30m/s의 속도까지 10초 안에 가속시키려면 어떻게 해야 하는가(얼마의 힘이 필요한가)?\n\n콩트(Comte)는 갈릴레이와 마찬가지로 형이상학적 질문에서 탈피하고 현상들의 (일반적) 관계에 대해서만 초점을 맞추는 것이 과학 혁명이 성공할 수 있는 본질이라고 봄.\n극단적으로 why의 문제는 how의 문제로 귀속된다고까지 주장함; 정확한 말을 아니지만 why의 본질을 담고 있음.\n\n\n\n\n\n\n\n\nPositive Philosophy\n\n\n\n인간 진보의 과정이 신학적 상태 (theological state) &gt; 형이상학적 상태 (metaphysical state) &gt; 실증적 상태 (positive state)로 나아간다는 논의에서 최종 상태인 실증적 상태에 대하여…\n\n최종적인 실증적 상태에서, 마음은 절대적 개념, 우주의 기원과 목적지, 그리고 현상의 원인에 대한 헛된 탐구를 포기하고, 그것들의 법칙, 즉 그들의 불변적 연속 및 유사성 관계(invariable relations of succession and resemblance)의 연구에 전념합니다. 적절히 결합된 추론과 관찰이 이러한 지식의 수단입니다. 현재 우리가 사실의 설명(explanation)에 대해 언급할 때 이해되는 것은 단순히 개별 현상과 일반적 사실들 사이의 연결을 확립하는 것이며…\n\n\n무게(weight)와 인력(attraction)이 무엇인지에 대해서는, 우리는 그것과는 아무 상관이 없습니다. 왜냐하면 그것은 전혀 지식의 문제가 아니기 때문입니다. 신학자들과 형이상학자들은 그러한 질문들에 대해 상상하고 정제할 수 있습니다; 그러나 실증적 철학은 그것들을 거부합니다.\n\n\n푸리에는 열(heat)에 관한 그의 훌륭한 일련의 연구에서, 그의 선배들이 열성 물질과 보편적 에테르의 작용에 대해 논쟁했을 때와는 달리, 한 번도 그 본질에 대해 질문하지 않고 열 현상의 가장 중요하고 정확한 법칙들과 많은 크고 새로운 진리들을 우리에게 제공했습니다.\n\nComte, Auguste. The positive philosophy of Auguste Comte. Blanchard, 1858.\n\n\n\nSewall Wright\n\n\n\nPath diagrams라는 데이터로부터 인과 관계에 대한 질문에 답하는 수학적 방법을 최초로 개발\n실제로 인과 분석의 툴로 이어지지는 못하였고,\nPath analysis(경로 분석)이라는 통계 기법으로 어어졌으나 뿌리 깊은 오해의 씨앗이 되었음.\n\n  \n\n기니피그의 털색에 영향을 미치는 요인을 설명하는 경로 다이어그램. D = 발달 요인(수태 후, 출생 전), E = 환경 요인(출생 후), G = 각 개별 부모의 유전적 요인, H = 부모 모두의 유전적 요인을 합친 것, O, O′ = 자손. 분석의 목적은 D, E, H의 영향력을 추정하는 것(다이어그램에서는 d, e, f로 표기)\n\n\n\n\n\n\nSewall Wright in 1954\nfrom Wikipedia\n\n\n\n\n\nPath Analysis\n\n\n\n\nPath Model\n\n\n\n\n\n\nLatent Variable Path Model\n\n\n\n\n\n\n\n\n\n\n\nLatent variables\n\n\n\n(다음은 수업 후 추가된 내용으로 참고만 하세요.)\n보통, 추상적인 개념적 단위인 구성개념(construct)를 수량화하기 위해 사용되는 변수를 잠재변수(latent variable)라고 함.\n관측된 변수가 아닌 이 잠재변수들로 모델링; \\(Y = f(X)\\)\n여러 개의 선형 모형이 나타남으로 동시에 여러 개의 \\(Y = f(X)\\) 형태의 방정식을 얻게 되고,\n이를 (잠재변수) 구조방정식모형((latent variable) structural equation model, SEM)이라고 함. (또는 latent variable path model)\n잠재변수의 예로 독해력이라는 구성개념을 수량화하고자 하는 경우,\n다음과 같이 독해력을 3가지 문항으로 측정하는 예를 생각해볼 수 있음.\n\n테스트 1: 참가자가 글을 읽고, 해당 글을 가장 잘 설명하는 그림을 4개의 선택지 중에서 선택.\n테스트 2: 참가자가 지시문을 읽고, 그 지시에 따라 행동을 실행 (예: “일어서서 테이블 주위를 걷고, 다시 앉으세요”).\n테스트 3: 참가자가 단어나 문장이 빠진 글을 읽고, 글의 의미를 바탕으로 빠진 단어나 문장을 채움.\n\n독해력과는 별개로 추가로 측정되는 능력이 다음과 같이 존재할 수 있음; unique variance\n\n테스트 1: 읽은 내용을 시각적 이미지로 변환하는 능력.\n테스트 2: 읽은 내용을 행동으로 옮기는 능력.\n테스트 3: 자신의 지식에서 가장 적합한 단어나 문장을 선택해 글에 삽입하는 능력.\n\n아래 그림에서 세 가지 독해력 검사에서 개인의 점수(test 1, test 2, test 3)는 “진정한 독해력”(구성개념), 오차(error), 각 검사의 고유한 특성에 의해 영향을 받는다는 가정을 나타냄.\n\n\n통계적으로 잠재 변수를 요인(factor)이라고도 부르며, 통계의 요인 분석(factor analysis)을 통해 얻어질 수 있음.\n이는 실제 측정된 변수들이 공통적으로 공유할 것으로 가정되는 요인을 추출하는 프로세스라고 볼 수 있으나,\n\n구성개념을 수치화하는 이런 방식에 대한 반론이 있으며, “구성 개념을 수량화”하기 위한 다른 대안들이 제안되고 있음\n\n가령, 사회경제적 지위(socioeconomic status, SES)와 같은 구성개념의 통계적 수량화에 의문이 제기되어 왔음.\n\n\n\n\n\n\n\n\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n사람들은 어떻게 인과성에 대한 지식을 얻게 되는가?\n어떤 경험 패턴이 이 연관성을 “인과적”이라고 확신시키는가?\n\n\n\n\n\n관찰을 기반으로 규칙성 발견하고 예측\n올빼미가 쥐의 움직임을 관찰하고 잠시 후 쥐가 어디에 있을지를 파악\n컴퓨터 바둑 프로그램이 수백만 개의 바둑 게임 데이터베이스를 연구하여 어떤 수와 승률이 높은지 알아내는 것\n하나의 이벤트를 관찰하면 다른 이벤트를 관찰할 가능성이 달라진다면, 하나의 이벤트가 다른 이벤트와 연관되어 있다고 말할 수 있음\n“치약을 구매한 고객이 치실도 구매할 가능성이 얼마나 되는가?”; \\(P(치실 ~| 치약~)\\)\n통계의 핵심: 상관관계, 회귀\n올빼미는 쥐가 왜 항상 A 지점에서 B 지점으로 가는지 이해하지 못해도 훌륭한 사냥꾼이 될 수 있음\n위스키 한 병을 들고 있는 보행자가 경적을 울릴 때 다르게 반응할 가능성이 있다는 것을 기계가 스스로 파악할 수 있는가?\n\nAssociation 단계의 한계: 유연성과 적응성의 부족\n\n\n\n\n\n\n관찰을 넘어, 세상에 대한 개입\n“치약 가격을 두 배로 올리면 치실 판매량은 어떻게 될까?”\n데이터에는 없는 새로운 종류의 지식을 요구\n통계학의 언어로는 이 질문을 표현하는 것조차 불충분함\n수동적으로 수집된 데이터만으로는 이러한 질문에 대답할 수 없음\n\n과거의 데이터를 이용하면?\n과거에 가격이 두 배 비쌌을 때의 치실 판매량으로 추론?\n이전에 가격이 두 배 비쌌을 때 다른 이유가 있었을 수 있음\n\n인과 관계를 파악하기 위해 전통적으로 실험을 통해 해결; 예. 테크 기업들의 실험들\n정확한 인과 관계 모델이 있으면 관찰 데이터만으로도 가능; \\(P(치실 ~| ~do(치약~))\\)\n사실, 일상 생활에서 항상 개입을 수행해서 그 관계/효과를 이해: 어떻게(How) 하면 두통이 사라질까?\n\n\n\n\n\n두통이 사라졌다면 왜(Why) 그럴까?\n\n약을 먹지 않았어도 두통이 사라졌을까?: 가상의 세계 (counterfactual world)\n\n나는 왜 탈락했을까? 내가 여자라면 합격했을까?\n“현재 치약을 구매한 고객이 가격을 두 배로 올려도 여전히 치약을 구매할 확률은 얼마인가?”\n\n우리는 현실 세계(고객이 현재 가격으로 치약을 구매했다는 것을 알고 있는)와 가상의 세계(가격이 두 배 높은 경우)와 비교\n\n보이는 세계  볼 수 있는 새로운 세계  볼 수 없는 세계(보이는 것과 모순)\n이를 위해서는 “이론” 또는 “자연의 법칙”이라고 볼 수 있는 근본적인 인과 과정의 모델이 필요\n\n\n\n\n\n\n\nTreatment Effect\n\n\n\n\nIndividual Treatment Effect: \\(\\tau_i \\equiv Y_i(1) - Y_i(0)\\): the fundamental problem of causal inference\nAverage Treatment Effect: \\(\\tau \\equiv E[Y_i(1) - Y_i(0)] = E[Y_i(1)] - E[Y_i(0)]\\): 여러 관측치(데이터)를 이용해 (노이즈를 제거하면서) 최적의 인과효과를 추정\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(T\\)\n\\(Y\\)\n\\(Y(1)\\)\n\\(Y(0)\\)\n\\(Y(1) - Y(0)\\)\n\n\n\n\n1\n0\n0\n\n0\n?\n\n\n2\n1\n1\n1\n\n?\n\n\n3\n1\n0\n0\n\n?\n\n\n4\n0\n1\n\n1\n?\n\n\n…\n…\n…\n…\n…\n?\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n인과 모델은 실제 개입(intervention) 없이도 가상의 상황에서 어떻게 되리라고 예측할 수 있는 추론(reasoning)의 능력을 가능하게 함: deep understanding\n\n달이 사라진다면?\n경험/관찰한 적이 없는 상황에 대해서도 예측할 수 있음\n유연하고 적응적인 행동을 가능하게 함\n\nML에서도, 인과 모델이 있다면 작은 데이터로 학습해도 빠르게 올바른 예측을 할 수 있음.\n사람이 몇 번의 경험으로만으로 운전, 요리 등을 학습할 수 있는 이유도 비슷하다고 볼 수 있음.\n\n\n전형적인 인과적 질문들\n\n\n\nHow effective is a given treatment in preventing a disease?\nWas it the new tax break that caused our sales to go up? Or our marketing campaign?\nWhat is the annual health-care costs attributed to obesity?\nCan hiring records prove an employer guilty of sex discrimination?\nI am about to quit my job, will I regret it?\n\n\n\n특정 치료법이 질병 예방에 얼마나 효과적일까요?\n새로운 세금 감면 혜택이 매출 상승의 원인이었을까요? 아니면 마케팅 캠페인 때문이었나요?\n비만으로 인한 연간 의료 비용은 얼마인가요?\n채용 기록으로 고용주의 성차별을 입증할 수 있나요?\n직장을 그만두려고 하는데 후회하게 될까요?\n\n\n\n\n\n\n\n개입(intervention)의 과학을 위한 인과를 표현하기 위한 새로운 수학적 언어가 요구됨!\n\n\n\n관찰(seeing)과 개입(doing)에 대한 대수학(algebra)\n\n\n\n\n\n\n\n관찰(seeing)\n\n개입(doing)\n\n\n\n\n잔디가 젖었다는 것을 보았을 때(see) 비가 내렸을 가능성은 얼마인가?\n\n잔디를 젖게 하면(doing) 비가 내릴 가능성은 얼마인가?\n\n\n\\(P(~rain~ | ~wet~)\\)\n\n\\(P( ~rain~ | ~do(wet)~)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n인과 효과: 개입했을 때 어떤 변화가 나타나는가? (why → how)\n\n\n\n\n물리학의 법칙도 소위 “방정식”으로 표현되었음; 방정식의 대칭성으로 인과적 모형으로 표현되지 못함\n\n인과의 문제를 현상 간의 관계에 대한 법칙으로 전개(why → how)\n\n\\(F = m*a\\)\n\\(a = \\frac{F}{m}\\)\n\\(m = \\frac{F}{a}\\)\n\n\n통계학은 확률이라는 수학적 언어로 표현되어 개입의 효과를 표현할 방법이 없음\n\n“잔디가 젖어 있으면, 비가 왔다”와 “이 스프링클러를 켜면, 잔디가 젖을 것이다”라는 정보가 주어지면,\n컴퓨터는 “이 스프링클러를 켜면, 비가 왔다”라고 결론 내릴 것임.\n\n\n\n\n\n\nSource: Causality: Models, Reasoning, and Inference (2000) by Judea Pearl\n\n\n\n\n\n\n\n신발을 신고 잠든 다음날 두통이 생긴다면?\nFork:Common Cause\n\n\nSource: Introduction to Causal Inference (ICI) by Brady Neal\n\n\n미모가 뛰어나면 연기력이 떨어지는가?\nCollider: Common Effect \n\n\n\n\n\n\n\n\n\nConfounding\n\n\n\n일반적으로, 표면적으로 드러난 변수간의 관계가 숨겨진 다른 변수들(lurking third variables)의 영향으로 진실한/인과적 관계가 아닌 경우, confounding 혹은 confounder가 존재한다고 함.\n극단적이지만 이해하지 쉬운 예로는\n\n초등학생 발 사이즈 → 독해력?\n\n머리 길이 → 우울증?\n\n\nSimpson’s paradox\n아래 첫번째 그림은 집단 전체에 대한 플랏이고, 두번째 그림은 나이대별로 나누어 본 플랏\n전체 집단을 보면 운동을 많이 할수록 콜레스테롤이 증가하는 것으로 보이나,\n나이대별로 보면, 상식적으로 운동이 긍정적 효과가 나타남.\n왜 그렇게 나타나는가?\n\nSource: The book of why by Judea Pearl\n\n\n\n\n위에서 다룬 역사적 사건들을 참고\n그 외 흥미로운 예시들\nCOVID-27\nSource: Introduction to Causal Inference (ICI) by Brady Neal\n일종의 Simpon’s paradox\n\n\n\n\n\n\n중증/경증 상태(C)가 common cause로 작용; C를 고정함으로써 common cause에 의한 confounding을 제거.\n\n\n\n\n\n\n\n처치(T)가 상태(C)의 원인으로 작용; B의 치료효과가 더 뛰어나나 오랜 대기 시간으로 인한 부정적 효과(경로: T → C → Y)가 매우 커서 처치의 (순수한) 치료 효과(경로: T → Y)보다 컸던 것.\n\n\n\n\n\n\n코딩 기술이 뛰어나면 협업능력이 떨어지는가? (Collider Bias)\n어느 회사에서 지원자의 코딩 능력과 협업 능력을 1점부터 5점까지 정량화하여,\n총점 8점 이상을 받은 지원자를 모두 채용한다고 했을 때,\n\n\n\n\n\n\n수집된 데이터의 특성에 따라 인과추론을 방해하거나(confounding)\n일반화할 수 있는 대상의 범위가 제한됨\n\n노인에 관한 데이터: 누가 사망했는가?\n\nSurvival bias: 일종의 collider bias\n예를 들어, 비만이 사망율에 미치는 효과에 대한 과소추정\n\n\n의료 분야에서 발견되는 패러독스\n\n비만은 당뇨 환자의 생존에 이익이 되는가?\n\n과거 기록을 이용?; 수녀들의 자서전 연구\n\n추적조사/종단연구(longitudinal study)\n\n회사 구성원에 대한 조사: 근속년수에 따른 샘플 속성의 변화\n누가 참여(안)했는가? 어떤 방식으로 참여했는가?\n\n관측되지 않은 데이터: 어떤 사람/대상이 왜 누락되었는가?\n어떤 사람들이 설문/실험에 참여했는가? 혹은 어떤 문항에 응답했는가?/하지 않았는가?\n\n어떤 유저들의 데이터인가? 가령, SNS의 기록은 누가 남기는가?\n코호트/특정세대의 특성: 그들만의 특성인가?\n\n\n\nAbraham Wald: “Where are the missing holes?”\n\nSource: War History Online\n\n\n\n\n\n\n개입없이 수동적으로 얻은 관찰 데이터의 분석에서는 항상 confounding이 존재할 기능성이 있음\n결정적인 인과관계를 파악하기 위해, 전통적으로 “통계학”의 시각에서 인과문제를 해결하기 위해 RCT (randomized controlled trial)라고 부르는 소위 gold standard한 실험 연구를 통해서 해결하고자 했음\n개념적으로는 “물리적 통제”라고 볼 수 있음; vs. “통계적 통제”\n두 그룹으로 집단을 randomly assign(무선/무작위 배정/할당): 모든 면에서 동질한 성향을 가짐. 예를 들어, 두 집단의 연령이 평균적으로 동일해짐.\n분야마다 효과를 제대로 검증하기 위한 많은 실험 설계들이 발전되었음; 연구방법론\n\n \n예를 들어, 걷기가 사망율에 미치는 효과를 검증하려면, 가령 600명을 300명씩 두 그룹으로 무작위로 나눈 후 한쪽은 1마일 이하를 걷도록 하고 나머지는 2마일 이상을 걷게 한 후 12년 후 사망율을 확인해야 함.\n하지만, 실험 연구는 자체로 많은 한계를 지님\n\n많은 경우 실험이 불가능하거나 완전한 통제가 어려움\n실험에서 처치한 구체적인 상황에서만 유효하고; 어느 지형을 어느 속도로 누구와 어떻게 걸었는지에 대한 실험 통제하에서\n따라서 그 효과 또한 일반화되어 표현하기 어려움\n반대로, 덜 통제된 실험의 경우 어떤 요인의 효과인지 불분명\n완전한 통제를 할수록 더 인위적인 상황이 연출됨; 자연스러운/현실적인 상황에서 적용된다는 보장이 없음\n실험 참여자는 어떻게 왜 참여한 것인가?",
    "crumbs": [
      "Home",
      "Introduction",
      "Causal Inference"
    ]
  },
  {
    "objectID": "contents/causal-inference.html#인과-추론을-향한-고군분투",
    "href": "contents/causal-inference.html#인과-추론을-향한-고군분투",
    "title": "Causal Inference",
    "section": "",
    "text": "담배는 폐암의 원인인가?\n\n1950년 ~ 1964년 미국에서 의사, 역학자, 통계학자 사이의 큰 논쟁\n통계적으로 입증할 수 있는가? 실험군-대조군의 실험적 방법론 이외의 다른 방법이 있는가?\n\n교란(confounding) 변수가 전혀 존재하지 않는다는 것을 입증할 수 있는가?; 흡연 유전자, 흡연자의 라이프스타일, 흡연자의 주위 환경 등\n\nUS Surgeon General이 임명한 특별 자문위원회의 고민 (1964)\n\n\n위원회는 보고서를 위해 1년 이상 노력했고, 주요 문제 중 하나는 “원인”이라는 단어의 사용이었습니다. 위원회 위원들은 19세기의 결정론적 인과성 개념을 제쳐두어야 했고, 통계도 제쳐두어야 했습니다. (아마도 코크런이) 보고서에 적었듯이, “통계적 방법은 연관성에서 인과 관계를 증명할 수 없습니다. 연관성(association)의 인과적 유의성(causal significance)은 통계적 확률에 대한 진술을 넘어서는 판단의 문제입니다. 속성이나 요인과 질병 또는 건강에 미치는 영향 사이의 연관성의 인과적 중요성을 판단하거나 평가하려면 여러 가지 기준을 사용해야 하며, 그 중 어느 것도 판단의 충분한 근거가 될 수 없습니다.”\n\n\n\n“Statistical methods cannot establish proof of a causal relationship in an association. The causal significance of an association is a matter of judgment which goes beyond any statement of statistical probability. To judge or evaluate the causal significance of the association between the attribute or agent and the disease, or effect upon health, a number of criteria must be utilized, no one of which is an all-sufficient basis for judgment.”\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n\n임산부의 흡연은 저체중아의 생존에 이로운가?\nBirth-weight Paradox\n\n1960년대 중반, Jacob Yerushalmy의 논문\n\n이미 여러 연구에서 흡연자 아기가 출생 시 체중이 더 가볍다는 것이 밝혀졌음; 저체중은 영아사망율을 높임\n저체중아 중, 어머니가 흡연자인 경우 생존율이 더 높았음!\n\n논문이 발표된 지 40년이 넘은 2006년까지 만족스럽게 설명되지 않았음\n흡연자 대신 흑인으로 대체해도 같은 현상을 발견\n의학에서 비슷한 패러독스가 자주 발견됨; 예. 당뇨 환자의 경우 비만이 생존에 이득이 되는 것처럼 나타남.\n\n  \n\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n학과별로 공정해도 대학은 차별할 수 있을까?\nUC Berkeley Admission Paradox\n\n1973년, 대학원에 진학한 남성과 여성의 합격 비율이 각각 44%, 35%였음\n입학 결정은 학과별로 독립적으로 내렸음\n학과별로는 여성의 합격 비율이 더 높았음!; 남성에 대한 역차별이 나타났음\nPeter Bickel(버클리대의 통계학자)와 William Kruskal(시카고대의 통계학자) 사이의 논쟁(해결하지 못한채 2005년 사망)\n\n학과별로 나누어 살펴보는 것으로 충분한가?\n어느 주 출신인지에 따른 차별까지 존재한다면 학과별로 나눠보는 것은 더 큰 오류를 발생시킬 수 있음\n\n즉, 두 요인을 모두 고려하면 다시 여성에 대한 차별이 있는 것으로 결론 날 수 있음.\n\n\n무분별한 편견(bias)이 양산되는 방식에 대한 전형적인 예시로 볼 수 있음\n\n편견(bias): associational 개념 - 관찰된 연관성에 대한 인식\n\n편견/편향은 인지적 부하없이(깊이 생각할 필요없음) 빠르게 대응할 수 있는 인지체계로 생존에 적응적으로 여겨짐.\n\n동물들의 무의식적/진화적 혐오까지도 포함\n\n단, 편견에 대한 비판의 원인은\n\n분포를 고려하지 않는 이분적인 태도\n편의적으로 인과관계를 추론하는 경향\n\n\n차별(discrimination): causal 개념 - 변화가 발생되도록 개입\n\n\n\n\n\n\n\n\n\n\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n효과는 없었으나 성공적인 정책?\n\n1990년대 최악의 시카고 공립학교의 개혁 정책\n고1에서 보충 과목을 없애고, 대학 진학 준비 과목을 수강하도록 함.\n이 중 대수 1 과목(“Algebra for All”)의 경우 3년 간 유의한 성적 개선이 없었음.\nGuanglei Hong(시카고대 인간발달)은 정책의 직접적 효과는 존재한다고 판별했음!\n\n정책이 두 가지 방식으로 (다른 방향으로) 작용했음.\n이후 “Double-Dose Algebra”으로 개선했음.",
    "crumbs": [
      "Home",
      "Introduction",
      "Causal Inference"
    ]
  },
  {
    "objectID": "contents/causal-inference.html#간략한-역사적-배경",
    "href": "contents/causal-inference.html#간략한-역사적-배경",
    "title": "Causal Inference",
    "section": "",
    "text": "인과에 대한 철학적 논쟁들\n\n\n\n\n정의, 인식 가능성, 필연성\n다중 원인, 상호작용  \n시간적 순서성, 인과의 피트백 \n결정론적 vs. 확률론적\n\n\n\n\nThe Grammar of Science (1892), by Karl Pearson\n\n특정 시퀀스가 과거에 발생하고 반복되었다는 것은 경험의 문제이며, 인과라는 개념 안에서 그렇게 표현합니다. 미래에도 계속 반복될 것이라는 것은 신념의 문제이며, 확률이라는 개념 안에서 그렇게 표현합니다. 과학은 어떤 경우에도 시퀀스에 내재된 필연성을 입증할 수 없으며, 시퀀스가 반드시 반복되어야 한다는 것을 절대적으로 확실하게 증명할 수도 없습니다. 과거에 대한 과학은 묘사이고 미래에 대한 과학은 믿음입니다;\n\n\n\nThat a certain sequence has occurred and recurred in the past is a matter of experience to which we give expression in the concept causation; that it will continue to recur in the future is a matter of belief to which we give expression in the concept probability. Science in no case can demon­strate any inherent necessity in a sequence, nor prove with absolute certainty that it must be repeated. Science for the past is a description, for the future a belief; (Pearson, 1892, p. 113).\n\n\n\n인과 관계라는 개념은 현상에서 개념적 과정을 통해 추출된 것으로, 논리적 필연도 아니고 실제 경험도 아닙니다…. 우주에 대한 더 넓은 관점에서보면 모든 현상은 상관관계로서 보이지만, 인과적으로는 관계하지 않는 것으로 보입니다.\n\n\n\nthe idea of causation is extracted by conceptual processes from phenomena, it is neither a logical necessity, nor an actual experience…. The wider view of the universe sees all phenomena as correlated, but not causally related. (Pearson, 1892, p. 177)\n\n\n\n\n피어슨은 인과관계를 분석의 언어에서 배제시키고, 이후 그 전통이 통계학에서 어어짐\nPositivism(실증주의) \n\n인과관계에서 “힘”, “필연성” 등과 같은 관찰 불가능한 형이상학적 요소를 제거하고, 오직 관찰 가능한 현상 간의 관계만을 인정\n오직 관찰된 패턴만을 반영할 수 있으며, 반복적인 규칙성(regularity of succession)을 통해 상관관계(correlation/association)로 설명될 수 있음\n경험주의 철학의 대표격인 흄(David Hume)의 철학적 영향을 받음: 인과관계를 “항상적 연접(constant conjunction)”으로 이해. 즉, 인과는 두 사건 사이의 필연적 연결이 아니라 단지 한 사건이 다른 사건 뒤에 규칙적으로 발생하는 패턴의 (심리적, 인지적) 구성물\n\n그렇다면, 과학에서 인과문제는?\n\nwhy의 문제를 how의 문제로 대수적 방정식/함수관계를 통해 대체했음\n\n가령, F=ma에서 1kg의 정지된 물체를 30m/s의 속도까지 10초 안에 가속시키려면 어떻게 해야 하는가(얼마의 힘이 필요한가)?\n\n콩트(Comte)는 갈릴레이와 마찬가지로 형이상학적 질문에서 탈피하고 현상들의 (일반적) 관계에 대해서만 초점을 맞추는 것이 과학 혁명이 성공할 수 있는 본질이라고 봄.\n극단적으로 why의 문제는 how의 문제로 귀속된다고까지 주장함; 정확한 말을 아니지만 why의 본질을 담고 있음.\n\n\n\n\n\n\n\n\nPositive Philosophy\n\n\n\n인간 진보의 과정이 신학적 상태 (theological state) &gt; 형이상학적 상태 (metaphysical state) &gt; 실증적 상태 (positive state)로 나아간다는 논의에서 최종 상태인 실증적 상태에 대하여…\n\n최종적인 실증적 상태에서, 마음은 절대적 개념, 우주의 기원과 목적지, 그리고 현상의 원인에 대한 헛된 탐구를 포기하고, 그것들의 법칙, 즉 그들의 불변적 연속 및 유사성 관계(invariable relations of succession and resemblance)의 연구에 전념합니다. 적절히 결합된 추론과 관찰이 이러한 지식의 수단입니다. 현재 우리가 사실의 설명(explanation)에 대해 언급할 때 이해되는 것은 단순히 개별 현상과 일반적 사실들 사이의 연결을 확립하는 것이며…\n\n\n무게(weight)와 인력(attraction)이 무엇인지에 대해서는, 우리는 그것과는 아무 상관이 없습니다. 왜냐하면 그것은 전혀 지식의 문제가 아니기 때문입니다. 신학자들과 형이상학자들은 그러한 질문들에 대해 상상하고 정제할 수 있습니다; 그러나 실증적 철학은 그것들을 거부합니다.\n\n\n푸리에는 열(heat)에 관한 그의 훌륭한 일련의 연구에서, 그의 선배들이 열성 물질과 보편적 에테르의 작용에 대해 논쟁했을 때와는 달리, 한 번도 그 본질에 대해 질문하지 않고 열 현상의 가장 중요하고 정확한 법칙들과 많은 크고 새로운 진리들을 우리에게 제공했습니다.\n\nComte, Auguste. The positive philosophy of Auguste Comte. Blanchard, 1858.\n\n\n\nSewall Wright\n\n\n\nPath diagrams라는 데이터로부터 인과 관계에 대한 질문에 답하는 수학적 방법을 최초로 개발\n실제로 인과 분석의 툴로 이어지지는 못하였고,\nPath analysis(경로 분석)이라는 통계 기법으로 어어졌으나 뿌리 깊은 오해의 씨앗이 되었음.\n\n  \n\n기니피그의 털색에 영향을 미치는 요인을 설명하는 경로 다이어그램. D = 발달 요인(수태 후, 출생 전), E = 환경 요인(출생 후), G = 각 개별 부모의 유전적 요인, H = 부모 모두의 유전적 요인을 합친 것, O, O′ = 자손. 분석의 목적은 D, E, H의 영향력을 추정하는 것(다이어그램에서는 d, e, f로 표기)\n\n\n\n\n\n\nSewall Wright in 1954\nfrom Wikipedia\n\n\n\n\n\nPath Analysis\n\n\n\n\nPath Model\n\n\n\n\n\n\nLatent Variable Path Model\n\n\n\n\n\n\n\n\n\n\n\nLatent variables\n\n\n\n(다음은 수업 후 추가된 내용으로 참고만 하세요.)\n보통, 추상적인 개념적 단위인 구성개념(construct)를 수량화하기 위해 사용되는 변수를 잠재변수(latent variable)라고 함.\n관측된 변수가 아닌 이 잠재변수들로 모델링; \\(Y = f(X)\\)\n여러 개의 선형 모형이 나타남으로 동시에 여러 개의 \\(Y = f(X)\\) 형태의 방정식을 얻게 되고,\n이를 (잠재변수) 구조방정식모형((latent variable) structural equation model, SEM)이라고 함. (또는 latent variable path model)\n잠재변수의 예로 독해력이라는 구성개념을 수량화하고자 하는 경우,\n다음과 같이 독해력을 3가지 문항으로 측정하는 예를 생각해볼 수 있음.\n\n테스트 1: 참가자가 글을 읽고, 해당 글을 가장 잘 설명하는 그림을 4개의 선택지 중에서 선택.\n테스트 2: 참가자가 지시문을 읽고, 그 지시에 따라 행동을 실행 (예: “일어서서 테이블 주위를 걷고, 다시 앉으세요”).\n테스트 3: 참가자가 단어나 문장이 빠진 글을 읽고, 글의 의미를 바탕으로 빠진 단어나 문장을 채움.\n\n독해력과는 별개로 추가로 측정되는 능력이 다음과 같이 존재할 수 있음; unique variance\n\n테스트 1: 읽은 내용을 시각적 이미지로 변환하는 능력.\n테스트 2: 읽은 내용을 행동으로 옮기는 능력.\n테스트 3: 자신의 지식에서 가장 적합한 단어나 문장을 선택해 글에 삽입하는 능력.\n\n아래 그림에서 세 가지 독해력 검사에서 개인의 점수(test 1, test 2, test 3)는 “진정한 독해력”(구성개념), 오차(error), 각 검사의 고유한 특성에 의해 영향을 받는다는 가정을 나타냄.\n\n\n통계적으로 잠재 변수를 요인(factor)이라고도 부르며, 통계의 요인 분석(factor analysis)을 통해 얻어질 수 있음.\n이는 실제 측정된 변수들이 공통적으로 공유할 것으로 가정되는 요인을 추출하는 프로세스라고 볼 수 있으나,\n\n구성개념을 수치화하는 이런 방식에 대한 반론이 있으며, “구성 개념을 수량화”하기 위한 다른 대안들이 제안되고 있음\n\n가령, 사회경제적 지위(socioeconomic status, SES)와 같은 구성개념의 통계적 수량화에 의문이 제기되어 왔음.",
    "crumbs": [
      "Home",
      "Introduction",
      "Causal Inference"
    ]
  },
  {
    "objectID": "contents/causal-inference.html#causal-revolution",
    "href": "contents/causal-inference.html#causal-revolution",
    "title": "Causal Inference",
    "section": "",
    "text": "Source: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n사람들은 어떻게 인과성에 대한 지식을 얻게 되는가?\n어떤 경험 패턴이 이 연관성을 “인과적”이라고 확신시키는가?\n\n\n\n\n\n관찰을 기반으로 규칙성 발견하고 예측\n올빼미가 쥐의 움직임을 관찰하고 잠시 후 쥐가 어디에 있을지를 파악\n컴퓨터 바둑 프로그램이 수백만 개의 바둑 게임 데이터베이스를 연구하여 어떤 수와 승률이 높은지 알아내는 것\n하나의 이벤트를 관찰하면 다른 이벤트를 관찰할 가능성이 달라진다면, 하나의 이벤트가 다른 이벤트와 연관되어 있다고 말할 수 있음\n“치약을 구매한 고객이 치실도 구매할 가능성이 얼마나 되는가?”; \\(P(치실 ~| 치약~)\\)\n통계의 핵심: 상관관계, 회귀\n올빼미는 쥐가 왜 항상 A 지점에서 B 지점으로 가는지 이해하지 못해도 훌륭한 사냥꾼이 될 수 있음\n위스키 한 병을 들고 있는 보행자가 경적을 울릴 때 다르게 반응할 가능성이 있다는 것을 기계가 스스로 파악할 수 있는가?\n\nAssociation 단계의 한계: 유연성과 적응성의 부족\n\n\n\n\n\n\n관찰을 넘어, 세상에 대한 개입\n“치약 가격을 두 배로 올리면 치실 판매량은 어떻게 될까?”\n데이터에는 없는 새로운 종류의 지식을 요구\n통계학의 언어로는 이 질문을 표현하는 것조차 불충분함\n수동적으로 수집된 데이터만으로는 이러한 질문에 대답할 수 없음\n\n과거의 데이터를 이용하면?\n과거에 가격이 두 배 비쌌을 때의 치실 판매량으로 추론?\n이전에 가격이 두 배 비쌌을 때 다른 이유가 있었을 수 있음\n\n인과 관계를 파악하기 위해 전통적으로 실험을 통해 해결; 예. 테크 기업들의 실험들\n정확한 인과 관계 모델이 있으면 관찰 데이터만으로도 가능; \\(P(치실 ~| ~do(치약~))\\)\n사실, 일상 생활에서 항상 개입을 수행해서 그 관계/효과를 이해: 어떻게(How) 하면 두통이 사라질까?\n\n\n\n\n\n두통이 사라졌다면 왜(Why) 그럴까?\n\n약을 먹지 않았어도 두통이 사라졌을까?: 가상의 세계 (counterfactual world)\n\n나는 왜 탈락했을까? 내가 여자라면 합격했을까?\n“현재 치약을 구매한 고객이 가격을 두 배로 올려도 여전히 치약을 구매할 확률은 얼마인가?”\n\n우리는 현실 세계(고객이 현재 가격으로 치약을 구매했다는 것을 알고 있는)와 가상의 세계(가격이 두 배 높은 경우)와 비교\n\n보이는 세계  볼 수 있는 새로운 세계  볼 수 없는 세계(보이는 것과 모순)\n이를 위해서는 “이론” 또는 “자연의 법칙”이라고 볼 수 있는 근본적인 인과 과정의 모델이 필요\n\n\n\n\n\n\n\nTreatment Effect\n\n\n\n\nIndividual Treatment Effect: \\(\\tau_i \\equiv Y_i(1) - Y_i(0)\\): the fundamental problem of causal inference\nAverage Treatment Effect: \\(\\tau \\equiv E[Y_i(1) - Y_i(0)] = E[Y_i(1)] - E[Y_i(0)]\\): 여러 관측치(데이터)를 이용해 (노이즈를 제거하면서) 최적의 인과효과를 추정\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(T\\)\n\\(Y\\)\n\\(Y(1)\\)\n\\(Y(0)\\)\n\\(Y(1) - Y(0)\\)\n\n\n\n\n1\n0\n0\n\n0\n?\n\n\n2\n1\n1\n1\n\n?\n\n\n3\n1\n0\n0\n\n?\n\n\n4\n0\n1\n\n1\n?\n\n\n…\n…\n…\n…\n…\n?\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n인과 모델은 실제 개입(intervention) 없이도 가상의 상황에서 어떻게 되리라고 예측할 수 있는 추론(reasoning)의 능력을 가능하게 함: deep understanding\n\n달이 사라진다면?\n경험/관찰한 적이 없는 상황에 대해서도 예측할 수 있음\n유연하고 적응적인 행동을 가능하게 함\n\nML에서도, 인과 모델이 있다면 작은 데이터로 학습해도 빠르게 올바른 예측을 할 수 있음.\n사람이 몇 번의 경험으로만으로 운전, 요리 등을 학습할 수 있는 이유도 비슷하다고 볼 수 있음.\n\n\n전형적인 인과적 질문들\n\n\n\nHow effective is a given treatment in preventing a disease?\nWas it the new tax break that caused our sales to go up? Or our marketing campaign?\nWhat is the annual health-care costs attributed to obesity?\nCan hiring records prove an employer guilty of sex discrimination?\nI am about to quit my job, will I regret it?\n\n\n\n특정 치료법이 질병 예방에 얼마나 효과적일까요?\n새로운 세금 감면 혜택이 매출 상승의 원인이었을까요? 아니면 마케팅 캠페인 때문이었나요?\n비만으로 인한 연간 의료 비용은 얼마인가요?\n채용 기록으로 고용주의 성차별을 입증할 수 있나요?\n직장을 그만두려고 하는데 후회하게 될까요?",
    "crumbs": [
      "Home",
      "Introduction",
      "Causal Inference"
    ]
  },
  {
    "objectID": "contents/causal-inference.html#structural-causal-modelscm",
    "href": "contents/causal-inference.html#structural-causal-modelscm",
    "title": "Causal Inference",
    "section": "",
    "text": "개입(intervention)의 과학을 위한 인과를 표현하기 위한 새로운 수학적 언어가 요구됨!\n\n\n\n관찰(seeing)과 개입(doing)에 대한 대수학(algebra)\n\n\n\n\n\n\n\n관찰(seeing)\n\n개입(doing)\n\n\n\n\n잔디가 젖었다는 것을 보았을 때(see) 비가 내렸을 가능성은 얼마인가?\n\n잔디를 젖게 하면(doing) 비가 내릴 가능성은 얼마인가?\n\n\n\\(P(~rain~ | ~wet~)\\)\n\n\\(P( ~rain~ | ~do(wet)~)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n인과 효과: 개입했을 때 어떤 변화가 나타나는가? (why → how)\n\n\n\n\n물리학의 법칙도 소위 “방정식”으로 표현되었음; 방정식의 대칭성으로 인과적 모형으로 표현되지 못함\n\n인과의 문제를 현상 간의 관계에 대한 법칙으로 전개(why → how)\n\n\\(F = m*a\\)\n\\(a = \\frac{F}{m}\\)\n\\(m = \\frac{F}{a}\\)\n\n\n통계학은 확률이라는 수학적 언어로 표현되어 개입의 효과를 표현할 방법이 없음\n\n“잔디가 젖어 있으면, 비가 왔다”와 “이 스프링클러를 켜면, 잔디가 젖을 것이다”라는 정보가 주어지면,\n컴퓨터는 “이 스프링클러를 켜면, 비가 왔다”라고 결론 내릴 것임.\n\n\n\n\n\n\nSource: Causality: Models, Reasoning, and Inference (2000) by Judea Pearl",
    "crumbs": [
      "Home",
      "Introduction",
      "Causal Inference"
    ]
  },
  {
    "objectID": "contents/causal-inference.html#confounding",
    "href": "contents/causal-inference.html#confounding",
    "title": "Causal Inference",
    "section": "",
    "text": "신발을 신고 잠든 다음날 두통이 생긴다면?\nFork:Common Cause\n\n\nSource: Introduction to Causal Inference (ICI) by Brady Neal\n\n\n미모가 뛰어나면 연기력이 떨어지는가?\nCollider: Common Effect \n\n\n\n\n\n\n\n\n\nConfounding\n\n\n\n일반적으로, 표면적으로 드러난 변수간의 관계가 숨겨진 다른 변수들(lurking third variables)의 영향으로 진실한/인과적 관계가 아닌 경우, confounding 혹은 confounder가 존재한다고 함.\n극단적이지만 이해하지 쉬운 예로는\n\n초등학생 발 사이즈 → 독해력?\n\n머리 길이 → 우울증?\n\n\nSimpson’s paradox\n아래 첫번째 그림은 집단 전체에 대한 플랏이고, 두번째 그림은 나이대별로 나누어 본 플랏\n전체 집단을 보면 운동을 많이 할수록 콜레스테롤이 증가하는 것으로 보이나,\n나이대별로 보면, 상식적으로 운동이 긍정적 효과가 나타남.\n왜 그렇게 나타나는가?\n\nSource: The book of why by Judea Pearl\n\n\n\n\n위에서 다룬 역사적 사건들을 참고\n그 외 흥미로운 예시들\nCOVID-27\nSource: Introduction to Causal Inference (ICI) by Brady Neal\n일종의 Simpon’s paradox\n\n\n\n\n\n\n중증/경증 상태(C)가 common cause로 작용; C를 고정함으로써 common cause에 의한 confounding을 제거.\n\n\n\n\n\n\n\n처치(T)가 상태(C)의 원인으로 작용; B의 치료효과가 더 뛰어나나 오랜 대기 시간으로 인한 부정적 효과(경로: T → C → Y)가 매우 커서 처치의 (순수한) 치료 효과(경로: T → Y)보다 컸던 것.\n\n\n\n\n\n\n코딩 기술이 뛰어나면 협업능력이 떨어지는가? (Collider Bias)\n어느 회사에서 지원자의 코딩 능력과 협업 능력을 1점부터 5점까지 정량화하여,\n총점 8점 이상을 받은 지원자를 모두 채용한다고 했을 때,",
    "crumbs": [
      "Home",
      "Introduction",
      "Causal Inference"
    ]
  },
  {
    "objectID": "contents/causal-inference.html#selection-bias",
    "href": "contents/causal-inference.html#selection-bias",
    "title": "Causal Inference",
    "section": "",
    "text": "수집된 데이터의 특성에 따라 인과추론을 방해하거나(confounding)\n일반화할 수 있는 대상의 범위가 제한됨\n\n노인에 관한 데이터: 누가 사망했는가?\n\nSurvival bias: 일종의 collider bias\n예를 들어, 비만이 사망율에 미치는 효과에 대한 과소추정\n\n\n의료 분야에서 발견되는 패러독스\n\n비만은 당뇨 환자의 생존에 이익이 되는가?\n\n과거 기록을 이용?; 수녀들의 자서전 연구\n\n추적조사/종단연구(longitudinal study)\n\n회사 구성원에 대한 조사: 근속년수에 따른 샘플 속성의 변화\n누가 참여(안)했는가? 어떤 방식으로 참여했는가?\n\n관측되지 않은 데이터: 어떤 사람/대상이 왜 누락되었는가?\n어떤 사람들이 설문/실험에 참여했는가? 혹은 어떤 문항에 응답했는가?/하지 않았는가?\n\n어떤 유저들의 데이터인가? 가령, SNS의 기록은 누가 남기는가?\n코호트/특정세대의 특성: 그들만의 특성인가?\n\n\n\nAbraham Wald: “Where are the missing holes?”\n\nSource: War History Online",
    "crumbs": [
      "Home",
      "Introduction",
      "Causal Inference"
    ]
  },
  {
    "objectID": "contents/causal-inference.html#experiments",
    "href": "contents/causal-inference.html#experiments",
    "title": "Causal Inference",
    "section": "",
    "text": "개입없이 수동적으로 얻은 관찰 데이터의 분석에서는 항상 confounding이 존재할 기능성이 있음\n결정적인 인과관계를 파악하기 위해, 전통적으로 “통계학”의 시각에서 인과문제를 해결하기 위해 RCT (randomized controlled trial)라고 부르는 소위 gold standard한 실험 연구를 통해서 해결하고자 했음\n개념적으로는 “물리적 통제”라고 볼 수 있음; vs. “통계적 통제”\n두 그룹으로 집단을 randomly assign(무선/무작위 배정/할당): 모든 면에서 동질한 성향을 가짐. 예를 들어, 두 집단의 연령이 평균적으로 동일해짐.\n분야마다 효과를 제대로 검증하기 위한 많은 실험 설계들이 발전되었음; 연구방법론\n\n \n예를 들어, 걷기가 사망율에 미치는 효과를 검증하려면, 가령 600명을 300명씩 두 그룹으로 무작위로 나눈 후 한쪽은 1마일 이하를 걷도록 하고 나머지는 2마일 이상을 걷게 한 후 12년 후 사망율을 확인해야 함.\n하지만, 실험 연구는 자체로 많은 한계를 지님\n\n많은 경우 실험이 불가능하거나 완전한 통제가 어려움\n실험에서 처치한 구체적인 상황에서만 유효하고; 어느 지형을 어느 속도로 누구와 어떻게 걸었는지에 대한 실험 통제하에서\n따라서 그 효과 또한 일반화되어 표현하기 어려움\n반대로, 덜 통제된 실험의 경우 어떤 요인의 효과인지 불분명\n완전한 통제를 할수록 더 인위적인 상황이 연출됨; 자연스러운/현실적인 상황에서 적용된다는 보장이 없음\n실험 참여자는 어떻게 왜 참여한 것인가?",
    "crumbs": [
      "Home",
      "Introduction",
      "Causal Inference"
    ]
  },
  {
    "objectID": "contents/misc.html",
    "href": "contents/misc.html",
    "title": "MISC.",
    "section": "",
    "text": "Human-centered AI?\nSuicide rates for girls are rising. Are smartphones to blame?\n\n\nHow Opioid Overdoses Reached Crisis Levels\n\nOverweight & Obesity Statistics - NIDDK\n\nLack of sex is driven maily by the young"
  },
  {
    "objectID": "contents/pollr.html",
    "href": "contents/pollr.html",
    "title": "설문",
    "section": "",
    "text": "설문 링크"
  },
  {
    "objectID": "contents/setup.html",
    "href": "contents/setup.html",
    "title": "Python 설정",
    "section": "",
    "text": "데이터 사이언스를 위한 Python 개발 환경\n몇 가지 선택지",
    "crumbs": [
      "Home",
      "Python Basics",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#클라우드-환경",
    "href": "contents/setup.html#클라우드-환경",
    "title": "Python 설정",
    "section": "클라우드 환경",
    "text": "클라우드 환경\nColab\n\n사용법: Colab Welcome\n클라우드 환경 vs. 구글 드라이브 mount\nColab AI assistant\n구글 드라이브의 데이터셋을 import:\n\n\npd.read_csv(\"drive/MyDrive/...\")\n\n\n패키지 업데이트\n\n!pip install --upgrade pandas numpy seaborn matplotlib statsmodels scikit-learn",
    "crumbs": [
      "Home",
      "Python Basics",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#로컬-환경",
    "href": "contents/setup.html#로컬-환경",
    "title": "Python 설정",
    "section": "로컬 환경",
    "text": "로컬 환경\nPython과 Conda Package Manager\nConda Cheatsheet: 기본적인 conda 명령어 요약\n\nMiniconda 설치\nAnaconda보다는 기본 패키지들이 미리 설치되지 않는 miniconda를 추천: miniconda install page\n\nWindows 경우: 설치시 물어보는 “add Miniconda to your PATH variable” 옵션을 켜고 설치할 것\n\nShell을 통해 작업하는데\n\nWindows 경우: Anaconda의 응용 프로그램으로 등록된 Anaconda Powershell Prompt를 이용\nMac의 경우: 기본 terminal을 이용\n커서 앞에 (base)가 보이면 conda가 설치된 것\n\n\n\n\n\n\n\nShell(Command Line Interface)에 대한 팁\n\n\n\n\n\nMac의 경우: 기본 terminal을 이용하되 기본 zsh shell 대신 다음 Oh-My-Zsh을 추천\nOh-My-Zsh!: 링크\nWindows의 경우: Windows Terminal 추천\n\n설치 링크는 구글링…\n명령프롬프트(CMD) vs. Powershell\nPowershell에서 conda를 사용하기 위해서는 몇 가지 설정 필요: 블로그 링크\n잘 안될 경우, conda 설치시 함께 설치되는 응용프로그램 콘다 powershell을 이용\n\n\n\n\n# Terminal (Mac) or Miniconda Powershell Prompt (Windows)\n\n(base)&gt; conda info # 콘다 정보 \n(base)&gt; conda update conda # 콘다 업데이트\n\n\nConda Environment\nconda/user guide\n환경 생성: miniconda에서 자체 제공하는 환경 (다른 가상환경 툴인 pyenv나 venv도 있음)\n(base)&gt; conda create --name myenv  # --name 대신 -n으로 축약 가능\n\n# 특정 버전의 파이썬과 함께 설치시\n(base)&gt; conda create --name myenv python=3.12\n환경 확인\n(base)&gt; conda env list\n# conda environments:\n#\n# base         */.../miniconda3\n# myenv         /.../miniconda3/envs/myenv\n환경 제거\n(base)&gt; conda env remove --name myenv\n환경 activate/deactivate\n(base)&gt; conda activate myenv\n(myenv)&gt; conda deactivate\n특정 환경 안의 파이썬 버전 확인\n(myenv)&gt; python --version\n\n\n환경(activated) 내에서 패키지 설치 및 제거\n\n\n\n\n\n\n패키지 repository(channel) 선택\n\n\n\n\n\nconda/managing channels\n다음을 통해 .condarc 환경파일에 configuration 추가\n(base)&gt; conda config --add channels conda-forge\n(base)&gt; conda config --set channel_priority strict  # 채널 순으로 검색, 버전 순이 아니고\n# 개별적으로 채널을 선택해서 install하려면 (특정 환경에 설치하려면 아래 conda environment 참조)\n(base)&gt; conda install scipy --channel conda-forge\n\n# pakcage가 있는 채널들\n(base)&gt; conda search scipy\n\n\n\n# 특정 환경을 activate한 후\n\n# Python을 update하거나 다른 버전을 설치하려면, 가령 3.12으로 업데이트 하려면\n(myenv)&gt; conda install python=3.12  # python update\n\n# 패키지 설치\n(myenv)&gt; conda install &lt;package name1&gt; &lt;package name2&gt; ...\n# 특정한 채널, conda-forge 통한 설치: --channel 대신 -c로 축약 가능\n(myenv)&gt; conda install --channel conda-forge &lt;package name&gt;\n\n# 제거\n(myenv)&gt; conda remove &lt;package name1&gt; &lt;package name2&gt; ...\n\n# 업데이트\n(myenv)&gt; conda update &lt;package name1&gt; &lt;package name2&gt; ...\n(myenv)&gt; conda update --all  # all packages\n\n# 패키지 리스트 확인\n(myenv)&gt; conda list\n환경 밖에서 특정 환경 안에 설치하려면 환경 이름 추가\n(base)&gt; conda install --name myenv &lt;package name1&gt;  # --name 대신 -n으로 축약 가능\npip을 이용한 패키지 설치: conda repository에 없는 패키지들을 설치하는 경우. 충돌의 우려 있음\n(myenv)&gt; pip install &lt;package name1&gt; &lt;package name2&gt; ...\n수업에 필요한 기본 패키지 설치\n# 수업에 필요한 기본 패키지 설치 (conda-forge 채널 선택)\n(myenv)&gt; conda install --channel conda-forge jupyter numpy pandas matplotlib seaborn scikit-learn statsmodels",
    "crumbs": [
      "Home",
      "Python Basics",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#sec-vscode",
    "href": "contents/setup.html#sec-vscode",
    "title": "Python 설정",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\nVS Code 설치\n\n개인마다 선호하는 text editor가 있으나 본 수업에서는 VS Code로 진행: download and install here\n\n\n\nExtensions\n\nPython\nPython Extension Pack 중\n\nIntelliCode\nPython Environment Manager\n\nJupyter\nPylance: 문법 체크, 자동완성, …\nDocs View\n\n안 보일시, 설정에서 language server를 default(Pylance)에서 Jedi로 바꾸면 해결\n\nCopilot…\n\n\n\nPreferences\n\nThemes\nFont, font size (notebook, markup, output)\n\n\n\nShortcuts\nShow Command Palette: ctrl(cmd) + shift + p, 또는 F1\nCell 안과 밖에서 다르게 작동\n\nundo / redo : ctrl(cmd) + z / ctrl(cmd) + shift + z\nmove: alt(option) + arrow up/down\ncopy : alt(option) + shift + arrow up/down\n\n코드 실행 방식 3가지: ctrl/shift/alt(option) + enter\nHelp: Keyboard shortcuts reference의 Basic editing 참고\n\n\n그 외\n\ninteractive mode\nexport\ndocs view: sticky mode\nvariables viewer, data viewer\nformatter: “Black formatter”\nsnippets: 구글링…\n\n\n\nVS Code내에서 terminal 사용\nTerminal: Select Default Profile에서 선택\n\nMac: zsh\nWindows: powershell",
    "crumbs": [
      "Home",
      "Python Basics",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#jupyter-notebooklab",
    "href": "contents/setup.html#jupyter-notebooklab",
    "title": "Python 설정",
    "section": "Jupyter Notebook/Lab",
    "text": "Jupyter Notebook/Lab\n\n\n\n\n\n\n콘다 환경 등록\n\n\n\n\n\n새로 만든 환경을 등록해줘야 함. 환경을 activate한 상태에서\n(myenv)&gt; ipython kernel install --user --name=myenv\n환경을 삭제해도 등록시킨 kernel 이름은 삭제되지 않으니 직접 삭제.\n등록된 커널 리스트를 확인\n(myenv)&gt; jupyter kernelspec list\n커널 삭제\n(myenv)&gt; jupyter kernelspec remove myenv\n\n\n\nJupyter Notebook 또는 lab 실행\n\nAnaconda 응용 프로그램을 이용해 실행하거나,\n쉘에서 실행하려면,\n\n# jupytet notebook\n(base)&gt; jupyter notebook\n\n# jupyter lab\n(base)&gt; jupyter lab\n등록한 커널을 선택 후 시작\n커널을 종료하려면, 쉘에서 Ctrl-C 두 번",
    "crumbs": [
      "Home",
      "Python Basics",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#python-packages-modules-functions",
    "href": "contents/setup.html#python-packages-modules-functions",
    "title": "Python 설정",
    "section": "Python Packages, Modules, Functions",
    "text": "Python Packages, Modules, Functions\nJupyter notebook 파일을 생성: filename.ipynb\n\nimport numpy as np\nimport pandas as pd\nfrom numpy.linalg import inv\n\n\n\n\n\n\n\n두 개 이상의 함수/모듈을 import하거나 as로 별칭 지정 가능\n\n\n\n\n\nfrom numpy.linalg import inv as inverse\nfrom numpy.linalg import inv, det\nfrom numpy.linalg import inv as inverse, det as determinant\n\n\n\nnp.linalg?  # 함수에 대한 도움말\nNumPy 패키지(package)의 linalg 모듈(module)\nnp.linalg  # linalg.py 파이썬 스트립트 파일\nlinalg 모듈 파일 안에 def으로 정의된 함수\n\nnp.linalg.inv  # 모듈 안에서 def으로 정의된 함수 inv()\n\nnp.linalg.inv?  # 함수에 대한 도움말\n예를 들어, 행렬의 역행렬을 구하는 함수 inv를 사용하려면\n\nrng = np.random.default_rng(123)  # random number generator\nx = rng.standard_normal((3, 3))  # 3x3 matrix from standard normal distribution\nx\n\n어떻게 import하느냐에 따라 다른 방식으로 사용\n\ninv(x)  # inverse matrix of x\n\n# inv 함수를 따로 import하지 않은 경우, numpy의 linalg 모듈을 통해 사용\nnp.linalg.inv(x)  # same as above\n\n주로 모듈 이름을 함께 쓰는 것이 관례인데,\n\n이는 코드의 가독성을 높이고,\n사용자 정의 함수와의 충돌을 방지하기 위함\n\n모듈 안에 정의된 함수들을 확인하려면: dir() 함수\n\ndir(np.linalg)  # 모듈 안에 정의된 함수들",
    "crumbs": [
      "Home",
      "Python Basics",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html",
    "href": "contents/two_cultures/two_cultures.html",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "",
    "text": "Statistical Science\n2001, Vol. 16, No. 3, 199–231"
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#introduction",
    "href": "contents/two_cultures/two_cultures.html#introduction",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "1. INTRODUCTION",
    "text": "1. INTRODUCTION\nStatistics starts with data. Think of the data as being generated by a black box in which a vector of input variables \\(\\mathbf{x}\\) (independent variables) go in one side, and on the other side the response variables \\(\\mathbf{y}\\) come out. Inside the black box, nature functions to associate the predictor variables with the response variables, so the picture is like this:\n\nThere are two goals in analyzing the data:\nPrediction. To be able to predict what the responses are going to be to future input variables;\nInformation. To extract some information about how nature is associating the response variables to the input variables.\nThere are two different approaches toward these goals:\n\nThe Data Modeling Culture\nThe analysis in this culture starts with assuming a stochastic data model for the inside of the black box. For example, a common data model is that data are generated by independent draws from\nresponse variables = \\(f\\)(predictor variables, random noise, parameters)\nThe values of the parameters are estimated from the data and the model then used for information and/or prediction. Thus the black box is filled in like this:\n\nModel validation. Yes–no using goodness-of-fit tests and residual examination.\nEstimated culture population. 98% of all statisticians.\n\n\nThe Algorithmic Modeling Culture\nThe analysis in this culture considers the inside of the box complex and unknown. Their approach is to find a function \\(f(\\mathbf{x})\\)—an algorithm that operates on \\(\\mathbf{x}\\) to predict the responses \\(\\mathbf{y}\\). Their black box looks like this:\n\nModel validation. Measured by predictive accuracy.\nEstimated culture population. 2% of statisticians, many in other fields.\nIn this paper I will argue that the focus in the statistical community on data models has:\n\nLed to irrelevant theoryand questionable scientific conclusions;\nKept statisticians from using more suitable algorithmic models;\nPrevented statisticians from working on exciting new problems;\n\nI will also review some of the interesting new developments in algorithmic modeling in machine learning and look at applications to three data sets."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#road-map",
    "href": "contents/two_cultures/two_cultures.html#road-map",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "2. ROAD MAP",
    "text": "2. ROAD MAP\nIt may be revealing to understand how I became a member of the small second culture. After a seven-year stint as an academic probabilist, I resigned and went into full-time free-lance consulting. After thirteen years of consulting I joined the Berkeley Statistics Department in 1980 and have been there since. My experiences as a consultant formed my views about algorithmic modeling. Section 3 describes two of the projects I worked on. These are given to show how my views grew from such problems.\nWhen I returned to the university and began reading statistical journals, the research was distant from what I had done as a consultant. All articles begin and end with data models. My observations about published theoretical research in statistics are in Section 4.\nData modeling has given the statistics field many successes in analyzing data and getting information about the mechanisms producing the data. But there is also misuse leading to questionable conclusions about the underlying mechanism. This is reviewed in Section 5. Following that is a discussion (Section 6) of how the commitment to data modeling has prevented statisticians from entering new scientific and commercial fields where the data being gathered is not suitable for analysis by data models.\nIn the past fifteen years, the growth in algorithmic modeling applications and methodology has been rapid. It has occurred largely outside statistics in a new community—often called machine learning—that is mostly young computer scientists (Section 7). The advances, particularly over the last five years, have been startling. Three of the most important changes in perception to be learned from these advances are described in Sections 8, 9, and 10, and are associated with the following names:\n\nRashomon: the multiplicity of good models;\nOccam: the conflict between simplicity and accuracy;\nBellman: dimensionality—curse or blessing?\n\nSection 11 is titled “Information from a Black Box” and is important in showing that an algorithmic model can produce more and more reliable information about the structure of the relationship between inputs and outputs than data models. This is illustrated using two medical data sets and a genetic data set. A glossary at the end of the paper explains terms that not all statisticians may be familiar with."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#projects-in-consulting",
    "href": "contents/two_cultures/two_cultures.html#projects-in-consulting",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "3. PROJECTS IN CONSULTING",
    "text": "3. PROJECTS IN CONSULTING\nAs a consultant I designed and helped supervise surveys for the Environmental Protection Agency (EPA) and the state and federal court systems. Controlled experiments were designed for the EPA, and I analyzed traffic data for the U.S. Department of Transportation and the California Transportation Department. Most of all, I worked on a diverse set of prediction projects. Here are some examples:\n\nPredicting next-day ozone levels.\nUsing mass spectra to identify halogen-containing compounds.\nPredicting the class of a ship from high altitude radar returns.\nUsing sonar returns to predict the class of a submarine.\nIdentity of hand-sent Morse Code.\nToxicity of chemicals.\nOn-line prediction of the cause of a freeway traffic breakdown.\nSpeech recognition.\nThe sources of delay in criminal trials in state court systems.\n\nTo understand the nature of these problems and the approaches taken to solve them, I give a fuller description of the first two on the list.\n\n3.1 The Ozone Project\nIn the mid- to late 1960s ozone levels became a serious health problem in the Los Angeles Basin. Three different alert levels were established. At the highest, all government workers were directed not to drive to work, children were kept off playgrounds and outdoor exercise was discouraged.\nThe major source of ozone at that time was automobile tailpipe emissions. These rose into the low atmosphere and were trapped there by an inversion layer. A complex chemical reaction, aided by sunlight, cooked away and produced ozone two to three hours after the morning commute hours. The alert warnings were issued in the morning, but would be more effective if they could be issued 12 hours in advance. In the mid-1970s, the EPA funded a large effort to see if ozone levels could be accurately predicted 12 hours in advance.\nCommuting patterns in the Los Angeles Basin are regular, with the total variation in any given daylight hour varying only a few percent from one weekday to another. With the total amount of emissions about constant, the resulting ozone levels depend on the meteorology of the preceding days. A large data base was assembled consisting of lower and upper air measurements at U.S. weather stations as far away as Oregon and Arizona, together with hourly readings of surface temperature, humidity, and wind speed at the dozens of air pollution stations in the Basin and nearby areas.\nAltogether, there were daily and hourly readings of over 450 meteorological variables for a period of seven years, with corresponding hourly values of ozone and other pollutants in the Basin. Let \\(\\mathbf{x}\\) be the predictor vector of meteorological variables on the \\(n\\)th day. There are more than 450 variables in \\(\\mathbf{x}\\) since information several days back is included. Let \\(y\\) be the ozone level on the \\((n + 1)\\)st day. Then the problem was to construct a function \\(f(\\mathbf{x})\\) such that for any future day and future predictor variables \\(\\mathbf{x}\\) for that day, \\(f(\\mathbf{x})\\) is an accurate predictor of the next day’s ozone level \\(y\\).\nTo estimate predictive accuracy, the first five years of data were used as the training set. The last two years were set aside as a test set. The algorithmic modeling methods available in the pre-1980s decades seem primitive now. In this project large linear regressions were run, followed by variable selection. Quadratic terms in, and interactions among, the retained variables were added and variable selection used again to prune the equations. In the end, the project was a failure—the false alarm rate of the final predictor was too high. I have regrets that this project can’t be revisited with the tools available today.\n\n\n3.2 The Chlorine Project\nThe EPA samples thousands of compounds a year and tries to determine their potential toxicity. In the mid-1970s, the standard procedure was to measure the mass spectra of the compound and to try to determine its chemical structure from its mass spectra.\nMeasuring the mass spectra is fast and cheap. But the determination of chemical structure from the mass spectra requires a painstaking examination by a trained chemist. The cost and availability of enough chemists to analyze all of the mass spectra produced daunted the EPA. Many toxic compounds contain halogens. So the EPA funded a project to determine if the presence of chlorine in a compound could be reliably predicted from its mass spectra.\nMass spectra are produced by bombarding the compound with ions in the presence of a magnetic field. The molecules of the compound split and the lighter fragments are bent more by the magnetic field than the heavier. Then the fragments hit an absorbing strip, with the position of the fragment on the strip determined by the molecular weight of the fragment. The intensity of the exposure at that position measures the frequency of the fragment. The resultant mass spectra has numbers reflecting frequencies of fragments from molecular weight 1 up to the molecular weight of the original compound. The peaks correspond to frequent fragments and there are many zeroes. The available data base consisted of the known chemical structure and mass spectra of 30,000 compounds.\nThe mass spectrum predictor vector \\(\\mathbf{x}\\) is of variable dimensionality. Molecular weight in the data base varied from 30 to over 10,000. The variable to be predicted is\n\\(y = 1\\): contains chlorine,\n\\(y = 2\\): does not contain chlorine.\nThe problem is to construct a function \\(f(\\mathbf{x})\\) that is an accurate predictor of \\(y\\) where \\(\\mathbf{x}\\) is the mass spectrum of the compound.\nTo measure predictive accuracy the data set was randomly divided into a 25,000 member training set and a 5,000 member test set. Linear discriminant analysis was tried, then quadratic discriminant analysis. These were difficult to adapt to the variable dimensionality. By this time I was thinking about decision trees. The hallmarks of chlorine in mass spectra were researched. This domain knowledge was incorporated into the decision tree algorithm by the design of the set of 1,500 yes–no questions that could be applied to a mass spectra of any dimensionality. The result was a decision tree that gave 95% accuracy on both chlorines and nonchlorines (see Breiman, Friedman, Olshen and Stone, 1984).\n\n\n3.3 Perceptions on Statistical Analysis\nAs I left consulting to go back to the university, these were the perceptions I had about working with data to find answers to problems:\n\nFocus on finding a good solution—that’s what consultants get paid for.\n\nLive with the data before you plunge into modeling.\n\nSearch for a model that gives a good solution, either algorithmic or data.\n\nPredictive accuracy on test sets is the criterion for how good the model is.\n\nComputers are an indispensable partner."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#return-to-the-university",
    "href": "contents/two_cultures/two_cultures.html#return-to-the-university",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "4. RETURN TO THE UNIVERSITY",
    "text": "4. RETURN TO THE UNIVERSITY\nI had one tip about what research in the university was like. A friend of mine, a prominent statistician from the Berkeley Statistics Department, visited me in Los Angeles in the late 1970s. After I described the decision tree method to him, his first question was, “What’s the model for the data?”\n\n4.1 Statistical Research\nUpon my return, I started reading the Annals of Statistics, the flagship journal of theoretical statistics, and was bemused. Every article started with\n\nAssume that the data are generated by the following model:\n\nfollowed by mathematics exploring inference, hypothesis testing and asymptotics. There is a wide spectrum of opinion regarding the usefulness of the theory published in the Annals of Statistics to the field of statistics as a science that deals with data. I am at the very low end of the spectrum. Still, there have been some gems that have combined nice theory and significant applications. An example is wavelet theory. Even in applications, data models are universal. For instance, in the Journal of the American Statistical Association (JASA), virtually every article contains a statement of the form:\n\nAssume that the data are generated by the following model:\n\nI am deeply troubled by the current and past use of data models in applications, where quantitative conclusions are drawn and perhaps policy decisions made."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#the-use-of-data-models",
    "href": "contents/two_cultures/two_cultures.html#the-use-of-data-models",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "5. THE USE OF DATA MODELS",
    "text": "5. THE USE OF DATA MODELS\nStatisticians in applied research consider data modeling as the template for statistical analysis: Faced with an applied problem, think of a data model. This enterprise has at its heart the belief that a statistician, by imagination and by looking at the data, can invent a reasonably good parametric class of models for a complex mechanism devised by nature. Then parameters are estimated and conclusions are drawn. But when a model is fit to data to draw quantitative conclusions:\n\nThe conclusions are about the model’s mechanism, and not about nature’s mechanism.\n\nIt follows that:\n\nIf the model is a poor emulation of nature, the conclusions may be wrong.\n\nThese truisms have often been ignored in the enthusiasm for fitting data models. A few decades ago, the commitment to data models was such that even simple precautions such as residual analysis or goodness-of-fit tests were not used. The belief in the infallibility of data models was almost religious. It is a strange phenomenon—once a model is made, then it becomes truth and the conclusions from it are infallible.\n\n5.1 An Example\nI illustrate with a famous (also infamous) example: assume the data is generated by independent draws from the model\n\\[\n(R) \\quad y = b_0 + \\sum_{1}^{M} b_m x_m + \\varepsilon,\n\\]\nwhere the coefficients \\({b_m}\\) are to be estimated, \\(\\varepsilon\\) is \\(N(0, \\sigma^2)\\) and \\(\\sigma^2\\) is to be estimated. Given that the data is generated this way, elegant tests of hypotheses, confidence intervals, distributions of the residual sum-of-squares and asymptotics can be derived. This made the model attractive in terms of the mathematics involved. This theory was used both by academic statisticians and others to derive significance levels for coefficients on the basis of model (R), with little consideration as to whether the data on hand could have been generated by a linear model. Hundreds, perhaps thousands of articles were published claiming proof of something or other because the coefficient was significant at the 5% level.\nGoodness-of-fit was demonstrated mostly by giving the value of the multiple correlation coefficient \\(R^2\\) which was often closer to zero than one and which could be over inflated by the use of too many parameters. Besides computing \\(R^2\\), nothing else was done to see if the observational data could have been generated by model (R). For instance, a study was done several decades ago by a well-known member of a university statistics department to assess whether there was gender discrimination in the salaries of the faculty. All personnel files were examined and a data base set up which consisted of salary as the response variable and 25 other variables which characterized academic performance; that is, papers published, quality of journals published in, teaching record, evaluations, etc. Gender appears as a binary predictor variable.\nA linear regression was carried out on the data and the gender coefficient was significant at the 5% level. That this was strong evidence of sex discrimination was accepted as gospel. The design of the study raises issues that enter before the consideration of a model—Can the data gathered answer the question posed? Is inference justified when your sample is the entire population? Should a data model be used? The deficiencies in analysis occurred because the focus was on the model and not on the problem.\nThe linear regression model led to many erroneous conclusions that appeared in journal articles waving the 5% significance level without knowing whether the model fit the data. Nowadays, I think most statisticians will agree that this is a suspect way to arrive at conclusions. At the time, there were few objections from the statistical profession about the fairy-tale aspect of the procedure, But, hidden in an elementary textbook, Mosteller and Tukey (1977) discuss many of the fallacies possible in regression and write “The whole area of guided regression is fraught with intellectual, statistical, computational, and subject matter difficulties.”\nEven currently, there are only rare published critiques of the uncritical use of data models. One of the few is David Freedman, who examines the use of regression models (1994); the use of path models (1987) and data modeling (1991, 1995). The analysis in these papers is incisive.\n\n\n5.2 Problems in Current Data Modeling\nCurrent applied practice is to check the data model fit using goodness-of-fit tests and residual analysis. At one point, some years ago, I set up a simulated regression problem in seven dimensions with a controlled amount of nonlinearity. Standard tests of goodness-of-fit did not reject linearity until the nonlinearity was extreme. Recent theory supports this conclusion. Work by Bickel, Ritov and Stoker (2001) shows that goodness-of-fit tests have very little power unless the direction of the alternative is precisely specified. The implication is that omnibus goodness-of-fit tests, which test in many directions simultaneously, have little power, and will not reject until the lack of fit is extreme.\nFurthermore, if the model is tinkered with on the basis of the data, that is, if variables are deleted or nonlinear combinations of the variables added, then goodness-of-fit tests are not applicable. Residual analysis is similarly unreliable. In a discussion after a presentation of residual analysis in a seminar at Berkeley in 1993, William Cleveland, one of the fathers of residual analysis, admitted that it could not uncover lack of fit in more than four to five dimensions. The papers I have read on using residual analysis to check lack of fit are confined to data sets with two or three variables.\nWith higher dimensions, the interactions between the variables can produce passable residual plots for a variety of models. A residual plot is a goodness-of-fit test, and lacks power in more than a few dimensions. An acceptable residual plot does not imply that the model is a good fit to the data.\nThere are a variety of ways of analyzing residuals. For instance, Landwher, Preibon and Shoemaker (1984, with discussion) gives a detailed analysis of fitting a logistic model to a three-variable data set using various residual plots. But each of the four discussants present other methods for the analysis. One is left with an unsettled sense about the arbitrariness of residual analysis.\nMisleading conclusions may follow from data models that pass goodness-of-fit tests and residual checks. But published applications to data often show little care in checking model fit using these methods or any other. For instance, many of the current application articles in JASA that fit data models have very little discussion of how well their model fits the data. The question of how well the model fits the data is of secondary importance compared to the construction of an ingenious stochastic model.\n\n\n5.3 The Multiplicity of Data Models\nOne goal of statistics is to extract information from the data about the underlying mechanism producing the data. The greatest plus of data modeling is that it produces a simple and understandable picture of the relationship between the input variables and responses. For instance, logistic regression in classification is frequentlyused because it produces a linear combination of the variables with weights that give an indication of the variable importance. The end result is a simple picture of how the prediction variables affect the response variable plus confidence intervals for the weights. Suppose two statisticians, each one with a different approach to data modeling, fit a model to the same data set. Assume also that each one applies standard goodness-of-fit tests, looks at residuals, etc., and is convinced that their model fits the data. Yet the two models give different pictures of nature’s mechanism and lead to different conclusions.\nMcCullah and Nelder (1989) write “Data will often point with almost equal emphasis on several possible models, and it is important that the statistician recognize and accept this.” Well said, but different models, all of them equallygood, may give different pictures of the relation between the predictor and response variables. The question of which one most accuratelyreflects the data is difficult to resolve. One reason for this multiplicity is that goodness-of-fit tests and other methods for checking fit give a yes–no answer. With the lack of power of these tests with data having more than a small number of dimensions, there will be a large number of models whose fit is acceptable. There is no way, among the y es–no methods for gauging fit, of determining which is the better model. A few statisticians know this. Mountain and Hsiao (1989) write, “It is difficult to formulate a comprehensive model capable of encompassing all rival models. Furthermore, with the use of finite samples, there are dubious implications with regard to the validity and power of various encompassing tests that rely on asymptotic theory.”\nData models in current use mayhave more damaging results than the publications in the social sciences based on a linear regression analysis. Just as the 5% level of significance became a de facto standard for publication, the Cox model for the analysis of survival times and logistic regression for survive– nonsurvive data have become the de facto standard for publication in medical journals. That different survival models, equallywell fitting, could give different conclusions is not an issue.\n\n\n5.4 Predictive Accuracy\nThe most obvious wayto see how well the model box emulates nature’s box is this: put a case \\(\\mathbf{x}\\) down nature’s box getting an output y. Similarly, put the same case \\(\\mathbf{x}\\) down the model box getting an output \\(y\\). The closeness of y and \\(y'\\) is a measure of how good the emulation is. For a data model, this translates as: fit the parameters in your model by using the data, then, using the model, predict the data and see how good the prediction is.\nPrediction is rarelyperfect. There are usuallymanyunmeasured variables whose effect is referred to as “noise.” But the extent to which the model box emulates nature’s box is a measure of how well our model can reproduce the natural phenomenon producing the data.\nMcCullagh and Nelder (1989) in their book on generalized linear models also think the answer is obvious. Theywrite, “At first sight it might seem as though a good model is one that fits the data very well; that is, one that makes \\(\\hat{\\mu}\\) (the model predicted value) veryclose to \\(y\\) (the response value).” Then theygo on to note that the extent of the agreement is biased by the number of parameters used in the model and so is not a satisfactory measure. They are, of course, right. If the model has too many parameters, then it may overfit the data and give a biased estimate of accuracy. But there are way s to remove the bias. To get a more unbiased estimate of predictive accuracy, cross-validation can be used, as advocated in an important early work by Stone (1974). If the data set is larger, put aside a test set.\nMosteller and Tukey (1977) were early advocates of cross-validation. They write, “Cross-validation is a natural route to the indication of the quality of any data-derived quantity . We plan to cross-validate carefully wherever we can.”\nJudging by the infrequency of estimates of predictive accuracy in JASA, this measure of model fit that seems natural to me (and to Mosteller and Tukey) is not natural to others. More publication of predictive accuracy estimates would establish standards for comparison of models, a practice that is common in machine learning."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#the-limitations-of-data-models",
    "href": "contents/two_cultures/two_cultures.html#the-limitations-of-data-models",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "6. THE LIMITATIONS OF DATA MODELS",
    "text": "6. THE LIMITATIONS OF DATA MODELS\nWith the insistence on data models, multivariate analysis tools in statistics are frozen at discriminant analysis and logistic regression in classification and multiple linear regression in regression. Nobody really believes that multivariate data is multivariate normal, but that data model occupies a large number of pages in everygraduate textbook on multivariate statistical analysis.\nWith data gathered from uncontrolled observations on complex systems involving unknown physical, chemical, or biological mechanisms, the a priori assumption that nature would generate the data through a parametric model selected by the statistician can result in questionable conclusions that cannot be substantiated by appeal to goodness-of-fit tests and residual analysis. Usually, simple parametric models imposed on data generated by complex systems, for example, medical data, financial data, result in a loss of accuracy and information as compared to algorithmic models (see Section 11).\nThere is an old saying “If all a man has is a hammer, then every problem looks like a nail.” The trouble for statisticians is that recently some of the problems have stopped looking like nails. I conjecture that the result of hitting this wall is that more complicated data models are appearing in current published applications. Bayesian methods combined with Markov Chain Monte Carlo are cropping up all over. This may signify that as data becomes more complex, the data models become more cumbersome and are losing the advantage of presenting a simple and clear picture of nature’s mechanism.\nApproaching problems by looking for a data model imposes an a priori straight jacket that restricts the ability of statisticians to deal with a wide range of statistical problems. The best available solution to a data problem might be a data model; then again it might be an algorithmic model. The data and the problem guide the solution. To solve a wider range of data problems, a larger set of tools is needed.\nPerhaps the damaging consequence of the insistence on data models is that statisticians have ruled themselves out of some of the most interesting and challenging statistical problems that have arisen out of the rapidly increasing ability of computers to store and manipulate data. These problems are increasingly present in many fields, both scientific and commercial, and solutions are being found by nonstatisticians."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#algorithmic-modeling",
    "href": "contents/two_cultures/two_cultures.html#algorithmic-modeling",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "7. ALGORITHMIC MODELING",
    "text": "7. ALGORITHMIC MODELING\nUnder other names, algorithmic modeling has been used by industrial statisticians for decades. See, for instance, the delightful book “Fitting Equations to Data” (Daniel and Wood, 1971). It has been used by psychometricians and social scientists. Reading a preprint of Gifi’s book (1990) many years ago uncovered a kindred spirit. It has made small inroads into the analysis of medical data starting with Richard Olshen’s work in the early 1980s. For further work, see Zhang and Singer (1999). Jerome Friedman and Grace Wahba have done pioneering work on the development of algorithmic methods. But the list of statisticians in the algorithmic modeling business is short, and applications to data are seldom seen in the journals. The development of algorithmic methods was taken up by a community outside statistics.\n\n7.1 ANew Research Community\nIn the mid-1980s two powerful new algorithms for fitting data became available: neural nets and decision trees. A new research community using these tools sprang up. Their goal was predictive accuracy. The community consisted of young computer scientists, physicists and engineers plus a few aging statisticians. They began using the new tools in working on complex prediction problems where it was obvious that data models were not applicable: speech recognition, image recognition, nonlinear time series prediction, handwriting recognition, prediction in financial markets.\nTheir interests range over many fields that were once considered happy hunting grounds for statisticians and have turned out thousands of interesting research papers related to applications and methodology. A large majority of the papers analyze real data. The criterion for any model is what is the predictive accuracy. An idea of the range of research of this group can be got by looking at the Proceedings of the Neural Information Processing Systems Conference (their main yearly meeting) or at the Machine Learning Journal.\n\n\n7.2 Theory in Algorithmic Modeling\nData models are rarely used in this community. The approach is that nature produces data in a black box whose insides are complex, mysterious, and, at least, partly unknown. What is observed is a set of \\(\\mathbf{x}\\)’s that go in and a subsequent set of \\(\\mathbf{y}\\)’s that come out. The problem is to find an algorithm \\(f(\\mathbf{x})\\) such that for future \\(\\mathbf{x}\\) in a test set, \\(f(\\mathbf{x})\\) will be a good predictor of \\(\\mathbf{y}\\).\nThe theory in this field shifts focus from data models to the properties of algorithms. It characterizes their “strength” as predictors, convergence if they are iterative, and what gives them good predictive accuracy. The one assumption made in the theory is that the data is drawn i.i.d. from an unknown multivariate distribution.\nThere is isolated work in statistics where the focus is on the theory of the algorithms. Grace Wahba’s research on smoothing spline algorithms and their applications to data (using cross-validation) is built on theory involving reproducing kernels in Hilbert Space (1990). The final chapter of the CART book (Breiman et al., 1984) contains a proof of the asymptotic convergence of the CART algorithm to the Bayes risk by letting the trees grow as the sample size increases. There are others, but the relative frequency is small.\nTheory resulted in a major advance in machine learning. Vladimir Vapnik constructed informative bounds on the generalization error (infinite test set error) of classification algorithms which depend on the “capacity” of the algorithm. These theoretical bounds led to support vector machines (see Vapnik, 1995, 1998) which have proved to be more accurate predictors in classification and regression then neural nets, and are the subject of heated current research (see Section 10).\nMy last paper “Some infinity theory for tree ensembles” (Breiman, 2000) uses a function space analysis to try and understand the workings of tree ensemble methods. One section has the heading, “My kingdom for some good theory.” There is an effective method for forming ensembles known as “boosting,” but there isn’t any finite sample size theory that tells us why it works so well.\n\n\n7.3 Recent Lessons\nThe advances in methodology and increases in predictive accuracy since the mid-1980s that have occurred in the research of machine learning has been phenomenal. There have been particularly exciting developments in the last five years. What has been learned? The three lessons that seem most important to one:\n\nRashomon: the multiplicity of good models;\nOccam: the conflict between simplicity and accuracy;\nBellman: dimensionality—curse or blessing?"
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#rashomon-and-the-multiplicity-of-good-models",
    "href": "contents/two_cultures/two_cultures.html#rashomon-and-the-multiplicity-of-good-models",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "8. RASHOMON AND THE MULTIPLICITY OF GOOD MODELS",
    "text": "8. RASHOMON AND THE MULTIPLICITY OF GOOD MODELS\nRashomon is a wonderful Japanese movie in which four people, from different vantage points, witness an incident in which one person dies and another is supposedlyraped. When theycome to testifyin court, theyall report the same facts, but their stories of what happened are verydifferent.\nWhat I call the Rashomon Effect is that there is often a multitude of different descriptions [equations fx] in a class of functions giving about the same minimum error rate. The most easily understood example is subset selection in linear regression. Suppose there are 30 variables and we want to find the best five variable linear regressions. There are about 140,000 five-variable subsets in competition. Usually we pick the one with the lowest residual sum-of-squares (RSS), or, if there is a test set, the lowest test error. But there may be (and generally are) many five-variable equations that have RSS within 1.0% of the lowest RSS (see Breiman, 1996a). The same is true if test set error is being measured.\nSo here are three possible pictures with RSS or test set error within 1.0% of each other:\nPicture 1\n\\[\ny = 2.1 + 3.8x_3 - 0.6x_8 + 83.2x_{12} - 2.1x_{17} + 3.2x_{27},\n\\]\nPicture 2\n\\[\ny = -8.9 + 4.6x_5 + 0.01x_6 + 12.0x_{15} + 17.5x_{21} + 0.2x_{22},\n\\]\nPicture 3\n\\[\ny = -76.7 + 9.3x_2 + 22.0x_7 - 13.2x_8 + 3.4x_{11} + 7.2x_{28}.\n\\]\nWhich one is better? The problem is that each one tells a different story about which variables are important.\nThe Rashomon Effect also occurs with decision trees and neural nets. In my experiments with trees, if the training set is perturbed only slightly, say by removing a random 2–3% of the data, I can get a tree quite different from the original but with almost the same test set error. I once ran a small neural net 100 times on simple three-dimensional data reselecting the initial weights to be small and random on each run. I found 32 distinct minima, each of which gave a different picture, and having about equal test set error.\nThis effect is closely connected to what I call instability (Breiman, 1996a) that occurs when there are many different models crowded together that have about the same training or test set error. Then a slight perturbation of the data or in the model construction will cause a skip from one model to another. The two models are close to each other in terms of error, but can be distant in terms of the form of the model.\nIf, in logistic regression or the Cox model, the common practice of deleting the less important covariates is carried out, then the model becomes unstable—there are too many competing models. Say you are deleting from 15 variables to 4 variables. Perturb the data slightly and you will very possibly get a different four-variable model and a different conclusion about which variables are important. To improve accuracy by weeding out less important covariates you run into the multiplicity problem. The picture of which covariates are important can vary significantly between two models having about the same deviance.\nAggregating over a large set of competing models can reduce the nonuniqueness while improving accuracy. Arena et al. (2000) bagged (see Glossary ) logistic regression models on a data base of toxic and nontoxic chemicals where the number of covariates in each model was reduced from 15 to 4 by standard best subset selection. On a test set, the bagged model was significantly more accurate than the single model with four covariates. It is also more stable. This is one possible fix. The multiplicity problem and its effect on conclusions drawn from models needs serious attention."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#occam-and-simplicity-vs.-accuracy",
    "href": "contents/two_cultures/two_cultures.html#occam-and-simplicity-vs.-accuracy",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "9. OCCAM AND SIMPLICITY VS. ACCURACY",
    "text": "9. OCCAM AND SIMPLICITY VS. ACCURACY\nOccam’s Razor, long admired, is usually interpreted to mean that simpler is better. Unfortunately, in prediction, accuracy and simplicity (interpretability) are in conflict. For instance, linear regression gives a fairly interpretable picture of the \\(\\mathbf{y}, \\mathbf{x}\\) relation. But its accuracy is usually less than that of the less interpretable neural nets. An example closer to my work involves trees.\nOn interpretability, trees rate an A+. A project I worked on in the late 1970s was the analysis of delay in criminal cases in state court systems. The Constitution gives the accused the right to a speedy trial. The Center for the State Courts was concerned that in many states, the trials were anything but speedy. It funded a study of the causes of the delay. I visited many states and decided to do the analysis in Colorado, which had an excellent computerized court data system. A wealth of information was extracted and processed.\nThe dependent variable for each criminal case was the time from arraignment to the time of sentencing. All of the other information in the trial history were the predictor variables. A large decision tree was grown, and I showed it on an overhead and explained it to the assembled Colorado judges. One of the splits was on District N which had a larger delay time than the other districts. I refrained from commenting on this. But as I walked out I heard one judge say to another, “I knew those guys in District N were dragging their feet.”\nWhile trees rate an A+ on interpretability, they are good, but not great, predictors. Give them, say, a B on prediction.\n\n9.1 Growing Forests for Prediction\nInstead of a single tree predictor, grow a forest of trees on the same data—say 50 or 100. If we are classifying, put the new x down each tree in the forest and get a vote for the predicted class. Let the forest prediction be the class that gets the most votes. There has been a lot of work in the last five years on ways to grow the forest. All of the well-known methods grow the forest by perturbing the training set, growing a tree on the perturbed training set, perturbing the training set again, growing another tree, etc. Some familiar methods are bagging (Breiman, 1996b), boosting (Freund and Schapire, 1996), arcing (Breiman, 1998), and additive logistic regression (Friedman, Hastie and Tibshirani, 1998).\nMy preferred method to date is random forests. In this approach successive decision trees are grown by introducing a random element into their construction. For example, suppose there are 20 predictor variables. At each node choose several of the 20 at random to use to split the node. Or use a random combination of a random selection of a few variables. This idea appears in Ho (1998), in Amit and Geman (1997) and is developed in Breiman (1999).\n\n\n9.2 Forests Compared to Trees\nWe compare the performance of single trees (CART) to random forests on a number of small and large data sets, mostly from the UCI repository (ftp.ics.uci.edu/pub/MachineLearningDatabases). A summary of the data sets is given in Table 1.\n\n\nTable 1: Data set descriptions\n\n\n\n\n\n\n\n\n\nData set\nTraining Sample size\nTest Sample size\nVariables\nClasses\n\n\n\n\nCancer\n699\n—\n9\n2\n\n\nIonosphere\n351\n—\n34\n2\n\n\nDiabetes\n768\n—\n8\n2\n\n\nGlass\n214\n—\n9\n6\n\n\nSoybean\n683\n—\n35\n19\n\n\nLetters\n15,000\n5000\n16\n26\n\n\nSatellite\n4,435\n2000\n36\n6\n\n\nShuttle\n43,500\n14,500\n9\n7\n\n\nDNA\n2,000\n1,186\n60\n3\n\n\nDigit\n7,291\n2,007\n256\n10\n\n\n\n\nTable 2 compares the test set error of a single tree to that of the forest. For the five smaller data sets above the line, the test set error was estimated by leaving out a random 10% of the data, then running CART and the forest on the other 90%. The left-out 10% was run down the tree and the forest and the error on this 10% computed for both. This was repeated 100 times and the errors averaged. The larger data sets below the line came with a separate test set. People who have been in the classification field for a while find these increases in accuracy startling. Some errors are halved. Others are reduced by one-third. In regression, where the forest prediction is the average over the individual tree predictions, the decreases in mean-squared test set error are similar.\n\n\nTable 2 Test set misclassification error (%)\n\n\nData set\nForest\nSingle tree\n\n\n\n\nBreast cancer\n2.9\n5.9\n\n\nIonosphere\n5.5\n11.2\n\n\nDiabetes\n24.2\n25.3\n\n\nGlass\n22.0\n30.4\n\n\nSoybean\n5.7\n8.6\n\n\nLetters\n3.4\n12.4\n\n\nSatellite\n8.6\n14.8\n\n\nShuttle ×103\n7.0\n62.0\n\n\nDNA\n3.9\n6.2\n\n\nDigit\n6.2\n17.1\n\n\n\n\n\n\n9.3 Random Forests are A + Predictors\nThe Statlog Project (Mitchie, Spiegelhalter and Taylor, 1994) compared 18 different classifiers. Included were neural nets, CART, linear and quadratic discriminant analysis, nearest neighbor, etc. The first four data sets below the line in Table 1 were the only ones used in the Statlog Project that came with separate test sets. In terms of rank of accuracy on these four data sets, the forest comes in 1, 1, 1, 1 for an average rank of 1.0. The next best classifier had an average rank of 7.3.\nThe fifth data set below the line consists of 16×16 pixel grayscale depictions of handwritten ZIP Code numerals. It has been extensively used by AT&T Bell Labs to test a variety of prediction methods. A neural net handcrafted to the data got a test set error of 5.1% vs. 6.2% for a standard run of random forest.\n\n\n9.4 The Occam Dilemma\nSo forests are A+ predictors. But their mechanism for producing a prediction is difficult to understand. Trying to delve into the tangled web that generated a plurality vote from 100 trees is a Herculean task. So on interpretability, they rate an F. Which brings us to the Occam dilemma:\n\nAccuracy generally requires more complex prediction methods. Simple and interpretable functions do not make the most accurate predictors.\n\nUsing complex predictors may be unpleasant, but the soundest path is to go for predictive accuracy first, then try to understand why. In fact, Section 10 points out that from a goal-oriented statistical viewpoint, there is no Occam’s dilemma. (For more on Occam’s Razor see Domingos, 1998, 1999.)"
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#bellman-and-the-curse-of-dimensionality",
    "href": "contents/two_cultures/two_cultures.html#bellman-and-the-curse-of-dimensionality",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "10. BELLMAN AND THE CURSE OF DIMENSIONALITY",
    "text": "10. BELLMAN AND THE CURSE OF DIMENSIONALITY\nThe title of this section refers to Richard Bellman’s famous phrase, “the curse of dimensionality.” For decades, the first step in prediction methodology was to avoid the curse. If there were too many prediction variables, the recipe was to find a few features (functions of the predictor variables) that “contain most of the information” and then use these features to replace the original variables. In procedures common in statistics such as regression, logistic regression and survival models the advised practice is to use variable deletion to reduce the dimensionality. The published advice was that high dimensionality is dangerous. For instance, a well-regarded book on pattern recognition (Meisel, 1972) states “the features must be relatively few in number.” But recent work has shown that dimensionality can be a blessing.\n\n10.1 Digging It Out in Small Pieces\nReducing dimensionality reduces the amount of information available for prediction. The more predictor variables, the more information. There is also information in various combinations of the predictor variables. Let’s try going in the opposite direction:\n\nInstead of reducing dimensionality, increase it by adding many functions of the predictor variables.\n\nThere may now be thousands of features. Each potentially contains a small amount of information. The problem is how to extract and put together these little pieces of information. There are two outstanding examples of work in this direction, The Shape Recognition Forest (Y. Amit and D. Geman, 1997) and Support Vector Machines (V. Vapnik, 1995, 1998).\n\n\n10.2 The Shape Recognition Forest\nIn 1992, the National Institute of Standards and Technology (NIST) set up a competition for machine algorithms to read handwritten numerals. They put together a large set of pixel pictures of handwritten numbers (223,000) written by over 2,000 individuals. The competition attracted wide interest, and diverse approaches were tried.\nThe Amit–Geman approach defined many thousands of small geometric features in a hierarchical assembly. Shallow trees are grown, such that at each node, 100 features are chosen at random from the appropriate level of the hierarchy; and the optimal split of the node based on the selected features is found.\nWhen a pixel picture of a number is dropped down a single tree, the terminal node it lands in gives probability estimates \\(p_0, p_1, \\ldots, p_9\\) that it represents numbers 0, 1, …, 9. Over 1,000 trees are grown, the probabilities averaged over this forest, and the predicted number is assigned to the largest averaged probability.\nUsing a 100,000 example training set and a 50,000 test set, the Amit–Geman method gives a test set error of 0.7%–close to the limits of human error.\n\n\n10.3 Support Vector Machines\nSuppose there is two-class data having prediction vectors in \\(M\\)-dimensional Euclidean space. The prediction vectors for class #1 are \\(\\{\\mathbf{x}_1\\}\\) and those for class #2 are \\(\\{\\mathbf{x}_2\\}\\). If these two sets of vectors can be separated by a hyperplane then there is an optimal separating hyperplane. “Optimal” is defined as meaning that the distance of the hyperplane to any prediction vector is maximal (see below).\nThe set of vectors in \\(\\{\\mathbf{x}_1\\}\\) and in \\(\\{\\mathbf{x}_2\\}\\) that achieve the minimum distance to the optimal separating hyperplane are called the support vectors. Their coordinates determine the equation of the hyperplane. Vapnik (1995) showed that if a separating hyperplane exists, then the optimal separating hyperplane has low generalization error (see Glossary).\n\nIn two-class data, separability by a hyperplane does not often occur. However, let us increase the dimensionality by adding as additional predictor variables all quadratic monomials in the original predictor variables; that is, all terms of the form \\(x_{m1}x_{m2}\\). A hyperplane in the original variables plus quadratic monomials in the original variables is a more complex creature. The possibility of separation is greater. If no separation occurs, add cubic monomials as input features. If there are originally 30 predictor variables, then there are about 40,000 features if monomials up to the fourth degree are added.\nThe higher the dimensionality of the set of features, the more likely it is that separation occurs. In the ZIP Code data set, separation occurs with fourth degree monomials added. The test set error is 4.1%. Using a large subset of the NIST data base as a training set, separation also occurred after adding up to fourth degree monomials and gave a test set error rate of 1.1%.\nSeparation can always be had by raising the dimensionality high enough. But if the separating hyperplane becomes too complex, the generalization error becomes large. An elegant theorem (Vapnik, 1995) gives this bound for the expected generalization error:\n\\[\n\\text{Ex-GE} \\leq \\text{Ex}(\\text{number of support vectors}) / (N - 1)\n\\]\nwhere \\(N\\) is the sample size and the expectation is over all training sets of size \\(N\\) drawn from the same underlying distribution as the original training set.\nThe number of support vectors increases with the dimensionality of the feature space. If this number becomes too large, the separating hyperplane will not give low generalization error. If separation cannot be realized with a relatively small number of support vectors, there is another version of support vector machines that defines optimality by adding a penalty term for the vectors on the wrong side of the hyperplane.\nSome ingenious algorithms make finding the optimal separating hyperplane computationally feasible. These devices reduce the search to a solution of a quadratic programming problem with linear inequality constraints that are of the order of the number N of cases, independent of the dimension of the feature space. Methods tailored to this particular problem produce speed-ups of an order of magnitude over standard methods for solving quadratic programming problems.\nSupport vector machines can also be used to provide accurate predictions in other areas (e.g., regression). It is an exciting idea that gives excellent performance and is beginning to supplant the use of neural nets. A readable introduction is in Cristianini and Shawe-Taylor (2000)."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#information-from-a-black-box",
    "href": "contents/two_cultures/two_cultures.html#information-from-a-black-box",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "11. INFORMATION FROM A BLACK BOX",
    "text": "11. INFORMATION FROM A BLACK BOX\nThe dilemma posed in the last section is that the models that best emulate nature in terms of predictive accuracy are also the most complex and inscrutable. But this dilemma can be resolved by realizing the wrong question is being asked. Nature forms the outputs \\(\\mathbf{y}\\) from the inputs \\(\\mathbf{x}\\) by means of a black box with complex and unknown interior.\n\nCurrent accurate prediction methods are also complex black boxes.\n\nSo we are facing two black boxes, where ours seems only slightly less inscrutable than nature’s. In data generated by medical experiments, ensembles of predictors can give cross-validated error rates significantly lower than logistic regression. My biostatistician friends tell me, “Doctors can interpret logistic regression.” There is no way they can interpret a black box containing fifty trees hooked together. In a choice between accuracy and interpretability, they’ll go for interpretability.\nFraming the question as the choice between accuracy and interpretability is an incorrect interpretation of what the goal of a statistical analysis is. The point of a model is to get useful information about the relation between the response and predictor variables. Interpretability is a way of getting information. But a model does not have to be simple to provide reliable information about the relation between predictor and response variables; neither does it have to be a data model.\n\nThe goal is not interpretability, but accurate information.\n\nThe following three examples illustrate this point. The first shows that random forests applied to a medical data set can give more reliable information about covariate strengths than logistic regression. The second shows that it can give interesting information that could not be revealed bya logistic regression. The third is an application to a microarraydata where it is difficult to conceive of a data model that would uncover similar information.\n\n11.1 Example I: Variable Importance in a Survival Data Set\nThe data set contains survival or nonsurvival of 155 hepatitis patients with 19 covariates. It is available at ftp.ics.uci.edu/pub/MachineLearning-Databases and was contributed by Gail Gong. The description is in a file called hepatitis.names. The data set has been previously analyzed by Diaconis and Efron (1983), and Cestnik, Konenenko and Bratko (1987). The lowest reported error rate to date, 17%, is in the latter paper.\nDiaconis and Efron refer to work by Peter Gregory of the Stanford Medical School who analyzed this data and concluded that the important variables were numbers 6, 12, 14, 19 and reports an estimated 20% predictive accuracy. The variables were reduced in two stages—the first was by informal data analysis. The second refers to a more formal (unspecified) statistical procedure which I assume was logistic regression.\nEfron and Diaconis drew 500 bootstrap samples from the original data set and used a similar procedure to isolate the important variables in each bootstrapped data set. The authors comment, “Of the four variables originally selected not one was selected in more than 60 percent of the samples. Hence the variables identified in the original analysis cannot be taken too seriously.” We will come back to this conclusion later.\n\nLogistic Regression\nThe predictive error rate for logistic regression on the hepatitis data set is 17.4%. This was evaluated by doing 100 runs, each time leaving out a randomly selected 10% of the data as a test set, and then averaging over the test set errors.\nUsually, the initial evaluation of which variables are important is based on examining the absolute values of the coefficients of the variables in the logistic regression divided by their standard deviations. Figure 1 is a plot of these values.\nThe conclusion from looking at the standardized coefficients is that variables 7 and 11 are the most important covariates. When logistic regression is run using only these two variables, the cross-validated error rate rises to 22.9%. Another way to find important variables is to run a best subsets search which, for any value k, finds the subset of k variables having lowest deviance.\nThis procedure raises the problems of instability and multiplicity of models (see Section 7.1). There are about 4,000 subsets containing four variables. Of these, there are almost certainly a substantial number that have deviance close to the minimum and give different pictures of what the underlying mechanism is.\n\n\n\nFig. 1. Standardized coefficients logistic regression.\n\n\n\n\n\nFig. 2. Variable importance-random forest.\n\n\n\n\nRandom Forests\nThe random forests predictive error rate, evaluated by averaging errors over 100 runs, each time leaving out 10% of the data as a test set, is 12.3% almost a 30% reduction from the logistic regression error.\nRandom forests consists of a large number of randomly constructed trees, each voting for a class. Similar to bagging (Breiman, 1996), a bootstrap sample of the training set is used to construct each tree. A random selection of the input variables is searched to find the best split for each node.\nTo measure the importance of the mth variable, the values of the mth variable are randomly permuted in all of the cases left out in the current bootstrap sample. Then these cases are run down the current tree and their classification noted. At the end of a run consisting of growing many trees, the percent increase in misclassification rate due to noising up each variable is computed. This is the measure of variable importance that is shown in Figure 1.\nRandom forests singles out two variables, the 12th and the 17th, as being important. As a verification both variables were run in random forests, individually and together. The test set error rates over 100 replications were 14.3% each. Running both together did no better. We conclude that virtually all of the predictive capability is provided by a single variable, either 12 or 17.\nTo explore the interaction between 12 and 17 a bit further, at the end of a random forest run using all variables, the output includes the estimated value of the probability of each class vs. the case number. This information is used to get plots of the variable values (normalized to mean zero and standard deviation one) vs. the probability of death. The variable values are smoothed using a weighted linear regression smoother. The results are in Figure 3 for variables 12 and 17.\n\n\n\nFig. 3. Variable 17 vs. probability #1.\n\n\nThe graphs of the variable values vs. class death probability are almost linear and similar. The two variables turn out to be highly correlated. Thinking that this might have affected the logistic regression results, it was run again with one or the other of these two variables deleted. There was little change.\nOut of curiosity, I evaluated variable importance in logistic regression in the same way that I did in random forests, by permuting variable values in the 10% test set and computing how much that increased the test set error. Not much help variables 12 and 17 were not among the 3 variables ranked as most important. In partial verification of the importance of 12 and 17, I tried them separately as single variables in logistic regression. Variable 12 gave a 15.7% error rate, variable 17 came in at 19.3%.\nTo go back to the original Diaconis–Efron analysis, the problem is clear. Variables 12 and 17 are surrogates for each other. If one of them appears important in a model built on a bootstrap sample, the other does not. So each one’s frequency of occurrence is automatically less than 50%. The paper lists the variables selected in ten of the samples. Either 12 or 17 appear in seven of the ten.\n\n\n\n11.2 Example II: Clustering in Medical Data\nThe Bupa liver data set is a two-class biomedical data set also available at ftp.ics.uci.edu/pub/MachineLearningDatabases. The covariates are:\n\nmcv mean corpuscular volume\nalkphos alkaline phosphotase\nsgpt alamine aminotransferase\nsgot aspartate aminotransferase\ngammagt gamma-glutamyl transpeptidase\ndrinks half-pint equivalents of alcoholic beverage drunk per day\n\nThe first five attributes are the results of blood tests thought to be related to liver functioning. The 345 patients are classified into two classes by the severity of their liver malfunctioning. Class two is severe malfunctioning. In a random forests run, the misclassification error rate is 28%. The variable importance given by random forests is in Figure 4.\n\n\n\nFig. 4. Variable importance—Bupa data.\n\n\nBlood tests 3 and 5 are the most important, followed by test 4. Random forests also outputs an intrinsic similarity measure which can be used to cluster. When this was applied, two clusters were discovered in class two. The average of each variable is computed and plotted in each of these clusters in Figure 5.\n\n\n\nFig. 5. Cluster averages—Bupa data.\n\n\nAn interesting facet emerges. The class two subjects consist of two distinct groups: those that have high scores on blood tests 3, 4, and 5 and those that have low scores on those tests.\n\n\n11.3 Example III: Microarray Data\nRandom forests was run on a microarray lymphoma data set with three classes, sample size of 81 and 4,682 variables (genes) without any variable selection [for more information about this data set, see Dudoit, Fridlyand and Speed, (2000)]. The error rate was low. What was also interesting from a scientific viewpoint was an estimate of the importance of each of the 4,682 gene expressions.\nThe graph in Figure 6 was produced by a run of random forests. This result is consistent with assessments of variable importance made using other algorithmic methods, but appears to have sharper detail.\n\n\n\nFig. 6. Microarray variable importance.\n\n\n\n\n11.4 Remarks about the Examples\nThe examples show that much information is available from an algorithmic model. Friedman (1999) derives similar variable information from a different way of constructing a forest. The similarity is that they are both built as ways to give low predictive error.\nThere are 32 deaths and 123 survivors in the hepatitis data set. Calling everyone a survivor gives a baseline error rate of 20.6%. Logistic regression lowers this to 17.4%. It is not extracting much useful information from the data, which may explain its inability to find the important variables. Its weakness might have been unknown and the variable importances accepted at face value if its predictive accuracy was not evaluated.\nRandom forests is also capable of discovering important aspects of the data that standard data models cannot uncover. The potentially interesting clustering of class two patients in Example II is an illustration. The standard procedure when fitting data models such as logistic regression is to delete variables; to quote from Diaconis and Efron (1983) again, “statistical experience suggests that it is unwise to fit a model that depends on 19 variables with only 155 data points available.” Newer methods in machine learning thrive on variables—the more the better. For instance, random forests does not overfit. It gives excellent accuracy on the lymphoma data set of Example III which has over 4,600 variables, with no variable deletion and is capable of extracting variable importance information from the data.\nThese examples illustrate the following points:\n\nHigher predictive accuracy is associated with more reliable information about the underlying data mechanism. Weak predictive accuracy can lead to questionable conclusions.\nAlgorithmic models can give better predictive accuracy than data models, and provide better information about the underlying mechanism."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#final-remarks",
    "href": "contents/two_cultures/two_cultures.html#final-remarks",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "12. FINAL REMARKS",
    "text": "12. FINAL REMARKS\nThe goals in statistics are to use data to predict and to get information about the underlying data mechanism. Nowhere is it written on a stone tablet what kind of model should be used to solve problems involving data. To make my position clear, I am not against data models per se. In some situations they are the most appropriate way to solve the problem. But the emphasis needs to be on the problem and on the data.\nUnfortunately, our field has a vested interest in data models, come hell or high water. For instance, see Dempster’s (1998) paper on modeling. His position on the 1990 Census adjustment controversy is particularly interesting. He admits that he doesn’t know much about the data or the details, but argues that the problem can be solved by a strong dose of modeling. That more modeling can make error ridden data accurate seems highly unlikely to me.\nTerrabytes of data are pouring into computers from many sources, both scientific, and commercial, and there is a need to analyze and understand the data. For instance, data is being generated at an awesome rate by telescopes and radio telescopes scanning the skies. Images containing millions of stellar objects are stored on tape or disk. Astronomers need automated ways to scan their data to find certain types of stellar objects or novel objects. This is a fascinating enterprise, and I doubt if data models are applicable. Yet I would enter this in my ledger as a statistical problem.\nThe analysis of genetic data is one of the most challenging and interesting statistical problems around. Microarray data, like that analyzed in Section 11.3 can lead to significant advances in understanding genetic effects. But the analysis of variable importance in Section 11.3 would be difficult to do accurately using a stochastic data model.\nProblems such as stellar recognition or analysis of gene expression data could be high adventure for statisticians. But it requires that they focus on solving the problem instead of asking what data model they can create. The best solution could be an algorithmic model, or maybe a data model, or maybe a combination. But the trick to being a scientist is to be open to using a wide variety of tools.\nThe roots of statistics, as in science, lie in working with data and checking theory against data. I hope in this century our field will return to its roots. There are signs that this hope is not illusory. Over the last ten years, there has been a noticeable move toward statistical work on real world problems and reaching out by statisticians toward collaborative work with other disciplines. I believe this trend will continue and, in fact, has to continue if we are to survive as an energetic and creative field."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#glossary",
    "href": "contents/two_cultures/two_cultures.html#glossary",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "GLOSSARY",
    "text": "GLOSSARY\nSince some of the terms used in this paper may not be familiar to all statisticians, I append some definitions.\nInfinite test set error. Assume a loss function \\(L(\\mathbf{y}, \\hat{\\mathbf{y}})\\) that is a measure of the error when \\(\\mathbf{y}\\) is the true response and \\(\\hat{\\mathbf{y}}\\) the predicted response. In classification, the usual loss is 1 if \\(\\mathbf{y} = \\hat{\\mathbf{y}}\\) and zero if \\(\\mathbf{y} = \\hat{\\mathbf{y}}\\). In regression, the usual loss is \\(\\left(\\mathbf{y} - \\hat{\\mathbf{y}}\\right)^2\\). Given a set of data (training set) consisting of \\(\\mathbf{y}_n, \\mathbf{x}_n\\) for \\(n = 1, 2, \\ldots, N\\), use it to construct a predictor function \\(\\phi(\\mathbf{x})\\) of \\(\\mathbf{y}\\). Assume that the training set is i.i.d drawn from the distribution of the random vector \\(\\mathbf{Y}, \\mathbf{X}\\). The infinite test set error is \\(E[L(\\mathbf{Y}, \\phi(\\mathbf{X}))]\\). This is called the generalization error in machine learning.\nThe generalization error is estimated either by setting aside a part of the data as a test set or by cross-validation.\nPredictive accuracy. This refers to the size of the estimated generalization error. Good predictive accuracy means low estimated error.\nTrees and nodes. This terminology refers to decision trees as described in the Breiman et al book (1984).\nDropping an \\(\\mathbf{x}\\) down a tree. When a vector of predictor variables is “dropped” down a tree, at each intermediate node it has instructions whether to go left or right depending on the coordinates of \\(\\mathbf{x}\\). It stops at a terminal node and is assigned the prediction given by that node.\nBagging. An acronym for “bootstrap aggregating.” Start with an algorithm such that given any training set, the algorithm produces a prediction function \\(\\phi(\\mathbf{x})\\). The algorithm can be a decision tree construction, logistic regression with variable deletion, etc. Take a bootstrap sample from the training set and use this bootstrap training set to construct the predictor \\(\\phi_1(\\mathbf{x})\\). Take another bootstrap sample and using this second training set construct the predictor \\(\\phi_2(\\mathbf{x})\\). Continue this way for \\(K\\) steps. In regression, average all of the \\(\\{\\phi_k(\\mathbf{x})\\}\\) to get the bagged predictor at \\(\\mathbf{x}\\). In classification, that class which has the plurality vote of the \\(\\{\\phi_k(\\mathbf{x})\\}\\) is the bagged predictor. Bagging has been shown effective in variance reduction (Breiman, 1996b).\nBoosting. This is a more complex way of forming an ensemble of predictors in classification than bagging (Freund and Schapire, 1996). It uses no randomization but proceeds by altering the weights on the training set. Its performance in terms of low prediction error is excellent (for details see Breiman, 1998)."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#acknowledgments",
    "href": "contents/two_cultures/two_cultures.html#acknowledgments",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "ACKNOWLEDGMENTS",
    "text": "ACKNOWLEDGMENTS\nMany of my ideas about data modeling were formed in three decades of conversations with my old friend and collaborator, Jerome Friedman. Conversations with Richard Olshen about the Cox model and its use in biostatistics helped me to understand the background. I am also indebted to William Meisel, who headed some of the prediction projects I consulted on and helped me make the transition from probability theory to algorithms, and to Charles Stone for illuminating conversations about the nature of statistics and science. I’m grateful also for the comments of the editor, Leon Gleser, which prompted a major rewrite of the first draft of this manuscript and resulted in a different and better paper."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#references",
    "href": "contents/two_cultures/two_cultures.html#references",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "REFERENCES",
    "text": "REFERENCES\n\nAmit, Y. and Geman, D. (1997). Shape quantization and recognition with randomized trees. Neural Computation 9 1545– 1588.\nArena, C., Sussman, N., Chiang, K., Mazumdar, S., Macina, O. and Li, W. (2000). Bagging Structure-Activity Relationships: A simulation study for assessing misclassification rates. Presented at the Second Indo-U.S. Workshop on Mathematical Chemistry, Duluth, MI. (Available at NSussman@server.ceoh.pitt.edu).\nBickel, P., Ritov, Y. and Stoker, T. (2001). Tailor-made tests for goodness of fit for semiparametric hypotheses. Unpublished manuscript.\nBreiman, L. (1996a). The heuristics of instability in model selection. Ann. Statist. 24 2350–2381.\nBreiman, L. (1996b). Bagging predictors. Machine Learning J. 26 123–140.\nBreiman, L. (1998). Arcing classifiers. Discussion paper, Ann. Statist. 26 801–824.\nBreiman. L. (2000). Some infinity theory for tree ensembles. (Available at www.stat.berkeley.edu/technical reports).\nBreiman, L. (2001). Random forests. Machine Learning J. 45 5– 32.\nBreiman, L. and Friedman, J. (1985). Estimating optimal transformations in multiple regression and correlation. J. Amer. Statist. Assoc. 80 580–619.\nBreiman, L., Friedman, J., Olshen, R. and Stone, C. (1984). Classification and Regression Trees. Wadsworth, Belmont, CA.\nCristianini, N. and Shawe-Taylor, J. (2000). An Introduction to Support Vector Machines. Cambridge Univ. Press.\nDaniel, C. and Wood, F. (1971). Fitting equations to data. Wiley, New York.\nDempster, A. (1998). Logicist statistic 1. Models and Modeling. Statist. Sci. 13 3 248–276.\nDiaconis, P. and Efron, B. (1983). Computer intensive methods in statistics. Scientific American 248 116–131.\nDomingos, P. (1998). Occam’s two razors: the sharp and the blunt. In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining (R. Agrawal and P. Stolorz, eds.) 37–43. AAAI Press, Menlo Park, CA.\nDomingos, P. (1999). The role of Occam’s razor in knowledge discovery. Data Mining and Knowledge Discovery 3 409–425.\nDudoit, S., Fridlyand, J. and Speed, T. (2000). Comparison of discrimination methods for the classification of tumors. (Available at www.stat.berkeley.edu/technical reports).\nFreedman, D. (1987). As others see us: a case study in path analysis (with discussion). J. Ed. Statist. 12 101–223.\nFreedman, D. (1991). Statistical models and shoe leather. Sociological Methodology 1991 (with discussion) 291–358.\nFreedman, D. (1991). Some issues in the foundations of statistics. Foundations of Science 1 19–83.\nFreedman, D. (1994). From association to causation via regression. Adv. in Appl. Math. 18 59–110.\nFreund, Y. and Schapire, R. (1996). Experiments with a new boosting algorithm. In Machine Learning: Proceedings of the Thirteenth International Conference 148–156. Morgan Kaufmann, San Francisco.\nFriedman, J. (1999). Greedy predictive approximation: a gradient boosting machine. Technical report, Dept. Statistics Stanford Univ.\nFriedman, J., Hastie, T. and Tibshirani, R. (2000). Additive logistic regression: a statistical view of boosting. Ann. Statist. 28 337–407.\nGifi, A. (1990). Nonlinear Multivariate Analysis. Wiley, New York.\nHo, T. K. (1998). The random subspace method for constructing decision forests. IEEE Trans. Pattern Analysis and Machine Intelligence 20 832–844.\nLandswher, J., Preibon, D. and Shoemaker, A. (1984). Graphical methods for assessing logistic regression models (with discussion). J. Amer. Statist. Assoc. 79 61–83.\nMcCullagh, P. and Nelder, J. (1989). Generalized Linear Models. Chapman and Hall, London.\nMeisel, W. (1972). Computer-Oriented Approaches to Pattern Recognition. Academic Press, New York.\nMichie, D., Spiegelhalter, D. and Taylor, C. (1994). Machine Learning, Neural and Statistical Classification. Ellis Horwood, New York.\nMosteller, F. and Tukey, J. (1977). Data Analysis and Regression. Addison-Wesley, Redding, MA.\nMountain, D. and Hsiao, C. (1989). A combined structural and flexible functional approach for model energy substitution. J. Amer. Statist. Assoc. 84 76–87.\nStone, M. (1974). Cross-validatory choice and assessment of statistical predictions. J. Roy. Statist. Soc. B 36 111–147.\nVapnik, V. (1995). The Nature of Statistical Learning Theory. Springer, New York.\nVapnik, V (1998). Statistical Learning Theory. Wiley, New York.\nWahba, G. (1990). Spline Models for Observational Data. SIAM, Philadelphia.\nZhang, H. and Singer, B. (1999). Recursive Partitioning in the Health Sciences. Springer, New York."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#d.-r.-cox",
    "href": "contents/two_cultures/two_cultures.html#d.-r.-cox",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "D. R. Cox",
    "text": "D. R. Cox\n\nD. R. Cox is an Honorary Fellow, Nuffield College, Oxford OX1 1NF, United Kingdom, and associate member, Department of Statistics, University of Oxford (e-mail: david.cox@nuffield.oxford.ac.uk).\n\nProfessor Breiman’s interesting paper gives both a clear statement of the broad approach underlying some of his influential and widely admired contributions and outlines some striking applications and developments. He has combined this with a critique of what, for want of a better term, I will call mainstream statistical thinking, based in part on a caricature. Like all good caricatures, it contains enough truth and exposes enough weaknesses to be thought-provoking.\nThere is not enough space to comment on all the many points explicitly or implicitly raised in the paper. There follow some remarks about a few main issues.\nOne of the attractions of our subject is the astonishingly wide range of applications as judged not only in terms of substantive field but also in terms of objectives, quality and quantity of data and so on. Thus any unqualified statement that “in applications” has to be treated sceptically. One of our failings has, I believe, been, in a wish to stress generality, not to set out more clearly the distinctions between different kinds of application and the consequences for the strategy of statistical analysis. Of course we have distinctions between decision-making and inference, between tests and estimation, and between estimation and prediction and these are useful but, I think, are, except perhaps the first, too phrased in terms of the technology rather than the spirit of statistical analysis. I entirely agree with Professor Breiman that it would be an impoverished and extremely unhistorical view of the subject to exclude the kind of work he describes simply because it has no explicit probabilistic base.\nProfessor Breiman takes data as his starting point. I would prefer to start with an issue, a question or a scientific hypothesis, although I would be surprised if this were a real source of disagreement. These issues may evolve, or even change radically, as analysis proceeds. Data looking for a question are not unknown and raise puzzles but are, I believe, atypical in most contexts. Next, even if we ignore design aspects and start with data, key points concern the precise meaning of the data, the possible biases arising from the method of ascertainment, the possible presence of major distorting measurement errors and the nature of processes underlying missing and incomplete data and data that evolve in time in a way involving complex interdependencies. For some of these, at least, it is hard to see how to proceed without some notion of probabilistic modeling.\nNext Professor Breiman emphasizes prediction as the objective, success at prediction being the criterion of success, as contrasted with issues of interpretation or understanding. Prediction is indeed important from several perspectives. The success of a theory is best judged from its ability to predict in new contexts, although one cannot dismiss as totally useless theories such as the rational action theory (RAT), in political science, which, as I understand it, gives excellent explanations of the past but which has failed to predict the real political world. In a clinical trial context it can be argued that an objective is to predict the consequences of treatment allocation to future patients, and so on.\nIf the prediction is localized to situations directly similar to those applying to the data there is then an interesting and challenging dilemma. Is it preferable to proceed with a directly empirical black-box approach, as favored by Professor Breiman, or is it better to try to take account of some underlying explanatory process? The answer must depend on the context but I certainly accept, although it goes somewhat against the grain to do so, that there are situations where a directly empirical approach is better. Short term economic forecasting and real-time flood forecasting are probably further exemplars. Key issues are then the stability of the predictor as practical prediction proceeds, the need from time to time for recalibration and so on.\nHowever, much prediction is not like this. Often the prediction is under quite different conditions from the data; what is the likely progress of the incidence of the epidemic of v-CJD in the United Kingdom, what would be the effect on annual incidence of cancer in the United States of reducing by 10% the medical use of X-rays, etc.? That is, it may be desired to predict the consequences of something only indirectly addressed by the data available for analysis. As we move toward such more ambitious tasks, prediction, always hazardous, without some understanding of underlying process and linking with other sources of information, becomes more and more tentative. Formulation of the goals of analysis solely in terms of direct prediction over the data set seems then increasingly unhelpful.\nThis is quite apart from matters where the direct objective is understanding of and tests of subject matter hypotheses about underlying process, the nature of pathways of dependence and so on.\nWhat is the central strategy of mainstream statistical analysis? This can most certainly not be discerned from the pages of Bernoulli, The Annals of Statistics or the Scandinavian Journal of Statistics nor from Biometrika and the Journal of Royal Statistical Society, Series B or even from the application pages of Journal of the American Statistical Association or Applied Statistics, estimable though all these journals are. Of course as we move along the list, there is an increase from zero to 100% in the papers containing analyses of “real” data. But the papers do so nearly always to illustrate technique rather than to explain the process of analysis and interpretation as such. This is entirely legitimate, but is completely different from live analysis of current data to obtain subject-matter conclusions or to help solve specific practical issues. Put differently, if an important conclusion is reached involving statistical analysis it will be reported in a subject-matter journal or in a written or verbal report to colleagues, government or business. When that happens, statistical details are typically and correctly not stressed. Thus the real procedures of statistical analysis can be judged only by looking in detail at specific cases, and access to these is not always easy. Failure to discuss enough the principles involved is a major criticism of the current state of theory.\nI think tentatively that the following quite commonly applies. Formal models are useful and often almost, if not quite, essential for incisive thinking. Descriptively appealing and transparent methods with a firm model base are the ideal. Notions of significance tests, confidence intervals, posterior intervals and all the formal apparatus of inference are valuable tools to be used as guides, but not in a mechanical way; they indicate the uncertainty that would apply under somewhat idealized, maybe very idealized, conditions and as such are often lower bounds to real uncertainty. Analyses and model development are at least partly exploratory. Automatic methods of model selection (and of variable selection in regression-like problems) are to be shunned or, if use is absolutely unavoidable, are to be examined carefully for their effect on the final conclusions. Unfocused tests of model adequacy are rarely helpful.\nBy contrast, Professor Breiman equates mainstream applied statistics to a relatively mechanical process involving somehow or other choosing a model, often a default model of standard form, and applying standard methods of analysis and goodness-of-fit procedures. Thus for survival data choose a priori the proportional hazards model. (Note, incidentally, that in the paper, often quoted but probably rarely read, that introduced this approach there was a comparison of several of the many different models that might be suitable for this kind of data.) It is true that many of the analyses done by nonstatisticians or by statisticians under severe time constraints are more or less like those Professor Breiman describes. The issue then is not whether they could ideally be improved, but whether they capture enough of the essence of the information in the data, together with some reasonable indication of precision as a guard against under or overinterpretation. Would more refined analysis, possibly with better predictive power and better fit, produce subject-matter gains? There can be no general answer to this, but one suspects that quite often the limitations of conclusions lie more in weakness of data quality and study design than in ineffective analysis.\nThere are two broad lines of development active at the moment arising out of mainstream statistical ideas. The first is the invention of models strongly tied to subject-matter considerations, representing underlying dependencies, and their analysis, perhaps by Markov chain Monte Carlo methods. In fields where subject-matter considerations are largely qualitative, we see a development based on Markov graphs and their generalizations. These methods in effect assume, subject in principle to empirical test, more and more about the phenomena under study. By contrast, there is an emphasis on assuming less and less via, for example, kernel estimates of regression functions, generalized additive models and so on. There is a need to be clearer about the circumstances favoring these two broad approaches, synthesizing them where possible.\nMy own interest tends to be in the former style of work. From this perspective Cox and Wermuth (1996, page 15) listed a number of requirements of a statistical model. These are to establish a link with background knowledge and to set up a connection with previous work, to give some pointer toward a generating process, to have primary parameters with individual clear subject-matter interpretations, to specify haphazard aspects well enough to lead to meaningful assessment of precision and, finally, that the fit should be adequate. From this perspective, fit, which is broadly related to predictive success, is not the primary basis for model choice and formal methods of model choice that take no account of the broader objectives are suspect in the present context. In a sense these are efforts to establish data descriptions that are potentially causal, recognizing that causality, in the sense that a natural scientist would use the term, can rarely be established from one type of study and is at best somewhat tentative.\nProfessor Breiman takes a rather defeatist attitude toward attempts to formulate underlying processes; is this not to reject the base of much scientific progress? The interesting illustrations given by Beveridge (1952), where hypothesized processes in various biological contexts led to important progress, even though the hypotheses turned out in the end to be quite false, illustrate the subtlety of the matter. Especially in the social sciences, representations of underlying process have to be viewed with particular caution, but this does not make them fruitless.\nThe absolutely crucial issue in serious mainstream statistics is the choice of a model that will translate key subject-matter questions into a form for analysis and interpretation. If a simple standard model is adequate to answer the subject matter question, this is fine: there are severe hidden penalties for overelaboration. The statistical literature, however, concentrates on how to do"
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#brad-efron",
    "href": "contents/two_cultures/two_cultures.html#brad-efron",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "Brad Efron",
    "text": "Brad Efron\n\nBrad Efron is Professor, Department of Statistics, Sequoia Hall, 390 Serra Mall, Stanford University, Stanford, California 94305–4065 (e-mail: brad@stat.stanford.edu).\n\nAt first glance Leo Breiman’s stimulating paper looks like an argument against parsimony and scientific insight, and in favor of black boxes with lots of knobs to twiddle. At second glance it still looks that way, but the paper is stimulating, and Leo has some important points to hammer home. At the risk of distortion I will try to restate one of those points, the most interesting one in my opinion, using less confrontational and more historical language.\nFrom the point of view of statistical development the twentieth century might be labeled “100 years of unbiasedness.” Following Fisher’s lead, most of our current statistical theory and practice revolves around unbiased or nearly unbiased estimates (particularly MLEs), and tests based on such estimates. The power of this theory has made statistics the the analysis, an important and indeed fascinating question, but a secondary step. Better a rough answer to the right question than an exact answer to the wrong question, an aphorism, due perhaps to Lord Kelvin, that I heard as an undergraduate in applied mathematics.\nI have stayed away from the detail of the paper but will comment on just one point, the interesting theorem of Vapnik about complete separation. This confirms folklore experience with empirical logistic regression that, with a largish number of explanatory variables, complete separation is quite likely to occur. It is interesting that in mainstream thinking this is, I think, regarded as insecure in that complete separation is thought to be a priori unlikely and the estimated separating plane unstable. Presumably bootstrap and cross-validation ideas may give here a quite misleading illusion of stability. Of course if the complete separator is subtle and stable Professor Breiman’s methods will emerge triumphant and ultimately it is an empirical question in each application as to what happens.\nIt will be clear that while I disagree with the main thrust of Professor Breiman’s paper I found it stimulating and interesting.\ndominant interpretational methodology in dozens of fields, but, as we say in California these days, it is power purchased at a price: the theory requires a modestly high ratio of signal to noise, sample size to number of unknown parameters, to have much hope of success. “Good experimental design” amounts to enforcing favorable conditions for unbiased estimation and testing, so that the statistician won’t find himself or herself facing 100 data points and 50 parameters.\nNow it is the twenty-first century when, as the paper reminds us, we are being asked to face problems that never heard of good experimental design. Sample sizes have swollen alarmingly while goals grow less distinct (“find interesting data structure”). New algorithms have arisen to deal with new problems, a healthy sign it seems to me even if the innovators aren’t all professional statisticians. There are enough physicists to handle the physics case load, but there are fewer statisticians and more statistics problems, and we need all the help we can get. An attractive feature of Leo’s paper is his openness to new ideas whatever their source.\nThe new algorithms often appear in the form of black boxes with enormous numbers of adjustable parameters (“knobs to twiddle”), sometimes more knobs than data points. These algorithms can be quite successful as Leo points out, sometimes more so than their classical counterparts. However, unless the bias-variance trade-off has been suspended to encourage new statistical industries, their success must hinge on some form of biased estimation. The bias may be introduced directly as with the “regularization” of overparameterized linear models, more subtly as in the pruning of overgrown regression trees, or surreptitiously as with support vector machines, but it has to be lurking somewhere inside the theory.\nOf course the trouble with biased estimation is that we have so little theory to fall back upon. Fisher’s information bound, which tells us how well a (nearly) unbiased estimator can possibly perform, is of no help at all in dealing with heavily biased methodology. Numerical experimentation by itself, unguided by theory, is prone to faddish wandering:\nRule 1. New methods always look better than old ones. Neural nets are better than logistic regression, support vector machines are better than neural nets, etc. In fact it is very difficult to run an honest simulation comparison, and easy to inadvertently cheat by choosing favorable examples, or by not putting as much effort into optimizing the dull old standard as the exciting new challenger.\nRule 2. Complicated methods are harder to criticize than simple ones. By now it is easy to check the efficiency of a logistic regression, but it is no small matter to analyze the limitations of a support vector machine. One of the best things statisticians do, and something that doesn’t happen outside our profession, is clarify the inferential basis of a proposed new methodology, a nice recent example being Friedman, Hastie, and Tibshirani’s analysis of “boosting,” (2000). The past half-century has seen the clarification process successfully at work on nonparametrics, robustness and survival analysis. There has even been some success with biased estimation in the form of Stein shrinkage and empirical Bayes, but I believe the hardest part of this work remains to be done. Papers like Leo’s are a call for more analysis and theory, not less.\nPrediction is certainly an interesting subject but Leo’s paper overstates both its role and our profession’s lack of interest in it.\n\nThe “prediction culture,” at least around Stanford, is a lot bigger than 2%, though its constituency changes and most of us wouldn’t welcome being typecast.\nEstimation and testing are a form of prediction: “In our sample of 20 patients drug A outperformed drug B; would this still be true if we went on to test all possible patients?”\nPrediction by itself is only occasionally sufficient. The post office is happy with any method that predicts correct addresses from hand-written scrawls. Peter Gregory undertook his study for prediction purposes, but also to better understand the medical basis of hepatitis. Most statistical surveys have the identification of causal factors as their ultimate goal.\n\nThe hepatitis data was first analyzed by Gail Gong in her 1982 Ph.D. thesis, which concerned prediction problems and bootstrap methods for improving on cross-validation. (Cross-validation itself is an uncertain methodology that deserves further critical scrutiny; see, for example, Efron and Tibshirani, 1996). The Scientific American discussion is quite brief, a more thorough description appearing in Efron and Gong (1983). Variables 12 or 17 (13 or 18 in Efron and Gong’s numbering) appeared as “important” in 60% of the bootstrap simulations, which might be compared with the 59% for variable 19, the most for any single explanator.\nIn what sense are variable 12 or 17 or 19 “important” or “not important”? This is the kind of interesting inferential question raised by prediction methodology. Tibshirani and I made a stab at an answer in our 1998 annals paper. I believe that the current interest in statistical prediction will eventually invigorate traditional inference, not eliminate it.\nA third front seems to have been opened in the long-running frequentist-Bayesian wars by the advocates of algorithmic prediction, who don’t really believe in any inferential school. Leo’s paper is at its best when presenting the successes of algorithmic modeling, which comes across as a positive development for both statistical practice and theoretical innovation. This isn’t an argument against traditional data modeling anymore than splines are an argument against polynomials. The whole point of science is to open up black boxes, understand their insides, and build better boxes for the purposes of mankind. Leo himself is a notably successful scientist, so we can hope that the present paper was written more as an advocacy device than as the confessions of a born-again black boxist."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#bruce-hoadley",
    "href": "contents/two_cultures/two_cultures.html#bruce-hoadley",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "Bruce Hoadley",
    "text": "Bruce Hoadley\n\nDr. Bruce Hoadley is with Fair, Isaac and Co., Inc., 120 N. Redwood Drive, San Rafael, California 94903-1996 (e-mail: BruceHoadley@ FairIsaac.com).\n\n\nINTRODUCTION\nProfessor Breiman’s paper is an important one for statisticians to read. He and Statistical Science should be applauded for making this kind of material available to a large audience. His conclusions are consistent with how statistics is often practiced in business. This discussion will consist of an anecdotal recital of my encounters with the algorithmic modeling culture. Along the way, areas of mild disagreement with Professor Breiman are discussed. I also include a few proposals for research topics in algorithmic modeling.\n\n\nCASE STUDY OF AN ALGORITHMIC MODELING CULTURE\nAlthough I spent most of my career in management at Bell Labs and Bellcore, the last seven years have been with the research group at Fair, Isaac. This company provides all kinds of decision support solutions to several industries, and is very well known for credit scoring. Credit scoring is a great example of the problem discussed by Professor Breiman. The input variables, \\(\\mathbf{x}\\), might come from company databases or credit bureaus. The output variable, \\(y\\), is some indicator of credit worthiness.\nCredit scoring has been a profitable business for Fair, Isaac since the 1960s, so it is instructive to look at the Fair, Isaac analytic approach to see how it fits into the two cultures described by Professor Breiman. The Fair, Isaac approach was developed by engineers and operations research people and was driven by the needs of the clients and the quality of the data. The influences of the statistical community were mostly from the nonparametric side things like jackknife and bootstrap.\nConsider an example of behavior scoring, which is used in credit card account management. For pedagogical reasons, I consider a simplified version (in the real world, things get more complicated) of monthly behavior scoring. The input variables, \\(\\mathbf{x}\\), in this simplified version, are the monthly bills and payments over the last 12 months. So the dimension of \\(\\mathbf{x}\\) is 24. The output variable is binary and is the indicator of no severe delinquency over the next 6 months. The goal is to estimate the function, \\(f(\\mathbf{x}) = \\log\\left(\\frac{P(\\mathbf{y} = 1 | \\mathbf{x})}{P(\\mathbf{y} = 0 | \\mathbf{x})}\\right)\\). Professor Breiman argues that some kind of simple logistic regression from the data modeling culture is not the way to solve this problem. I agree. Let’s take a look at how the engineers at Fair, Isaac solved this problem—way back in the 1960s and 1970s.\nThe general form used for \\(f(\\mathbf{x})\\) was called a segmented scorecard. The process for developing a segmented scorecard was clearly an algorithmic modeling process.\nThe first step was to transform \\(\\mathbf{x}\\) into many interpretable variables called prediction characteristics. This was done in stages. The first stage was to compute several time series derived from the original two. An example is the time series of months delinquent—a nonlinear function. The second stage was to define characteristics as operators on the time series. For example, the number of times in the last six months that the customer was more than two months delinquent. This process can lead to thousands of characteristics. A subset of these characteristics passes a screen for further analysis.\nThe next step was to segment the population based on the screened characteristics. The segmentation was done somewhat informally. But when I looked at the process carefully, the segments turned out to be the leaves of a shallow-to-medium tree. And the tree was built sequentially using mostly binary splits based on the best splitting characteristics—defined in a reasonable way. The algorithm was manual, but similar in concept to the CART algorithm, with a different purity index.\nNext, a separate function, \\(f(\\mathbf{x})\\), was developed for each segment. The function used was called a scorecard. Each characteristic was chopped up into discrete intervals or sets called attributes. A scorecard was a linear function of the attribute indicator (dummy) variables derived from the characteristics. The coefficients of the dummy variables were called score weights.\nThis construction amounted to an explosion of dimensionality. They started with 24 predictors. These were transformed into hundreds of characteristics and pared down to about 100 characteristics. Each characteristic was discretized into about 10 attributes, and there were about 10 segments. This makes 100 × 10 × 10 = 10 000 features. Yes indeed, dimensionality is a blessing.\nWhat Fair, Isaac calls a scorecard is now elsewhere called a generalized additive model (GAM) with bin smoothing. However, a simple GAM would not do. Client demand, legal considerations and robustness over time led to the concept of score engineering. For example, the score had to be monotonically decreasing in certain delinquency characteristics. Prior judgment also played a role in the design of scorecards. For some characteristics, the score weights were shrunk toward zero in order to moderate the influence of these characteristics. For other characteristics, the score weights were expanded in order to increase the influence of these characteristics. These adjustments were not done willy-nilly. They were done to overcome known weaknesses in the data.\nSo how did these Fair, Isaac pioneers fit these complicated GAM models back in the 1960s and 1970s? Logistic regression was not generally available. And besides, even today’s commercial GAM software will not handle complex constraints. What they did was to maximize (subject to constraints) a measure called divergence, which measures how well the score, \\(S\\), separates the two populations with different values of \\(y\\). The formal definition of divergence is \\(\\frac{2(E[S|y = 1] - E[S|y = 0])^2}{(V[S|y=1] + V[S|y=0])}\\). This constrained fitting was done with a heuristic nonlinear programming algorithm. A linear transformation was used to convert to a log odds scale.\nCharacteristic selection was done by analyzing the change in divergence after adding (removing) each candidate characteristic to (from) the current best model. The analysis was done informally to achieve good performance on the test sample. There were no formal tests of fit and no tests of score weight statistical significance. What counted was performance on the test sample, which was a surrogate for the future real world.\nThese early Fair, Isaac engineers were ahead of their time and charter members of the algorithmic modeling culture. The score formula was linear in an exploded dimension. A complex algorithm was used to fit the model. There was no claim that the final score formula was correct, only that it worked well on the test sample. This approach grew naturally out of the demands of the business and the quality of the data. The overarching goal was to develop tools that would help clients make better decisions through data. What emerged was a very accurate and palatable algorithmic modeling solution, which belies Breiman’s statement: “The algorithmic modeling methods available in the pre-1980s decades seem primitive now.” At a recent ASA meeting, I heard talks on treed regression, which looked like segmented scorecards to me.\nAfter a few years with Fair, Isaac, I developed a talk entitled, “Credit Scoring—A Parallel Universe of Prediction and Classification.” The theme was that Fair, Isaac developed in parallel many of the concepts used in modern algorithmic modeling.\nCertain aspects of the data modeling culture crept into the Fair, Isaac approach. The use of divergence was justified by assuming that the score distributions were approximately normal. So rather than making assumptions about the distribution of the inputs, they made assumptions about the distribution of the output. This assumption of normality was supported by a central limit theorem, which said that sums of many random variables are approximately normal—even when the component random variables are dependent and multiples of dummy random variables.\nModern algorithmic classification theory has shown that excellent classifiers have one thing in common, they all have large margin. Margin, \\(M\\), is a random variable that measures the comfort level with which classifications are made. When the correct classification is made, the margin is positive; it is negative otherwise. Since margin is a random variable, the precise definition of large margin is tricky. It does not mean that \\(E[M]\\) is large. When I put my data modeling hat on, I surmised that large margin means that \\(E[M]/\\sqrt{V[M]}\\) is large. Lo and behold, with this definition, large margin means large divergence.\nSince the good old days at Fair, Isaac, there have been many improvements in the algorithmic modeling approaches. We now use genetic algorithms to screen very large structured sets of prediction characteristics. Our segmentation algorithms have been automated to yield even more predictive systems. Our palatable GAM modeling tool now handles smooth splines, as well as splines mixed with step functions, with all kinds of constraint capability. Maximizing divergence is still a favorite, but we also maximize constrained GLM likelihood functions. We also are experimenting with computationally intensive algorithms that will optimize any objective function that makes sense in the business environment. All of these improvements are squarely in the culture of algorithmic modeling.\n\n\nOVERFITTING THE TEST SAMPLE\nProfessor Breiman emphasizes the importance of performance on the test sample. However, this can be overdone. The test sample is supposed to represent the population to be encountered in the future. But in reality, it is usually a random sample of the current population. High performance on the test sample does not guarantee high performance on future samples, things do change. There are practices that can be followed to protect against change.\nOne can monitor the performance of the models over time and develop new models when there has been sufficient degradation of performance. For some of Fair, Isaac’s core products, the redevelopment cycle is about 18–24 months. Fair, Isaac also does “score engineering” in an attempt to make the models more robust over time. This includes damping the influence of individual characteristics, using monotone constraints and minimizing the size of the models subject to performance constraints on the current test sample. This score engineering amounts to moving from very nonparametric (no score engineering) to more semiparametric (lots of score engineering).\n\n\nSPIN-OFFS FROM THE DATA MODELING CULTURE\nIn Section 6 of Professor Breiman’s paper, he says that “multivariate analysis tools in statistics are frozen at discriminant analysis and logistic regression in classification” This is not necessarily all that bad. These tools can carry you very far as long as you ignore all of the textbook advice on how to use them. To illustrate, I use the saga of the Fat Scorecard.\nEarly in my research days at Fair, Isaac, I was searching for an improvement over segmented scorecards. The idea was to develop first a very good global scorecard and then to develop small adjustments for a number of overlapping segments. To develop the global scorecard, I decided to use logistic regression applied to the attribute dummy variables. There were 36 characteristics available for fitting. A typical scorecard has about 15 characteristics. My variable selection was structured so that an entire characteristic was either in or out of the model. What I discovered surprised me. All models fit with anywhere from 27 to 36 characteristics had the same performance on the test sample. This is what Professor Breiman calls “Rashomon and the multiplicity of good models.” To keep the model as small as possible, I chose the one with 27 characteristics. This model had 162 score weights (logistic regression coefficients), whose P values ranged from 0.0001 to 0.984, with only one less than 0.05; i.e., statistically significant. The confidence intervals for the 162 score weights were useless. To get this great scorecard, I had to ignore the conventional wisdom on how to use logistic regression.\nSo far, all I had was the scorecard GAM. So clearly I was missing all of those interactions that just had to be in the model. To model the interactions, I tried developing small adjustments on various overlapping segments. No matter how hard I tried, nothing improved the test sample performance over the global scorecard. I started calling it the Fat Scorecard.\nEarlier, on this same data set, another Fair, Isaac researcher had developed a neural network with 2,000 connection weights. The Fat Scorecard slightly outperformed the neural network on the test sample. I cannot claim that this would work for every data set. But for this data set, I had developed an excellent algorithmic model with a simple data modeling tool.\nWhy did the simple additive model work so well? One idea is that some of the characteristics in the model are acting as surrogates for certain interaction terms that are not explicitly in the model. Another reason is that the scorecard is really a sophisticated neural net. The inputs are the original inputs. Associated with each characteristic is a hidden node. The summation functions coming into the hidden nodes are the transformations defining the characteristics. The transfer functions of the hidden nodes are the step functions (compiled from the score weights)—all derived from the data. The final output is a linear function of the outputs of the hidden nodes. The result is highly nonlinear and interactive, when looked at as a function of the original inputs.\nThe Fat Scorecard study had an ingredient that is rare. We not only had the traditional test sample, but had three other test samples, taken one, two, and three years later. In this case, the Fat Scorecard outperformed the more traditional thinner scorecard for all four test samples. So the feared overfitting to the traditional test sample never materialized. To get a better handle on this you need an understanding of how the relationships between variables evolve over time.\nI recently encountered another connection between algorithmic modeling and data modeling. In classical multivariate discriminant analysis, one assumes that the prediction variables have a multivariate normal distribution. But for a scorecard, the prediction variables are hundreds of attribute dummy variables, which are very nonnormal. However, if you apply the discriminant analysis algorithm to the attribute dummy variables, you can get a great algorithmic model, even though the assumptions of discriminant analysis are severely violated.\n\n\nA SOLUTION TO THE OCCAM DILEMMA\nI think that there is a solution to the Occam dilemma without resorting to goal-oriented arguments. Clients really do insist on interpretable functions, \\(f(\\mathbf{x})\\). Segmented palatable scorecards are very interpretable by the customer and are very accurate. Professor Breiman himself gave single trees an A+ on interpretability. The shallow-to-medium tree in a segmented scorecard rates an A++. The palatable scorecards in the leaves of the trees are built from interpretable (possibly complex) characteristics. Sometimes we can’t implement them until the lawyers and regulators approve. And that requires super interpretability. Our more sophisticated products have 10 to 20 segments and up to 100 characteristics (not all in every segment). These models are very accurate and very interpretable.\nI coined a phrase called the “Ping-Pong theorem.” This theorem says that if we revealed to Professor Breiman the performance of our best model and gave him our data, then he could develop an algorithmic model using random forests, which would outperform our model. But if he revealed to us the performance of his model, then we could develop a segmented scorecard, which would outperform his model. We might need more characteristics, attributes and segments, but our experience in this kind of contest is on our side.\nHowever, all the competing models in this game of Ping-Pong would surely be algorithmic models. But some of them could be interpretable.\n\n\nTHE ALGORITHM TUNING DILEMMA\nAs far as I can tell, all approaches to algorithmic model building contain tuning parameters, either explicit or implicit. For example, we use penalized objective functions for fitting and marginal contribution thresholds for characteristic selection. With experience, analysts learn how to set these tuning parameters in order to get excellent test sample or cross-validation results. However, in industry and academia, there is sometimes a little tinkering, which involves peeking at the test sample. The result is some bias in the test sample or cross-validation results. This is the same kind of tinkering that upsets test of fit pureness. This is a challenge for the algorithmic modeling approach. How do you optimize your results and get an unbiased estimate of the generalization error?\n\n\nGENERALIZING THE GENERALIZATION ERROR\nIn most commercial applications of algorithmic modeling, the function, \\(f(\\mathbf{x})\\), is used to make decisions. In some academic research, classification is used as a surrogate for the decision process, and misclassification error is used as a surrogate for profit. However, I see a mismatch between the algorithms used to develop the models and the business measurement of the model’s value. For example, at Fair, Isaac, we frequently maximize divergence. But when we argue the model’s value to the clients, we don’t necessarily brag about the great divergence. We try to use measures that the client can relate to. The ROC curve is one favorite, but it may not tell the whole story. Sometimes, we develop simulations of the client’s business operation to show how the model will improve their situation. For example, in a transaction fraud control process, some measures of interest are false positive rate, speed of detection and dollars saved when 0.5% of the transactions are flagged as possible frauds. The 0.5% reflects the number of transactions that can be processed by the current fraud management staff. Perhaps what the client really wants is a score that will maximize the dollars saved in their fraud control system. The score that maximizes test set divergence or minimizes test set misclassifications does not do it. The challenge for algorithmic modeling is to find an algorithm that maximizes the generalization dollars saved, not generalization error.\nWe have made some progress in this area using ideas from support vector machines and boosting. By manipulating the observation weights used in standard algorithms, we can improve the test set performance on any objective of interest. But the price we pay is computational intensity.\n\n\nMEASURING IMPORTANCE—IS IT REALLY POSSIBLE?\nI like Professor Breiman’s idea for measuring the importance of variables in black box models. A Fair, Isaac spin on this idea would be to build accurate models for which no variable is much more important than other variables. There is always a chance that a variable and its relationships will change in the future. After that, you still want the model to work. So don’t make any variable dominant.\nI think that there is still an issue with measuring importance. Consider a set of inputs and an algorithm that yields a black box, for which \\(x_1\\) is important. From the “Ping Pong theorem” there exists a set of input variables, excluding \\(x_1\\) and an algorithm that will yield an equally accurate black box. For this black box, \\(x_1\\) is unimportant.\n\n\nIN SUMMARY\nAlgorithmic modeling is a very important area of statistics. It has evolved naturally in environments with lots of data and lots of decisions. But you can do it without suffering the Occam dilemma; for example, use medium trees with interpretable"
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#emanuel-parzen",
    "href": "contents/two_cultures/two_cultures.html#emanuel-parzen",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "Emanuel Parzen",
    "text": "Emanuel Parzen\n\nEmanuel Parzen is Distinguished Professor, Department of Statistics, Texas A&M University, 415 C Block Building, College Station, Texas 77843 (e-mail: eparzen@stat.tamu.edu).\n\n\n1. BREIMAN DESERVES OUR APPRECIATION\nI strongly support the view that statisticians must face the crisis of the difficulties in their practice of regression. Breiman alerts us to systematic blunders (leading to wrong conclusions) that have been committed applying current statistical practice of data modeling. In the spirit of “statistician, avoid doing harm” I propose that the first goal of statistical ethics should be to guarantee to our clients that any mistakes in our analysis are unlike any mistakes that statisticians have made before.\nThe two goals in analyzing data which Leo calls prediction and information I prefer to describe as “management” and “science.” Management seeks profit, practical answers (predictions) useful for decision making in the short run. Science seeks truth, fundamental knowledge about nature which provides understanding and control in the long run. As a historical note, Student’s t-test has many scientific applications but was invented by Student as a management tool to make Guinness beer better (bitter?).\nBreiman does an excellent job of presenting the case that the practice of statistical science, using only the conventional data modeling culture, needs reform. He deserves much thanks for alerting us to the algorithmic modeling culture. Breiman warns us that “if the model is a poor emulation of nature, the conclusions maybe wrong.” This situation, which I call “the right answer to the wrong question,” is called by statisticians “the error of the third kind.” Engineers at M.I.T. define “suboptimization” as “elegantly solving the wrong problem.”\nGAMs in the leaves. They are very accurate and interpretable. And you can do it with data modeling tools as long as you (i) ignore most textbook advice, (ii) embrace the blessing of dimensionality, (iii) use constraints in the fitting optimizations (iv) use regularization, and (v) validate the results.\nBreiman presents the potential benefits of algorithmic models (better predictive accuracy than data models, and consequently better information about the underlying mechanism and avoiding questionable conclusions which results from weak predictive accuracy) and support vector machines (which provide almost perfect separation and discrimination between two classes by increasing the dimension of the feature set). He convinces me that the methods of algorithmic modeling are important contributions to the tool kit of statisticians.\nIf the profession of statistics is to remain healthy, and not limit its research opportunities, statisticians must learn about the cultures in which Breiman works, but also about many other cultures of statistics.\n\n\n2. HYPOTHESES TO TEST TO AVOID BLUNDERS OF STATISTICAL MODELING\nBreiman deserves our appreciation for pointing out generic deviations from standard assumptions (which I call bivariate dependence and two-sample conditional clustering) for which we should routinely check. “Test null hypothesis” can be a useful algorithmic concept if we use tests that diagnose in a model-free way the directions of deviation from the null hypothesis model.\nBivariate dependence (correlation) may exist between features [independent (input) variables] in a regression causing them to be proxies for each other and our models to be unstable with different forms of regression models being equally well fitting. We need tools to routinely test the hypothesis of statistical independence of the distributions of independent (input) variables.\nTwo sample conditional clustering arises in the distributions of independent (input) variables to discriminate between two classes, which we call the conditional distribution of input variables X given each class. Class I may have only one mode (cluster) at low values of X while class II has two modes\n(clusters) at low and high values of X. We would like to conclude that high values of X are observed only for members of class II but low values of X occur for members of both classes. The hypothesis we propose testing is equality of the pooled distribution of both samples and the conditional distribution of sample I, which is equivalent to P class IX = P class I. For successful discrimination one seeks to increase the number (dimension) of inputs (features) X to make P class IX close to 1 or 0.\n\n\n3. STATISTICAL MODELING, MANY CULTURES, STATISTICAL METHODS MINING\nBreiman speaks of two cultures of statistics; I believe statistics has many cultures. At specialized workshops (on maximum entropy methods or robust methods or Bayesian methods or ) a main topic of conversation is “Why don’t all statisticians think like us?”\nI have my own eclectic philosophy of statistical modeling to which I would like to attract serious attention. I call it “statistical methods mining” which seeks to provide a framework to synthesize and apply the past half-century of methodological progress in computationally intensive methods for statistical modeling, including EDA (exploratory data analysis), FDA (functional data analysis), density estimation, Model DA (model selection criteria data analysis), Bayesian priors on function space, continuous parameter regression analysis and reproducing kernels, fast algorithms, Kalman filtering, complexity, information, quantile data analysis, nonparametric regression, conditional quantiles.\nI believe “data mining” is a special case of “data modeling.” We should teach in our introductory courses that one meaning of statistics is “statistical data modeling done in a systematic way” by an iterated series of stages which can be abbreviated SIEVE (specify problem and general form of models, identify tentatively numbers of parameters and specialized models, estimate parameters, validate goodness-of-fit of estimated models, estimate final model nonparametrically or algorithmically). MacKay and Oldford (2000) brilliantly present the statistical method as a series of stages PPDAC (problem, plan, data, analysis, conclusions).\n\n\n4. QUANTILE CULTURE, ALGORITHMIC MODELS\nA culture of statistical data modeling based on quantile functions, initiated in Parzen (1979), has been my main research interest since 1976. In my discussion to Stone (1977) I outlined a novel approach to estimation of conditional quantile functions which I only recently fully implemented. I would like to extend the concept of algorithmic statistical models in two ways: (1) to mean data fitting by representations which use approximation theory and numerical analysis; (2) to use the notation of probability to describe empirical distributions of samples (data sets) which are not assumed to be generated by a random mechanism.\nMy quantile culture has not yet become widely applied because “you cannot give away a good idea, you have to sell it” (by integrating it in computer programs usable by applied statisticians and thus promote statistical methods mining).\nA quantile function \\(Q_u\\), \\(0 \\leq u \\leq 1\\), is the inverse \\(F^{-1}(u)\\) of a distribution function \\(F(\\mathbf{x})\\), \\(-\\infty &lt; \\mathbf{x} &lt; \\infty\\). Its rigorous definition is \\(Q_u = \\inf_{\\mathbf{x}} F(\\mathbf{x}) \\geq u\\). When \\(F\\) is continuous with density \\(f\\), \\(F(Q_u) = u\\), \\(q_u = Q(u) = 1/f(Q_u)\\). We use the notation \\(Q\\) for a true unknown quantile function, \\(\\tilde{Q}\\) for a raw estimator from a sample, and \\(\\hat{Q}\\) for a smooth estimator of the true \\(Q\\).\nConcepts defined for \\(Q_u\\) can be defined also for other versions of quantile functions. Quantile functions can “compress data” by a five-number summary, values of \\(Q_u\\) at \\(u = 0.5, 0.25, 0.75, 0.1, 0.9\\) (or \\(0.05, 0.95\\)). Measures of location and scale are \\(Q_M = 0.5 - Q_{0.25} + Q_{0.75}\\), \\(Q_D = 2(Q_{0.75} - Q_{0.25})\\). To use quantile functions to identify distributions fitting data we propose the quantile quartile function \\(Q_u = Q_u - Q_M/Q_D\\). Five-number summary of distribution becomes \\(Q_M, Q_D, Q_{0.5}, \\text{skewness}, Q_{0.1}, \\text{left-tail}, Q_{0.9}, \\text{right-tail}\\). Elegance of \\(Q_u\\) is its universal values at \\(u = 0.25, 0.75\\). Values \\(Q_u &gt; 1\\) are outliers as defined by Tukey EDA.\nFor the fundamental problem of comparison of two distributions \\(F\\) and \\(G\\) we define the comparison distribution \\(D_u(F, G)\\) and comparison density \\(d_u(F, G) = D_u(F, G)\\). For \\(F\\) and \\(G\\) continuous, define \\(D_u(F, G) = G^{-1}(u)\\), \\(d_u(F, G) = g^{-1}(u)/f^{-1}(u)\\) assuming \\(f(\\mathbf{x}) = 0\\) implies \\(g(\\mathbf{x}) = 0\\), written \\(G \\sim F\\). For \\(F\\) and \\(G\\) discrete with probability mass functions \\(p_F\\) and \\(p_G\\) define (assuming \\(G \\sim F\\)) \\(d_u(F, G) = p_G^{-1}(u)/p_F^{-1}(u)\\).\nOur applications of comparison distributions often assume \\(F\\) to be an unconditional distribution and \\(G\\) a conditional distribution. To analyze bivariate data \\(\\mathbf{X}, \\mathbf{Y}\\) a fundamental tool is dependence density \\(d_t(u) = d_u(F_Y, F_{Y|\\mathbf{X}=\\mathbf{Q}_X(t)})\\). When \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are jointly continuous,\n\\[\nd(t, u)\n\\]\n= \\(f_{X,Y}(Q_X(t), Q_Y(u))/f_X(Q_X(t))f_Y(Q_Y(u)).\\)\nThe statistical independence hypothesis \\(F_X \\circ F_Y = F_X \\circ F_Y\\) is equivalent to \\(d_t(u) = 1\\), all \\(t, u\\). A fundamental formula for estimation of conditional quantile functions is\n\\[\nQ_{Y|X=x}(u) = Q_Y(D^{-1}(u; F_Y, F_{Y|X=x}))\n\\]\n= \\(Q_Y(s), u = D(s; F_Y, F_{Y|X=x}).\\)\nTo compare the distributions of two univariate samples, let \\(\\mathbf{Y}\\) denote the continuous response variable and \\(\\mathbf{X}\\) be binary \\(0, 1\\) denoting the population from which \\(\\mathbf{Y}\\) is observed. The comparison density is defined (note \\(F_Y\\) is the pooled distribution function)\n\\[\nd_1(u) = d(u; F_Y, F_{Y|X=1})\n\\]\n= \\(P[X = 1|Y = Q_Y(u)]/P[X = 1].\\)\n\n\n5. QUANTILE IDEAS FOR HIGH DIMENSIONAL DATA ANALYSIS\nBy high dimensional data we mean multivariate data \\(\\mathbf{Y}_1, \\ldots, \\mathbf{Y}_m\\). We form approximate high dimensional comparison densities \\(d_{u_1, \\ldots, u_m}\\) to test statistical independence of the variables and, when we have two samples, \\(d_{1, \\ldots, 1}\\) to test equality of sample I with pooled sample. All our distributions are empirical distributions but we use notation for true distributions in our formulas. Note that\n\\[\n\\int_0^1 du_1 \\ldots \\int_0^1 du_m d(u_1, \\ldots, u_m) d_1(u_1, \\ldots, u_m) = 1.\n\\]\nA decile quantile bin \\(B_{k_1, \\ldots, k_m}\\) is defined to be the set of observations \\(\\mathbf{Y}_1, \\ldots, \\mathbf{Y}_m\\) satisfying, for \\(j = 1, \\ldots, m\\), \\(Q_{\\mathbf{Y}_j}((k_j - 1)/10) &lt; \\mathbf{Y}_j \\leq Q_{\\mathbf{Y}_j}(k_j/10)\\). Instead of deciles \\(k/10\\) we could use \\(k/M\\) for another base \\(M\\).\nTo test the hypothesis that \\(\\mathbf{Y}_1, \\ldots, \\mathbf{Y}_m\\) are statistically independent we form for all \\(k_j = 1, \\ldots, 10\\),\n\\[\nd(k_1, \\ldots, k_m) = P[\\text{Bin}(k_1, \\ldots, k_m)]/\n\\] \\[\nP[\\text{Bin}(k_1, \\ldots, k_m)| \\text{independence}].\n\\]\nTo test equality of distribution of a sample from population I and the pooled sample we form\n\\[\nd_1(k_1, \\ldots, k_m)\n\\]\n= \\(P[\\text{Bin}(k_1, \\ldots, k_m)|\\) population \\(I\\) ]/\n\\(P[\\text{Bin}(k_1, \\ldots, k_m)|\\) pooled sample]\nfor all \\(k_1, \\ldots, k_m\\) such that the denominator is positive and otherwise defined arbitrarily. One can show (letting \\(\\mathbf{X}\\) denote the population observed)\n\\[\nd_1(k_1, \\ldots, k_m) = P[\\mathbf{X} = I] \\text{ observation from}\n\\] \\[\n\\text{Bin}(k_1, \\ldots, k_m) / P[\\mathbf{X} = I].\n\\]\nTo test the null hypotheses in ways that detect directions of deviations from the null hypothesis our recommended first step is quantile data analysis of the values \\(d_{k_1, \\ldots, k_m}\\) and \\(d_{1, k_1, \\ldots, k_m}\\).\nI appreciate this opportunity to bring to the attention of researchers on high dimensional data analysis the potential of quantile methods. My conclusion is that statistical science has many cultures and statisticians will be more successful when they emulate Leo Breiman and apply as many cultures as possible (which I call statistical methods mining). Additional references are on my web site at stat.tamu.edu."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#d.-r.-cox-1",
    "href": "contents/two_cultures/two_cultures.html#d.-r.-cox-1",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "D. R. COX",
    "text": "D. R. COX\nProfessor Cox is a worthy and thoughtful adversary. We walk down part of the trail together and then sharply diverge. To begin, I quote: “Professor Breiman takes data as his starting point. I would prefer to start with an issue, a question, or a scientific hypothesis,” I agree, but would expand the starting list to include the prediction of future events. I have never worked on a project that has started with “Here is a lot of data; let’s look at it and see if we can get some ideas about how to use it.” The data has been put together and analyzed starting with an objective.\n\nC1 Data Models Can Be Useful\nProfessor Cox is committed to the use of data models. I readily acknowledge that there are situations where a simple data model may be useful and appropriate; for instance, if the science of the mechanism producing the data is well enough known to determine the model apart from estimating parameters. There are also situations of great complexity posing important issues and questions in which there is not enough data to resolve the questions to the accuracy desired. Simple models can then be useful in giving qualitative understanding, suggesting future research areas and the kind of additional data that needs to be gathered.\nAt times, there is not enough data on which to base predictions; but policy decisions need to be made. In this case, constructing a model using whatever data exists, combined with scientific common sense and subject-matter knowledge, is a reasonable path. Professor Cox points to examples when he writes:\n\nOften the prediction is under quite different conditions from the data; what is the likely progress of the incidence of the epidemic of v-CJD in the United Kingdom, what would be the effect on annual incidence of cancer in the United States reducing by 10% the medical use of X-rays, etc.? That is, it may be desired to predict the consequences of something only indirectly addressed by the data available for analysis prediction, always hazardous, without some understanding of the underlying process and linking with other sources of information, becomes more and more tentative.\n\nI agree.\n\n\nC2 Data Models Only\nFrom here on we part company. Professor Cox’s discussion consists of a justification of the use of data models to the exclusion of other approaches. For instance, although he admits, ” I certainly accept, although it goes somewhat against the grain to do so, that there are situations where a directly empirical approach is better” the two examples he gives of such situations are short-term economic forecasts and real-time flood forecasts—among the less interesting of all of the many current successful algorithmic applications. In his view, the only use for algorithmic models is short-term forecasting; there are no comments on the rich information about the data and covariates available from random forests or in the many fields, such as pattern recognition, where algorithmic modeling is fundamental.\nHe advocates construction of stochastic data models that summarize the understanding of the phenomena under study. The methodology in the Cox and Wermuth book (1996) attempts to push understanding further by finding casual orderings in the covariate effects. The sixth chapter of this book contains illustrations of this approach on four data sets.\nThe first is a small data set of 68 patients with seven covariates from a pilot study at the University of Mainz to identify psychological and socioeconomic factors possibly important for glucose control in diabetes patients. This is a regression-type problem with the response variable measured by GHb (glycosylated haemoglobin). The model fitting is done by a number of linear regressions and validated by the checking of various residual plots. The only other reference to model validation is the statement, “\\(R^2=0.34\\), reasonably large by the standards usual for this field of study.” Predictive accuracy is not computed, either for this example or for the three other examples.\nMy comments on the questionable use of data models apply to this analysis. Incidentally, I tried to get one of the data sets used in the chapter to conduct an alternative analysis, but it was not possible to get it before my rejoinder was due. It would have been interesting to contrast our two approaches.\n\n\nC3 Approach to Statistical Problems\nBasing my critique on a small illustration in a book is not fair to Professor Cox. To be fairer, I quote his words about the nature of a statistical analysis:\n\nFormal models are useful and often almost, if not quite, essential for incisive thinking. Descriptively appealing and transparent methods with a firm model base are the ideal. Notions of significance tests, confidence intervals, posterior intervals, and all the formal apparatus of inference are valuable tools to be used as guides, but not in a mechanical way; they indicate the uncertainty that would apply under somewhat idealized, maybe very idealized, conditions and as such are often lower bounds to real uncertainty. Analyses and model development are at least partly exploratory. Automatic methods of model selection (and of variable selection in regression like problems) are to be shunned or, if use is absolutely unavoidable, are to be examined carefully for their effect on the final conclusions. Unfocused tests of model adequacy are rarely helpful.\n\nGiven the right kind of data: relatively small sample size and a handful of covariates, I have no doubt that his experience and ingenuity in the craft of model construction would result in an illuminating model. But data characteristics are rapidly changing. In many of the most interesting current problems, the idea of starting with a formal model is not tenable.\n\n\nC4 Changes in Problems\nMy impression from Professor Cox’s comments is that he believes every statistical problem can be best solved by constructing a data model. I believe that statisticians need to be more pragmatic. Given a statistical problem, find a good solution, whether it is a data model, an algorithmic model or (although it is somewhat against my grain), a Bayesian data model or a completely different approach.\nMy work on the 1990 Census Adjustment (Breiman, 1994) involved a painstaking analysis of the sources of error in the data. This was done by a long study of thousands of pages of evaluation documents. This seemed the most appropriate way of answering the question of the accuracy of the adjustment estimates.\nThe conclusion that the adjustment estimates were largely the result of bad data has never been effectively contested and is supported by the results of the Year 2000 Census Adjustment effort. The accuracy of the adjustment estimates was, arguably, the most important statistical issue of the last decade, and could not be resolved by any amount of statistical modeling.\nA primary reason why we cannot rely on data models alone is the rapid change in the nature of statistical problems. The realm of applications of statistics has expanded more in the last twenty-five years than in any comparable period in the history of statistics.\nIn an astronomy and statistics workshop this year, a speaker remarked that in twenty-five years we have gone from being a small sample-size science to a very large sample-size science. Astronomical data bases now contain data on two billion objects comprising over 100 terabytes and the rate of new information is accelerating.\nA recent biostatistics workshop emphasized the analysis of genetic data. An exciting breakthrough is the use of microarrays to locate regions of gene activity. Here the sample size is small, but the number of variables ranges in the thousands. The questions are which specific genes contribute to the occurrence of various types of diseases.\nQuestions about the areas of thinking in the brain are being studied using functional MRI. The data gathered in each run consists of a sequence of 150,000 pixel images. Gigabytes of satellite information are being used in projects to predict and understand short- and long-term environmental and weather changes.\nUnderlying this rapid change is the rapid evolution of the computer, a device for gathering, storing and manipulation of incredible amounts of data, together with technological advances incorporating computing, such as satellites and MRI machines.\nThe problems are exhilarating. The methods used in statistics for small sample sizes and a small number of variables are not applicable. John Rice, in his summary talk at the astronomy and statistics workshop said, “Statisticians have to become opportunistic.” That is, faced with a problem, they must find a reasonable solution by whatever method works. One surprising aspect of both workshops was how opportunistic statisticians faced with genetic and astronomical data had become. Algorithmic methods abounded.\n\n\nC5 Mainstream Procedures and Tools\nProfessor Cox views my critique of the use of data models as based in part on a caricature. Regarding my references to articles in journals such as JASA, he states that they are not typical of mainstream statistical analysis, but are used to illustrate technique rather than explain the process of analysis. His concept of mainstream statistical analysis is summarized in the quote given in my Section C3. It is the kind of thoughtful and careful analysis that he prefers and is capable of.\nFollowing this summary is the statement:\n\nBy contrast, Professor Breiman equates mainstream applied statistics to a relatively mechanical process involving somehow or other choosing a model, often a default model of standard form, and applying standard methods of analysis and goodness-of-fit procedures.\n\nThe disagreement is definitional—what is “mainstream”? In terms of numbers my definition of mainstream prevails, I guess, at a ratio of at least 100 to 1. Simply count the number of people doing their statistical analysis using canned packages, or count the number of SAS licenses.\nIn the academic world, we often overlook the fact that we are a small slice of all statisticians and an even smaller slice of all those doing analyses of data. There are many statisticians and nonstatisticians in diverse fields using data to reach conclusions and depending on tools supplied to them by SAS, SPSS, etc. Their conclusions are important and are sometimes published in medical or other subject-matter journals. They do not have the statistical expertise, computer skills, or time needed to construct more appropriate tools. I was faced with this problem as a consultant when confined to using the BMDP linear regression, stepwise linear regression, and discriminant analysis programs. My concept of decision trees arose when I was faced with nonstandard data that could not be treated by these standard methods.\nWhen I rejoined the university after my consulting years, one of my hopes was to provide better general purpose tools for the analysis of data. The first step in this direction was the publication of the CART book (Breiman et al., 1984). CART and other similar decision tree methods are used in thousands of applications yearly in many fields. It has proved robust and reliable. There are others that are more recent; random forests is the latest. A preliminary version of random forests is free source with f77 code, S+ and R interfaces available at www.stat.berkeley.edu/users/breiman.\nA nearly completed second version will also be put on the web site and translated into Java by the Weka group. My collaborator, Adele Cutler, and I will continue to upgrade, add new features, graphics, and a good interface.\nMy philosophy about the field of academic statistics is that we have a responsibility to provide the many people working in applications outside of academia with useful, reliable, and accurate analysis tools. Two excellent examples are wavelets and decision trees. More are needed."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#brad-efron-1",
    "href": "contents/two_cultures/two_cultures.html#brad-efron-1",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "Brad Efron",
    "text": "Brad Efron\nBrad seems to be a bit puzzled about how to react to my article. I’ll start with what appears to be his biggest reservation.\n\nE1 From Simple to Complex Models\nBrad is concerned about the use of complex models without simple interpretability in their structure, even though these models may be the most accurate predictors possible. But the evolution of science is from simple to complex.\nThe equations of general relativity are considerably more complex and difficult to understand than Newton’s equations. The quantum mechanical equations for a system of molecules are extraordinarily difficult to interpret. Physicists accept these complex models as the facts of life, and do their best to extract usable information from them.\nThere is no consideration given to trying to understand cosmology on the basis of Newton’s equations or nuclear reactions in terms of hard ball models for atoms. The scientific approach is to use these complex models as the best possible descriptions of the physical world and try to get usable information out of them.\nThere are many engineering and scientific applications where simpler models, such as Newton’s laws, are certainly sufficient—say, in structural design. Even here, for larger structures, the model is complex and the analysis difficult. In scientific fields outside statistics, answering questions is done by extracting information from increasingly complex and accurate models.\nThe approach I suggest is similar. In genetics, astronomy and many other current areas statistics is needed to answer questions, construct the most accurate possible model, however complex, and then extract usable information from it.\nRandom forests is in use at some major drug companies whose statisticians were impressed by its ability to determine gene expression (variable importance) in microarray data. They were not concerned about its complexity or black-box appearance.\n\n\nE2 Prediction\n\nLeo’s paper overstates both its [prediction’s] role, and our profession’s lack of interest in it Most statistical surveys have the identification of causal factors as their ultimate role.\n\nMy point was that it is difficult to tell, using goodness-of-fit tests and residual analysis, how well a model fits the data. An estimate of its test set accuracy is a preferable assessment. If, for instance, a model gives predictive accuracy only slightly better than the “all survived” or other baseline estimates, we can’t put much faith in its reliability in the identification of causal factors.\nI agree that often “… statistical surveys have the identification of casual factors as their ultimate role.” I would add that the more predictively accurate the model is, the more faith can be put into the variables that it fingers as important.\n\n\nE3 Variable Importance\nA significant and often overlooked point raised by Brad is what meaning can one give to statements that “variable X is important or not important.” This has puzzled me on and off for quite a while. In fact, variable importance has always been defined operationally. In regression the “important” variables are defined by doing “best subsets” or variable deletion.\nAnother approach used in linear methods such as logistic regression and survival models is to compare the size of the slope estimate for a variable to its estimated standard error. The larger the ratio, the more “important” the variable. Both of these definitions can lead to erroneous conclusions.\nMy definition of variable importance is based on prediction. A variable might be considered important if deleting it seriously affects prediction accuracy. This brings up the problem that if two variables are highly correlated, deleting one or the other of them will not affect prediction accuracy. Deleting both of them may degrade accuracy considerably. The definition used in random forests spots both variables.\n“Importance” does not yet have a satisfactory theoretical definition (I haven’t been able to locate the article Brad references but I’ll keep looking). It depends on the dependencies between the output variable and the input variables, and on the dependencies between the input variables. The problem begs for research.\n\n\nE4 Other Reservations\n\nSample sizes have swollen alarmingly while goals grow less distinct (“find interesting data structure”).\n\nI have not noticed any increasing fuzziness in goals, only that they have gotten more diverse. In the last two workshops I attended (genetics and astronomy) the goals in using the data were clearly laid out. “Searching for structure” is rarely seen even though data may be in the terabyte range.\n\nThe new algorithms often appear in the form of black boxes with enormous numbers of adjustable parameters (“knobs to twiddle”).\n\nThis is a perplexing statement and perhaps I don’t understand what Brad means. Random forests has only one adjustable parameter that needs to be set for a run, is insensitive to the value of this parameter over a wide range, and has a quick and simple way for determining a good value. Support vector machines depend on the settings of 1–2 parameters. Other algorithmic models are similarly sparse in the number of knobs that have to be twiddled.\n\nNew methods always look better than old ones. Complicated models are harder to criticize than simple ones.\n\nIn 1992 I went to my first NIPS conference. At that time, the exciting algorithmic methodology was neural nets. My attitude was grim skepticism. Neural nets had been given too much hype, just as AI had been given and failed expectations. I came away a believer. Neural nets delivered on the bottom line! In talk after talk, in problem after problem, neural nets were being used to solve difficult prediction problems with test set accuracies better than anything I had seen up to that time.\nMy attitude toward new and/or complicated methods is pragmatic. Prove that you’ve got a better mousetrap and I’ll buy it. But the proof had better be concrete and convincing.\nBrad questions where the bias and variance have gone. It is surprising when, trained in classical bias variance terms and convinced of the curse of dimensionality, one encounters methods that can handle thousands of variables with little loss of accuracy. It is not voodoo statistics; there is some simple theory that illuminates the behavior of random forests (Breiman, 1999). I agree that more theoretical work is needed to increase our understanding.\nBrad is an innovative and flexible thinker who has contributed much to our field. He is opportunistic in problem solving and may, perhaps not overtly, already have algorithmic modeling in his bag of tools."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#bruce-hoadley-1",
    "href": "contents/two_cultures/two_cultures.html#bruce-hoadley-1",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "Bruce Hoadley",
    "text": "Bruce Hoadley\nI thank Bruce Hoadley for his description of the algorithmic procedures developed at Fair, Isaac since the 1960s. They sound like people I would enjoy working with. Bruce makes two points of mild contention. One is the following:\n\nHigh performance (predictive accuracy) on the test sample does not guarantee high performance on future samples; things do change.\n\nI agree—algorithmic models accurate in one context must be modified to stay accurate in others. This does not necessarily imply that the way the model is constructed needs to be altered, but that data gathered in the new context should be used in the construction.\nHis other point of contention is that the Fair, Isaac algorithm retains interpretability, so that it is possible to have both accuracy and interpretability. For clients who like to know what’s going on, that’s a sellable item. But developments in algorithmic modeling indicate that the Fair, Isaac algorithm is an exception.\nA computer scientist working in the machine learning area joined a large money management company some years ago and set up a group to do portfolio management using stock predictions given by large neural nets. When we visited, I asked how he explained the neural nets to clients. “Simple,” he said; “We fit binary trees to the inputs and outputs of the neural nets and show the trees to the clients. Keeps them happy!” In both stock prediction and credit rating, the priority is accuracy. Interpretability is a secondary goal that can be finessed."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#manny-parzen",
    "href": "contents/two_cultures/two_cultures.html#manny-parzen",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "Manny Parzen",
    "text": "Manny Parzen\nManny Parzen opines that there are not two but many modeling cultures. This is not an issue I want to fiercely contest. I like my division because it is pretty clear cut—are you modeling the inside of the box or not? For instance, I would include Bayesians in the data modeling culture. I will keep my eye on the quantile culture to see what develops.\nMost of all, I appreciate Manny’s openness to the issues raised in my paper. With the rapid changes in the scope of statistical problems, more open and concrete discussion of what works and what doesn’t should be welcomed.\n\nWHERE ARE WE HEADING?\nMany of the best statisticians I have talked to over the past years have serious concerns about the viability of statistics as a field. Oddly, we are in a period where there has never been such a wealth of new statistical problems and sources of data. The danger is that if we define the boundaries of our field in terms of familiar tools and familiar problems, we will fail to grasp the new opportunities."
  },
  {
    "objectID": "contents/two_cultures/two_cultures.html#additional-references",
    "href": "contents/two_cultures/two_cultures.html#additional-references",
    "title": "Statistical Modeling: The Two Cultures",
    "section": "ADDITIONAL REFERENCES",
    "text": "ADDITIONAL REFERENCES\n\nBeverdige, W. V. I (1952) The Art of Scientific Investigation. Heinemann, London.\nBreiman, L. (1994) The 1990 Census adjustment: undercount or bad data (with discussion)? Statist. Sci. 9 458–475.\nCox, D. R. and Wermuth, N. (1996) Multivariate Dependencies. Chapman and Hall, London.\nEfron, B. and Gong, G. (1983) A leisurelylook at the bootstrap, the jackknife, and cross-validation. Amer. Statist. 37 36–48.\nEfron, B. and Tibshirani, R. (1996) Improvements on crossvalidation: the 632+ rule. J. Amer. Statist. Assoc. 91 548–560.\nEfron, B. and Tibshirani, R. (1998) The problem of regions. Ann. Statist. 26 1287–1318.\nGong, G. (1982) Cross-validation, the jackknife, and the bootstrap: excess error estimation in forward logistic regression. Ph.D. dissertation, Stanford Univ.\nMacKay, R. J. and Oldford, R. W. (2000) Scientific method, statistical method, and the speed of light. Statist. Sci. 15 224–253.\nParzen, E. (1979) Nonparametric statistical data modeling (with discussion). J. Amer. Statist. Assoc. 74 105–131.\nStone, C. (1977) Consistent nonparametric regression. Ann. Statist. 5 595–645."
  },
  {
    "objectID": "contents/eda.html",
    "href": "contents/eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n다음 네 가지의 시각화 패키지를 사용해서 그 차이를 확인해 볼 것임.\nMatplotlib 방식\n두 가지 interface를 제공하는데, 혼동을 야기함\npandas/seaborn 방식",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/eda.html#위도-경도-값의-활용",
    "href": "contents/eda.html#위도-경도-값의-활용",
    "title": "Exploratory Data Analysis",
    "section": "위도, 경도 값의 활용",
    "text": "위도, 경도 값의 활용\n\n\n\n\n\n\nShow Matplotlib styles\n\n\n\nplt.style.available\n\n\n\n# set the style\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\nlat, lon = housing['latitude'], housing['longitude']\n\n## MATLAB 스타일\n# figure() 함수를 직접 호출\nplt.figure(figsize=(7, 5)) # create a plot figure, figsize는 생략가능\n\n# scatter() 함수를 직접 호출\nplt.scatter(x=lon, y=lat, label=None, edgecolors=\"w\", linewidths=.4, alpha=0.3)\n\n# set the labels\nplt.xlabel('longitude')\nplt.ylabel('latitude')\nplt.axis('equal') # set the aspect of the plot to be equal\n\nplt.show()\n\n\n\n\n\n\n\n\n\n## 객체 방식\n# figure, axes라는 객체를 생성 후 메서드를 호출\nfig, ax = plt.subplots(figsize=(7, 5)) \n\n# ax의 메서드인 .scatter로 그래프를 그림\nax.scatter(x=lon, y=lat, label=None, edgecolors=\"w\", linewidths=.4, alpha=0.3)\n\n# ax의 메서드인 .set_xlabel, .set_ylabel로 라벨을 지정\nax.set_xlabel('longitude')\nax.set_ylabel('latitude')\nax.axis('equal')  # set the aspect of the plot to be equal\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# pandas의 plot 메서드를 사용하는 방식\nhousing.plot.scatter(x=\"longitude\", y=\"latitude\", alpha=0.3)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n# 다음과 동일함\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.3)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n\n\n\npandas가 제공하는 plots\n\n‘line’ : line plot (default)\n‘bar’ : vertical bar plot\n‘barh’ : horizontal bar plot\n‘hist’ : histogram\n‘box’ : boxplot\n‘kde’ : Kernel Density Estimation plot\n‘density’ : same as ‘kde’\n‘area’ : area plot\n‘pie’ : pie plot\n‘scatter’ : scatter plot (DataFrame only)\n‘hexbin’ : hexbin plot (DataFrame only)\n\n\n# NEAR OCEAN에 해당하는 부분만 시각화\nhousing2 = housing.query('ocean_proximity == \"NEAR OCEAN\"')\n\nhousing2.plot.scatter(x=\"longitude\", y=\"latitude\", alpha=0.3, figsize=(7, 5))\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n\n\n# Seaborn을 사용하는 방식\nplt.figure(figsize=(7, 5))\nsns.scatterplot(housing, x=\"longitude\", y=\"latitude\", hue=\"ocean_proximity\", alpha=0.5)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n  The San Francisco Bay Area\n\n\n\n\n집값과의 관계를 보기 위해, 집값을 컬러에 매핑하면,\n\nhousing.plot.scatter(\n    x=\"longitude\",\n    y=\"latitude\",\n    s=housing[\"population\"] / 100,  # point size\n    c=\"median_house_value\",  # color\n    alpha=0.3,  # transparency\n    cmap=\"flare\",  # color map\n    figsize=(7, 5),\n)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Annotation 추가\n\n\n\n\n\n아래 코드를 추가하여 도시 이름을 표시\npath = \"https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks_v1/data/california_cities.csv\"\ncities = pd.read_csv(path)\n\npopular_cities = cities.query('population_total &gt; 400000')\nlat, lon, names = popular_cities['latd'], popular_cities['longd'], popular_cities[\"city\"]\n\nplt.scatter(lon, lat, c=\"w\", alpha=1)\nfor name, lat, lon in zip(names, lat, lon):\n    plt.annotate(name, (lon, lat), xytext=(5, 5), textcoords=\"offset points\", color=\"k\")\n\n\n\n\n\n\n\n\n\nColor 사용에 관한 체계적 가이드\n\n\n\nChoosing color pallettes from Seaborn website",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/eda.html#데이터의-분포",
    "href": "contents/eda.html#데이터의-분포",
    "title": "Exploratory Data Analysis",
    "section": "데이터의 분포",
    "text": "데이터의 분포\nHistogram, density plot, boxplot\n\n# pandas의 DataFrame 메서드인 hist()를 사용\nhousing.hist(bins=50, figsize=(9, 6))\nplt.show()\n\n\n\n\n\n\n\n\n\n# density plot\nhousing.plot.density(bw_method=0.2, subplots=True, layout=(3, 3), sharex=False, sharey=False, figsize=(9, 6))\nplt.show()\n\n\n\n\n\n\n\n\n\n# Using matplotlib\nfig, ax = plt.subplots(3, 3, figsize=(9, 6))\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n\nfor i in range(3):\n    for j in range(3):\n        ax[i, j].hist(housing.iloc[:, i * 3 + j], bins=30)\n        ax[i, j].set_title(housing.columns[i * 3 + j])        \n\n\n\n\n\n\n\n\n\n# 한 변수의 각 레벨/카테고리별로 그리기, using pandas\nhousing.plot.hist(column=[\"median_house_value\"], by=\"ocean_proximity\", sharey=False, sharex=True, figsize=(6, 8), bins=50)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\n\n\n\n\n\n\nsource: R for Data Science\n\n\n\n\n\n# Using pandas\nhousing.plot.box(column=\"median_house_value\", by=\"ocean_proximity\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Using seaborn\nplt.figure(figsize=(9, 5))\nsns.boxplot(housing, x=\"ocean_proximity\", y=\"median_house_value\", hue=\"median_age_cat\", fill=False, gap=.2)\nplt.show()",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/eda.html#두-연속-변수간의-관계",
    "href": "contents/eda.html#두-연속-변수간의-관계",
    "title": "Exploratory Data Analysis",
    "section": "두 연속 변수간의 관계",
    "text": "두 연속 변수간의 관계\n\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"] / housing[\"households\"]\nhousing[\"bedrooms_per_household\"] = housing[\"total_bedrooms\"] / housing[\"households\"]\nhousing[\"people_per_household\"] = housing[\"population\"] / housing[\"households\"]\n\n또는 assign()를 사용\n\nhousing.assign(\n    rooms_per_household = lambda x: x[\"total_rooms\"] / x[\"households\"],\n    bedrooms_per_household = lambda x: x[\"total_bedrooms\"] / x[\"households\"],\n    people_per_household = lambda x: x[\"population\"] / x[\"households\"]\n).head(1)\n\n   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88               41.00       880.00          129.00   \n\n   population  households  median_income  median_house_value ocean_proximity  \\\n0      322.00      126.00           8.33           452600.00        NEAR BAY   \n\n  median_age_cat  rooms_per_household  bedrooms_per_household  \\\n0          40-52                 6.98                    1.02   \n\n   people_per_household  \n0                  2.56  \n\n\n\nhousing.value_counts(\"median_house_value\").sort_index()\n\nmedian_house_value\n14999.00       4\n17500.00       1\n22500.00       4\n            ... \n499100.00      1\n500000.00     27\n500001.00    965\nName: count, Length: 3842, dtype: int64\n\n\n\n# median_house_value &lt; 500001 값으로 필터링\nhousing = housing.query('median_house_value &lt; 500001')\n\n\nxvar = \"rooms_per_household\"\nyvar = \"median_house_value\"\n\n# matplotlib의 객체 방식\nfig, ax = plt.subplots()\nhousing.plot.scatter(ax=ax, x=xvar, y=yvar, alpha=0.1, figsize=(7, 5))\n\n# fitted line of natural spline: 아래 노트 참고\nnspline_fit = nspline(housing, xvar, yvar, df_n=15).sort_values(xvar)\nnspline_fit.plot.line(ax=ax, x=xvar, y=yvar, c=\".3\", figsize=(7, 5))\n\nplt.xlim(0, 10)\nplt.ylim(0, 500000)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNatural spline fit\n\n\n\n\n\ndef nspline(df, x, y, df_n=5):\n    from statsmodels.formula.api import ols\n\n    df = df[[x, y]].dropna()\n    formula = f\"{y} ~ cr({x}, df={df_n})\"\n    df[y] = ols(formula, data=df).fit().fittedvalues\n\n    return df\n\n\n\n해변에 가까운 정도(ocean_proximity) 따라 나누어 보면,\n\n# divide plots by ocean_proximity\nfig, ax = plt.subplots(1, 4, figsize=(12, 3))\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n\ntypes = ['NEAR OCEAN', '&lt;1H OCEAN', 'NEAR BAY', 'INLAND']\nfor i, op in enumerate(types):\n\n    df = housing.query(f'ocean_proximity == \"{op}\"')\n    df.plot.scatter(ax=ax[i], x=xvar, y=yvar, alpha=0.1)\n\n    # fitted line of natural spline\n    nspline_fit = nspline(df, xvar, yvar, df_n=15).sort_values(xvar)\n    nspline_fit.plot.line(ax=ax[i], x=xvar, y=yvar, c=\".3\")\n    \n    ax[i].set_title(op)\n    ax[i].set_xlim(1, 12)\n    ax[i].set_ylim(0, 500000)\n\nplt.show()\n\n\n\n\n\n\n\n\nseaborn.object 방식\n\n(\n    so.Plot(housing, x='rooms_per_household', y='median_house_value')\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet('ocean_proximity')\n    .limit(x=(1, 12), y=(0, 500000))\n    .layout(size=(8.9, 3))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n범주형 변수의 순서 할당\n\n\n\npd.Categorical을 사용하여 범주형 변수의 순서를 지정할 수 있음.\nhousing[\"ocean_proximity\"] = pd.Categorical(\n    housing[\"ocean_proximity\"],\n    categories=[\"NEAR BAY\", \"NEAR OCEAN\", \"&lt;1H OCEAN\", \"INLAND\"],\n    ordered=True,\n)\n\n\n해변에 가까운 정도(ocean_proximity)와 집의 연령(median_age_cat)에 따라 나누어 보면,\n\n(\n    so.Plot(\n        housing.query('ocean_proximity != \"ISLAND\"'),\n        x=\"rooms_per_household\",\n        y=\"median_house_value\",\n    )\n    .add(so.Dots(alpha=0.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet(col=\"ocean_proximity\", row=\"median_age_cat\")\n    .limit(x=(1, 12), y=(0, 500000))\n    .layout(size=(8, 8))\n)\n\n\n\n\n\n\n\n\n해안에 가까운 정도(ocean_proximity)가 고정되어 있을 때, 그 안에서 여전히\n경도(longitude)가 작을수록, 집값(median_house_value)이 변화하는지 살펴보면,\n\n(\n    so.Plot(\n        housing.query('ocean_proximity != \"ISLAND\"'),\n        x='longitude',\n        y='median_house_value')\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet(\"ocean_proximity\")\n    .layout(size=(8.9, 3))\n    .share(x=False)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nPopulation과의 관계가 있을까?\n\n\nShow the code\nhousing2 = housing.copy()\nhousing2[\"ocean_proximity\"] = (\n    housing\n    .query('ocean_proximity not in [\"ISLAND\", \"INLAND\"]')[\"ocean_proximity\"]\n    .cat.remove_unused_categories()\n)\n\n(\n    so.Plot(housing2, x='longitude', y='population')\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet(\"ocean_proximity\")\n    .layout(size=(8.9, 3))\n    .share(x=False)\n    .limit(y=(0, 3000))\n)\n\n\n\n\n\n\n\n\n\nPanelized spline fit: pyGAM 참고",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/knn.html",
    "href": "contents/knn.html",
    "title": "K-Nearest Neighbors",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "contents/knn.html#k-nearest-neighbors-in-classification",
    "href": "contents/knn.html#k-nearest-neighbors-in-classification",
    "title": "K-Nearest Neighbors",
    "section": "K-Nearest Neighbors in Classification",
    "text": "K-Nearest Neighbors in Classification\n앞서 분류(classification) 문제의 경우에 Bayes classifier를 정의했는데,\n\\(f(x): P(Y = C_k|X=x)\\)가 최대인 클래스에 할당; \\(\\underset{k}{\\mathrm{argmax}}~ P(Y = C_k|X=x)\\)\n간단히 말해, 어떤 관측치 \\(x\\)에 대해서, 포함될 가능성이 가장 높은 클래스에 할당하는 것임\n(조건부 확률분포 \\(P(Y|X=x)\\)에서 가장 높은 확률을 가지는 클래스에 할당)\n이를 위해서 \\(P(Y|X)\\)를 추정하는 가장 단순하며 직관적인 방법으로 K-Nearest Neighbors(K-최근접 이웃)가 있음\n대표적인 non-parametric 방법론임\n\n주어진 (test) 관측치 \\(x\\)에 대해서 가장 가까운 K개의 (training) 값을 찾음\n이 K개의 값들이 속한 클래스을 파악하면,\n각 클래스에 속할 비율, 즉 확률을 추정할 수 있음\n\n클래스 \\(C_j\\)에 대해서,\n\\(\\displaystyle P(Y = C_j|X=x) = \\frac{1}{K} \\sum_{i \\in N_k} I(y_i = C_j)\\)\n\n\n\n\n\n\n\n\n\n\n오렌지 배경: 오렌지 클래스에 할당된 평면 안의 값들,\n\n파란 배경: 파란 클래스에 할당된 평면 안의 값들\n\n\n\n\n다음과 같이 각 클래스에 속할 확률을 설정하여, 클래스 별로 100개씩의 데이터를 추출한 예를 보면 (simulated data),\nBayes classifier가 되는 즉, \\(P(Y=orange|X) = P(Y=blue|X)\\)인 \\(X\\)을 찾을 수 있음 (=\\(\\frac{1}{2}\\))\n이를 Bayes decision boundary라고 함. (보라색 점선)\n\n이 때, KNN은 Bayes decision boundary를 추정하는데 사용될 수 있음.\n\nK가 증가함에 따라 flexibiliy가 떨어지고 선형에 가까운 결정 경계를 생성\nK = 100의 경우 variance는 낮지만 bias가 높은 classifier에 해당\nK = 1, K = 100에 대한 test error rate은 각각 0.1695, 0.1925\n\n\nK의 값이 bias-variance trade-off를 결정함으로, 과적합이 되지 않도록 cross-validation과 같은 방법을 통해 최적의 K를 찾아야 함\n\n\nSource: pp. 38-39, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "contents/knn.html#k-nearest-neighbors-regression",
    "href": "contents/knn.html#k-nearest-neighbors-regression",
    "title": "K-Nearest Neighbors",
    "section": "K-Nearest Neighbors regression",
    "text": "K-Nearest Neighbors regression\nKNN의 추정 방식은 자연스럽게 회귀(regression) 문제에 적용할 수 있음\n\n주어진 (test) 관측치 \\(x\\)에 대해서 가장 가까운 K개의 (training) 값을 찾음; 집합 \\(N\\)으로 표현\n이 K개의 값들의 평균을 계산하여, 예측값을 구함\n\n\\(f(x) = E(Y|X=x)\\)의 추정치로서 \\(E(Y|x_i \\in N)\\)를 사용\n\n\n식으로 표현하면,\n\\(\\displaystyle \\hat f(x) = \\frac{1}{K} \\sum_{x_i \\in N} y_i\\)\n\n\nSource: p. 112, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\nLinear regression과의 비교\nX와 Y가 선형관계를 가질 때 (N = 50),\n\n\n\nSource: p. 113, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\nX와 Y가 선형관계에서 벗어날 때 (N = 50),\n\n\n\nSource: p. 114, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n\n실제 X, Y의 관계가 비선형적일 수록 KNN이 더 나은 예측을 할 수 있음\n하지만, 변수의 수가 증가하면 테스트 관측치 \\(x_0\\)에서 가까운 관측치 수가 기하급수적으로 줄어\n\\(x_0\\)에 가장 가까운 K 관측치는 매우 멀리 떨어져 있어, 그 평균값들인 예측값의 정확도가 떨어질 수 있음; the curse of dimensionality\n\n\n\n\nFIGURE 2.7. A simulation example, demonstrating the curse of dimensionality and its effect on MSE, bias and variance. The input features are uniformly distributed in \\([−1, 1]^p\\) for \\(p\\) = 1,…, 10 The top left panel shows the target function (no noise) in \\(\\mathbb{R}\\): \\(f(X) = e^{−8||X||^2}\\), and demonstrates the error that 1-nearest neighbor makes in estimating \\(f(0)\\). The training point is indicated by the blue tick mark. The top right panel illustrates why the radius of the 1-nearest neighborhood increases with dimension \\(p\\). The lower left panel shows the average radius of the 1-nearest neighborhoods. The lower-right panel shows the MSE, squared bias and variance curves as a function of dimension \\(p\\).\n\n\n\nSource: p. 25, The Elements of Statistical Learning (2e) by Hastie, T., Tibshirani, R., & Friedman, J.\n\n특히, Y와 관련이 적은 예측변수들이 많을 경우에는 더욱 그러함\n아래 그림은 Y와 관련이 별로 없는 변수들이 늘어날 때, KNN과 선형회귀의 성능의 변화를 보여줌 (N = 50);\nKNN의 성능이 빠르게 떨어짐\n\n\n\nFIGURE 3.20. Test MSE for linear regression (black dashed lines) and KNN(green curves) as the number of variables p increases. The true function is non-linear in the first variable, as in the lower panel in Figure 3.19, and does not depend on the additional variables. The performance of linear regression deteriorates slowly in the presence of these additional noise variables, whereas KNN’s performance degrades much more quickly as p increases.\n\n\n\nSource: p. 115, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n\n일반적으로, parametric 방법이 예측변수 개수 대비 관측치가 적을 때 non-parametric 보다 성능이 우수함.\nKNN의 경우 해석가능하지 않기 때문에, 예측변수가 적을 때에도 선형회귀를 사용하는 것이 더 나을 수 있음",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "contents/knn.html#regression",
    "href": "contents/knn.html#regression",
    "title": "K-Nearest Neighbors",
    "section": "Regression",
    "text": "Regression\nMajor League Baseball Data from the 1986 and 1987 seasons\n\nfrom ISLP import load_data\n\nHitters = load_data('Hitters') # from ISLP\nHitters = Hitters.dropna()\nHitters.head(3)\n\n   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n1    315    81      7    24   38     39     14    3449    835      69    321   \n2    479   130     18    66   72     76      3    1624    457      63    224   \n3    496   141     20    65   78     37     11    5628   1575     225    828   \n\n   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n1   414     375      N        W      632       43      10  475.00         N  \n2   266     263      A        W      880       82      14  480.00         A  \n3   838     354      N        E      200       11       3  500.00         N  \n\n\n\n# suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nX = Hitters.drop('Salary', axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = Hitters['Salary']\n\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, \ntest_size=0.5, random_state=1)\n\n# 모든 변수들로부터 거리를 적절히 계산하려면 단위를 일치해야 함\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nK=3일 때의 KNN regression\n\nfrom sklearn.neighbors import KNeighborsRegressor\nK = 3\nknn = KNeighborsRegressor(n_neighbors=K)\nknn_pred = knn.fit(X_train_scaled, y_train).predict(X_test_scaled)\n\nprint(\n    f\"RMSE: {root_mean_squared_error(y_test, knn_pred):.2f}\\n\"\n    f\"R2: {r2_score(y_test, knn_pred):.2f}\"\n)\n\nRMSE: 315.50\nR2: 0.42\n\n\n여러 K값에 대한 grid search를 통해 최적의 K를 찾음\n10-fold cross-validation을 통해 test error rate을 계산\n\nkfold = skm.KFold(n_splits=10, shuffle=True, random_state=1)\nknn = KNeighborsRegressor()\n\nK = np.arange(1, 51)\nparam_grid = {\"n_neighbors\": K}\n\ngrid = skm.GridSearchCV(knn, param_grid, cv=kfold, scoring=\"neg_mean_squared_error\")\ngrid.fit(X_train_scaled, y_train)\n\nGridSearchCV(cv=KFold(n_splits=10, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=10, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='neg_mean_squared_error') best_estimator_: KNeighborsRegressorKNeighborsRegressor(n_neighbors=np.int64(8))  KNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor(n_neighbors=np.int64(8)) \n\n\n\ngrid.best_params_\n\n{'n_neighbors': np.int64(8)}\n\n\n각 K에 대한 test error rate을 시각화\n\nplt.figure(figsize=(6, 4.5), dpi=70)\nplt.errorbar(\n    1/K,\n    -grid.cv_results_[\"mean_test_score\"],\n    yerr=grid.cv_results_[\"std_test_score\"] / np.sqrt(10),\n)\nplt.axvline(1/grid.best_params_['n_neighbors'], color=\".5\", linestyle=\":\")\nplt.xlabel(\"1/K\")\nplt.ylabel(\"Cross-validated MSE\")\nplt.show()\n\n\n\n\n\n\n\n\nK=8에 대한 test error\n\nK = grid.best_params_['n_neighbors']\nknn = KNeighborsRegressor(n_neighbors=K)\nknn_pred = knn.fit(X_train_scaled, y_train).predict(X_test_scaled)\n\nprint(\n    f\"RMSE: {root_mean_squared_error(y_test, knn_pred):.2f}\\n\"\n    f\"R2: {r2_score(y_test, knn_pred):.2f}\"\n)\n\nRMSE: 318.17\nR2: 0.41\n\n\n\n\n\n\n\n\nPlot validation curves\n\n\n\n\n\nvalidation_curve()를 사용하여, traing score와 validation score를 간단히 그릴 수 있음\ntrain_scores, val_scores = skm.validation_curve(knn, X_train_scaled, y_train, param_name='n_neighbors', param_range=K, cv=kfold, scoring='neg_mean_squared_error')  # default: r2\n\nplt.plot(K, -train_scores.mean(1), label='training score')\nplt.plot(K, -val_scores.mean(1), label='validation score')\nplt.legend(frameon=False)\nplt.xlabel('K')\nplt.ylabel('Cross-validated MSE')",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "contents/knn.html#classification",
    "href": "contents/knn.html#classification",
    "title": "K-Nearest Neighbors",
    "section": "Classification",
    "text": "Classification\n\nPalmer Archipelago penguin data\n앞서 살펴봤던 Palmer Penguins의 데이터를 이용\n부리의 길이(bill_length)와 깊이(bill_depth)로부터 종(species)을 분류하는 문제\n\npenguins = sns.load_dataset('penguins')\npenguins.head(3)\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen           39.10          18.70             181.00   \n1  Adelie  Torgersen           39.50          17.40             186.00   \n2  Adelie  Torgersen           40.30          18.00             195.00   \n\n   body_mass_g     sex  \n0      3750.00    Male  \n1      3800.00  Female  \n2      3250.00  Female  \n\n\n\nShow the code\npenguins = sns.load_dataset('penguins')\n\nsns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"body_mass_g\", hue=\"species\",\n    height=4, aspect=1.2, palette=colors[:3]\n)\n\nsns.displot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"body_mass_g\", hue=\"species\",\n    kind=\"kde\", height=4, aspect=1.2, palette=colors[:3]\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# 모든 변수들로부터 거리를 적절히 계산하려면 단위를 일치해야 함: 이 예제에서는 필요없음\n\n# Fit KNN Classifier\nknn5 = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\nknn5.fit(X_train_sub, y_train_sub)\nknn5_pred = knn5.predict(X_test_sub)\n\nprint(\n    f\"Accuracy: {knn5.score(X_test_sub, y_test_sub)}\"\n)\n\nAccuracy: 0.94\n\n\n\n\nfrom ISLP import confusion_table\nconfusion_table(knn5_pred, y_test_sub)\n\nTruth      Adelie  Chinstrap  Gentoo\nPredicted                           \nAdelie         35          0       0\nChinstrap       0         11       0\nGentoo          3          2      35\n\n\n\nkfold = skm.KFold(n_splits=10, shuffle=True, random_state=1)\nknn = KNeighborsClassifier(weights=\"distance\")\n\nK = np.arange(1, 21)\nparam_grid = {\"n_neighbors\": K}\n\ngrid = skm.GridSearchCV(knn, param_grid, cv=kfold, scoring=\"accuracy\")\ngrid.fit(X_train_sub, y_train_sub)\n\nGridSearchCV(cv=KFold(n_splits=10, random_state=1, shuffle=True),\n             estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=10, random_state=1, shuffle=True),\n             estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='accuracy') best_estimator_: KNeighborsClassifierKNeighborsClassifier(n_neighbors=np.int64(15))  KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_neighbors=np.int64(15)) \n\n\n\ngrid.best_params_\n\n{'n_neighbors': np.int64(15)}\n\n\n\ngrid.best_estimator_.fit(X_train_sub, y_train_sub)\nknn_pred = grid.best_estimator_.predict(X_test_sub)\n\nprint(\n    f\"Accuracy: {grid.best_estimator_.score(X_test_sub, y_test_sub)}\\n\"\n)\n\nTruth      Adelie  Chinstrap  Gentoo\nPredicted                           \nAdelie         36          1       0\nChinstrap       0         10       0\nGentoo          2          2      35\n\n\nK = np.arange(1, 21)\n\ntrain_scores, val_scores = skm.validation_curve(knn, X_train_sub, y_train_sub, param_name='n_neighbors', param_range=K, cv=kfold, scoring='accuracy')\n\nplt.plot(1/K, train_scores.mean(1), label='training score')\nplt.plot(1/K, val_scores.mean(1), label='validation score')\nplt.legend(frameon=False)\nplt.xlabel('1/K')\nplt.ylabel('Cross-validated accuracy')\nplt.show()\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(\"\\nClassification Report:\\n\", classification_report(y_test_sub, y_pred))\n\n              precision    recall  f1-score   support\n\n      Adelie       0.97      0.95      0.96        38\n   Chinstrap       1.00      0.77      0.87        13\n      Gentoo       0.90      1.00      0.95        35\n\n    accuracy                           0.94        86\n   macro avg       0.96      0.91      0.93        86\nweighted avg       0.95      0.94      0.94        86\n\n\n\n\n\nWine recognition dataset\n\nfrom sklearn import datasets\nwine = datasets.load_wine(as_frame=True)\nprint(wine.DESCR)\n\n\nwine_df = wine['frame']\nwine_df['target'] = wine_df['target'].map({0: 'C0', 1: 'C1', 2: 'C3'})\nwine_df.head(3)\n\n   alcohol  malic_acid  ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0    14.23        1.71 2.43              15.60     127.00           2.80   \n1    13.20        1.78 2.14              11.20     100.00           2.65   \n2    13.16        2.36 2.67              18.60     101.00           2.80   \n\n   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity  hue  \\\n0        3.06                  0.28             2.29             5.64 1.04   \n1        2.76                  0.26             1.28             4.38 1.05   \n2        3.24                  0.30             2.81             5.68 1.03   \n\n   od280/od315_of_diluted_wines  proline target  \n0                          3.92  1065.00     C0  \n1                          3.40  1050.00     C0  \n2                          3.17  1185.00     C0  \n\n\n\nShow the code\nsns.relplot(\n    data=wine_df,\n    x=\"color_intensity\", y=\"alcohol\", hue=\"target\",\n    height=4, aspect=1.2, palette=colors[:3]\n)\n\nsns.displot(\n    data=wine_df,\n    x=\"color_intensity\", y=\"alcohol\", hue=\"target\",\n    kind=\"kde\", height=4, aspect=1.2, palette=colors[:3]\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nX = wine_df[['color_intensity', 'alcohol']]\ny = wine_df['target']\n\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.25, random_state=1)\n\n# 모든 변수들로부터 거리를 적절히 계산하려면 단위를 일치해야 함\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn3 = KNeighborsClassifier(n_neighbors=3)\nknn3.fit(X_train_scaled, y_train)\nknn3_pred = knn3.predict(X_test_scaled)\n\nprint(\n    f\"Accuracy: {knn3.score(X_test_scaled, y_test):.2f}\\n\"\n)\n\nAccuracy: 0.87\n\n\n\n\nfrom ISLP import confusion_table\nconfusion_table(knn3_pred, y_test)\n\nTruth      C0  C1  C3\nPredicted            \nC0         17   1   4\nC1          0  16   0\nC3          1   0   6\n\n\n\nkfold = skm.KFold(n_splits=10, shuffle=True, random_state=0)\nknn = KNeighborsClassifier()\n\nK = np.arange(1, 51)\nparam_grid = {\"n_neighbors\": K}\n\ngrid = skm.GridSearchCV(knn, param_grid, cv=kfold, scoring=\"accuracy\")\ngrid.fit(X_train_scaled, y_train)\n\nGridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='accuracy') best_estimator_: KNeighborsClassifierKNeighborsClassifier(n_neighbors=np.int64(11))  KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_neighbors=np.int64(11)) \n\n\n\ngrid.best_params_\n\n{'n_neighbors': np.int64(11)}\n\n\n\ngrid.best_estimator_.fit(X_train_scaled, y_train)\nknn_pred = grid.best_estimator_.predict(X_test_scaled)\n\nconfusion_table(knn_pred, y_test)\n\nTruth      C0  C1  C3\nPredicted            \nC0         18   2   4\nC1          0  15   0\nC3          0   0   6\n\n\nK = np.arange(1, 51)\n\ntrain_scores, val_scores = skm.validation_curve(knn, X_train_scaled, y_train, param_name='n_neighbors', param_range=K, cv=kfold, scoring='accuracy')\n\nplt.plot(1/K, train_scores.mean(1), label='training score')\nplt.plot(1/K, val_scores.mean(1), label='validation score')\nplt.legend(frameon=False)\nplt.xlabel('1/K')\nplt.ylabel('Cross-validated accuracy')\nplt.show()\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, knn_pred))\n\n              precision    recall  f1-score   support\n\n          C0       0.75      1.00      0.86        18\n          C1       1.00      0.88      0.94        17\n          C3       1.00      0.60      0.75        10\n\n    accuracy                           0.87        45\n   macro avg       0.92      0.83      0.85        45\nweighted avg       0.90      0.87      0.86        45\n\n\n\n\n\nMNIST handwritten digits dataset\n\nimport tensorflow as tf\n\n# Load the MNIST dataset\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# Shape of the data/labels\nprint(f\"Training data shape: {X_train.shape}\")\nprint(f\"Training labels shape: {y_train.shape}\")\nprint(f\"Testing data shape: {X_test.shape}\")\nprint(f\"Testing labels shape: {y_test.shape}\")\n\nTraining data shape: (60000, 28, 28)\nTraining labels shape: (60000,)\nTesting data shape: (10000, 28, 28)\nTesting labels shape: (10000,)\n\n\n\nfig, axes = plt.subplots(5, 5, figsize=(5, 5))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X_train[i], cmap='gray')\n    ax.set_title(f\"Label: {y_train[i]}\")\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# randomly select subsets\nnp.random.seed(123)\n\nn1, n2 = 2000, 2000\n\nidx1 = np.random.randint(0, 60000, n1)\nidx2 = np.random.randint(0, 10000, n2)\n\nX_train_sub = X_train[idx1, :]\ny_train_sub = y_train[idx1]\nX_test_sub = X_test[idx2, :]\ny_test_sub = y_test[idx2]\n\n# Reshape the data\nX_train_sub = X_train_sub.reshape(n1, 28 * 28)\nX_test_sub = X_test_sub.reshape(n1, 28 * 28)\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Fit KNN Classifier\nknn = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\nknn.fit(X_train_sub, y_train_sub)\n\n# Predictions\ny_pred = knn.predict(X_test_sub)\n\nprint(\"Predictive Accuracy:\", accuracy_score(y_test_sub, y_pred))\n\nPredictive Accuracy: 0.9115\n\n\n\n# 오분류된 샘플만 시각화\nmis_idx = np.where(y_pred != y_test_sub)[0]\n\nn_show = min(25, len(mis_idx))\nfig, axes = plt.subplots(5, 5, figsize=(5, 5))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; n_show:\n        idx = mis_idx[i]\n        ax.imshow(X_test_sub[idx].reshape(28, 28), cmap='gray')\n        ax.set_title(f\"True: {y_test_sub[idx]}, Pred: {y_pred[idx]}\", fontsize=8)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "contents/logistic.html",
    "href": "contents/logistic.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Home",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#emperical-probability-odds",
    "href": "contents/logistic.html#emperical-probability-odds",
    "title": "Logistic Regression",
    "section": "Emperical probability/ Odds",
    "text": "Emperical probability/ Odds\n이제 y를 직접 예측하기보다, 확률을 예측하는 방식을 취하면,\n\n각 두께를 가진 나무들의 개수 (m)와\n그 중에 태풍으로 죽은 나무의 수 (died)를 고려하면,\n나무의 두께에 따라 죽은 나무 수의 비율 (p= died/m)을 계산할 수 있음. 이를 emperical probability라고 함.\n사실, 이 p는 binary response (0, 1)의 conditional mean(평균)인데,\n통계적으로 표현하면 \\(E(Y|d=d_i)\\)이며 선형모형의 mean function를 제공.\n\n\nblowbs_bn = blowbs.groupby(\"d\")[\"y\"].agg([(\"died\", \"sum\"), (\"m\", \"count\"), (\"p\", \"mean\")]).reset_index()\nblowbs_bn\n\n       d  died   m    p\n0   5.00     7  90 0.08\n1   6.00     7  92 0.08\n2   7.00    18  91 0.20\n..   ...   ...  ..  ...\n22 28.00     2   2 1.00\n23 29.00     1   2 0.50\n24 32.00     1   1 1.00\n\n[25 rows x 4 columns]\n\n\n\ncode\n(\n    so.Plot(blowbs_bn, x='d', y='p')\n    .add(so.Dot(), pointsize='m')\n    .add(so.Dots(alpha=.3, color=\".6\"), so.Jitter(y=.1), x=blowbs.d, y=blowbs.y)\n    # .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(color=\"#87bc45\"), so.PolyFit(1))\n    .limit(y=(-0.2, 1.2))\n    .layout(size=(8, 5))\n    .label(title='Emperical Probability', y = 'Proportion of Died', x='Diameter (cm)')\n)\n\n\n\n\n\n\n\n\n\nOLS estimate으로도 충분한가?\n1차보다는 고차 다항함수로 fit한다면?\n\n우선 d를 log2 변환해서 살펴보면,\n\ncode\nblowbs_bn[\"log2d\"] = np.log2(blowbs_bn[\"d\"])\n(\n    so.Plot(blowbs_bn, x='log2d', y='p')\n    .add(so.Dot(), pointsize='m')\n    .add(so.Dots(alpha=.3, color=\".6\"), so.Jitter(y=.1), x=blowbs.log2d, y=blowbs.y)\n    .add(so.Line(color=\"#87bc45\"), so.PolyFit(1))\n    .limit(y=(-0.2, 1.2))\n    .layout(size=(8, 5))\n    .label(title='Emperical Probability', y = 'Proportion of Died', x='Log of Diameter (cm)')\n)\n\n\n\n\n\n\n\n\n위에서 살펴본 OLS의 문제들 즉,\n\n잔차에 패턴이 보인다는 것은 충분히 좋은 모형이 아니라는 것을 의미하고,\n예측값이 확률을 의미하지 못할 수 있음.\n잔차의 분포도 Gaussian과는 거리가 멈.\n\n이런 문제들을 해결하고 예측값이 분명한 “확률”의 의미를 품도록 여러 방식이 제시되는데 주로 사용되는 것이 logistic regression임.\n\n\n\n\n\n\nImportant\n\n\n\nBinary outcome을 예측하는 logistic regression 모형은 binary 값을 예측하는 것이 아니고, 확률 값을 예측하는 것임.\n이후에 이를 이용해 binary outcome을 예측.\n\n예를 들어, 두께가 5cm인 (특정 종의) 나무가 태풍에 쓰러질 확률(true probability)을 파악하고자 함.\n이 때, 관측값은 5cm인 나무 중 쓰러진 나무의 “비율”이고, 이 관측치들로부터 true probability를 추정하고자 함.\n\nOdds의 정의: 실패할 확률 대비 성공할 확률의 비율\n\\(\\displaystyle odds = \\frac{P(Y = 1)}{P(Y = 0)} = \\frac{p}{1-p}\\)\n예를 들어, 5cm 두께의 나무는 90그루 중 7그루가 죽었으므로 83그루는 살았음.\n즉, 죽음:생존 = 7:83 \\(\\approx\\) 1:12 이고 odds = 7/83 = 0.084; 생존할 가능성 대비 죽을 가능성이 8.4%임.\n확률로 표현하면, \\(odds = \\frac{\\frac{7}{90}}{1 - \\frac{7}{90}} = \\frac{7}{90-7} = \\frac{7}{83}\\)\n확률과 odds, logit(log odds)의 관계\n\n\n\n\nblowbs_bn = blowbs_bn.assign(odds = lambda x: x.p / (1 - x.p))\n# p = 1인 경우 odds가 무한대가 되므로 편의상 inf 값을 50으로 대체\nblowbs_bn[\"odds\"] = blowbs_bn[\"odds\"].apply(lambda x: 50 if x == np.inf else x)\nblowbs_bn\n\n       d  died   m    p  log2d  odds\n0   5.00     7  90 0.08   2.32  0.08\n1   6.00     7  92 0.08   2.58  0.08\n2   7.00    18  91 0.20   2.81  0.25\n..   ...   ...  ..  ...    ...   ...\n22 28.00     2   2 1.00   4.81 50.00\n23 29.00     1   2 0.50   4.86  1.00\n24 32.00     1   1 1.00   5.00 50.00\n\n[25 rows x 6 columns]\n\n\n이 odds를 선형모형으로 나무두께로 예측하려고 하는 것인데, 예를 들어,\n\\(\\widehat{odds} = b_0 + b_1 \\cdot \\log_{2} d\\)\nodds 값의 범위는 (0, \\(\\infty\\))이므로, 선형모형으로 fit하는 것이 적절하지 않음.\n한편, odds를 log 변환하면, 그 범위는 (\\(-\\infty\\), \\(\\infty\\))가 되어 선형모형으로 fit하는 것이 적절해짐.(또한, 가우시안 분포를 가정했을 때 최적임)\n이 때, 예측모형은 log odds가 \\(x\\)에 대해 선형적으로 연결된다고 가정하는 것임. 즉,\n\\(\\displaystyle \\log \\widehat{odds} = \\log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right) = b_0 + b_1 \\cdot \\log_{2} d\\)\n이를 logit 함수로 간단히 표현하면; 전통적 통계에서 선호\n\\(\\displaystyle logit(\\hat p) = b_0 + b_1 \\cdot \\log_{2} d\\),   \\(\\displaystyle logit(x) := \\log\\left(\\frac{x}{1-x}\\right)\\)\nlogit의 역함수인 sigmoid 함수로 표현하면; machine learning에서 선호\n\\(\\displaystyle \\hat p = \\sigma(b_0 + b_1 \\cdot \\log_{2} d)\\),   \\(\\displaystyle \\sigma(x) := \\frac{1}{1 + e^{-x}}\\)\n이를 확률로 표현하면,\n\\(\\displaystyle P(Y=1|X=x) = E(Y|X=x) = \\sigma(b_0 + b_1 \\cdot \\log_{2} d)\\)\n즉, logit은 예측변수 \\(x\\) 와 선형적으로 연결되는데 반해, 확률 \\(p\\) 는 예측변수 \\(x\\) 와 비선형적 관계를 맺음\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMean function \\(E(Y|X)\\)을 변형하는 방식으로 표현할 때,\n\\(f(E(Y|X))=\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_n X_n\\)\n\n이 때, \\(f\\)를 link function이라고 하고,\n여기서는 logit 함수를 link function으로 사용함.\n\nMachine learning에서는, 그 역함수를 이용해 선형함수 쪽을 변형해 다음과 같이 표현하는데,\n\\(E(Y|X)=f(\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_n X_n)\\)\n\n이 때, \\(f\\)를 activation function이라고 부름.\n\n여기서는 sigmoid 함수를 activation function으로 사용함.\n\nsigmoid 함수는 logisitic 함수라고도 부름.\n\n\n한편, 각 클래스 내에서 \\(X\\)가 multivariate Gaussian 분포를 따르고, 클래스 간에 covariance matrix가 동일하다고 가정하면,\nlog odds가 \\(X\\)에 대해 선형적으로 연결된다는 것을 보일 수 있음.\n\n여기서는 쓰러진 나무와 산 나무의 두께가 Gaussian 분포를 따르고,\n그 두 분포의 분산이 동일하다면, log odds가 \\(X\\)에 대해 선형적 연결될 수 있음.\n나무의 두께(d)를 log2 변환한 이유임 (오른쪽 그림)\n\n\ncode\nblowbs[\"label\"] = blowbs[\"y\"].map({0: \"alive\", 1: \"died\"})\n(\n    so.Plot(blowbs, x='d', color='label')\n    .add(so.Area(), so.KDE(common_norm=False))\n    #.scale(color=so.Nominal())\n    .scale(color=['#70e20c', '#7d2be2'])\n    .layout(size=(8, 5))\n    .label(x=\"Diameter (cm)\", y=\"Density\", color=\"Label\")\n).show()\n\n(\n    so.Plot(blowbs, x='log2d', color='label')\n    .add(so.Area(), so.KDE(common_norm=False))\n    .scale(color=['#70e20c', '#7d2be2'])\n    .layout(size=(8, 5))\n    .label(x=\"Log of Diameter (cm)\", y=\"Density\", color=\"Label\")\n).show()\n\n\n\n\n\n\n\n\n\n\n\n이제, log odds을 구해, \\(X\\)에 대해 선형적인 패턴이 있는지 확인해보면,\n\nblowbs_bn = blowbs_bn.assign(log_odds = lambda x: np.log(x.odds))\nblowbs_bn   \n\n       d  died   m    p  log2d  odds  log_odds\n0   5.00     7  90 0.08   2.32  0.08     -2.47\n1   6.00     7  92 0.08   2.58  0.08     -2.50\n2   7.00    18  91 0.20   2.81  0.25     -1.40\n..   ...   ...  ..  ...    ...   ...       ...\n22 28.00     2   2 1.00   4.81 50.00      3.91\n23 29.00     1   2 0.50   4.86  1.00      0.00\n24 32.00     1   1 1.00   5.00 50.00      3.91\n\n[25 rows x 7 columns]\n\n\n관측값들과 emperical probability와 log odds을 함께 살펴보면,\n\ncode\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.p, size=blowbs_bn.m, c=\"#008fd5\", sizes=(20, 200), ax=ax)\n\ndef jitter(values, j):\n    return values + np.random.normal(0, j, values.shape)\n\nsns.scatterplot(x=blowbs.log2d, y=jitter(blowbs.y, 0.02), alpha=.3, c=\".6\", ax=ax)\n\nfor i, row in blowbs_bn.iterrows():\n    ax.annotate(f\"{row.died:n}/{row.m:n}\", xy=(row.log2d, row.p), xytext=(row.log2d, row.p-0.05), size=9)\n\nax.set_xticks(blowbs_bn.log2d.unique())\nax.set_xticklabels(blowbs_bn.log2d.unique().round(2))\nax.tick_params(axis='x', rotation=45)\nax.set_title(\"Emperical Probability\")\nax.legend_.remove()\n\n\n\n\n\n\n\n\nLogit 값(분홍색)을 추가해서 그리면,\n\ncode\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.p, size=blowbs_bn.m, sizes=(20, 200), c=\"#008fd5\", ax=ax, legend=False)\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.log_odds, size=blowbs_bn.m, sizes=(20, 200), color=\"#f46a9b\", ax=ax)\ndef jitter(values, j):\n    return values + np.random.normal(0, j, values.shape)\n\nsns.scatterplot(x=blowbs.log2d, y=jitter(blowbs.y, 0.02), alpha=.3, c=\".6\", ax=ax)\n\nfor i, row in blowbs_bn.iterrows():\n    ax.annotate(f\"logit{row.died:n}/{row.m:n}\", xy=(row.log2d, row.log_odds), xytext=(row.log2d, row.log_odds-0.4), size=9)\n\nax.set_xticks(blowbs_bn.log2d.unique())\nax.set_xticklabels(blowbs_bn.log2d.unique().round(2))\nax.tick_params(axis='x', rotation=45)\nax.set_ylabel(\"P & Logit\")\nax.set_title(\"Emperical Probability and Logit\")\nax.legend_.remove()\n\n# # polyfit 5\n# x = np.linspace(blowbs_bn.log2d.min(), blowbs_bn.log2d.max(), 100)\n# y = np.polyval(np.polyfit(blowbs_bn.log2d, blowbs_bn.log_odds, 5), x)\n# sns.lineplot(x=x, y=y, ax=ax, color=\".6\")\n\n# # polyfit 1\n# y = np.polyval(np.polyfit(blowbs_bn.log2d, blowbs_bn.log_odds, 1), x)\n# sns.lineplot(x=x, y=y, ax=ax, color=\".6\")\n\n\n\n\n\n\n\n\n위의 logit값을 선형모형으로 예측하는 모형: \\(\\displaystyle \\log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right) = b_0 + b_1 \\cdot \\log_{2} d\\)\n파라미터 \\(b_0, b_1\\)의 추정은 잔차들의 제곱의 합을 최소로 하는 OLS 방식은 부적절하며, 대신에 Maximum Likelihood Estimation을 사용함.\n\n아이디어는 관측치가 전체적으로 관찰될 likelihood가 최대가 되도록 \\(b_0, b_1\\)을 선택하는 것임\n이를 위해서 적절한 확률모형을 결합시켜야 함.\n선택하는 확률 모형은 Bernoulli 분포임; 평균 \\(E(Y|X) = p\\)이고, \\(logit(E(Y|X)) = b_0 + b_1 \\cdot \\log_{2} d\\)\n\n발생 비율값으로 변환해 Binomial distribution (이항분포)로 전개하는 방식도 있음; binomial logit model\n\n관찰값은 Bernoulli 분포로부터 발생했다고 가정함으로써, 실제 관찰값들이 관찰될 확률/가능도를 기준으로 파라미터를 추정할 수 있음.\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\nblowbs.sort_values(\"log2d\")\n\n         d    s  y           spp  log2d  label\n2102  5.00 0.45  0  black spruce   2.32  alive\n724   5.00 0.18  0  black spruce   2.32  alive\n723   5.00 0.18  1  black spruce   2.32   died\n...    ...  ... ..           ...    ...    ...\n1784 29.00 0.38  1  black spruce   4.86   died\n1079 29.00 0.25  0  black spruce   4.86  alive\n3455 32.00 0.80  1  black spruce   5.00   died\n\n[659 rows x 6 columns]\n\n\n\n각 likelihood는 관측치들이 모두 독립적으로 발생했다고 가정했을 때의 확률값이고,\n각 \\(x_i\\)(그에 대응하는 \\(p_i\\))에 대해 관측치(\\(y_i\\))가 관찰된 확률은 간결하게 다음과 같이 표현할 수 있음.\n\\[p_i^{y_i} (1-p_i)^{1-y_i}\\]\n\\[\\displaystyle \\text{log likelihood}=\\log \\prod_{i=1}^{n}{P_i}=\\sum_{i=1}^{n} y_i \\cdot \\log p_i + (1-y_i) \\cdot \\log(1-p_i)\\]\n모든 데이터가 관찰될 likelihood = \\(p_1^7 (1-p_1)^{83} \\cdot p_2^7 (1-p_2)^{85} \\cdot p_3^{18} \\cdot (1-p_3)^{73} \\cdots\\)\n이 때, 모형을 예측변수 \\(X\\)의 1차 다항함수로 fit한다면,   \\(\\displaystyle \\log\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 \\cdot x_i\\) 인데,\n변형하면   \\(\\displaystyle p_i = \\sigma(\\beta_0 + \\beta_1 x_i) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot x_i)}}\\)\nLikelihood = \\(\\displaystyle \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.32)}}\\right)^7 \\left(1 - \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.32)}}\\right)^{83} \\cdot \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.58)}}\\right)^7 \\left(1 - \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.58)}}\\right)^{85} \\cdots\\)\n이 Likelihood가 최대가 되도록 \\(\\beta_0, \\beta_1\\)의 추정치를 찾음.\n\n\n\n\n\n\nNote\n\n\n\n위의 Log likelihood = \\(\\displaystyle \\sum_{i=1}^{n} y_i \\cdot \\log p_i + (1-y_i) \\cdot \\log(1-p_i)\\)는\n머신러닝에서 손실함수로 사용되는 cross-entropy loss와 (거의) 동일함.\n\\(L = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\cdot \\log p_i + (1-y_i) \\cdot \\log (1-p_i)\\)\n첫 번째 항: \\(\\displaystyle \\sum -y_i \\cdot \\log p_i = \\sum y_i \\cdot \\log \\frac{1}{p_i} = \\sum y_i \\cdot \\log \\frac{y_i}{p_i}= \\sum \\left(- y_i \\cdot \\log p_i - (-y_i \\cdot \\log y_i)\\right)\\)\n= cross-entropy - entropy (부정확하게 예측되어 발생하는 정보 손실량: Kullback-Leibler divergence)\n\n\n\n\n\n\n\n\nRegularization\n\n\n\n앞서 Ridge, Lasso에서와 비슷한 원리로 OLS가 아닌 ML(maximum likelihood) estimator에 penalty를 적용하는 방식임.\n즉, likeihood에 대한 계산에서 penalty term을 추가함\n에를 들어, Logistic regression with L2 regularization: 다음 손실함수를 최소화하도록 파라미터를 추정\n\\(\\displaystyle L = \\frac{1}{n} \\left[ - \\sum_{i=1}^{n} \\left(y_i \\cdot \\log p_i + (1-y_i) \\cdot \\log(1-p_i)\\right) + \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2 \\right]\\)\n\n\n\nimport statsmodels.formula.api as smf\n\nmod = smf.logit('y ~ log2d', data=blowbs).fit()\nprint(mod.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.499165\n         Iterations 6\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                  659\nModel:                          Logit   Df Residuals:                      657\nMethod:                           MLE   Df Model:                            1\nDate:                Tue, 27 May 2025   Pseudo R-squ.:                  0.2316\nTime:                        23:59:25   Log-Likelihood:                -328.95\nconverged:                       True   LL-Null:                       -428.10\nCovariance Type:            nonrobust   LLR p-value:                 4.888e-45\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -7.8162      0.628    -12.437      0.000      -9.048      -6.584\nlog2d          2.2408      0.190     11.773      0.000       1.868       2.614\n==============================================================================\n\n\n\n\n\n\n\n\nScikit-learn implementation\n\n\n\nScikit-learn의 LogisticRegression()은 디폴트로 \\(l2\\) regularization을 사용함: 참고 문서\n우선, train-test split을 통해 데이터를 나눈 후, LogisticRegression()을 적용하면,\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nX = blowbs[['log2d']]\ny = blowbs['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n\nlr = LogisticRegression(penalty=None)  # l1, l2 regularization 가능\nlr.fit(X_train, y_train)\n\nprint(lr.coef_, lr.intercept_)\n# [[2.08]] [-7.26]\n\n# As a DataFrame with column names\ncoefs = pd.DataFrame({\"coef\": lr.coef_[0], \"name\": X.columns})\ncoefs\n#    coef   name\n# 0  2.08  log2d\n\n\n위 fitted model의 예측값들\n\ncode\nfig, ax = plt.subplots(1, 1, figsize=(9, 5))\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.p, size=blowbs_bn.m, c=\"#008fd5\", sizes=(20, 200), ax=ax)\n\ndef jitter(values, j):\n    return values + np.random.normal(0, j, values.shape)\n\nsns.scatterplot(x=blowbs.log2d, y=jitter(blowbs.y, 0.02), alpha=.3, c=\".6\", ax=ax)\n\nfor i, row in blowbs_bn.iterrows():\n    ax.annotate(f\"{row.died:n}/{row.m:n}\", xy=(row.log2d, row.p), xytext=(row.log2d, row.p-0.05), size=9)\n\nax.set_xticks(blowbs_bn.log2d.unique())\nax.set_xticklabels(blowbs_bn.log2d.unique().round(2))\n# x-axis with 45 degree rotation\nax.tick_params(axis='x', rotation=45)\n\n# fitted line\nsns.lineplot(x=blowbs.log2d, y=mod.predict(blowbs[\"log2d\"]), ax=ax, color=\"#87bc45\")\n\nplt.show()",
    "crumbs": [
      "Home",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#prediction-values",
    "href": "contents/logistic.html#prediction-values",
    "title": "Logistic Regression",
    "section": "Prediction Values",
    "text": "Prediction Values\n예측값\nLogistic regression에서는 세 가지 타입의 예측값들이 있음.\n\nPredicted probability:   \\(\\displaystyle \\hat{p} = \\frac{1}{1+e^{-(b_0 + b_1 \\cdot x)}}\\)\nOdds:   \\(\\displaystyle odds = \\frac{\\hat{p}}{1 - \\hat{p}} = e^{b_0 + b_1 \\cdot x}\\),  Odds ratio:   \\(\\displaystyle \\frac{odds_1}{odds_2} = e^{b_1 \\cdot (x_1 - x_2)}\\)\nLog odds:   \\(\\displaystyle logit = b_0 + b_1 \\cdot x\\)\n\n이 확률값을 이용해 binary outcome인 클래스를 예측할 수 있음.\n\n# using statsmodels package, not scikit-learn\nblowbs_pred = blowbs.assign(\n    pred_prob = mod.predict(blowbs[\"log2d\"]),\n    pred_odds = lambda x: x.pred_prob / (1 - x.pred_prob),\n    pred_logit = lambda x: mod.predict(blowbs[\"log2d\"], which=\"linear\")\n)\nblowbs_pred.iloc[:, 1:]\n\n        s  y           spp  log2d  label  pred_prob  pred_odds  pred_logit\n17   0.02  0  black spruce   3.17  alive       0.33       0.49       -0.71\n24   0.03  0  black spruce   3.46  alive       0.48       0.94       -0.06\n25   0.03  0  black spruce   3.17  alive       0.33       0.49       -0.71\n...   ... ..           ...    ...    ...        ...        ...         ...\n3646 0.94  1  black spruce   3.17   died       0.33       0.49       -0.71\n3647 0.94  1  black spruce   4.09   died       0.79       3.83        1.34\n3661 0.98  1  black spruce   3.00   died       0.25       0.33       -1.09\n\n[659 rows x 8 columns]\n\n\n예를 들어, 두께(log2d)가 3인 나무의 경우,\n\nProbabiliy: 태풍에 쓰러질 확률은 25%로 예측되며,\nOdds: 태풍에 쓰러지지 않을 가능성 대비 쓰러질 가능성의 비율은 1:0.33이므로 대략 3:1로 예측됨. 즉 다시 말하면, 1 그루가 쓰러진다면 3 그루는 쓰러지지 않을 것으로 예측함.\n\nOdds가 1이면 event의 확률(p)이 0.5\nOdds가 1보다 작으면 event의 확률(p)이 0.5보다 작고,\nOdds가 1보다 크면 event의 확률(p)이 0.5보다 큼.\n\nLog odds (logit): 확률 p의 [0, 1]의 값을 무한한 값으로 늘려 linearly fit할 수 있게 함.\n\n\n\n\n\n\n\n모형의 파라미터 해석\n\n\n\n\n\nOdds의 비율 (odds ratio, OR)을 통해 해석\n\n\\(\\displaystyle odds: \\frac{\\hat{p}}{1 - \\hat{p}} = e^{b_0 + b_1 \\cdot x}=e^{b_0}\\cdot e^{b_1 \\cdot x}\\)  로부터\n\\(x\\)가 1 증가할 때 odds의 비율: \\(\\displaystyle odds ~ratio: \\frac{\\frac{\\hat{p_2}}{1 - \\hat{p_2}}}{\\frac{\\hat{p_1}}{1 - \\hat{p_1}}}  = e^{b_1 \\cdot (x+1) - b_1 \\cdot x} = e^{b_1}\\)\n즉, \\(x\\)가 1 증가하면, odds가 “몇 배”로 증가하는지를 나타냄.\n따라서, odds ratio가 1보다 크면 (\\(b_1\\)이 양수) \\(x\\)가 1 증가할 때, event의 odds가 커지며,\nodds ratio가 1보다 작으면 (\\(b_1\\)이 음수) event의 odds가 줄어듬.\n\n위의 경우, \\(\\displaystyle \\widehat{odds} = e^{-7.82 + 2.24 \\cdot log_2(d)}\\) 이므로 odds ratio = \\(e^{2.24} = 9.4\\)\n\n해석하면, 나무의 두께가 (log2 scale로) 1 늘어남 (2배 증가)에 따라 태풍에 나무가 쓰러질 odds가 9.4배 증가함\n다시 말하면, 나무의 두께가 (log2 scale로) 1 늘어남 (2배 증가)에 따라 태풍에 나무가 쓰러지지 않을 가능성 대비 쓰러질 가능성이 9.4배 증가함.\n나무의 두께 (원래 d)로 말하면, 두께가 10% 두꺼워지면, \\(e^{2.24 \\cdot log_2(1.1)}=1.36\\) 배, 즉 odds가 36% 증가함.\n\n\\(b_0\\): d = 1일 때의 odds이므로 \\(\\displaystyle e^{-7.82 + 2.34 \\cdot 0} = e^{-7.82} = 0.004\\), 즉 두께가 1cm 일 때 태풍에 나무가 쓰러지지 않을 가능성 대비 쓰러질 가능성은 0.004임.\n\n\n\n\n\n\n\n\n\nScikit-learn implementation\n\n\n\n\n\nScikit-learn의 LogisticRegression()은 디폴트로 \\(l2\\) regularization을 사용함: 참고 문서\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nX = blowbs[['log2d']]\ny = blowbs['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n\nlr = LogisticRegression(penalty=None)  # l1, l2 regularization 가능\nlr.fit(X_train, y_train)\n\n# predict_proba: 각 클래스에 대한 확률을 반환\n# predict: 예측된 클래스를 반환 (threshold=0.5)\ntest_pred = X_test.assign(\n    y=y_test,\n    pred_prob=lr.predict_proba(X_test)[:, 1],  # 확률값\n    pred_class=lr.predict(X_test),  # 클래스\n    pred_odds=lambda x: x.pred_prob / (1 - x.pred_prob),  # odds\n    pred_logit=lambda x: np.log(x.pred_odds),  # log odds\n)\ntest_pred.head(5)\n\n#       log2d  y  pred_prob  pred_class  pred_odds  pred_logit\n# 2890   3.46  1       0.48           0       0.94       -0.07\n# 574    3.81  0       0.66           1       1.93        0.66\n# 96     3.17  0       0.34           0       0.51       -0.67\n# 2773   3.46  1       0.48           0       0.94       -0.07\n# 1813   3.58  1       0.55           1       1.21        0.19",
    "crumbs": [
      "Home",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#predictive-accuracy",
    "href": "contents/logistic.html#predictive-accuracy",
    "title": "Logistic Regression",
    "section": "Predictive Accuracy",
    "text": "Predictive Accuracy\n모형의 예측 정확성\n분류 문제의 경우, 보통 두 단계를 거쳐 예측\n\nInference: 클래스에 속할 확률을 추정; 확률을 추정하지 않는 알고리즘도 있음.\nDecision: 추정된 확률을 기반으로 클래스를 결정\n\n기본적으로 0.5 이상이면 1로, 그렇지 않으면 0으로 분류\nThreshold를 조정해 분류를 조정할 수 있음\n\n예를 들어, 확실한 경우(p &gt; 0.8)에만 1로 분류\n\n애매한 확률 구간의 경우 결정을 유보할 수 있음; reject option\n\n\n이 때, 모형의 예측 정확성을 평가하는 두 가지 방식이 있음.\n\n확률 대한 예측 정확성 (evaluating predicted probability)\n클래스에 대한 예측 정확성 (evaluating predicted class)\n\n\nEvaluation of predicted probability\n\n\n\n\n\n\nDeviance Residuals\n\n\n\n\n\n이제 이 모형이 좋은 모형인지 살펴보기 위해 residual, 잔차를 살펴볼 수 있는가?\n\n\n\nPearson residual: \\(\\displaystyle \\frac{y_i - \\hat{p}_i}{\\sqrt{\\hat{p}_i (1-\\hat{p}_i)}}\\)\nDeviance residual: \\(\\displaystyle sign(y_i - \\hat{p}_i) \\sqrt{-2[y_i \\log\\hat{p}_i + (1-y_i) \\log(1-\\hat{p}_i)]}\\)\n\nDeviance를 이용해 OLS에서의 \\(R^2\\)와 비슷한 개념을 구성: Pseudo R-squared\nModel deviance, \\(D = -2[\\log likelihood - \\log likelihood_{\\text{perfect}})]\\)\n\nPerfect model의 likelihood: 1\nNull deviance: 절편만 있는 모형의 deviance; 856.2073760911842\nPseudo R-squared: \\(\\displaystyle \\frac{Null~Deviance - Deviance}{Null~Deviance}\\)\nCox-Snell’s Pseudo R-squared\nNagelkerke’s Pseudo R-squared\n\n“Coefficient of discrimination” (Tjur, 2009): average \\(\\hat{p}\\) when \\(y=1\\) - average \\(\\hat{p}\\) when \\(y=0\\)\n\n\n\n클래스별 예측 확률의 분포/히스토그램\n\n\ncode\ntest_pred[\"y2\"] = test_pred[\"y\"].map({0: \"survived (y=0)\", 1: \"died (y=1)\"})\n(\n    so.Plot(test_pred, x='pred_prob')\n    .add(so.Bars(color=\"#ef9b20\"), so.Hist(bins=12))\n    .facet(\"y2\")\n    .label(x=\"Predicted Probability\", y=\"Count\")\n    .layout(size=(7, 4))\n)\n\n\n\n\n\n\n\n\n\n각 (확률 값의) bin에 포함되는 관측치의 실제 값(\\(y \\in \\{0, 1\\}\\))의 비율을 계산해 시각화\n\ncode\nfrom sklearn.calibration import calibration_curve\n\n# 3. calibration_curve로 실제 비율 계산\nprob_true, prob_pred = calibration_curve(\n    test_pred[\"y\"], test_pred[\"pred_prob\"], n_bins=10, strategy=\"uniform\"\n)\n\n(\n    so.Plot(x=prob_pred, y=prob_true)\n    .add(so.Line(marker=\"o\", color=\"#f46a9b\"))\n    .add(so.Line(color=\".3\", linestyle=\":\"), y=prob_pred)\n    .scale(x=so.Continuous().tick(at=np.linspace(0, 1, 11)))\n    .label(\n        x=\"(10-Binned) Predicted Probability\",\n        y=\"Observed Event Percentage\\n(Emperical Probability)\",\n        title=\"Calibration Plot\",\n    )\n    .layout(size=(5, 4.5))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation of predicted class\n\n\n\n\n\n\nImportant\n\n\n\n예측된 확률을 기반으로 binary outcome/class으로 예측하여, 모형의 예측력을 평가\n\n예측된 확률값에 대해 임계치를 정하여, 예를 들어 0.5보다 크면 1, 0.5보다 작으면 0으로 분류하여, 이 binary 예측값과 실제값을 비교하여, 예측력을 평가\nConfusion matrix\nROC curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThreshold: 0.5인 경우, accuracy rate: 0.78\n\n\n\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\n203\n54\n\n\ndied(1)\n19\n54\n\n\n\n\n\n \n\n\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\nTrue Negative\nFalse Positive\n\n\ndied(1)\nFalse Negative\nTrue Positive\n\n\n\n\n\n\n다른 threshold를 적용하면,\n\n\n\nThreshold: 0.1인 경우,\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\n42\n4\n\n\ndied(1)\n180\n104\n\n\n\nAccuracy rate: 0.44\n\n\n \n\n\nThreshold: 0.9인 경우\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\n222\n104\n\n\ndied(1)\n0\n4\n\n\n\nAccuracy rate: 0.68\n\n\n \n\n\n\n\ncode\nfrom sklearn.metrics import accuracy_score\nfrom ISLP import confusion_table\n\n# cutoff = 0.5\ncm = confusion_table(test_pred[\"pred_class\"], test_pred[\"y\"])\nscore = accuracy_score(test_pred[\"y\"], test_pred[\"pred_class\"])\n\ndisplay(cm)\nprint(f\"Accuracy rate: {score:.2f}\")\n\n# cutoff = 0.1\ncm = confusion_table(test_pred[\"pred_prob\"] &gt; 0.1, test_pred[\"y\"])\nscore = accuracy_score(test_pred[\"y\"], test_pred[\"pred_prob\"] &gt; 0.1)\n\ndisplay(cm)\nprint(f\"Accuracy rate: {score:.2f}\")\n\n# cutoff = 0.9\ncm = confusion_table(test_pred[\"pred_prob\"] &gt; 0.9, test_pred[\"y\"])\nscore = accuracy_score(test_pred[\"y\"], test_pred[\"pred_prob\"] &gt; 0.9)\n\ndisplay(cm)\nprint(f\"Accuracy rate: {score:.2f}\")\n\n\n\n\n\n\n분류모형의 성능을 평가하기 위한 여러 지표들\nPrecision & Recall: 정보 검색 시스템의 탐색 능력을 평가하는데에서 유래\n\nPrecision (정밀도): \\(\\hat{y}=1\\)일 때, \\(y=1\\)일 확률\n\n시스템이 스팸으로 분류한 이메일 중 실제 스팸인 이메일의 비율\n\n높은 precision은 스팸으로 잘못 분류된 정상 이메일이 적다는 것을 의미\n하지만, 스팸 이메일이 정상 이메일함에 나타날 수 있음.\n\n정보검색에서 반환된 문서들 중 실제로 관련 있는 문서의 비율\n\n높은 precision은 사용자가 검색 결과에서 불필요한 정보를 적게 얻고, 대부분 유용한 정보를 얻는다는 것을 의미\n하지만, 관련은 있지만 놓친 정보는 많을 수 있음.\n검색 결과의 정확도를 평가\n\n검사를 통해 암으로 진단받은 사람이 실제로 암을 가지고 있을 확률\n\n높은 precision은 암 진단을 받은 사람에게 잘못된 암 진단을 내리지 않는 것을 의미\n\n\nRecall (재현율): \\(y=1\\)일 때, \\(\\hat{y}=1\\)일 확률\n\n실제 스팸 이메일 중에서 시스템이 스팸으로 올바르게 분류한 비율\n\n높은 recall은 대부분의 스팸 이메일이 정확히 스팸으로 분류된다는 것을 의미\n하지만, 정상 이메일이 스팸함으로 분류될 수 있음.\n\n정보검색에서 관련 문서들 중에서 실제로 시스템이 반환한 문서의 비율\n\n높은 recall은 사용자가 찾고자 하는 모든 관련 정보를 검색 결과에서 얻을 수 있다는 것을 의미\n하지만, 관련 없는 정보도 많이 포함될 수 있음.\n검색 시스템의 탐색 능력을 평가\n\n실제로 암을 가진 사람 중에서 검사를 통해 암으로 진단받은 사람의 비율\n\n높은 recall은 암 환자를 놓치지 않고 모두 찾아낸다는 것을 의미\n\n참 양성, true positive rate (TPR), 또는 sensitivity라고도 함\n\n\nprecision & recall trade-off: 이 둘 사이에는 종종 상충 관계가 있음. Precision을 높이기 위해 예측 확률에 대한 더 엄격한 기준을 사용하면 recall이 낮아질 수 있고, 반대로 recall을 높이기 위해 더 느슨한 기준을 사용하면 precision이 낮아질 수 있음.\n\n정보 검색의 맥락: precision을 높여 사용자가 불필요한 정보를 받지 않게 해주며, recall을 높여 필요한 정보를 놓치지 않도록 함.\n스팸 필터링의 맥락: precision을 높여 자주 스팸 메일함을 확인하지 않아도 되도록 하며, recall을 높여 대부분의 스팸 이메일이 차단되어 정상 메일함에서 보이지 않도록 쾌적한 환경을 제공할 수 있음.\n\nSensitivity & specificity: 의학 분야에서 진단 테스트의 정확성을 평가하는데에서 유래\nPrecision & recall이 주로 양성(positive) 클래스에 초점을 두는 반면, sensitivity & specificity는 양성(positive)와 음성(negative) 클래스 모두에 초점을 둠.\n특히, 이상치 탐지라든가 희귀 질병 진단 등 불균형 데이터셋에서는 precision & recall이 더 유용할 수 있음.\n\nSensitivity (민감도): \\(y=1\\)일 때, \\(\\hat{y}=1\\)일 확률\n\nRecall, true positive rate (TPR)\n높은 민감도는 질병이 있는 사람을 놓치지 않고 모두 찾아내는 것(참 양성)을 의미\n\nSpecificity (특이도): \\(y=0\\)일 때, \\(\\hat{y}=0\\)일 확률\n\nTrue negative rate (TNR)\n실제로 질병이 없는 사람을 얼마나 잘 (질병이 없다고) 식별하는지(참 음성)를 나타냄\n높은 특이도는 질병이 없는 사람에게 질병이 있다고 잘못 진단하지 않는 것을 의미함.\n\n희귀 질병을 진단하는 경우, 정상인(negative)을 올바로 식별하는 것은 매우 쉽기 때문에 질병이 없는 사람에 대한 판별력이 과대추정될 수 있음; precision이 더 유용할 수 있음.\n\n1 - FPR (False Positive Rate; false alarm)\n0, 1을 바꿨을 때의 즉, 0을 기준으로 했을 때의 recall\n특이도가 높은 테스트라면 질병이 없다는 판정은 신뢰할 만함. 즉, 0(음성)을 기준으로한 sensitivity가 높은 것이 됨.\n\n\n\n\n\n\n\n\n거짓 음성 비율: False negative rate (FNR); \\(y=1\\)일 때, \\(\\hat{y}=0\\)일 확률: 1 - sensitivity\n- 정상이라고 진단된 환자가 실제로 암인 확률\n거짓 양성 비율: False positive rate (FPR) 또는 False alarm; \\(y=0\\)일 때, \\(\\hat{y}=1\\)일 확률: 1 - specificity\n- 정상인 사람이 검사를 통해 암으로 진단받을 확률\n\n\n\n\n\n위의 confusion matrix로부터 지표들을 계산하면,\n\n\n\nThreshold: 0.1인 경우,\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\n42\n4\n\n\ndied(1)\n180\n104\n\n\n\n\n\n \n\n\nThreshold: 0.9인 경우\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\n222\n104\n\n\ndied(1)\n0\n4\n\n\n\n\n\n\n\n\n\n\nrecall = 104 / (4 + 104) = 0.96\nprecision = 104 / (180 + 104) = 0.37\nspecificity = 42 / (42 + 180) = 0.19\naccuracy = (42 + 104) / (42 + 4 + 180 + 104) = 0.44\n\n\n\n \n\n\n\nrecall = 4 / (4 + 104) = 0.04\nprecision = 4 / (0 + 4) = 1.00\nspecificity = 222 / (222 + 0) = 1.00\naccuracy = (222 + 4) / (222 + 104 + 0 + 4) = 0.68\n\n\n\n\nReceiver operating characteristic (ROC) curve\n예측된 확률에 대한 임계치를 조정함에 따라 옳은 예측과 틀린 예측의 비율이 어떻게 달라지는지 살펴봄으로써 임계치를 설정하는데 도움을 줌\n\n잘못된 예측에 대한 비용이 다르다면, 특정 임계치를 선택하는 것을 고려해야 함\n\n\n\ncode\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(test_pred.y, test_pred.pred_prob)\nroc = pd.DataFrame(\n    {\n        \"thresholds\": thresholds.round(2),\n        \"False Pos\": fpr,\n        \"sensitivity(TPR)\": tpr,\n        \"specificity(TNR)\": 1 - fpr\n    }\n)\nroc.sort_values(\"thresholds\").head(10)\n\n## ROC curve\n# 위에서 얻은 roc 데이터을 사용하여 그리거나\n# RocCurveDisplay를 이용\nfrom sklearn.metrics import RocCurveDisplay\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n\n# scikit-learn의 visualization 문서 참조\n# https://scikit-learn.org/stable/visualizations.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nfrom sklearn.metrics import precision_recall_curve\n\nprecision, recall, thresholds = precision_recall_curve(test_pred.y, \ntest_pred.pred_prob)\npr = pd.DataFrame(\n    {\n        \"thresholds\": np.append(thresholds.round(2), 1),\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1-score\": 2 * precision * recall / (precision + recall)\n    }\n).query(\"recall &gt; 0.1\")\npr.sort_values(\"thresholds\").head(10)\n\n# precision-recall curve\n# 위에서 얻은 pr 데이터을 사용하여 그리거나\n# PrecisionRecallDisplay를 이용\nfrom sklearn.metrics import PrecisionRecallDisplay\npr_display = PrecisionRecallDisplay(precision=precision, recall=recall).plot()\n\n# scikit-learn의 visualization 문서 참조\n# https://scikit-learn.org/stable/visualizations.html\n\n\n\n\n\n\n\n\n\n위에서 언급한 클래스 불균형의 예들: 이상치 탐지, 희귀 질병 진단 등에서는 precision & recall이 더 유용할 수 있음.\n아래는 양성:음성 = 3:97인 경우의 예\n\n\ncode\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n\n# Create an imbalanced dataset\nX, y = make_classification(n_samples=10000, n_features=20, n_classes=2,\n                           weights=[0.97, 0.03], random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train a random forest classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Get the probability scores for the testing set\ny_score = clf.predict_proba(X_test)[:, 1]\n\n## 혹은 logistic regression\n# lr = LogisticRegression()\n# y_score = lr.predict_proba(X_test)[:, 1]\n\n# Calculate the ROC curve\nfpr, tpr, th = roc_curve(y_test, y_score)\nspc = 1 - fpr\nroc_auc = auc(fpr, tpr)\n\n# Calculate the Precision-Recall curve\nprecision, recall, th2 = precision_recall_curve(y_test, y_score)\npr_auc = average_precision_score(y_test, y_score)\n\n# Plot the ROC curve\nplt.figure(figsize=(11, 10), dpi=80)\nplt.subplot(2,2,1)\nplt.plot(spc, tpr, color='#87bc45', lw=2, label='AUC = %0.2f' % roc_auc)\nplt.plot([1, 0], [0, 1], color='k', lw=1, linestyle='-')\nplt.xlim([0.0, 1.02])\nplt.ylim([0.0, 1.02])\nplt.xlabel('Specificity')\nplt.ylabel('Sensitivity')\nplt.title('Sensitivity-Specificity Curve')\nplt.legend(loc=\"lower left\")\n\n# Plot the Precision-Recall curve\nplt.subplot(2,2,2)\nplt.plot(recall, precision, color='#87bc45', lw=2, label='AUC = %0.2f' % pr_auc)\nplt.xlim([0.0, 1.02])\nplt.ylim([0.0, 1.02])\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc=\"lower left\")\n\ndf1 = pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr, \"spc\": spc, \"th\": th})\ndf2 = pd.DataFrame({\"precision\": precision, \"recall\": recall, \"th\": np.append(th2, 1)})\ndf_merge = df1.merge(df2)\n\nplt.subplot(2,2,4)\nplt.plot(df_merge[\"spc\"], df_merge[\"precision\"], color='#00A08A', lw=2)\nplt.plot(df_merge[\"recall\"], df_merge[\"precision\"], color='#87bc45', lw=2)\nplt.xlim([0.0, 1.02])\nplt.ylim([0.0, 1.02])\nplt.ylabel('Precision')\nplt.xlabel('Specificity/Recall')\nplt.legend([\"Specificity\", \"Recall\"])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nClassifier의 전체적 성능에 대한 지표\nROC curve:\n\nAUC: Area Under the Curve = Concordance Index\n\n각 specificity값에 대한 sensitivity의 합; 모형(classifier) 대한 전반적 평가\n0.5: random guess, 1: perfect prediction\n\nConcordance Index(c-index): 모든 서로 다른 클래스의 Y쌍, 예를 들어 \\((0_i, 1_j)\\)에 대해서 해당하는 예측된 확률의 크기가 \\(p_i &lt; p_j\\) 인 비율, 즉 순서가 맞는(concordance) 비율\n\n\n\n\n\n\n\n \n\n\n\n위 blowdown 데이터셋 대한 모형의 AUC, concordance index 계산\n\n\n\n\n\n\nCode\n\n\n\n\n\nfrom sklearn.metrics import roc_auc_score, auc\n\nroc_auc = roc_auc_score(test_pred.y, test_pred.pred_prob)\nprint(f\"AUC: {roc_auc:.2f}\")  # 또는 auc(fpr, tpr)\n\n\n\nAUC: 0.81\n\n\n\n\n\n\nCode\n\n\n\n\n\ndef concordance(y, pred_prob):\n    concord = 0\n    total = 0\n\n    test_pred = pd.DataFrame({\"y\": y, \"pred_prob\": pred_prob})\n    for idx, case in test_pred.iterrows():\n        other_cases = test_pred[test_pred.index != idx]\n\n        # 같은 케이스 제외\n        df = other_cases[case[\"y\"] != other_cases[\"y\"]]\n\n        concord += (\n            (case[\"y\"] - df[\"y\"] &gt; 0) == (case[\"pred_prob\"] - df[\"pred_prob\"] &gt; 0)\n        ).sum()\n        total += len(df)\n\n    return concord / total\n\nprint(f\"Concordance index: {concordance(test_pred.y, test_pred.pred_prob):.2f}\")\n\n\n\nConcordance index: 0.81\n\n\n\nPrecision-recall:\n\nAverage precision: the area under the precision-recall curve\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nfrom sklearn.metrics import average_precision_score\n\naverage_precision = average_precision_score(test_pred.y, test_pred.pred_prob)\nprint(f\"Average Precision: {average_precision:.2f}\")\n\n\n\nAverage Precision: 0.68\n\nThe classification report in scikit-learn\nThershold: 0.4인 경우,\n\n\ncode\nfrom sklearn.metrics import classification_report, recall_score\n\nthreshold = .4\npred_class = test_pred[\"pred_prob\"] &gt;= threshold\nprint(classification_report(test_pred[\"y\"], pred_class))\n\n\n              precision    recall  f1-score   support\n\n           0       0.86      0.82      0.84       222\n           1       0.66      0.73      0.69       108\n\n    accuracy                           0.79       330\n   macro avg       0.76      0.77      0.77       330\nweighted avg       0.80      0.79      0.79       330\n\n\n\n\n\n\n\n\n\nspecificity\n\n\n\n\nspecificity for positive = recall for negative: 0.82\nmacro avg of recall: sensitivity와 specificity의 평균: 0.77\n\n\n\n\nF1-score = \\(\\displaystyle \\left(\\frac{\\text{precision}^{-1} + \\text{recall}^{-1}}{2}\\right)^{-1}\\)\n\nPrecision과 recall의 조화평균\n보수적인 지표임. 즉, precision과 recall 중 하나라도 낮으면 F1-score도 낮아짐\n\n\n\n\n\n\n\n\n\nClassifier로서 전반적인 모형의 성능 vs. 특정 임계치에서의 모형의 성능 vs. 확률모형\n\nClassifier로서 전반적인 모형의 성능: AUC 등\n비용을 고려한 특정 임계치에서의 모형의 성능\n\n잘못된 예측에 대한 비용이 다르다면, 임계치를 조정하여, 잘못된 예측에 대한 비용을 줄일 수 있음\n만약, 농작물에 대한 피해라고 가정하면,\n\nCosts: 농작물 피해, 펜스 설치비, 노동력 등\nBenefits: 수확물의 가치\n거짓 음성을 낮춰야 하는 경우: 예를 들어, 농작물의 작은 피해도 심각한 결과를 초래하는 경우\n거짓 양성을 낮춰야 하는 경우: 예를 들어, 농작물의 피해 예방을 위한 비용이 큰 경우\n\n만약, 와인 셀러가 와인의 품질(high:양성 vs. low:음성)을 성분들로 예측하는 모형을 만든다면, (in Stefanie Molin’s book)\n\nCosts: 높은 품질의 와인을 낮은 품질로 예측하면, 와인 품평가에게 신뢰를 잃을 수 있음\nBenefits: 낮은 품질의 와인을 높은 품질로 예측하면, 낮은 품질의 와인을 높은 가격에 팔아 수익으로 이어질 수 있음\n거짓 음성을 낮춰야 하는 경우: 예를 들어, 영세한 와이너리가 수익이 중요한 경우\n거짓 양성을 낮춰야 하는 경우: 예를 들어, 고품질의 와인을 생산하는 것으로 유명한 와이너리; 네임밸류를 유지하기 위해. 반면, 비싼 와인이 싸게 팔리는 것은 감당할 수 있음.\n\n\n확률모형: 확률을 정확히 예측하는 모형의 추구;\n\n확률값 혹은 확률 분포(Bayesian 접근)로 communicate\nDecision maker는 당사자",
    "crumbs": [
      "Home",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#decision-boundary",
    "href": "contents/logistic.html#decision-boundary",
    "title": "Logistic Regression",
    "section": "Decision Boundary",
    "text": "Decision Boundary\n앞선, Log odds(logit):   \\(\\displaystyle log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)  = b_0 + b_1 \\cdot X\\)\n예측변수가 2개라면,\nLog odds(logit):   \\(\\displaystyle log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)  = b_0 + b_1 \\cdot X_1 + b_2 \\cdot X_2\\)\n또는 \\(\\displaystyle \\hat p = \\sigma(b_0 + b_1 \\cdot X_1 + b_2 \\cdot X_2)\\),   \\(\\displaystyle \\sigma := \\frac{1}{1 + e^{-x}}\\)\n\n왼편의 logit 함수(또는 sigmoid 함수)는 증가함수이므로, threshold를 정하면 선형 결정 경계(decision boundary)가 나타남: hyperplane\n비선형 함수로 모형을 세우면, decision boundary가 비선형으로 나타남.\n\n예를 들어, \\(b_0 + b_1 \\cdot X + b_2 \\cdot X^2\\) 등\n\n\n밑은 두 예측변수 \\(X_1= log2d\\) (diameter)와 \\(X_2=s\\) (severity)로 나무가 쓰러질지 여부를 선형함수로 만들었을 때의 decision boundary (threshold=0.5)\n즉, Log odds(logit):   \\(\\displaystyle log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)  = b_0 + b_1 \\cdot X_1 + b_2 \\cdot X_2\\)\n\n\n\n\n\n\n\n\n\nBodyM Dataset\nAWS: BodyM Dataset\n\n2,779명 대상으로 8,324장의 정면 및 측면 실루엣 사진과 키, 몸무게, 그리고 14가지 신체 측정치 제공\n인종 분포: 백인 40%, 아시아인 30%, 흑인/아프리카계 미국인 14%, 아메리카 원주민 또는 알래스카 원주민 1%, 기타 15%(히스패닉 15%)\n\n\\(Y\\) = 성별,\n\\(X_1\\) = 키(height), \\(X_2\\) = 어깨 너비(shoulder-breath)에 대한 2차항을 포함한 선형함수로 모형을 세웠을 때의 decision boundary",
    "crumbs": [
      "Home",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#basis-functions",
    "href": "contents/logistic.html#basis-functions",
    "title": "Logistic Regression",
    "section": "Basis Functions",
    "text": "Basis Functions\n기저 함수\n관측치(input vector) \\(\\mathbf{x}\\)에 대해, basis function \\(\\phi(\\mathbf{x})\\)를 사용하여, 비선형 모형을 선형모형의 형태로 변환(파라미터에 대한 선형)\n\\[\\mathbf{y} = b_0 + b_1 \\cdot \\phi_1(\\mathbf{x}) + b_2 \\cdot \\phi_2(\\mathbf{x}) + ... + b_n \\cdot \\phi_{m-1}(\\mathbf{x})\\]\n예를 들어, 두 예측변수의 관측치 \\(\\mathbf{x} = (x_1, x_2) = (log2d, s)\\) (각각 diameter와 severity)에 대해서 모든 2차항을 추가한다면\n\\[\\phi_i(\\mathbf{x}): \\mathbb{R}^2 \\to \\mathbb{R}\\]\nwhere \\(\\phi_1(\\mathbf{x}) = x_1, ~ \\phi_2(\\mathbf{x}) = x_2, ~ \\phi_3(\\mathbf{x}) = x_1^2, ~ \\phi_4(\\mathbf{x}) = x_1 \\cdot x_2, ~ \\phi_5(\\mathbf{x}) = x_2^2\\)\n\n즉, 5개의 예측 변수가 생성: \\(x_1, x_2, x_1^2, ~ x_1 \\cdot x_2, ~x_2^2\\)\n이는 2차원 예측변수 공간을 5차원으로 확장하는 것과 같음: \\((x_1, x_2) \\rightarrow (\\phi_1, \\phi_2, \\phi_3, \\phi_4, \\phi_5)\\)\n이는 Kernel trick과 유사함: 고차원으로 변환하여 선형모형을 적용하는 것\nFeature engineering의 일종임. Deep learning에서는 연구자가 직접 만들기보다는 모형이 학습할 수 있도록 함.\n\n\n\ncode\n# 2차식을 추가\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression\nfrom ISLP import confusion_table\nfrom sklearn.metrics import accuracy_score\n\npoly = PolynomialFeatures(degree=2)\nX_train_poly = poly.fit_transform(X_train)\nX_test_poly = poly.transform(X_test)\n\nlr_poly = LogisticRegression(penalty=None, max_iter=1000)\nlr_blowsdn_poly = lr_poly.fit(X_train_poly, y_train)\n\ntest_pred = X_test.assign(\n    y=y_test,\n    pred_prob=lr_blowsdn_poly.predict_proba(X_test_poly)[:, 1],  # 확률값\n    pred_class=lr_blowsdn_poly.predict(X_test_poly),  # 클래스\n)\n\n# cutoff = 0.5\ncm = confusion_table(test_pred[\"pred_class\"], test_pred[\"y\"])\nscore = accuracy_score(test_pred[\"y\"], test_pred[\"pred_class\"])\n\n\n\n\n\nFigure 1: Logistic Regression Decision Boundary with Kernel Tricks\n\n\n\n\n\n\n\n(a) \\(\\phi: (x_1, x_2) \\rightarrow (x_1, x_2, x_1^2, x_1 x_2, x_2^2) \\in \\mathbb{R^5}\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(\\phi: (x_1, x_2) \\rightarrow (x_1, x_2, x_1^2 + x_2^2) \\in \\mathbb{R^3}\\)\n\n\n\n\n\n\n\n\n\n\n\n선형모형은 basis function을 고려하면 간단한 neural network(신경망)으로 볼 수 있음.\n\n자주 사용되는 basis function\n\nPolynomial basis: \\(\\phi_i(x) = \\mathbf{x}^i\\)\nGaussian (Radial) basis: \\(\\displaystyle \\phi_i(\\mathbf{x}) = \\exp\\left(-\\frac{||\\mathbf{x} - \\mu_i||^2}{2 s^2}\\right)\\)\nSigmoidal basis: \\(\\displaystyle \\phi_i(\\mathbf{x}) = \\sigma\\left(\\frac{||\\mathbf{x} - \\mu_i||}{s}\\right)\\), \\(\\displaystyle \\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\n\n\n\n다른 모형에서도 유연하게 비슷한 아이디어를 적용할 수 있음.\n\n가령, KNN에서 이웃하는 데이터들 간의 거리를 고려해서 확률을 계산할 수 있음.\n\n즉, Gaussian의 분포에 비례하여 근접한 이웃에 더 큰 가중치를 부여함.\n\n\n\n\n\n\n\nKNN with Gaussian kernel\ndef gaussian_kernel(distances, sigma=1):\n  \"\"\"Calculates Gaussian kernel weights.\"\"\"\n  return np.exp(-distances**2 / (2 * sigma**2))\n\n# Create KNN classifier with custom weights\nknn_gaussin = KNeighborsClassifier(n_neighbors=10, weights=lambda x: gaussian_kernel(x, sigma=1))",
    "crumbs": [
      "Home",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#neural-networkdeep-learning",
    "href": "contents/logistic.html#neural-networkdeep-learning",
    "title": "Logistic Regression",
    "section": "Neural Network/Deep Learning",
    "text": "Neural Network/Deep Learning\n\n앞서 Gaussian basis (radial basis) function (RBF)과 같은 방식으로 비선형 모형 변환이 유용하나, 계산량이 많고, 과적합을 방지하기 위한 조심스러운 정규화가 필요함.\n또는, Support Vector Machine (SVM)도 비슷하게 basis function을 이용해 비선형 모형을 만드나, 확률적 출력을 제공하지 않고 다중 클래스의 일반화가 어려움.\nRBF 및 SVM 같은 모델은 최근에는 deep learning에 의해 대체되고 있음.\n\n대규모 데이터셋을 효과적으로 활용 가능.\n깊은 계층 구조의 표현을 학습하여 복잡한 예측 문제에서 높은 정확도 달성.\n\n\n\nFIGURE 10.1. Neural network with a single hidden layer. The hidden layer computes activations \\(A_k = h_k(X)\\) that are nonlinear transformations of linear combinations of the inputs \\(X1, X2, . . . , Xp\\). Hence these \\(A_k\\) are not directly observed. The functions \\(h_k(\\cdot)\\) are not fixed in advance, but are learned during the training of the network. The output layer is a linear model that uses these activations \\(A_k\\) as inputs, resulting in a function \\(f(X)\\).\n\n\n\n\n\n\nFIGURE 10.2. Activation functions. The piecewise-linear ReLU function is popular for its efficiency and computability. We have scaled it down by a factor of five for ease of comparison.\n\n\n\nHidden layers: \\(A_k = g(w_{k0} + w_{k1}X_1 + w_{k2}X_2 + w_{k3}X_3 + w_{k4}X_4)\\)\n\n\\(g\\)는 activation function으로 logistic에서처럼 sigmoid나 현대적으로는 ReLU를 사용\n\n각 \\(A_k\\)가 1에 가까우면 firing, 0에 가까우면 silent한다고 마치 뇌의 뉴런이 신호가 전달되는 것에 비유\n\\(Y(X) = \\beta_0 + \\beta_1 A_1 + \\beta_2 A_2 + \\cdots + \\beta_5 A_5\\)\n\nThe handwritten digit recognition example\n\n1980년대 후반 숫자를 인식하는 문제가 신경망 기술의 발전에 큰 촉매로 작용했음.\n신경망의 구조의 개선을 거쳐 인간 수준의 성능을 달성하는데 30년 이상 걸렸음.\n\n아래 28x28 pixels의 음영을 나타내는 0~255(8bit) 값을 입력값으로 받음.\n\n\n\n\n60,000개의 training set을 이용하여 학습\n인풋 레이어의 유닛(unit)의 수: 28x28 = 784개 (예측변수의 수)\n각각의 hidden layer의 유닛의 수는 256개, 128개\n총 파라미터의 수는 235,146개; 예측변수보다 월등히 많음\n한편 multinomial logistic으로 필요한 파라미터의 수는 785x9 = 7,065개\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: 3blue1brown\n\n\n\n\n Source: An Introduction to Statistical Learning (2e)\n\nConvolutional Deep Belief Networks: Hierarchical Representations\n\n\nSource: p.301, Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H\n\n\n\nFigure 2. The first layer bases (top) and the second layer bases (bottom) learned from natural images. Each second layer basis (filter) was visualized as a weighted linear combination of the first layer bases.\n\n\n\n\n\nFigure 3. Columns 1-4: the second layer bases (top) and the third layer bases (bottom) learned from specific object categories. Column 5: the second layer bases (top) and the third layer bases (bottom) learned from a mixture of four object categories (faces, cars, airplanes, motorbikes).\n\n\n\n\nSource: Convolutional Deep Belief Networks\n\n\n\nGoogle Neural Network Playground",
    "crumbs": [
      "Home",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#multiclass-k-2",
    "href": "contents/logistic.html#multiclass-k-2",
    "title": "Logistic Regression",
    "section": "Multiclass (K > 2)",
    "text": "Multiclass (K &gt; 2)\nMultinomial logistic regression\n클래스 \\(C_k\\)에 속할 확률을 예측하는 모형: Base 클래스를 기준으로 나머지 클래스에 대한 확률을 예측\nBinary 경우에서의 확률 \\(\\displaystyle P(Y=1|X=x) = \\frac{1}{1+e^{-(b_0 + b_1 \\cdot x)}}=\\frac{e^{b_0 + b_1 \\cdot x}}{1 + e^{b_0 + b_1 \\cdot x}}\\)을 다음과 같이 확장할 수 있음.\n\n\\(\\displaystyle P(Y=C_k|X=x) = \\frac{e^{\\beta_{k0} + \\beta_{k1} \\cdot x}}{1 + \\sum_{j=1}^{K-1} e^{\\beta_{j0} + \\beta_{j1} \\cdot x}}, ~~ k = 1, 2, ..., K-1\\)\n\\(k=K\\) (Base 클래스)의 경우 \\(\\displaystyle P(Y=C_K|X=x) = \\frac{1}{1 + \\sum_{j=1}^{K-1} e^{\\beta_{j0} + \\beta_{j1} \\cdot x}}\\)\n\n이 때, 마지막 클래스 \\(C_K\\)에 대한 클래스 \\(C_k\\)의 비율은 다음과 같이 나타남.\n\\(\\displaystyle log\\left(\\frac{P(Y=C_k|X=x)}{P(Y=C_K|X=x)}\\right) = \\beta_{k0} + \\beta_{k1} \\cdot x\\)\n이는 binary경우 \\(log(odds)\\) 인 \\(\\displaystyle log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right) = b_0 + b_1 \\cdot x\\)의 확장으로 볼 수 있음.\n\n\n\n\n\n\nNote\n\n\n\nMachine learning에서는 주로 base 클래스 없이 모든 클래스에 대한 확률을 예측:\nSoftmax coding: \\(\\displaystyle P(Y=C_k|X=x) = \\frac{e^{\\beta_{k0} + \\beta_{k1} \\cdot x}}{\\sum_{j=1}^{K} e^{\\beta_{j0} + \\beta_{j1} \\cdot x}}\\)\n두 클래스 \\(C_k\\)와 \\(C_{k'}\\)의 사이의 확률의 비율은\n\\(\\displaystyle log\\left(\\frac{P(Y=C_k|X=x)}{P(Y=C_{k'}|X=x)}\\right) = (\\beta_{k0} - \\beta_{k'0}) + (\\beta_{k1} - \\beta_{k'1}) \\cdot x\\)\n\n\n\nBlowdown data\n가령, 나무의 종(9 species)을 분류하는 문제를 생각해보면,\n나무의 두께(log2d), 쓰러졌는지 여부(y), 피해 심각도(s)로 9개의 종에 대한 확률을 예측하는 모형을 만들 수 있음.\n\nLoad data\n# Multinomial logistic regression\nblowdown = pd.read_csv('data/blowdown2.csv')\nX = blowdown.drop(columns=[\"spp\", \"d\"])\ny = blowdown[\"spp\"]  # classify the species\nblowdown\n\n\n\n\n\n\n\n\n\n\nd\ns\ny\nspp\nlog2d\n\n\n\n\n0\n9.00\n0.02\n0\nbalsam fir\n3.17\n\n\n1\n14.00\n0.02\n0\nbalsam fir\n3.81\n\n\n2\n18.00\n0.02\n0\nbalsam fir\n4.17\n\n\n...\n...\n...\n...\n...\n...\n\n\n3663\n19.00\n0.98\n1\njackpine\n4.25\n\n\n3664\n37.00\n0.98\n1\nblack ash\n5.21\n\n\n3665\n48.00\n0.98\n1\nblack ash\n5.58\n\n\n\n\n3666 rows × 5 columns\n\n\n\n\n# split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0, stratify=y)\n\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train);\n\n\n\nPredicted probability\npd.options.display.max_columns = 15\n\n# predicted probability\ntest_pred_prob = pd.DataFrame(lr.predict_proba(X_test), columns=lr.classes_)\npd.concat(\n    [X_test.assign(target=y_test).reset_index(drop=True), test_pred_prob], axis=1\n) # 데이터셋 병합\n\n\n\n\n\n\n\n\n\ns\ny\nlog2d\ntarget\naspen\nbalsam fir\nblack ash\nblack spruce\ncedar\njackpine\npaper birch\nred maple\nred pine\n\n\n\n\n0\n0.61\n1\n4.58\npaper birch\n0.29\n0.00\n0.02\n0.04\n0.24\n0.02\n0.33\n0.01\n0.04\n\n\n1\n0.26\n0\n4.32\ncedar\n0.14\n0.02\n0.01\n0.05\n0.20\n0.26\n0.11\n0.04\n0.18\n\n\n2\n0.86\n1\n5.17\naspen\n0.42\n0.00\n0.09\n0.01\n0.10\n0.01\n0.34\n0.00\n0.02\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1830\n0.09\n0\n3.70\ncedar\n0.05\n0.05\n0.00\n0.13\n0.25\n0.22\n0.05\n0.05\n0.20\n\n\n1831\n0.41\n1\n3.32\nblack spruce\n0.05\n0.01\n0.00\n0.33\n0.44\n0.02\n0.06\n0.02\n0.06\n\n\n1832\n0.20\n0\n3.00\ncedar\n0.01\n0.05\n0.00\n0.31\n0.24\n0.12\n0.01\n0.06\n0.21\n\n\n\n\n1833 rows × 13 columns\n\n\n\n\nPredicted class\n# prediction\ntest_pred = X_test.assign(\n    target=y_test,\n    pred_class=lr.predict(X_test),\n    pred_prob=lr.predict_proba(X_test).max(axis=1),\n)\ntest_pred\n\n\n\n\n\n\n\n\n\n\ns\ny\nlog2d\ntarget\npred_class\npred_prob\n\n\n\n\n2811\n0.61\n1\n4.58\npaper birch\npaper birch\n0.33\n\n\n1105\n0.26\n0\n4.32\ncedar\njackpine\n0.26\n\n\n3566\n0.86\n1\n5.17\naspen\naspen\n0.42\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n255\n0.09\n0\n3.70\ncedar\ncedar\n0.25\n\n\n1934\n0.41\n1\n3.32\nblack spruce\ncedar\n0.44\n\n\n816\n0.20\n0\n3.00\ncedar\nblack spruce\n0.31\n\n\n\n\n1833 rows × 6 columns\n\n\n\n# confusion matrix\nconfusion_table(test_pred[\"pred_class\"], test_pred[\"y\"])\n\n\n\n\n\n\n\n\nTruth\naspen\nbalsam fir\nblack ash\nblack spruce\ncedar\njackpine\npaper birch\nred maple\nred pine\n\n\nPredicted\n\n\n\n\n\n\n\n\n\n\n\n\n\naspen\n38\n0\n6\n0\n7\n12\n30\n0\n7\n\n\nbalsam fir\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nblack ash\n2\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nblack spruce\n8\n19\n0\n189\n88\n41\n0\n13\n74\n\n\ncedar\n63\n7\n0\n111\n278\n46\n79\n25\n76\n\n\njackpine\n21\n4\n0\n8\n47\n45\n24\n11\n28\n\n\npaper birch\n80\n0\n18\n4\n27\n11\n114\n1\n11\n\n\nred maple\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nred pine\n6\n7\n0\n17\n38\n23\n4\n12\n52\n\n\n\n\n\n\n\nPrecision, Recall: one-versus-all(rest)\n해당 클래스를 positive로 두고 나머지 클래스를 negative로 두어, 각 클래스에 대한 precision과 recall을 계산\n\n# classification report: one-versus-all\nprint(classification_report(test_pred[\"y\"], test_pred[\"pred_class\"]))\n\n              precision    recall  f1-score   support\n\n       aspen       0.38      0.17      0.24       218\n  balsam fir       0.00      0.00      0.00        37\n   black ash       0.33      0.04      0.07        25\nblack spruce       0.44      0.57      0.50       329\n       cedar       0.41      0.57      0.48       485\n    jackpine       0.24      0.25      0.25       178\n paper birch       0.43      0.45      0.44       251\n   red maple       0.00      0.00      0.00        62\n    red pine       0.33      0.21      0.26       248\n\n    accuracy                           0.39      1833\n   macro avg       0.28      0.25      0.25      1833\nweighted avg       0.36      0.39      0.36      1833\n\n\n\n\n# accuracy\nprint(accuracy_score(test_pred[\"y\"], test_pred[\"pred_class\"]))\n\n0.3911620294599018\n\n\n\n\nMNIST digits data\n\n\nLoad data\nimport tensorflow as tf\n\n# Load the MNIST dataset\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# randomly select a subset of the data\nnp.random.seed(123)\n\nn1, n2 = 2000, 2000  # 2000 training and 2000 test samples\n\nidx1 = np.random.randint(0, 60000, n1)\nidx2 = np.random.randint(0, 10000, n2)\n\nX_train_sub = X_train[idx1, :]\ny_train_sub = y_train[idx1]\nX_test_sub = X_test[idx2, :]\ny_test_sub = y_test[idx2]\n\n# Reshape the data\nX_train_sub = X_train_sub.reshape(n1, 28 * 28)  # 2-dim array: (2000, 784)\nX_test_sub = X_test_sub.reshape(n2, 28 * 28)\n\n\n\n# Fit Logistic Regression with polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Create polynomial features\npoly = PolynomialFeatures(degree=2)\nX_train_poly = poly.fit_transform(X_train_sub)\nX_test_poly = poly.transform(X_test_sub)\n\n# Fit the model\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train_poly, y_train_sub)\n\n# Prediction\ny_pred_poly = lr.predict(X_test_poly)\n\n\n\nPredicted probability\npd.options.display.max_columns = 15\n\n# predicted probability\ntest_pred_prob = pd.DataFrame(lr.predict_proba(X_test_poly), columns=lr.classes_)\ntest_pred_prob[\"target\"] = y_test_sub\ntest_pred_prob\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\ntarget\n\n\n\n\n0\n0.01\n0.00\n0.00\n0.00\n0.00\n0.98\n0.00\n0.00\n0.02\n0.00\n8\n\n\n1\n0.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n4\n\n\n2\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n7\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1997\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0\n\n\n1998\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0\n\n\n1999\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n6\n\n\n\n\n2000 rows × 11 columns\n\n\n\n\n\nConfusion Matrix\nfrom ISLP import confusion_table\nconfusion_table(y_pred_poly, y_test_sub)\n\n\n\n\n\n\n\n\nTruth\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nPredicted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n186\n0\n2\n0\n0\n4\n5\n0\n1\n1\n\n\n1\n0\n222\n2\n1\n0\n3\n0\n6\n1\n1\n\n\n2\n2\n1\n179\n3\n5\n0\n0\n6\n2\n0\n\n\n3\n1\n0\n7\n191\n0\n2\n1\n1\n8\n3\n\n\n4\n0\n1\n0\n0\n173\n1\n4\n0\n3\n8\n\n\n5\n1\n1\n1\n4\n1\n156\n3\n0\n4\n1\n\n\n6\n0\n4\n2\n1\n4\n3\n187\n0\n4\n0\n\n\n7\n3\n0\n1\n4\n0\n0\n0\n179\n2\n7\n\n\n8\n0\n1\n0\n1\n3\n3\n0\n0\n174\n3\n\n\n9\n0\n0\n1\n1\n6\n1\n0\n4\n0\n192\n\n\n\n\n\n\n\n\n\nClassification Report\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# accuracy\nprint(\"Predictive Accuracy:\", accuracy_score(y_test_sub, y_pred_poly))\n\n# classification report: one-versus-all\nprint(\"\\nClassification Report:\\n\", classification_report(y_test_sub, y_pred_poly))\n\n\nPredictive Accuracy: 0.9195\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.93      0.96      0.95       193\n           1       0.94      0.97      0.95       230\n           2       0.90      0.92      0.91       195\n           3       0.89      0.93      0.91       206\n           4       0.91      0.90      0.91       192\n           5       0.91      0.90      0.90       173\n           6       0.91      0.94      0.92       200\n           7       0.91      0.91      0.91       196\n           8       0.94      0.87      0.91       199\n           9       0.94      0.89      0.91       216\n\n    accuracy                           0.92      2000\n   macro avg       0.92      0.92      0.92      2000\nweighted avg       0.92      0.92      0.92      2000",
    "crumbs": [
      "Home",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/model-basic-implementation.html",
    "href": "contents/model-basic-implementation.html",
    "title": "Model Basics: Implementations",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Basics",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/model-basic-implementation.html#visualising-models",
    "href": "contents/model-basic-implementation.html#visualising-models",
    "title": "Model Basics: Implementations",
    "section": "Visualising models",
    "text": "Visualising models\nFitted models을 이해하기 위해 모델이 예측하는 부분(prediction)과 모델이 놓친 부분(residuals)을 시각화해서 보는 것이 유용함\n\nPredictions: the pattern that the model has captured\n우선, 예측 변수들의 데이터 값을 커버하는 grid를 구성\n\nsim1 = pd.read_csv(\"data/sim1.csv\")\nsim1\n\n     x     y\n0    1  4.20\n1    1  7.51\n2    1  2.13\n..  ..   ...\n27  10 24.97\n28  10 23.35\n29  10 21.98\n\n[30 rows x 2 columns]\n\n\n\n# create a grid for the range of x sim1: new data\ngrid = pd.DataFrame(dict(x=np.linspace(sim1.x.min(), sim1.x.max(), 10)))\n\n모델에 grid를 입력하여 prediction값을 추가\n\n# a model for sim1\nfrom statsmodels.formula.api import ols\nsim1_mod = ols(\"y ~ x\", data=sim1).fit()\n\ngrid[\"pred\"] = sim1_mod.predict(grid) # column 이름이 매치되어야 함\ngrid\n\n       x  pred\n0   1.00  6.27\n1   2.00  8.32\n2   3.00 10.38\n..   ...   ...\n7   8.00 20.63\n8   9.00 22.68\n9  10.00 24.74\n\n[10 rows x 2 columns]\n\n\nprediction을 시각화\n\n\nShow the code\n(\n    so.Plot(sim1, x='x', y='y')\n    .add(so.Dot(color=\".8\"))\n    .add(so.Line(marker=\".\", pointsize=10), x=grid.x, y=grid.pred)  # prediction!\n    .layout(size=(4.5, 3.5))\n)\n\n\n\n\n\n\n\n\n\n\n\nResiduals: what the model has missed.\n\\(e = Y - \\hat{Y}\\) : 관측값 - 예측값\n\nsim1[\"resid\"] = sim1_mod.resid  # Y - Y_hat\n\n\nsim1\n\n     x     y  fitted  resid\n0    1  4.20    6.27  -2.07\n1    1  7.51    6.27   1.24\n2    1  2.13    6.27  -4.15\n..  ..   ...     ...    ...\n27  10 24.97   24.74   0.23\n28  10 23.35   24.74  -1.39\n29  10 21.98   24.74  -2.76\n\n[30 rows x 4 columns]\n\n\n우선, residuals의 분포를 시각화해서 살펴보면,\n\nsns.set_theme(style=\"whitegrid\")\nsim1[\"resid\"].hist(bins=20);\n\n\n\n\n\n\n\n\n예측 변수와 residuals의 관계를 시각화해서 보면,\n\n(\n    so.Plot(sim1, x='x', y='resid')\n    .add(so.Dot())\n    .add(so.Line(), so.PolyFit(5))\n    .layout(size=(5, 4))\n)\n\n\n\n\n\n\n\n\n위의 residuals은 특별한 패턴을 보이지 않아야 모델이 데이터의 패턴을 잘 잡아낸 것으로 판단할 수 있음.\n아래는 원래 데이터와 일차 선형 모형에 대한 예측값의 관계를 시각화한 것\n\n\n\n\n\n\n\n\n\nResiduals에 패턴이 보이는 경우",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Basics",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/model-basic-implementation.html#categorical-variables",
    "href": "contents/model-basic-implementation.html#categorical-variables",
    "title": "Model Basics: Implementations",
    "section": "Categorical variables",
    "text": "Categorical variables\npredictor가 카테고리 변수인 경우 이를 numeric으로 바꾸어야 함\n\nformula y ~ sex의 경우, \\(y=a_0 +a_1sex\\) 로 변환될 수 없음 (성별을 연산할 수 없음)\n실제로, formula는 \\(sex[T.male]\\) 라는 indicator/dummy variable을 새로 만들어 membership을 나타내 줌: dummy-coding 또는 one-hot encoding 이라고 부름.\n\n\\(y=a_0 +a_1sex[T.male]\\)   (남성일 때, \\(sex[T.male]=1\\), 그렇지 않은 경우 0)\n\nMachine learning 알리고즘 중에 non-parametric 모델은 카테고리 변수를 직접 처리할 수 있음\n\nPatsy의 formula는 편리하게 범주형 변수를 알아서 처리해 주기 때문에 범주형 변수의 복잡한 처리과정을 걱정할 필요가 없음.\n직접 변환하려면 pandas의 pd.get_dummies()나 scikit-learn의 OneHotEncoder를 이용\n\n2개의 범주인 경우: 한 개의 변수 sex[T.male]이 생성\n\n\ndf = pd.DataFrame({\"sex\": [\"male\", \"female\", \"male\"], \"response\": [10, 21, 13]})\ndf\n\n      sex  response\n0    male        10\n1  female        21\n2    male        13\n\n\n\n# Design matrix\nfrom patsy import dmatrices\ny, X = dmatrices('response ~ sex', data=df, return_type=\"dataframe\")\nX\n\n   Intercept  sex[T.male]\n0       1.00         1.00\n1       1.00         0.00\n2       1.00         1.00\n\n\n\n세 개의 범주인 경우: 두 개의 변수 sex[T.male], sex[T.neutral]가 생성\n일반적으로 n개의 범주를 가진 변수인 경우 n-1개의 변수가 생성\n\n\ndf = pd.DataFrame({\"sex\": [\"male\", \"female\", \"male\", \"neutral\"], \"response\": [10, 21, 13, 5]})\ndf\n\n       sex  response\n0     male        10\n1   female        21\n2     male        13\n3  neutral         5\n\n\n\ny, X = dmatrices(\"response ~ sex\", data=df, return_type=\"dataframe\")\nX\n\n   Intercept  sex[T.male]  sex[T.neutral]\n0       1.00         1.00            0.00\n1       1.00         0.00            0.00\n2       1.00         1.00            0.00\n3       1.00         0.00            1.00\n\n\n직접 변환: pandas의 get_dummies를 이용\n\npd.get_dummies(df[\"sex\"], prefix=\"sex\")  # drop_first=True\n\n   sex_female  sex_male  sex_neutral\n0       False      True        False\n1        True     False        False\n2       False      True        False\n3       False     False         True\n\n\n\n실제 예를 들어서 살펴보면,\nData: sim2.csv\n\nsim2 = pd.read_csv(\"data/sim2.csv\")\nsim2\n\n    x    y\n0   a 1.94\n1   a 1.18\n2   a 1.24\n.. ..  ...\n37  d 2.13\n38  d 2.49\n39  d 0.30\n\n[40 rows x 2 columns]\n\n\n\nsim2.plot.scatter(x=\"x\", y=\"y\");\n\n\n\n\n\n\n\n\nFit a model to Data\n\nmod2 = ols('y ~ x', data=sim2).fit()\nmod2.params\n\nIntercept   1.15\nx[T.b]      6.96\nx[T.c]      4.98\nx[T.d]      0.76\ndtype: float64\n\n\n모델을 이용해 새로운 데이터의 예측값을 구하면\n\ngrid = pd.DataFrame({\"x\": list(\"abcd\")})  # new data\ngrid[\"pred\"] = mod2.predict(grid)\ngrid\n\n   x  pred\n0  a  1.15\n1  b  8.12\n2  c  6.13\n3  d  1.91\n\n\n예측값을 시각화하면,\n각 카테고리 별로 평균값으로 예측… why?\n\n# plt.scatter scatter plot, 점의 내부가 채워지지 않도록 설정\nplt.scatter(sim2[\"x\"], sim2[\"y\"])\nplt.scatter(grid[\"x\"], grid[\"pred\"], color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\nfitted model의 파라미터를 보면,\nformula y ~ x는 실제 \\(\\hat{y} = a_0 + a_1x[T.b] + a_2 x[T.c] + a_3 x[T.d]\\) 으로 해석\n\ny, X = dmatrices(\"y ~ x\", data=sim2, return_type=\"dataframe\")\npd.concat([X, sim2[\"x\"]], axis=1).sample(8)\n\n    Intercept  x[T.b]  x[T.c]  x[T.d]  x\n7        1.00    0.00    0.00    0.00  a\n18       1.00    1.00    0.00    0.00  b\n15       1.00    1.00    0.00    0.00  b\n..        ...     ...     ...     ... ..\n8        1.00    0.00    0.00    0.00  a\n9        1.00    0.00    0.00    0.00  a\n5        1.00    0.00    0.00    0.00  a\n\n[8 rows x 5 columns]\n\n\nSaratogaHouses 데이터셋에 적용해 보면,\n\nhouses = sm.datasets.get_rdataset(\"SaratogaHouses\", \"mosaicData\").data\n\nsns.boxplot(y=\"price\", x=\"fuel\", data=houses, fill=False);\n\n\n\n\n\n\n\n\n\nmod_houses = ols(\"price ~ fuel\", data=houses)\nmod_houses.fit().params\n\nIntercept     164937.57\nfuel[T.gas]    63597.52\nfuel[T.oil]    23796.83\ndtype: float64\n\n\n\\(\\widehat{price}\\) = $164,937 + $635,979· fuel[T.gas] + $23,796·fuel[T.oil]\n\n절편 $164,937: dummy variables의 값이 모두 0일 때, 즉 electric일 때의 평균 가격\nfuel[T.gas] = 0, fuel[T.oil] = 0, fuel[T.electric] = 1 일 때\n각 기울기는 electric일과 해당 fuel 종류의 평균 가격의 차이를 의미\n\n\ny, X = dmatrices(\"price ~ fuel\", data=houses, return_type=\"dataframe\")\nX\n\n      Intercept  fuel[T.gas]  fuel[T.oil]\n0          1.00         0.00         0.00\n1          1.00         1.00         0.00\n2          1.00         1.00         0.00\n...         ...          ...          ...\n1725       1.00         1.00         0.00\n1726       1.00         1.00         0.00\n1727       1.00         1.00         0.00\n\n[1728 rows x 3 columns]",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Basics",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/model-basic-implementation.html#interactions",
    "href": "contents/model-basic-implementation.html#interactions",
    "title": "Model Basics: Implementations",
    "section": "Interactions",
    "text": "Interactions\n\nTwo continuous\n두 연속변수가 서로 상호작용하는 경우: not additive, but multiplicative\n\n각각의 효과가 더해지는 것을 넘어서서 서로의 효과를 증폭시키거나 감소시키는 경우\n강수량과 풍속이 함께 항공편의 지연을 가중시키는 경우\n운동량과 식사량이 함께 체중 감량에 영향을 미치는 경우\n\n\n\nShow the code\nnp.random.seed(123)\nx1 = np.random.uniform(0, 10, 200)\nx2 = 2*x1 - 1 + np.random.normal(0, 12, 200)\ny = x1 + x2 + x1*x2 + np.random.normal(0, 50, 200)\ndf = pd.DataFrame(dict(precip=x1, wind=x2, delay=y))\ndf\n\n\n     precip   wind  delay\n0      6.96   4.04  95.70\n1      2.86   5.60  31.23\n2      2.27   8.37 -30.97\n..      ...    ...    ...\n197    7.45  16.38 186.67\n198    4.73 -18.56 -96.90\n199    1.22  -5.63 -22.95\n\n[200 rows x 3 columns]\n\n\n\n# additive model\nmod1 = ols('delay ~ precip + wind', data=df).fit()\n\n# interaction model\nmod2 = ols('delay ~ precip + wind + precip:wind', data=df).fit()\n\nmod2: y ~ x1 + x2 + x1:x2는 \\(\\hat{y} = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\) 로 변환되고,\n변형하면, \\(\\hat{y} = a_0 + a_1x_1 + (a_2 + a_3x_1)x_2\\)\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous and Categorical\n연속변수와 범주형 변수가 서로 상호작용하는 경우\n\n운동량이 건강에 미치는 효과: 혼자 vs. 단체\n\nData: sim3.csv\n\nsim3 = pd.read_csv(\"data/sim3.csv\")\nsim3\n\n     x1 x2  rep     y  sd\n0     1  a    1 -0.57   2\n1     1  a    2  1.18   2\n2     1  a    3  2.24   2\n..   .. ..  ...   ...  ..\n117  10  d    1  6.56   2\n118  10  d    2  5.06   2\n119  10  d    3  5.14   2\n\n[120 rows x 5 columns]\n\n\n\n\nCode\n(\n    so.Plot(sim3, x='x1', y='y', color='x2')\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(), so.PolyFit(5), color=None)\n)\n\n\n\n\n\n\n\n\n\n두 가지 모델로 fit할 수 있음\n\nmod1 = ols('y ~ x1 + x2', data=sim3).fit()\nmod2 = ols('y ~ x1 * x2', data=sim3).fit() # 같은 의미 'y ~ x1 + x2 + x1:x2'\n\nformula y ~ x1 * x2는 \\(\\hat{y} = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\)로 변환됨\n\n하지만, 여기서는 x2가 범주형 변수라 dummy-coding후 적용됨.\nDesign matrix를 확인해 보면,\n\ny, X = dmatrices(\"y ~ x1 + x2\", data=sim3, return_type=\"dataframe\")\nX.iloc[:, 1:]\n\n     x2[T.b]  x2[T.c]  x2[T.d]    x1\n0       0.00     0.00     0.00  1.00\n1       0.00     0.00     0.00  1.00\n2       0.00     0.00     0.00  1.00\n..       ...      ...      ...   ...\n117     0.00     0.00     1.00 10.00\n118     0.00     0.00     1.00 10.00\n119     0.00     0.00     1.00 10.00\n\n[120 rows x 4 columns]\n\n\n\ny, X = dmatrices(\"y ~ x1 * x2\", data=sim3, return_type=\"dataframe\")\nX.iloc[:, 1:]\n\n     x2[T.b]  x2[T.c]  x2[T.d]    x1  x1:x2[T.b]  x1:x2[T.c]  x1:x2[T.d]\n0       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n1       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n2       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n..       ...      ...      ...   ...         ...         ...         ...\n117     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n118     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n119     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n\n[120 rows x 7 columns]\n\n\n\ngrid = sim3.value_counts([\"x1\", \"x2\"]).reset_index().drop(columns=\"count\")\ngrid[\"mod1\"] =  mod1.predict(grid)\ngrid[\"mod2\"] =  mod2.predict(grid)\ngrid_long = grid.melt(id_vars=[\"x1\", \"x2\"], var_name=\"model\", value_name=\"pred\")\ngrid_full = grid_long.merge(sim3[[\"x1\", \"x2\", \"y\"]])\n\n\ngrid_full\n\n     x1 x2 model  pred     y\n0     1  a  mod1  1.67 -0.57\n1     1  a  mod1  1.67  1.18\n2     1  a  mod1  1.67  2.24\n..   .. ..   ...   ...   ...\n237  10  d  mod2  3.98  6.56\n238  10  d  mod2  3.98  5.06\n239  10  d  mod2  3.98  5.14\n\n[240 rows x 5 columns]\n\n\n\n\nCode\n(\n    so.Plot(grid_full, x=\"x1\", y=\"y\", color=\"x2\")\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(), y=\"pred\")\n    .facet(\"model\")\n    .layout(size=(8, 5))\n)\n\n\n\n\n\n\n\n\n\n\ninteraction이 없는 모형 mod1의 경우, 네 범주에 대해 기울기가 동일하고 절편의 차이만 존재\ninteraction이 있는 모형 mod2의 경우, 네 범주에 대해 기울기가 다르고 절편도 다름\n\n\n\\(y = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\)에서 \\(x_1x_2\\)항이 기울기를 변할 수 있도록 해줌 \\(y = a_0 + a_2x_2 + (a_1 + a_3x_2)x_1\\)으로 변형하면, \\(x_1\\)의 기울기는 \\(a_1 + a_3 x_2\\)\n\n\n\n\n\n\n\nFitted models\n\n\n\n\n\nmod1 = ols('y ~ x1 + x2', data=sim3).fit()\nmod1.params\n# Intercept    1.87\n# x2[T.b]      2.89\n# x2[T.c]      4.81\n# x2[T.d]      2.36\n# x1          -0.20\n\nmod2 = ols('y ~ x1 * x2', data=sim3).fit() # 같은 의미 'y ~ x1 + x2 + x1:x2'\nmod2.params\n# Intercept     1.30\n# x2[T.b]       7.07\n# x2[T.c]       4.43\n# x2[T.d]       0.83\n# x1           -0.09\n# x1:x2[T.b]   -0.76\n# x1:x2[T.c]    0.07\n# x1:x2[T.d]    0.28\n\n\n\n두 모형을 비교하여 중 더 나은 모형을 선택하기 위해, residuals을 차이를 살펴보면,\n\nsim3[\"mod1\"] = mod1.resid\nsim3[\"mod2\"] = mod2.resid\n\nsim3_long = sim3.melt(\n    id_vars=[\"x1\", \"x2\"],\n    value_vars=[\"mod1\", \"mod2\"],\n    var_name=\"model\",\n    value_name=\"resid\",\n)\nsim3_long\n\n     x1 x2 model  resid\n0     1  a  mod1  -2.25\n1     1  a  mod1  -0.49\n2     1  a  mod1   0.56\n3     1  b  mod1   2.87\n..   .. ..   ...    ...\n236  10  c  mod2  -0.64\n237  10  d  mod2   2.59\n238  10  d  mod2   1.08\n239  10  d  mod2   1.16\n\n[240 rows x 4 columns]\n\n\n\n\nCode\n(\n    so.Plot(sim3_long, x=\"x1\", y=\"resid\", color=\"x2\")\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(linestyle=\":\", color=\".5\"), so.Agg(lambda x: 0))\n    .facet(\"x2\", \"model\")\n    .layout(size=(9, 6))\n    .scale(color=\"Set2\")\n)\n\n\n\n\n\n\n\n\n\n\n둘 중 어떤 모델이 더 나은지에 대한 정확한 통계적 비교가 가능하나 (잔차의 제곱의 평균인 RMSE나 잔차의 절대값의 평균인 MAE 등)\n여기서는 직관적으로 어느 모델이 데이터의 패턴을 더 잘 잡아냈는지를 평가하는 것으로 충분\n잔차를 직접 들여다봄으로써, 어느 부분에서 어떻게 예측이 잘 되었는지, 잘 안 되었는지를 면밀히 검사할 수 있음\ninteraction 항이 있는 모형이 더 나은 모형\n\nSaratogaHouses 데이터에서 가령, livingArea와 centralAir의 interaction을 살펴보면,\n\nhouses = sm.datasets.get_rdataset(\"SaratogaHouses\", \"mosaicData\").data\n(\n    so.Plot(houses, x='livingArea', y='price')\n    .add(so.Dots(color='.6'))\n    .add(so.Line(color=\"orangered\"), so.PolyFit(1))\n    .facet(\"centralAir\")\n    .label(title=\"Central Air: {}\".format)\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\nmod1 = ols('price ~ livingArea + centralAir', data=houses).fit()\nmod2 = ols('price ~ livingArea * centralAir', data=houses).fit()\n\ndisplay(mod1.params, mod2.params)\n\nIntercept           14144.05\ncentralAir[T.Yes]   28450.58\nlivingArea            106.76\ndtype: float64\n\n\nIntercept                       44977.64\ncentralAir[T.Yes]              -53225.75\nlivingArea                         87.72\nlivingArea:centralAir[T.Yes]       44.61\ndtype: float64\n\n\nR-squared 비교\n\ndisplay(mod1.rsquared, mod2.rsquared)\n\n0.5253223149339137\n\n\n0.5430362101820772",
    "crumbs": [
      "Home",
      "Linear Models",
      "Model Basics",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/pandas.html",
    "href": "contents/pandas.html",
    "title": "NumPy and pandas",
    "section": "",
    "text": "파이썬의 모든 것은 객체(object)이며, 특정 클래스(class)의 인스턴스(instance)임\n인스턴스는 클래스에서 정의된 속성(attribute)과 메서드(method)를 전달 받음(inherited)\n\n\nx = \"Love me tender\"  # x: string object\n\nx는 str 클래스(빵의 틀)의 인스턴스(빵)가 되면서, str 클래스의 속성과 함수를 사용할 수 있게 됨\n\nx.upper()  # upper()라는 함수를 호출\n\n'LOVE ME TENDER'\n\n\n이는 다음과 같이 원래 str 클래스의 메서드를 사용하는 것과 동일함\n\nstr.upper(x)\n\n'LOVE ME TENDER'\n\n\n마찬가지로,\n\nx.count('e')  # 함수에 인자(argument)가 포함된 경우\n\n4\n\n\n\nstr.count(x, \"e\")\n\n4\n\n\n\n이렇게, 각 인스턴스가 가지는 함수를 호출해서 적용할 수 있는데\n이 때 그 함수를 메서드(method)라고 함\n이는 각 클레스에서 고유하게 정의된 함수들을 사용할 수 있게 함\n\n이 경우 str이라는 클래스에서 정의된 함수들을 사용할 수 있음\n\n파이썬의 고유한 함수들, 예를 들어\n\ntype(x)\n\nstr\n\n\n\nlen(x)  # 문자열의 길이\n\n14\n\n\npandas 패키지의 한 클래스를 살펴보면,\n\nimport pandas as pd\npd.DataFrame?\n\n\npd.DataFrame\n\npandas.core.frame.DataFrame\n\n\nDataFrame이라는 클래스가 정의되는데, 이는 사실 다음과 같은 폴더 위치에 있는 frame.py 마듈에서 정의된 클래스임\npd.core.frame.DataFrame\n\ndf = pd.DataFrame({'mango': [1, 2, 3], 'apple': [4, 5, 6]})\ndf\n\n   mango  apple\n0      1      4\n1      2      5\n2      3      6\n\n\n\n이 때, df는 DataFrame 클래스의 인스턴스(instance)가 되면서, DataFrame 클래스(class)에서 정의된 속성(attribute)과 함수를 사용할 수 있게 됨.\n\n이 함수를 메서드(method)라고 함\n\n\ndf.columns  # columns라는 속성을 추출\n\nIndex(['mango', 'apple'], dtype='object')\n\n\n\ndf.head(2)  # head()라는 함수를 호출\n\n   mango  apple\n0      1      4\n1      2      5\n\n\n\ndf.columns.sort_values()  # df.columns는 Index object이고, 이에 대한 sort_values()라는 함수를 호출\n\nIndex(['apple', 'mango'], dtype='object')",
    "crumbs": [
      "Home",
      "Python Basics",
      "NumPy & pandas"
    ]
  },
  {
    "objectID": "contents/pandas.html#python-objects",
    "href": "contents/pandas.html#python-objects",
    "title": "NumPy and pandas",
    "section": "",
    "text": "파이썬의 모든 것은 객체(object)이며, 특정 클래스(class)의 인스턴스(instance)임\n인스턴스는 클래스에서 정의된 속성(attribute)과 메서드(method)를 전달 받음(inherited)\n\n\nx = \"Love me tender\"  # x: string object\n\nx는 str 클래스(빵의 틀)의 인스턴스(빵)가 되면서, str 클래스의 속성과 함수를 사용할 수 있게 됨\n\nx.upper()  # upper()라는 함수를 호출\n\n'LOVE ME TENDER'\n\n\n이는 다음과 같이 원래 str 클래스의 메서드를 사용하는 것과 동일함\n\nstr.upper(x)\n\n'LOVE ME TENDER'\n\n\n마찬가지로,\n\nx.count('e')  # 함수에 인자(argument)가 포함된 경우\n\n4\n\n\n\nstr.count(x, \"e\")\n\n4\n\n\n\n이렇게, 각 인스턴스가 가지는 함수를 호출해서 적용할 수 있는데\n이 때 그 함수를 메서드(method)라고 함\n이는 각 클레스에서 고유하게 정의된 함수들을 사용할 수 있게 함\n\n이 경우 str이라는 클래스에서 정의된 함수들을 사용할 수 있음\n\n파이썬의 고유한 함수들, 예를 들어\n\ntype(x)\n\nstr\n\n\n\nlen(x)  # 문자열의 길이\n\n14\n\n\npandas 패키지의 한 클래스를 살펴보면,\n\nimport pandas as pd\npd.DataFrame?\n\n\npd.DataFrame\n\npandas.core.frame.DataFrame\n\n\nDataFrame이라는 클래스가 정의되는데, 이는 사실 다음과 같은 폴더 위치에 있는 frame.py 마듈에서 정의된 클래스임\npd.core.frame.DataFrame\n\ndf = pd.DataFrame({'mango': [1, 2, 3], 'apple': [4, 5, 6]})\ndf\n\n   mango  apple\n0      1      4\n1      2      5\n2      3      6\n\n\n\n이 때, df는 DataFrame 클래스의 인스턴스(instance)가 되면서, DataFrame 클래스(class)에서 정의된 속성(attribute)과 함수를 사용할 수 있게 됨.\n\n이 함수를 메서드(method)라고 함\n\n\ndf.columns  # columns라는 속성을 추출\n\nIndex(['mango', 'apple'], dtype='object')\n\n\n\ndf.head(2)  # head()라는 함수를 호출\n\n   mango  apple\n0      1      4\n1      2      5\n\n\n\ndf.columns.sort_values()  # df.columns는 Index object이고, 이에 대한 sort_values()라는 함수를 호출\n\nIndex(['apple', 'mango'], dtype='object')",
    "crumbs": [
      "Home",
      "Python Basics",
      "NumPy & pandas"
    ]
  },
  {
    "objectID": "contents/pandas.html#numpy",
    "href": "contents/pandas.html#numpy",
    "title": "NumPy and pandas",
    "section": "NumPy",
    "text": "NumPy\n\n수학적 symbolic 연산에 대한 구현이라고 볼 수 있으며,\n행렬(matrix) 또는 벡터(vector)를 ndarray (n-dimensional array)이라는 이름으로 구현함.\n\n사실상 정수(int)나 실수(float)의 한가지 타입으로 이루어짐.\n\n고차원의 arrays 가능\n\n\n\nSource: Medium.com\n가령, 다음과 같은 행렬 연산이 있다면,\n\\(\\begin{bmatrix}1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix}2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix}0 \\\\ 2 \\\\ 4 \\end{bmatrix}\\)\n\nimport numpy as np\n\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6]]) # 3x2 matrix\nX = np.array([[2],\n              [-1]]) # 2x1 matrix\n\nA @ X  # A * X : matrix multiplication\n\narray([[0],\n       [2],\n       [4]])\n\n\n\nA.dot(X)  # A @ X와 동일\n\narray([[0],\n       [2],\n       [4]])\n\n\n\nA + A # element-wise addition\n\narray([[ 2,  4],\n       [ 6,  8],\n       [10, 12]])\n\n\n\n2 * A - 1 # braodcasting\n\narray([[ 1,  3],\n       [ 5,  7],\n       [ 9, 11]])\n\n\n\nnp.exp(A) # element-wise\n\narray([[  2.72,   7.39],\n       [ 20.09,  54.6 ],\n       [148.41, 403.43]])\n\n\n\nPython vs. NumPy\n\na = 2**31 - 1\nprint(a)\nprint(a + 1)\n\n2147483647\n2147483648\n\n\n\na = np.array([2**31 - 1], dtype='int32')\nprint(a)\nprint(a + 1)\n\n[2147483647]\n[-2147483648]\n\n\n\nSource: Ch.4 in Python for Data Analysis (3e) by Wes McKinney",
    "crumbs": [
      "Home",
      "Python Basics",
      "NumPy & pandas"
    ]
  },
  {
    "objectID": "contents/pandas.html#pandas",
    "href": "contents/pandas.html#pandas",
    "title": "NumPy and pandas",
    "section": "pandas",
    "text": "pandas\nSeries & DataFrame\n\nSeries\n1개의 칼럼으로 이루어진 데이터 포멧: 1d numpy array에 labels을 부여한 것으로 볼 수 있음.\nDataFrame의 각 칼럼들을 Series로 이해할 수 있음.\n\nSource: Practical Data Science\n\n\nDataFrame\n각 칼럼들이 한 가지 데이터 타입으로 이루어진 tabular형태 (2차원)의 데이터 포맷\n\n각 칼럼은 기본적으로 한 가지 데이터 타입인 것이 이상적이나, 다른 타입이 섞여 있을 수 있음\nNumPy의 2차원 array의 각 칼럼에 labels을 부여한 것으로 볼 수도 있으나, 여러 다른 기능들이 추가됨\nNumPy의 경우 고차원의 array를 다룰 수 있음: ndarray\n\n고차원의 DataFrame과 비슷한 것은 xarray가 존재\n\nLabels와 index를 제외한 데이터 값은 거의 NumPy ndarray로 볼 수 있음\n(pandas.array 존재)\n\n\nSource: Practical Data Science\nNumPy의 ndarray &lt;-&gt; pandas의 DataFrame 상호 변환\n\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6]]) # 3x2 matrix\n\ndf = pd.DataFrame(A, columns=[\"A1\", \"A2\"])\ndf\n\n   A1  A2\n0   1   2\n1   3   4\n2   5   6\n\n\n\n# 데이터 값들은 NumPy array\ndf.values  # 함수 호출이 아니라 속성(attribute) 접근이므로 ()가 없음\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n한 개의 Column을 추출\n보통 Series로 반환됨\n\n\ns = df[\"A1\"] # A1 칼럼 선택\ns  # DataFrame의 column 이름이 Series의 name으로 전환\n\n# 0    1\n# 1    3\n# 2    5\n# Name: A1, dtype: int64\n\n\n\ndf\n\n#    A1  A2\n# 0   1   2\n# 1   3   4\n# 2   5   6\n\n\n\ntype(s)\n\npandas.core.series.Series\n\n\n\ns.values  # 또는 s.to_numpy(); Series의 값은 NumPy 1d array\n\narray([1, 3, 5])\n\n\n\ndf2 = df[[\"A1\"]]  # list로 들어가면 DataFrame으로 반환\ndf2\n\n   A1\n0   1\n1   3\n2   5\n\n\n\ntype(df2)\n\npandas.core.frame.DataFrame\n\n\n\n\nIndex objects\nframe = pd.DataFrame(np.arange(6).reshape((2, 3)),\n                     index=pd.Index([\"Ohio\", \"Colorado\"], name=\"state\"),\n                     columns=pd.Index([\"one\", \"two\", \"three\"], name=\"number\"))\nframe\n\n\n\nnumber    one  two  three\nstate                    \nOhio        0    1      2\nColorado    3    4      5\n\n\n\n\nframe.index  # 함수 호출이 아니라 속성(attribute) 접근이므로 ()가 없음\n\nIndex(['Ohio', 'Colorado'], dtype='object', name='state')\n\n\n\nframe.columns # columns도 index object\n\nIndex(['one', 'two', 'three'], dtype='object', name='number')\n\n\nIndex는 times series에 특화\n파일 다운로드: Bike Sharing in Washington D.C. Dataset\n\nbike = pd.read_csv('data/day.csv', index_col='dteday', parse_dates=True)\nbike.head(3)\n\n            instant  season  yr  mnth  holiday  weekday  workingday  \\\ndteday                                                                \n2011-01-01        1       1   0     1        0        6           0   \n2011-01-02        2       1   0     1        0        0           0   \n2011-01-03        3       1   0     1        0        1           1   \n\n            weathersit  temp  atemp  hum  windspeed  casual  registered   cnt  \ndteday                                                                         \n2011-01-01           2  0.34   0.36 0.81       0.16     331         654   985  \n2011-01-02           2  0.36   0.35 0.70       0.25     131         670   801  \n2011-01-03           1  0.20   0.19 0.44       0.25     120        1229  1349  \n\n\n\nbike.plot(kind='line', y=['casual', 'registered'], figsize=(8, 4), title='Bike Sharing')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDataFrame의 연산\nNumPy의 ndarray들이 연산되는 방식과 동일하게 series나 DataFrame들의 연산 가능함\n\ndf + 2 * df\n\n   A1  A2\n0   3   6\n1   9  12\n2  15  18\n\n\n\nnp.log(df)\n\n    A1   A2\n0 0.00 0.69\n1 1.10 1.39\n2 1.61 1.79\n\n\n사실 연산은 index를 align해서 시행됨\n\n\n\nnumber    one  two  three\nstate                    \nOhio        0    1      2\nColorado    3    4      5\n\n\nnumber  one  two  three\nstate                  \nOhio      0    2      4\nFloria    6    8     10\n\n\n\nframe1 + frame2\n\n\n\nnumber    one  two  three\nstate                    \nColorado  NaN  NaN    NaN\nFloria    NaN  NaN    NaN\nOhio     0.00 3.00   6.00",
    "crumbs": [
      "Home",
      "Python Basics",
      "NumPy & pandas"
    ]
  },
  {
    "objectID": "contents/pandas.html#missing",
    "href": "contents/pandas.html#missing",
    "title": "NumPy and pandas",
    "section": "Missing",
    "text": "Missing\nNaN, NA, None\n\npandas에서는 missing을 명명하는데 R의 컨벤션을 따라 NA (not available)라 부름.\n\n대부분의 경우에서 NumPy object NaN(np.nan)을 NA을 나타내는데 사용됨.\n\nnp.nan은 실제로 floating-point의 특정 값으로 float64 데이터 타입임. Integer 또는 string type에서 약간 이상하게 작동될 수 있음.\n\nPython object인 None은 pandas에서 NA로 인식함.\n\n현재 NA라는 새로운 pandas object 실험 중임\n\nNA의 handling에 대해서는 교재 참고\n.dropna(), .fillna(), .isna(), .notna()\n\nMckinney’s: 7.1 Handling Missing Data,\nWorking with missing data\n\n\ns = pd.Series([1, 2, np.nan])\ns\n\n0   1.00\n1   2.00\n2    NaN\ndtype: float64\n\n\n\n# type을 변환: float -&gt; int\ns.astype(\"Int64\")\n\n0       1\n1       2\n2    &lt;NA&gt;\ndtype: Int64\n\n\n\ns = pd.Series([\"a\", \"b\", np.nan])\ns\n\n0      a\n1      b\n2    NaN\ndtype: object\n\n\n\n# type을 변환: object -&gt; string\ns.astype(\"string\")\n\n0       a\n1       b\n2    &lt;NA&gt;\ndtype: string\n\n\n\ns = pd.Series([1, 2, np.nan, None, pd.NA])\ns\n\n0       1\n1       2\n2     NaN\n3    None\n4    &lt;NA&gt;\ndtype: object\n\n\nMissing인지를 확인: .isna(), .notna()\n\ns.isna() # or s.isnull()\n\n0    False\n1    False\n2     True\n3     True\n4     True\ndtype: bool\n\n\n\ns.notna() # or s.notnull()\n\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n\n\npandas에서는 ExtensionDtype이라는 새로운 데이터 타입이 도입되었음.\n\ns2 = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\ns2.dtype  # date type 확인\n\nInt8Dtype()\n\n\n\nimport pyarrow as pa\ns2 = pd.Series([2, pd.NA], dtype=pd.ArrowDtype(pa.uint16()))\ns2.dtype\n\nuint16[pyarrow]\n\n\npandas dtypes 참고",
    "crumbs": [
      "Home",
      "Python Basics",
      "NumPy & pandas"
    ]
  },
  {
    "objectID": "contents/shrinkage.html",
    "href": "contents/shrinkage.html",
    "title": "Shrinkage Implementations",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 5, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nfrom ISLP import load_data\nfrom ISLP.models import ModelSpec\n\nimport sklearn.model_selection as skm\nimport sklearn.linear_model as sklm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import root_mean_squared_error, r2_score\nMajor League Baseball Data from the 1986 and 1987 seasons\nHitters = load_data('Hitters') # from ISLP\nHitters = Hitters.dropna()\nHitters\n\n     AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  \\\n1      315    81      7    24   38     39     14    3449    835      69   \n2      479   130     18    66   72     76      3    1624    457      63   \n3      496   141     20    65   78     37     11    5628   1575     225   \n..     ...   ...    ...   ...  ...    ...    ...     ...    ...     ...   \n319    475   126      3    61   43     52      6    1700    433       7   \n320    573   144      9    85   60     78      8    3198    857      97   \n321    631   170      9    77   44     31     11    4908   1457      30   \n\n     CRuns  CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary  \\\n1      321   414     375      N        W      632       43      10   475.0   \n2      224   266     263      A        W      880       82      14   480.0   \n3      828   838     354      N        E      200       11       3   500.0   \n..     ...   ...     ...    ...      ...      ...      ...     ...     ...   \n319    217    93     146      A        W       37      113       7   385.0   \n320    470   420     332      A        E     1314      131      12   960.0   \n321    775   357     249      A        W      408        4       3  1000.0   \n\n    NewLeague  \n1           N  \n2           A  \n3           N  \n..        ...  \n319         A  \n320         A  \n321         A  \n\n[263 rows x 20 columns]",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Regularization",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/shrinkage.html#ridge-regression",
    "href": "contents/shrinkage.html#ridge-regression",
    "title": "Shrinkage Implementations",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nimport sklearn.linear_model as sklm\nfrom sklearn.preprocessing import StandardScaler\n\n# standardize X\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# create an array of alpha values\nlambdas = np.logspace(-2, 10, 100) / y.std()  \n\nridge_coefs = []\nfor alpha in lambdas:\n    ridge = sklm.Ridge(alpha=alpha)\n    ridge.fit(X_scaled, y)\n    ridge_coefs.append(ridge.coef_)\nridge_coefs = np.array(ridge_coefs)\n\n\nridge_coefs = pd.DataFrame(ridge_coefs, columns=X.columns, index=lambdas)\nridge_coefs.index.name = 'lambda'\nridge_coefs.head(3)\n\n               AtBat        Hits      HmRun       Runs        RBI       Walks  \\\nlambda                                                                          \n0.000022 -291.094162  337.829131  37.853033 -60.571234 -26.994245  135.073570   \n0.000029 -291.094036  337.828697  37.852775 -60.570833 -26.994007  135.073465   \n0.000039 -291.093868  337.828124  37.852433 -60.570303 -26.993693  135.073326   \n\n              Years      CAtBat      CHits     CHmRun       CRuns        CRBI  \\\nlambda                                                                          \n0.000022 -16.694157 -391.033663  86.691953 -14.178832  480.740064  260.684702   \n0.000029 -16.694414 -391.032055  86.693349 -14.177901  480.737788  260.683034   \n0.000039 -16.694753 -391.029931  86.695195 -14.176671  480.734779  260.680828   \n\n              CWalks    PutOuts    Assists     Errors   League_N  Division_W  \\\nlambda                                                                         \n0.000022 -213.891103  78.761297  53.732322 -22.160934  31.248776  -58.414130   \n0.000029 -213.890731  78.761297  53.732268 -22.160957  31.248781  -58.414151   \n0.000039 -213.890240  78.761297  53.732197 -22.160987  31.248787  -58.414180   \n\n          NewLeague_N  \nlambda                 \n0.000022   -12.348893  \n0.000029   -12.348919  \n0.000039   -12.348954  \n\n\n\nridge_coefs.plot(figsize=(8, 6))\nplt.xlabel('$\\lambda$')\nplt.ylabel('Standardized coefficients')\nplt.xscale('log')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\\(\\lambda\\)값 중 하나를 살펴보면, 가령 50번째 \\(\\lambda\\)값과 그에 대응하는 회귀계수들은\n\npd.options.display.max_rows = 0\nbeta_hat = ridge_coefs.iloc[50, :]\nprint(f\"lambda: {lambdas[50]}, \\nbeta hats: \\n{beta_hat}\")\n\nlambda: 25.48679637046264, \nbeta hats: \nAtBat         -60.554231\nHits           94.445543\nHmRun         -11.652839\nRuns           29.082098\nRBI            20.558595\nWalks          61.290878\nYears         -32.326762\nCAtBat         11.255280\nCHits          72.074056\nCHmRun         52.153738\nCRuns          76.011242\nCRBI           73.044427\nCWalks        -45.183088\nPutOuts        70.812650\nAssists        18.834114\nErrors        -22.285540\nLeague_N       23.712255\nDivision_W    -59.725489\nNewLeague_N    -5.619252\nName: 25.48679637046264, dtype: float64\n\n\n\nEstimating Test Error of Ridge Regression\n한 예로, \\(\\lambda = 25\\)로 Ridge regression을 수행하면,\n\nfrom sklearn.pipeline import Pipeline\n\nridge = sklm.Ridge(alpha=25)\nscaler = StandardScaler()  # standardize\n\nridge_scaled = Pipeline([('scaler', scaler), ('ridge', ridge)])  \nridge_scaled.fit(X, y)\n\nPipeline(steps=[('scaler', StandardScaler()), ('ridge', Ridge(alpha=25))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()), ('ridge', Ridge(alpha=25))])  StandardScaler?Documentation for StandardScalerStandardScaler()  Ridge?Documentation for RidgeRidge(alpha=25) \n\n\n\n\n\n\n\n\nUsing Pipeline\n\n\n\n파이프라인을 사용해, scikit-learn의 여러 class들을(예. estimator, tranformer) 순서대로 적용되도록 묶을 수 있음\nTuple의 list로 입력: (name, estimator)\nridge_scaled = Pipeline([('scaler', scaler), ('ridge', ridge)])  \nridge_scaled.fit(X, y)\n이름을 지정하지 않고 간략히 처리하는 경우, Pipeline 대신 make_pipeline을 사용할 수 있음\n이 경우, 이름은 자동으로 생성됨\nfrom sklearn.pipeline import make_pipeline\n\nridge_scaled = make_pipeline(scaler, ridge)\nridge_scaled.fit(X, y)\n\n\n각 변수의 파라미터 추정치\n\nridge.coef_\n\narray([-61.83616,  95.40164, -11.71174,  29.01527,  20.50437,  61.68053,\n       -32.74913,  10.86313,  72.57968,  52.3169 ,  76.67548,  73.45853,\n       -46.12262,  70.93771,  19.03332, -22.37589,  23.82647, -59.80705,\n        -5.75064])\n\n\n이제, 최적의 \\(\\lambda\\)를 찾기 위해 cross-validation을 수행하는데,\n기준이 되는 대표적 metric은 MSE(mean squared error) 또는 \\(R^2\\)\n다른 metric에 대해서는 sklearn.metric 참고\n\nimport sklearn.model_selection as skm\n\nridge = sklm.Ridge()\nscaler = StandardScaler()\nridge_scaled = Pipeline([('scaler', scaler), ('ridge', ridge)])  \n\nK = 5\nkfold = skm.KFold(K, shuffle=True, random_state=0)\nparam_grid = {'ridge__alpha': lambdas}  # ridge: the name of the step in the pipeline, alpha: the parameter name\n\ngrid = skm.GridSearchCV(ridge_scaled, \n                        param_grid,\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\ngrid.fit(X, y)\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('ridge', Ridge())]),\n             param_grid={'ridge__alpha': array([       0.00002,        0.00003,        0.00004,        0.00005,\n              0.00007,        0.00009,        0.00012,        0.00016,\n              0.00021,        0.00027,        0.00036,        0.00048,\n              0.00063,        0.00083,        0.0011 ,        0.00146,\n              0.00193,        0.00255,        0.00337,        0.00445,\n              0.00589,        0.00778,        0.010...\n          36126.87535,    47757.60309,    63132.74068,    83457.76772,\n         110326.25731,   145844.81929,   192798.26791,   254867.9637 ,\n         336920.44865,   445389.00483,   588778.05255,   778329.93498,\n        1028906.36814,  1360153.66596,  1798043.09927,  2376907.15964,\n        3142131.38041,  4153712.76566,  5490963.82383,  7258731.02346,\n        9595615.22556, 12684838.61151, 16768610.12221, 22167115.72313])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('ridge', Ridge())]),\n             param_grid={'ridge__alpha': array([       0.00002,        0.00003,        0.00004,        0.00005,\n              0.00007,        0.00009,        0.00012,        0.00016,\n              0.00021,        0.00027,        0.00036,        0.00048,\n              0.00063,        0.00083,        0.0011 ,        0.00146,\n              0.00193,        0.00255,        0.00337,        0.00445,\n              0.00589,        0.00778,        0.010...\n          36126.87535,    47757.60309,    63132.74068,    83457.76772,\n         110326.25731,   145844.81929,   192798.26791,   254867.9637 ,\n         336920.44865,   445389.00483,   588778.05255,   778329.93498,\n        1028906.36814,  1360153.66596,  1798043.09927,  2376907.15964,\n        3142131.38041,  4153712.76566,  5490963.82383,  7258731.02346,\n        9595615.22556, 12684838.61151, 16768610.12221, 22167115.72313])},\n             scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', Ridge(alpha=np.float64(2.732865634209876)))])  StandardScaler?Documentation for StandardScalerStandardScaler()  Ridge?Documentation for RidgeRidge(alpha=np.float64(2.732865634209876)) \n\n\n\ngrid.best_params_\n\n{'ridge__alpha': np.float64(2.732865634209876)}\n\n\n\ngrid.best_estimator_\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', Ridge(alpha=np.float64(2.732865634209876)))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', Ridge(alpha=np.float64(2.732865634209876)))])  StandardScaler?Documentation for StandardScalerStandardScaler()  Ridge?Documentation for RidgeRidge(alpha=np.float64(2.732865634209876)) \n\n\n\ngrid.best_estimator_.fit(X, y).named_steps['ridge'].coef_\n\narray([-230.65706,  247.37135,    5.00202,   -6.18649,    2.11026,\n        111.2861 ,  -49.87498, -118.09574,  123.57104,   56.01087,\n        222.83363,  121.82137, -155.27562,   77.90012,   41.12019,\n        -24.88459,   30.4831 ,  -61.48008,  -13.76634])\n\n\n\nplt.figure(figsize=(8, 6), dpi=70)\nplt.errorbar(\n    lambdas,\n    -grid.cv_results_[\"mean_test_score\"],\n    yerr=grid.cv_results_[\"std_test_score\"] / np.sqrt(K),\n)\nplt.axvline(grid.best_params_[\"ridge__alpha\"], color=\".5\", linestyle=\":\")\nplt.xlabel(\"$\\lambda$\")\nplt.ylabel(\"Cross-validated MSE\")\nplt.xscale(\"log\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR-squared 값으로 비교하면\n\n\n\n\n\nR-squared 값으로 비교하면, (default 값)\ngrid = skm.GridSearchCV(ridge_scaled, \n                        param_grid,\n                        cv=kfold,\n                        scoring='r2') # default\ngrid.fit(X, y)\n위와 마찬가지로 plot을 그리면,\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCross-validation이 통합되어 있는 간편한 함수: RidgeCV, LassoCV, ElasticNetCV\n\n\n\n# suppress warnings for ElasticNet\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nkfold = skm.KFold(5, shuffle=True, random_state=0)\n\n# RidgeCV 함수에 mse_path가 없으므로, 대신 ElasticNetCV를 사용\nridgeCV = sklm.ElasticNetCV(alphas=lambdas, l1_ratio=0, cv=kfold)  # l1_ratio=0: L2 regularization, l1_ratio=1: L1 regularization\npipeCV = Pipeline(steps=[(\"scaler\", scaler), (\"ridge\", ridgeCV)])\npipeCV.fit(X, y)\n\n\nridgeCV.alpha_\n\nnp.float64(0.01360153665961598)\n\n\n\nridgeCV.coef_\n\narray([-213.94939,  229.32917,    1.36088,    0.44074,    5.24782,\n        106.26368,  -51.59113,  -92.22839,  120.087  ,   58.1399 ,\n        197.63691,  114.42331, -144.621  ,   77.54784,   38.9252 ,\n        -25.15106,   30.2249 ,  -61.76237,  -13.56158])\n\n\n\n\n\n\n\n\n\\(\\lambda\\)에 따른 변화를 보려면\n\n\n\ntuned_ridge = pipeCV.named_steps[\"ridge\"]\n\nplt.figure(figsize=(8, 6), dpi=40)\nplt.errorbar(\n    lambdas[::-1],\n    tuned_ridge.mse_path_.mean(axis=1),\n    yerr=tuned_ridge.mse_path_.std(axis=1) / np.sqrt(K),\n)\nplt.axvline(tuned_ridge.alpha_, c=\".5\", ls=\":\")\nplt.xlabel(\"$\\lambda$\")\nplt.ylabel(\"Cross-validated MSE\")\nplt.xscale(\"log\")\nplt.show()\n\n\n\n\n\nEvaluating Test Error of Cross-Validated Ridge\n앞서 적절한 \\(\\lambda\\)를 찾기 위해 cross-validation을 활용하는 방법을 살펴보았음.\n타당한 “test” error를 구하기 위해서는 처음에 training set만을 이용해 \\(\\lambda\\)를 찾은 후,\ntest set을 이용해 error를 구해야 함.\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.5, random_state=123)\n\n# standardize X\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Fit the model on the training set\nkfold = skm.KFold(n_splits=5, shuffle=True, random_state=2)\nridgeCV = sklm.RidgeCV(alphas=lambdas, cv=kfold)\n\nridgeCV.fit(X_train_scaled, y_train)\n\nRidgeCV(alphas=array([275.03283, 256.49654, 239.20953, 223.08761, 208.05225, 194.03023,\n       180.95324, 168.7576 , 157.3839 , 146.77675, 136.88449, 127.65893,\n       119.05515, 111.03123, 103.5481 ,  96.5693 ,  90.06085,  83.99105,\n        78.33034,  73.05113,  68.12773,  63.53615,  59.25403,  55.2605 ,\n        51.53613,  48.06277,  44.8235 ,  41.80255,  38.98519,  36.35772,\n        33.90733,  31.62209,  29.49087,  27.50328,  25.64965,  23.9...\n         4.18025,   3.89852,   3.63577,   3.39073,   3.16221,   2.94909,\n         2.75033,   2.56497,   2.3921 ,   2.23088,   2.08052,   1.9403 ,\n         1.80953,   1.68758,   1.57384,   1.46777,   1.36884,   1.27659,\n         1.19055,   1.11031,   1.03548,   0.96569,   0.90061,   0.83991,\n         0.7833 ,   0.73051,   0.68128,   0.63536,   0.59254,   0.55261,\n         0.51536,   0.48063,   0.44823,   0.41803,   0.38985,   0.36358,\n         0.33907,   0.31622,   0.29491,   0.27503]),\n        cv=KFold(n_splits=5, random_state=2, shuffle=True))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RidgeCV?Documentation for RidgeCViFittedRidgeCV(alphas=array([275.03283, 256.49654, 239.20953, 223.08761, 208.05225, 194.03023,\n       180.95324, 168.7576 , 157.3839 , 146.77675, 136.88449, 127.65893,\n       119.05515, 111.03123, 103.5481 ,  96.5693 ,  90.06085,  83.99105,\n        78.33034,  73.05113,  68.12773,  63.53615,  59.25403,  55.2605 ,\n        51.53613,  48.06277,  44.8235 ,  41.80255,  38.98519,  36.35772,\n        33.90733,  31.62209,  29.49087,  27.50328,  25.64965,  23.9...\n         4.18025,   3.89852,   3.63577,   3.39073,   3.16221,   2.94909,\n         2.75033,   2.56497,   2.3921 ,   2.23088,   2.08052,   1.9403 ,\n         1.80953,   1.68758,   1.57384,   1.46777,   1.36884,   1.27659,\n         1.19055,   1.11031,   1.03548,   0.96569,   0.90061,   0.83991,\n         0.7833 ,   0.73051,   0.68128,   0.63536,   0.59254,   0.55261,\n         0.51536,   0.48063,   0.44823,   0.41803,   0.38985,   0.36358,\n         0.33907,   0.31622,   0.29491,   0.27503]),\n        cv=KFold(n_splits=5, random_state=2, shuffle=True)) \n\n\n\nfrom sklearn.metrics import root_mean_squared_error, r2_score\n\n# Calculate the test RMSE and R-squared using the test set\ndef print_metrics(y, x):\n    print(f\"Test RMSE = {root_mean_squared_error(y, x):.2f}\")\n    print(f\"Test R-squared = {r2_score(y, x):.2f}\")\n\nprint_metrics(y_test, ridgeCV.predict(X_test_scaled))\n\nTest RMSE = 330.19\nTest R-squared = 0.34\n\n\n\n\n\n\n\n\nShuffleSplit을 이용한 trick\n\n\n\n\n\nShuffleSplit으로 1번 교차검증을 하는 방식으로, training set과 test set을 나누는 방법\nouter_valid = skm.ShuffleSplit(n_splits=1, test_size=0.25, random_state=1)\n\ninner_cv = skm.KFold(n_splits=5, shuffle=True, random_state=2)\nridgeCV = sklm.RidgeCV(alphas=lambdas, cv=inner_cv)\npipeCV = Pipeline(steps=[(\"scaler\", scaler), (\"ridge\", ridgeCV)])\n\nresults = skm.cross_validate(\n    pipeCV, X, y, cv=outer_valid, scoring=\"neg_mean_squared_error\"\n)\n\n-results[\"test_score\"]\n# array([132026.01644])\n\n\n\n\n\nHiger order polynomial regression + Ridge\n고차항을 추가한 후 Ridge regression을 수행하면,\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nridge = sklm.Ridge()\npoly = PolynomialFeatures(degree=2)  # 모든 2차항을 포함\npoly_ridge = Pipeline([('poly', poly), ('ridge', ridge)])\n\nK = 5\nkfold = skm.KFold(K, shuffle=True, random_state=0)\nlambdas = np.logspace(-2, 10, 100) / y.std()\nparam_grid = {'ridge__alpha': lambdas}\n\ngrid_poly_ridge = skm.GridSearchCV(poly_ridge, \n                        param_grid,\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\ngrid_poly_ridge.fit(X_train_scaled, y_train);\n\n\nprint_metrics(y_test, grid_poly_ridge.best_estimator_.predict(X_test_scaled))\n\nTest RMSE = 360.57\nTest R-squared = 0.21",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Regularization",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/shrinkage.html#lasso-regression",
    "href": "contents/shrinkage.html#lasso-regression",
    "title": "Shrinkage Implementations",
    "section": "Lasso Regression",
    "text": "Lasso Regression\nRidge에서와 마찬가지로, 우선 training/test set으로 나눈 후,\ntraining set에서 cross-validation을 이용해 최적의 \\(\\lambda\\)를 찾은 후,\ntest set을 이용해 test error를 구해보면,\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.5, random_state=1)\n\n# standardize X\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# create an array of alpha values\nlambdas = np.logspace(2, 6, 100) / y.std()\n\nlasso_coefs = []\nfor alpha in lambdas:\n    lasso = sklm.Lasso(alpha=alpha)\n    lasso.fit(X_train_scaled, y_train)\n    lasso_coefs.append(lasso.coef_)\nlasso_coefs = np.array(lasso_coefs)\n\n\nlasso_coefs = pd.DataFrame(lasso_coefs, columns=X.columns, index=lambdas)\nlasso_coefs.index.name = 'lambda'\nlasso_coefs.sample(5)\n\n                 AtBat        Hits      HmRun  Runs        RBI      Walks  \\\nlambda                                                                      \n58.877805     0.000000   28.063740   0.000000   0.0  15.507350  54.777702   \n3.964920   -282.087576  298.363238 -25.946751  -0.0  43.706836  87.276887   \n344.848531    0.000000    0.000000   0.000000   0.0   0.000000   0.000000   \n85.421614     0.000000   14.620521   0.000000   0.0  22.543629  43.842263   \n14.584482    -0.000000   55.000151  -0.000000   0.0   0.000000  71.600000   \n\n               Years  CAtBat  CHits     CHmRun     CRuns        CRBI   CWalks  \\\nlambda                                                                          \n58.877805   0.000000     0.0    0.0   0.000000  0.000000  171.424457  0.00000   \n3.964920   -1.315728    -0.0    0.0  94.297054  1.854077  136.835250 -5.38028   \n344.848531  0.000000     0.0    0.0   0.000000  0.000000    0.000000  0.00000   \n85.421614   0.000000     0.0    0.0   0.000000  0.000000  150.116450  0.00000   \n14.584482   0.000000     0.0    0.0  25.159039  0.000000  185.973787  0.00000   \n\n               PutOuts    Assists  Errors   League_N  Division_W  NewLeague_N  \nlambda                                                                         \n58.877805    94.315200   0.000000     0.0   0.000000  -13.044781          0.0  \n3.964920    124.505182  13.670514    -0.0  15.677443  -59.922602         -0.0  \n344.848531    0.000000  -0.000000     0.0  -0.000000   -0.000000         -0.0  \n85.421614    79.406766   0.000000     0.0   0.000000   -0.000000          0.0  \n14.584482   116.481406  -0.000000    -0.0   0.940461  -57.024540          0.0  \n\n\n\n\nPlot the lasso coefficients\nlasso_coefs.plot(figsize=(8, 6))\nplt.xlabel('$\\lambda$')\nplt.ylabel('Standardized coefficients')\nplt.xscale('log')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEvaluating Test Error of Cross-Validated Ridge\n5-fold cross-validation을 이용해 최적의 \\(\\lambda\\)를 찾으면,\n\n# Fit the model on the training set\nK = 5\nkfold = skm.KFold(n_splits=K, shuffle=True, random_state=1)\nlassoCV = sklm.LassoCV(cv=kfold)\nlassoCV.fit(X_train_scaled, y_train)\n\nLassoCV(cv=KFold(n_splits=5, random_state=1, shuffle=True))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LassoCV?Documentation for LassoCViFittedLassoCV(cv=KFold(n_splits=5, random_state=1, shuffle=True)) \n\n\n\nlassoCV.alpha_\n\nnp.float64(0.6353615151306538)\n\n\n\nlassoCV.coef_\n\narray([-297.37844,  254.07362,  -32.11397,  -13.73482,   85.00807,\n         89.97288,   17.75389, -969.62686,  857.24631,  154.46851,\n        179.9078 ,    0.     ,   -1.71849,  125.64956,   55.05843,\n        -28.67818,   53.39268,  -57.39778,  -30.45363])\n\n\nTest error with optimal \\(\\lambda\\)\n\n# Calculate the test RMSE and R-squared using the test set\nprint_metrics(y_test, lassoCV.predict(X_test_scaled))\n\nTest RMSE = 328.84\nTest R-squared = 0.37\n\n\n\n\nPlot the cross-validated RMSE\nlambdas = lassoCV.alphas_\nlasso_path = np.sqrt(lassoCV.mse_path_).mean(axis=1)\nlasso_path_se = np.sqrt(lassoCV.mse_path_).std(axis=1) / np.sqrt(K)\n\nplt.figure(figsize=(8, 6), dpi=70)\nplt.errorbar(lambdas, lasso_path, yerr=lasso_path_se)\nplt.axvline(lassoCV.alpha_, color=\".5\", linestyle=\":\")\nplt.xlabel(\"$\\lambda$\")\nplt.ylabel(\"Cross-validated RMSE\")\nplt.xscale(\"log\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nlasso_l20 = sklm.Lasso(alpha=20)\nlasso_l20.fit(X_train_scaled, y_train)\n\ncoefs = lasso_l20.coef_\ncoefs\n\narray([ -0.     ,  52.94574,   0.     ,   0.     ,   0.     ,  69.58717,\n         0.     ,   0.     ,   0.     ,  22.35575,   0.     , 184.34984,\n         0.     , 113.96798,  -0.     ,  -0.     ,   0.     , -51.76979,\n         0.     ])\n\n\n19개의 변수 중 다음 6개의 변수만 선택하여, 소위 variable/feature selection을 수행하게 됨.\n반면, 앞서 최적의 \\(\\lambda=0.75\\)에서는 17개의 변수가 선택되었음.\n\nX.columns[coefs.nonzero()]\n\nIndex(['Hits', 'Walks', 'CHmRun', 'CRBI', 'PutOuts', 'Division_W'], dtype='object')\n\n\nTest error with \\(\\lambda=20\\)\n\nprint_metrics(y_test, lasso_l20.predict(X_test_scaled))\n\nTest RMSE = 323.47\nTest R-squared = 0.39\n\n\n\n\n\n\n\n\nFeature selection using SelectFromModel\n\n\n\nSelectFromModel()를 이용해, Lasso regression을 통해 선택된 변수만을 사용할 수 있음\nfrom sklearn.feature_selection import SelectFromModel\n\nmodel_reduced = SelectFromModel(lasso_l20, prefit=True)\nX_new = model_reduced.transform(X)\nX_new\n# array([[  81,   39,   69,  414,  632,    1],\n#        [ 130,   76,   63,  266,  880,    1],\n#        [ 141,   37,  225,  838,  200,    0],\n#        ...,\n#        [ 126,   52,    7,   93,   37,    1],\n#        [ 144,   78,   97,  420, 1314,    0],\n#        [ 170,   31,   30,  357,  408,    1]])",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Regularization",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/shrinkage.html#comparisons",
    "href": "contents/shrinkage.html#comparisons",
    "title": "Shrinkage Implementations",
    "section": "Comparisons",
    "text": "Comparisons\n모두 함께 비교하면,\nHitters의 예는 19개의 예측변수와 N=263개의 관측값만으로 연봉을 예측하는 문제임.\n그 중 50%의 training set으로 모델을 fitting한 후, 나머지 50%의 test set으로 모델을 평가함.\n만약, 선형회귀모델을 사용한다면,\n\nlr = sklm.LinearRegression()\nlr.fit(X_train_scaled, y_train)\nprint_metrics(y_test, lr.predict(X_test_scaled))\n\nTest RMSE = 341.60\nTest R-squared = 0.32\n\n\n이전 Ridge, Ridge with a higher order polynomial, Lasso, Lasso with variable selection에 대한 결과를 비교해보면,\n\n\nRidge\nTest RMSE = 318.85\nTest R-squared = 0.41\n\n\n\n\n\nRidge with polynomial of degree 2\nTest RMSE = 304.79\nTest R-squared = 0.46\n\n\n\n\n\nLasso\nTest RMSE = 328.84\nTest R-squared = 0.37\n\n\n\n\n\nLasso with 6 predictors only (lambda = 20)\nTest RMSE = 323.47\nTest R-squared = 0.39",
    "crumbs": [
      "Home",
      "Machine Learning Basics",
      "Regularization",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/subsetting.html",
    "href": "contents/subsetting.html",
    "title": "Subsetting",
    "section": "",
    "text": "Load Packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nDataFrame의 일부를 선택하는 subsetting의 방식에 여러 가지 있음\nflights.head(3)\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1    517.00             515       2.00    830.00   \n1  2013      1    1    533.00             529       4.00    850.00   \n2  2013      1    1    542.00             540       2.00    923.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n0             819      11.00      UA    1545  N14228    EWR  IAH    227.00  \n1             830      20.00      UA    1714  N24211    LGA  IAH    227.00  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00",
    "crumbs": [
      "Home",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#bracket",
    "href": "contents/subsetting.html#bracket",
    "title": "Subsetting",
    "section": "Bracket [ ]",
    "text": "Bracket [ ]\nBracket안에 labels이 있는 경우 columns을 select\n\nA single string: Series로 반환\n\nA list of a single string: DataFrame으로 반환\n\nA list of strings\n\n\nflights['dest']  # return as a Series\n\n0         IAH\n1         IAH\n         ... \n336774    CLE\n336775    RDU\nName: dest, Length: 336776, dtype: object\n\n\n\nflights[['dest']]  # return as a DataFrame\n\n       dest\n0       IAH\n1       IAH\n...     ...\n336774  CLE\n336775  RDU\n\n[336776 rows x 1 columns]\n\n\n\nflights[['origin', 'dest']]\n\n       origin dest\n0         EWR  IAH\n1         LGA  IAH\n...       ...  ...\n336774    LGA  CLE\n336775    LGA  RDU\n\n[336776 rows x 2 columns]\n\n\nBracket안에 numbers가 있는 경우 rows를 select: position-based\n\nSlicing만 허용\nFirst index는 포함, last index는 제외\n[1, 5, 8]과 같이 특정 rows를 선택하는 것은 허용안됨\n\n\nflights[2:5]\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n3  2013      1    1    544.00             545      -1.00   1004.00   \n4  2013      1    1    554.00             600      -6.00    812.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00  \n3            1022     -18.00      B6     725  N804JB    JFK  BQN    183.00  \n4             837     -25.00      DL     461  N668DN    LGA  ATL    116.00  \n\n\n 만약, 아래와 같이 index가 number일 때 out of order가 된 경우에도 row position으로 적용됨\n\n\n   origin dest  arr_delay\n42    LGA  DFW      48.00\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n33    EWR  MSP      29.00\n\n\n\ndf_outoforder[2:4]\n\n   origin dest  arr_delay\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n\n\n Chaining with brackets\n\nflights[['origin', 'dest']][2:5]\n# 순서 바꿔어도 동일: flights[2:5][['origin', 'dest']]\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL",
    "crumbs": [
      "Home",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#dot-notation-.",
    "href": "contents/subsetting.html#dot-notation-.",
    "title": "Subsetting",
    "section": "Dot notation .",
    "text": "Dot notation .\n편리하나 주의해서 사용할 필요가 있음\n\n\n\n\n\n\nNote\n\n\n\n\nspace 또는 . 이 있는 변수명 사용 불가\nmethods와 동일한 이름의 변수명 사용 불가: 예) 변수명이 count인 경우 df.count는 df의 method로 인식\n새로운 변수를 만들어 값을 assgin할 수 없음: 예) df.new_var = 1 불가; 대신 df[\"new_var\"] = 1\n만약, 다음과 같이 변수을 지정했을 때 vars_names=[\"origin\", \"dest\"],\n\ndf[vars_names]는 \"orign\"과 \"dest\" columns을 선택\ndf.vars_names는 vars_names이라는 이름의 column을 의미\n\n\n\n\n\nflights.dest  # flihgts[\"dest\"]와 동일\n\n0         IAH\n1         IAH\n         ... \n336774    CLE\n336775    RDU\nName: dest, Length: 336776, dtype: object",
    "crumbs": [
      "Home",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#loc-.iloc",
    "href": "contents/subsetting.html#loc-.iloc",
    "title": "Subsetting",
    "section": ".loc & .iloc",
    "text": ".loc & .iloc\n각각 location, integer location의 약자\ndf.(i)loc[row_indexer, column_indexer]\n\n.loc: label-based indexing\n\nIndex가 number인 경우도 label로 처리\nSlicing의 경우 first, last index 모두 inclusive\n\n\nflights.loc[2:5, ['origin', 'dest']]  # 2:5는 index의 label, not position\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL\n5    EWR  ORD\n\n\n다음과 같이 index가 labels인 경우는 혼동의 염려 없음\n\n\n       origin dest\nred       JFK  MIA\nblue      JFK  BQN\ngreen     LGA  ATL\nyellow    EWR  ORD\n\n\n\ndf_labels.loc[\"blue\":\"green\", :]\n\n      origin dest\nblue     JFK  BQN\ngreen    LGA  ATL\n\n\n하지만, index가 number인 경우는 혼동이 있음\n앞서 본 예에서처럼 index가 out of order인 경우 loc은 다르게 작동\n\n\n   origin dest  arr_delay\n42    LGA  DFW      48.00\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n33    EWR  MSP      29.00\n\n\n\ndf_outoforder.loc[2:14, :]  # position 아님\n\n   origin dest  arr_delay\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n\n\n\ndf_outoforder.loc[[25, 33], :]  # slicing이 아닌 특정 index 선택\n\n   origin dest  arr_delay\n25    EWR  ORD      32.00\n33    EWR  MSP      29.00\n\n\n\nflights.loc[2:5, 'dest']  # returns as a Series\n\n2    MIA\n3    BQN\n4    ATL\n5    ORD\nName: dest, dtype: object\n\n\n\nflights.loc[2:5, ['dest']]  # return as a DataFrame\n\n  dest\n2  MIA\n3  BQN\n4  ATL\n5  ORD\n\n\n\n\n\n\n\n\nTip\n\n\n\n생략 표시\nflights.loc[2:5, :]  # ':' means all\nflights.loc[2:5]\nflights.loc[2:5, ]  # flights.loc[ , ['dest', 'origin']]은 에러\n\n\n\n# select a single row\nflights.loc[2, :]  # returns as a Series, column names as its index\n\nyear         2013\nmonth           1\n            ...  \ndest          MIA\nair_time   160.00\nName: 2, Length: 15, dtype: object\n\n\n\n# select a single row\nflights.loc[[2], :]  # returns as a DataFrame\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00  \n\n\n\n\n\n.iloc: position-based indexing\n\nSlicing의 경우 as usual: first index는 inclusive, last index는 exclusive\n\n\nflights.iloc[2:5, 12:14]  # 2:5는 index의 position, last index는 미포함\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL\n\n\n\nflights.iloc[2:5, 12]  # return as a Series\n\n2    JFK\n3    JFK\n4    LGA\nName: origin, dtype: object\n\n\n\nflights.iloc[2:5, :]\n# 다음 모두 가능\n# flights.iloc[2:5]\n# flights.iloc[2:5, ]\n\n# flights.iloc[, 2:5]는 에러\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n3  2013      1    1    544.00             545      -1.00   1004.00   \n4  2013      1    1    554.00             600      -6.00    812.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00  \n3            1022     -18.00      B6     725  N804JB    JFK  BQN    183.00  \n4             837     -25.00      DL     461  N668DN    LGA  ATL    116.00  \n\n\n\nflights.iloc[2:5, [12]]  # return as a DataFrame\n\n  origin\n2    JFK\n3    JFK\n4    LGA\n\n\n\nflights.iloc[[2, 5, 7], 12:14]  # 특정 위치의 rows 선택\n\n  origin dest\n2    JFK  MIA\n5    EWR  ORD\n7    LGA  IAD\n\n\n\n\n\n\n\n\nNote\n\n\n\n단 하나의 scalar 값을 추출할 때, 빠른 처리를 하는 다음을 사용할 수 있음\n.at[i, j], .iat[i, j]",
    "crumbs": [
      "Home",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#series의-indexing",
    "href": "contents/subsetting.html#series의-indexing",
    "title": "Subsetting",
    "section": "Series의 indexing",
    "text": "Series의 indexing\nDataFrame과 같은 방식으로 이해\nIndex가 numbers인 경우\n\n\n42    DFW\n2     MIA\n25    ORD\n14    DFW\n33    MSP\nName: dest, dtype: object\n\n\n\ns.loc[25:14]\n\n25    ORD\n14    DFW\nName: dest, dtype: object\n\n\n\ns.iloc[2:4]\n\n25    ORD\n14    DFW\nName: dest, dtype: object\n\n\n\ns[:3]\n\n42    DFW\n2     MIA\n25    ORD\nName: dest, dtype: object\n\n\n\n\n\n\n\n\nNote\n\n\n\n다음과 같은 경우 혼동스러움\ns[3] # 3번째? label 3?\n#&gt; errors occur\n\n\n Index가 lables인 경우 다음과 같이 편리하게 subsetting 가능\n\n\nred       MIA\nblue      BQN\ngreen     ATL\nyellow    ORD\nName: dest, dtype: object\n\n\n\ns[\"red\":\"green\"]\n\nred      MIA\nblue     BQN\ngreen    ATL\nName: dest, dtype: object\n\n\n\ns[[\"red\", \"green\"]]\n\nred      MIA\ngreen    ATL\nName: dest, dtype: object",
    "crumbs": [
      "Home",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#boolean-indexing",
    "href": "contents/subsetting.html#boolean-indexing",
    "title": "Subsetting",
    "section": "Boolean indexing",
    "text": "Boolean indexing\n\nBracket [ ] 이나 loc을 이용\niloc은 적용 안됨\n\n\nBracket [ ]\n\nnp.random.seed(123)\nflights_6 = flights[:100][[\"dep_delay\", \"arr_delay\", \"origin\", \"dest\"]].sample(6)\nflights_6\n\n    dep_delay  arr_delay origin dest\n8       -3.00      -8.00    JFK  MCO\n70       9.00      20.00    LGA  ORD\n..        ...        ...    ...  ...\n63      -2.00       2.00    JFK  LAX\n0        2.00      11.00    EWR  IAH\n\n[6 rows x 4 columns]\n\n\n\nflights_6[flights_6[\"dep_delay\"] &lt; 0]\n\n    dep_delay  arr_delay origin dest\n8       -3.00      -8.00    JFK  MCO\n82      -1.00     -26.00    JFK  SFO\n63      -2.00       2.00    JFK  LAX\n\n\n\nidx = flights_6[\"dep_delay\"] &lt; 0\nidx # bool type의 Series\n\n8      True\n70    False\n      ...  \n63     True\n0     False\nName: dep_delay, Length: 6, dtype: bool\n\n\n\n# Select a column with the boolean indexing\nflights_6[idx][\"dest\"]\n\n8     MCO\n82    SFO\n63    LAX\nName: dest, dtype: object\n\n\n\n\n\n\n\n\nNote\n\n\n\n사실, boolean indexing을 할때, DataFrame/Series의 index와 match함\n대부분 염려하지 않아도 되나 다음과 같은 결과 참고\n# Reset index\nidx_reset = idx.reset_index(drop=True)\n# 0     True\n# 1    False\n# 2     True\n# 3    False\n# 4     True\n# 5    False\n# Name: dep_delay, dtype: bool\n\nflights_6[idx_reset][\"dest\"]\n#&gt; IndexingError: Unalignable boolean Series provided as indexer \n#&gt; (index of the boolean Series and of the indexed object do not match)\n\n# Index가 없는 numpy array로 boolean indexing을 하는 경우 문제없음\nflights_6[idx_reset.to_numpy()][\"dest\"]\n# 8     MCO\n# 82    SFO\n# 63    LAX\n# Name: dest, dtype: object\n\n\n\nbool_idx = flights_6[[\"dep_delay\", \"arr_delay\"]] &gt; 0\nbool_idx\n\n    dep_delay  arr_delay\n8       False      False\n70       True       True\n..        ...        ...\n63      False       True\n0        True       True\n\n[6 rows x 2 columns]\n\n\n\nidx_any = bool_idx.any(axis=1)\nidx_any\n\n8     False\n70     True\n      ...  \n63     True\n0      True\nLength: 6, dtype: bool\n\n\n\nbool_idx.all(axis=1)\n\n8     False\n70     True\n      ...  \n63    False\n0      True\nLength: 6, dtype: bool\n\n\n\n\nnp.where() 활용\nnp.where(boolean condition, value if True, value if False)\n\nflights_6[\"delayed\"] = np.where(idx, \"delayed\", \"on-time\")\nflights_6\n\n    dep_delay  arr_delay origin dest  delayed\n8       -3.00      -8.00    JFK  MCO  delayed\n70       9.00      20.00    LGA  ORD  on-time\n..        ...        ...    ...  ...      ...\n63      -2.00       2.00    JFK  LAX  delayed\n0        2.00      11.00    EWR  IAH  on-time\n\n[6 rows x 5 columns]\n\n\n\nnp.where(flights_6[\"dest\"].str.startswith(\"S\"), \"S\", \"T\")  # str method: \"S\"로 시작하는지 여부\n\narray(['T', 'T', 'S', 'S', 'T', 'T'], dtype='&lt;U1')\n\n\n\nflights_6[\"dest_S\"] = np.where(flights_6[\"dest\"].str.startswith(\"S\"), \"S\", \"T\")\nflights_6\n\n    dep_delay  arr_delay origin dest  delayed dest_S\n8       -3.00      -8.00    JFK  MCO  delayed      T\n70       9.00      20.00    LGA  ORD  on-time      T\n..        ...        ...    ...  ...      ...    ...\n63      -2.00       2.00    JFK  LAX  delayed      T\n0        2.00      11.00    EWR  IAH  on-time      T\n\n[6 rows x 6 columns]\n\n\n\n\nloc\n\nflights_6.loc[idx, \"dest\"]  # flights_6[idx][\"dest\"]과 동일\n\n8     MCO\n82    SFO\n63    LAX\nName: dest, dtype: object\n\n\n만약 column 이름에 “time”을 포함하는 columns만 선택하고자 하면\n\nSeries/Index object는 str method 존재\nstr.contains(), str.startswith(), str.endswith()\n자세한 사항은 7.4 String Manipulation/String Functions in pandas by Wes McKinney\n\n\ncols = flights.columns.str.contains(\"time\")  # str method: \"time\"을 포함하는지 여부\ncols\n\narray([False, False, False,  True,  True, False,  True,  True, False,\n       False, False, False, False, False,  True])\n\n\n\n# Columns 쪽으로 boolean indexing\nflights.loc[:, cols]\n\n        dep_time  sched_dep_time  arr_time  sched_arr_time  air_time\n0         517.00             515    830.00             819    227.00\n1         533.00             529    850.00             830    227.00\n...          ...             ...       ...             ...       ...\n336774       NaN            1159       NaN            1344       NaN\n336775       NaN             840       NaN            1020       NaN\n\n[336776 rows x 5 columns]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nChained indexing으로 값을 assign하는 경우 copy vs. view 경고 메세지\nflights[flights[\"arr_delay\"] &lt; 0][\"arr_delay\"] = 0\n/var/folders/mp/vcywncl97ml2q4c_5k2r573m0000gn/T/ipykernel_96692/3780864177.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n 경고가 제시하는데로 .loc을 이용하여 assign\nflights.loc[flights[\"arr_delay\"] &lt; 0, \"arr_delay\"] = 0",
    "crumbs": [
      "Home",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#summary",
    "href": "contents/subsetting.html#summary",
    "title": "Subsetting",
    "section": "Summary",
    "text": "Summary\n\nBracket [ ]의 경우\n\n간단히 columns을 선택하고자 할때 column labels: df[[\"var1\", \"var2\"]]\n간단히 rows를 선택하고자 할때 numerical indexing: df[:10]\n\nDot-notation은\n\npandas의 methods와 중복된 이름을 피하고,\nassignment의 왼편에는 사용을 피할 것\n\n가능하면 분명한 loc 또는 iloc을 사용\n\nloc[:, [\"var1\", \"var2\"]]는 df[[\"var1\", \"var2\"]]과 동일\niloc[:10, :]은 df[:10]와 동일\nloc의 경우, index가 숫자라 할지라도 label로 처리됨\nloc은 iloc과는 다른게 slicing(:)에서 first, last index 모두 inclusive\n\nBoolean indexing의 경우\n\nBracket [ ]: df[bool_idx]\nloc: df.loc[bool_idx, :]\niloc 불가\n\nAssignment를 할때는,\n\nchained indexing을 피하고: df[:5][\"dest\"]\nloc or iloc 사용:\n\ndf.loc[:4, \"dest\"]: index가 0부터 정렬되어 있다고 가정했을 때, slicing에서 위치 하나 차이남\ndf.iloc[:5, 13]: “dest”의 column 위치 13\n\n\n한 개의 column 혹은 row을 선택하면 Series로 반환: df[\"var1\"] 또는 df.loc[2, :]\n\n\n\n\n\n\n\nNote\n\n\n\nNumpy의 indexing에 대해서는 교재 참고\nCh.4/Basic Indexing and Slicing in Python Data Analysis by Wes McKinney",
    "crumbs": [
      "Home",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/vis-intro.html",
    "href": "contents/vis-intro.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Python의 시각화 라이브러리는 다양하게 개발되어지고 있으며, 각기 특성이 달라 하나로만 쓰기 어려운 상황임\n\n\nMatplotlib\n\n가장 오래된 Python과 잘 통합된 널리 사용되는 라이브러리\n거의 가능한 모든 플랏을 그릴 수 있음\n한편, 디테일한 부분을 모두 specify해야 함으로써 많은 코딩이 요구되며, interactive 또는 web graphs에 취약함\n\npandas\n\nMatplotlib로 구현된 DataFrame의 method로 간략하게 시각화가 가능하며, 빠르게 데이터를 들여다볼 수 있음\n\nSeaborn & the seaborn.objects interface\n\nMatplotlib 위에 개발된 간결한 문법의 high-level 언어\nDecalative: 변수들이 어떤 시각화 속성과 위치를 지니는지만 specify\n“Grammer of graphics”라는 시각화 문법에 충실하고자 the seaborn.objects로 새롭게 변화 중\n\n\n\n\nAltair\n\n“Grammer of graphics”를 충실히 따라 설계됨\n각 plot이 이미지가 아닌 data + specification으로 이루어짐: 이미지가 저장되지 않고, 브라우저에서 이미지로 complie되어 생성됨\nWeb-based interactive 시각화인 D3에 그 모체를 두며, Vega/Vega-Lite로부터 파생됨\njavascript-based로 interactive 시각화에 용이하나 Python과 연계가 부족한 부분이 있고, 지원/커뮤니티가 미흡함\n\nBokeh\nPlotly\n다양한 언어(R, Python, Julia)을 지원하며, 기업 수준의 상용화 제품들도 있으며, 지원군 많음\n\nJake VanderPlas의 2017년 발표 자료 중: The Python Visualization Landscape\n\nSource: Jake VanderPlas - The Python Visualization Landscape PyCon 2017",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#대표적-도구들",
    "href": "contents/vis-intro.html#대표적-도구들",
    "title": "Data Visualization",
    "section": "",
    "text": "Matplotlib\n\n가장 오래된 Python과 잘 통합된 널리 사용되는 라이브러리\n거의 가능한 모든 플랏을 그릴 수 있음\n한편, 디테일한 부분을 모두 specify해야 함으로써 많은 코딩이 요구되며, interactive 또는 web graphs에 취약함\n\npandas\n\nMatplotlib로 구현된 DataFrame의 method로 간략하게 시각화가 가능하며, 빠르게 데이터를 들여다볼 수 있음\n\nSeaborn & the seaborn.objects interface\n\nMatplotlib 위에 개발된 간결한 문법의 high-level 언어\nDecalative: 변수들이 어떤 시각화 속성과 위치를 지니는지만 specify\n“Grammer of graphics”라는 시각화 문법에 충실하고자 the seaborn.objects로 새롭게 변화 중\n\n\n\n\nAltair\n\n“Grammer of graphics”를 충실히 따라 설계됨\n각 plot이 이미지가 아닌 data + specification으로 이루어짐: 이미지가 저장되지 않고, 브라우저에서 이미지로 complie되어 생성됨\nWeb-based interactive 시각화인 D3에 그 모체를 두며, Vega/Vega-Lite로부터 파생됨\njavascript-based로 interactive 시각화에 용이하나 Python과 연계가 부족한 부분이 있고, 지원/커뮤니티가 미흡함\n\nBokeh\nPlotly\n다양한 언어(R, Python, Julia)을 지원하며, 기업 수준의 상용화 제품들도 있으며, 지원군 많음\n\nJake VanderPlas의 2017년 발표 자료 중: The Python Visualization Landscape\n\nSource: Jake VanderPlas - The Python Visualization Landscape PyCon 2017",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#the-grammer-of-graphics",
    "href": "contents/vis-intro.html#the-grammer-of-graphics",
    "title": "Data Visualization",
    "section": "The Grammer of Graphics",
    "text": "The Grammer of Graphics\nA coherent system for describing and building graphs\nSource: Fundamentals of Data Visualization by Claus O. Wilke\nAesthetics and types of data\n\n데이터의 값을 특정 aesthetics에 mapping\n\n데이터의 타입은 다음과 같이 나누어짐\n\ncontinuous / discrete\nquatitative / qualitative\ncategorical unordered (nominal) / categorical ordered (ordinal)\n\n성별, 지역 / 등급, 랭킹\nordinal: 등간격을 가정\n\n퀄리티 good, fair, poor는 등간격이라고 봐야하는가?\n랭킹은?\n선호도 1, 2, …, 8; continuous?\n임금 구간?\n\n\n\n데이터 타입에 따라 좀 더 적절한 aesthetic mapping이 있으며,\n같은 정보를 품고 있는 시각화라도 더 적절한 representation이 존재\nBertin’s Semiology of Graphics (1967)\nLevels of organization\n\nSource: Jake VanderPlas’ presentation at PyCon 2018\n\nCase 1\n예를 들어, 다음과 같이 1) 지역별로 2) 날짜에 따른 3) 온도의 변화를 나타낸다면,\n즉, x축의 위치에 날짜 정보를, y축의 위치에 온도 정보를, 색깔에 지역 정보를 할당했음.\n\n한편, 아래는 x축의 위치에 압축된 날짜 정보를, y축의 위치에 지역 정보를, 색깔에 압축된 온도 정보를 할당했음.\n\n\n\nCase 2\n다음은 GDP, mortality, population, region의 네 정보를 다른 방식으로 mapping한 결과임.",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#탐색적-exploratory-vs.-정보전달-communicative",
    "href": "contents/vis-intro.html#탐색적-exploratory-vs.-정보전달-communicative",
    "title": "Data Visualization",
    "section": "탐색적 (Exploratory) vs. 정보전달 (Communicative)",
    "text": "탐색적 (Exploratory) vs. 정보전달 (Communicative)\n\n분석도구: 현미경, 연장도구\n강점이자 약점",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#interative-plots",
    "href": "contents/vis-intro.html#interative-plots",
    "title": "Data Visualization",
    "section": "Interative Plots",
    "text": "Interative Plots\nAltair\n\n\n\n\n\n\n\nPlotly",
    "crumbs": [
      "Home",
      "Exploratory Data Analysis",
      "Visualize"
    ]
  }
]