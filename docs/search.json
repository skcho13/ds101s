[
  {
    "objectID": "contents/wrangling.html",
    "href": "contents/wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Exploratory Data Analysis",
      "Wrangling"
    ]
  },
  {
    "objectID": "contents/wrangling.html#nyc-taxi-and-limousine-commission-record-data",
    "href": "contents/wrangling.html#nyc-taxi-and-limousine-commission-record-data",
    "title": "Wrangling",
    "section": "NYC Taxi and Limousine Commission Record Data",
    "text": "NYC Taxi and Limousine Commission Record Data\n\ntaxi = sns.load_dataset(\"taxis\")\ntaxi\n\n                  pickup             dropoff  passengers  distance  fare  tip  \\\n0    2019-03-23 20:21:09 2019-03-23 20:27:24           1      1.60  7.00 2.15   \n1    2019-03-04 16:11:55 2019-03-04 16:19:00           1      0.79  5.00 0.00   \n2    2019-03-27 17:53:01 2019-03-27 18:00:25           1      1.37  7.50 2.36   \n...                  ...                 ...         ...       ...   ...  ...   \n6430 2019-03-23 22:55:18 2019-03-23 23:14:25           1      4.14 16.00 0.00   \n6431 2019-03-04 10:09:25 2019-03-04 10:14:29           1      1.12  6.00 0.00   \n6432 2019-03-13 19:31:22 2019-03-13 19:48:02           1      3.85 15.00 3.36   \n\n      tolls  total   color      payment            pickup_zone  \\\n0      0.00  12.95  yellow  credit card        Lenox Hill West   \n1      0.00   9.30  yellow         cash  Upper West Side South   \n2      0.00  14.16  yellow  credit card          Alphabet City   \n...     ...    ...     ...          ...                    ...   \n6430   0.00  17.30   green         cash    Crown Heights North   \n6431   0.00   6.80   green  credit card          East New York   \n6432   0.00  20.16   green  credit card            Boerum Hill   \n\n                      dropoff_zone pickup_borough dropoff_borough  \n0              UN/Turtle Bay South      Manhattan       Manhattan  \n1            Upper West Side South      Manhattan       Manhattan  \n2                     West Village      Manhattan       Manhattan  \n...                            ...            ...             ...  \n6430                Bushwick North       Brooklyn        Brooklyn  \n6431  East Flatbush/Remsen Village       Brooklyn        Brooklyn  \n6432               Windsor Terrace       Brooklyn        Brooklyn  \n\n[6433 rows x 14 columns]\n\n\n\ntaxi.describe(include=\"object\")\n\n         color      payment     pickup_zone           dropoff_zone  \\\ncount     6433         6389            6407                   6388   \nunique       2            2             194                    203   \ntop     yellow  credit card  Midtown Center  Upper East Side North   \nfreq      5451         4577             230                    245   \n\n       pickup_borough dropoff_borough  \ncount            6407            6388  \nunique              4               5  \ntop         Manhattan       Manhattan  \nfreq             5268            5206  \n\n\n\n열의 이름 수정\nrename()\n\ntaxi.rename(columns={\"passengers\": \"persons\", \"payment\": \"pay\"}, inplace=True)\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00 0.00   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color          pay            pickup_zone  \\\n0   0.00  12.95  yellow  credit card        Lenox Hill West   \n1   0.00   9.30  yellow         cash  Upper West Side South   \n2   0.00  14.16  yellow  credit card          Alphabet City   \n\n            dropoff_zone pickup_borough dropoff_borough  \n0    UN/Turtle Bay South      Manhattan       Manhattan  \n1  Upper West Side South      Manhattan       Manhattan  \n2           West Village      Manhattan       Manhattan  \n\n\n\n\n값의 대체\n\nidx_cash = (taxi[\"pay\"] == \"cash\")  # Boolean index\n\n# 직접 값을 대입\ntaxi.loc[idx_cash, \"tip\"] = np.nan  # missing(NA) values\n\nnp.where()의 활용\n(boolean condition, value if True, value if False)\n\ntaxi[\"tip\"] = np.where(idx_cash, np.nan, taxi[\"tip\"])\n\nmap()의 활용\ndictionary로 입력\n\ntaxi[\"pay\"] = taxi[\"pay\"].map({\"credit card\": \"Card\", \"cash\": \"Cash\"})\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay            pickup_zone           dropoff_zone  \\\n0   0.00  12.95  yellow  Card        Lenox Hill West    UN/Turtle Bay South   \n1   0.00   9.30  yellow  Cash  Upper West Side South  Upper West Side South   \n2   0.00  14.16  yellow  Card          Alphabet City           West Village   \n\n  pickup_borough dropoff_borough  \n0      Manhattan       Manhattan  \n1      Manhattan       Manhattan  \n2      Manhattan       Manhattan  \n\n\n\n\n새로운 변수 생성\n기본적인 Series들의 연산\n\ntaxi[\"total2\"] = taxi[\"total\"] - taxi[\"tip\"]\n\nassign()의 활용\n\ntaxi = taxi.assign(\n    tip_ratio = lambda x: x[\"tip\"] / x[\"total2\"],\n    tip_ratio_per = lambda x: x.tip_ratio / x.persons  # dot notation\n)\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay            pickup_zone           dropoff_zone  \\\n0   0.00  12.95  yellow  Card        Lenox Hill West    UN/Turtle Bay South   \n1   0.00   9.30  yellow  Cash  Upper West Side South  Upper West Side South   \n2   0.00  14.16  yellow  Card          Alphabet City           West Village   \n\n  pickup_borough dropoff_borough  total2  tip_ratio  tip_ratio_per  \n0      Manhattan       Manhattan   10.80       0.20           0.20  \n1      Manhattan       Manhattan     NaN        NaN            NaN  \n2      Manhattan       Manhattan   11.80       0.20           0.20  \n\n\n\n# total2의 NA값 원래대로 복구\ntaxi[\"total2\"] = np.where(idx_cash, taxi[\"total\"], taxi[\"total2\"])\n\n잠시, 택시요금과 팁 간의 관계를 살펴보면,\n\ntaxi.plot.scatter(x=\"total2\", y=\"tip\", alpha=.2, figsize=(5, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\ntaxi.plot.scatter(x=\"total2\", y=\"tip_ratio\", alpha=.2, figsize=(5, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(taxi, x='total2', y='tip_ratio')\n    .add(so.Dot(alpha=.5, pointsize=2))\n    .add(so.Line(color=\".2\"), so.PolyFit(5))\n    .facet(\"persons\", wrap=4)\n    .layout(size=(8, 5))\n    .label(title=\"{} passengers\".format)\n)\n\n\n\n\n\n\n\n\n\n\n필터링\nquery()의 활용\n\nConditional operators\n&gt;, &gt;=, &lt;, &lt;=,\n== (equal to), != (not equal to)\nand, & (and)\nor, | (or)\nnot, ~ (not)\nin (includes), not in (not included)\n\n\n(\n    taxi.query('color == \"green\" & pickup_borough == \"Manhattan\"')\n    .head(3)\n)\n\n                  pickup             dropoff  persons  distance  fare  tip  \\\n5453 2019-03-29 18:12:27 2019-03-29 18:20:40        1      1.51  7.50 1.20   \n5454 2019-03-06 11:11:33 2019-03-06 11:15:15        1      0.45  4.50  NaN   \n5465 2019-03-27 20:55:35 2019-03-27 21:01:55        1      1.43  7.00 3.00   \n\n      tolls  total  color   pay        pickup_zone           dropoff_zone  \\\n5453   0.00  10.50  green  Card  East Harlem North      East Harlem South   \n5454   0.00   5.30  green  Cash  East Harlem North      East Harlem South   \n5465   0.00  11.30  green  Card     Central Harlem  Upper West Side North   \n\n     pickup_borough dropoff_borough  total2  tip_ratio  tip_ratio_per  \n5453      Manhattan       Manhattan    9.30       0.13           0.13  \n5454      Manhattan       Manhattan    5.30        NaN            NaN  \n5465      Manhattan       Manhattan    8.30       0.36           0.36  \n\n\n\n\n정렬\n\n(\n    taxi.query('persons &gt; 1')\n    .sort_values(\"tip_ratio\", ascending=False)\n)\n\n                  pickup             dropoff  persons  distance  fare  tip  \\\n2923 2019-03-06 23:17:06 2019-03-06 23:24:12        2      1.60  7.50 7.00   \n3272 2019-03-28 11:15:04 2019-03-28 11:21:01        3      0.62  5.50 5.00   \n4912 2019-03-27 18:04:45 2019-03-27 18:11:07        2      0.55  5.50 5.00   \n...                  ...                 ...      ...       ...   ...  ...   \n6405 2019-03-21 18:28:55 2019-03-21 18:51:08        2      3.88 16.50  NaN   \n6412 2019-03-20 17:33:25 2019-03-20 17:42:48        5      1.40  8.00  NaN   \n6420 2019-03-16 15:39:23 2019-03-16 15:46:18        2      1.20  7.00  NaN   \n\n      tolls  total   color   pay                   pickup_zone  \\\n2923   0.00  18.30  yellow  Card  Penn Station/Madison Sq West   \n3272   0.00  13.80  yellow  Card      Financial District North   \n4912   0.00  14.80  yellow  Card                   Murray Hill   \n...     ...    ...     ...   ...                           ...   \n6405   0.00  18.30   green  Cash                   Boerum Hill   \n6412   0.00   9.80   green  Cash              Brooklyn Heights   \n6420   0.00   7.80   green  Cash                Central Harlem   \n\n               dropoff_zone pickup_borough dropoff_borough  total2  tip_ratio  \\\n2923              Hudson Sq      Manhattan       Manhattan   11.30       0.62   \n3272           Battery Park      Manhattan       Manhattan    8.80       0.57   \n4912    UN/Turtle Bay South      Manhattan       Manhattan    9.80       0.51   \n...                     ...            ...             ...     ...        ...   \n6405             Ocean Hill       Brooklyn        Brooklyn   18.30        NaN   \n6412            Fort Greene       Brooklyn        Brooklyn    9.80        NaN   \n6420  Upper West Side North      Manhattan       Manhattan    7.80        NaN   \n\n      tip_ratio_per  \n2923           0.31  \n3272           0.19  \n4912           0.26  \n...             ...  \n6405            NaN  \n6412            NaN  \n6420            NaN  \n\n[1659 rows x 17 columns]\n\n\n\n\nGrouping\n\ngroupby()로 데이터를 의미있는 그룹으로 나눈 후, 다음과 같은 통계치를 계산\nsize(), count(), sum(), mean(), min(), max()\n\n  \n\nSource: Ch.10 in Python for Data Analysis (3e) by Wes McKinney\n\n\ntaxi.groupby(\"color\").size()  # size(): 열의 개수\n\ncolor\ngreen      982\nyellow    5451\ndtype: int64\n\n\n\ntaxi.groupby([\"color\", \"pay\"]).size()  # size(): 열의 개수\n\ncolor   pay \ngreen   Card     577\n        Cash     400\nyellow  Card    4000\n        Cash    1412\ndtype: int64\n\n\n\ntaxi.groupby([\"color\", \"pay\"])[\"total2\"].mean()\n\ncolor   pay \ngreen   Card   18.76\n        Cash   11.40\nyellow  Card   17.08\n        Cash   15.61\nName: total2, dtype: float64\n\n\n\ndf =(\n    taxi.groupby([\"color\", \"pay\"])[\"total2\"].mean()\n    .unstack()  # wide format으로 변환\n)\ndf\n\npay     Card  Cash\ncolor             \ngreen  18.76 11.40\nyellow 17.08 15.61\n\n\n\ndf[\"total\"] = df.sum(axis=1)\ndf\n\npay     Card  Cash  total\ncolor                    \ngreen  18.76 11.40  30.15\nyellow 17.08 15.61  32.68\n\n\n\ndf.assign(\n    Card_pct = lambda x: x.Card / x.total * 100,\n    Cash_pct = lambda x: x.Cash / x.total * 100\n)\n\npay     Card  Cash  total  Card_pct  Cash_pct\ncolor                                        \ngreen  18.76 11.40  30.15     62.21     37.79\nyellow 17.08 15.61  32.68     52.25     47.75\n\n\n여러 함수를 동시에 적용: agg()\n\ntaxi.groupby([\"color\", \"pay\"])[\"total2\"].agg([\"mean\", \"std\", \"size\"])\n\n             mean   std  size\ncolor  pay                   \ngreen  Card 18.76 14.73   577\n       Cash 11.40 10.41   400\nyellow Card 17.08 12.00  4000\n       Cash 15.61 12.38  1412\n\n\n가장 일반적인 방식으로 appy()를 사용하여 사용자 정의 함수를 적용\n\n# standardize\ndef standardize(x):\n    return (x - x.mean()) / x.std()\n\ntaxi.groupby(\"color\")[\"tip_ratio\"].apply(standardize)\n\ncolor       \ngreen   5451     NaN\n        5452   -0.95\n        5453    0.23\n                ... \nyellow  5448    0.33\n        5449    0.33\n        5450   -1.12\nName: tip_ratio, Length: 6433, dtype: float64\n\n\n\n\n시간 데이터의 처리\n시간을 표시하는 datetime64 타입을 이용해 시간 데이터를 처리하며,\ndt accessor를 사용\n\n# pickup 시간으로부터 요일을 추출\ntaxi[\"day\"] = taxi[\"pickup\"].dt.day_name().str[:3]  # 요일의 앞 3글자\n\n\n# pickup 시간으로부터 시간대를 추출\ntaxi[\"hour\"] = taxi[\"pickup\"].dt.hour\n\n\n# 택시를 탄 시간(분)을 계산\ntaxi[\"duration\"] = (taxi[\"dropoff\"] - taxi[\"pickup\"]).dt.total_seconds() / 60\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay            pickup_zone           dropoff_zone  \\\n0   0.00  12.95  yellow  Card        Lenox Hill West    UN/Turtle Bay South   \n1   0.00   9.30  yellow  Cash  Upper West Side South  Upper West Side South   \n2   0.00  14.16  yellow  Card          Alphabet City           West Village   \n\n  pickup_borough dropoff_borough  total2  tip_ratio  tip_ratio_per  day  hour  \\\n0      Manhattan       Manhattan   10.80       0.20           0.20  Sat    20   \n1      Manhattan       Manhattan    9.30        NaN            NaN  Mon    16   \n2      Manhattan       Manhattan   11.80       0.20           0.20  Wed    17   \n\n   duration  \n0      6.25  \n1      7.08  \n2      7.40  \n\n\n요일과 시간대별로 팁의 비율은 다른가?\n\ntaxi.groupby([\"day\", \"hour\"])[\"tip_ratio\"].mean()\n\nday  hour\nFri  0      0.17\n     1      0.19\n     2      0.12\n            ... \nWed  21     0.18\n     22     0.19\n     23     0.23\nName: tip_ratio, Length: 167, dtype: float64\n\n\npivot_table()를 활용할 수도 있음\n\ntaxi.pivot_table(\"tip_ratio\", \"hour\", \"day\")\n\nday   Fri  Mon  Sat  Sun  Thu  Tue  Wed\nhour                                   \n0    0.17 0.19 0.17 0.18 0.17 0.22 0.19\n1    0.19  NaN 0.18 0.18 0.20 0.20 0.18\n2    0.12 0.18 0.18 0.19 0.22 0.20 0.17\n...   ...  ...  ...  ...  ...  ...  ...\n21   0.17 0.18 0.15 0.18 0.18 0.17 0.18\n22   0.17 0.17 0.18 0.18 0.16 0.18 0.19\n23   0.18 0.22 0.17 0.19 0.19 0.18 0.23\n\n[24 rows x 7 columns]\n\n\n\n# day를 categorical 타입으로 변환\ntaxi[\"day\"] = pd.Categorical(taxi[\"day\"], categories=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n\n\ndf = taxi.pivot_table(\"tip_ratio\", \"hour\", \"day\")\n\nfig, axes = plt.subplots(6, 4, figsize=(8, 8), sharex=True, sharey=True)\nfig.subplots_adjust(wspace=0.1, hspace=0.3)\n\nfor i, ax in enumerate(axes.flat):\n    df.iloc[i].plot.bar(ax=ax, ylim=(0.1, 0.23))\n    ax.set_title(f\"hour={i}\", fontsize=9)\n\n\n\n\n\n\n\n\n\ntaxi_day_hour = (\n    taxi.groupby([\"day\", \"hour\"])[\"tip_ratio\"]\n    .agg([\"mean\", \"size\"])\n    .reset_index()\n    .rename(columns={\"mean\": \"tip_ratio\", \"size\": \"n\"})\n)\ntaxi_day_hour\n\n     day  hour  tip_ratio   n\n0    Mon     0       0.19  13\n1    Mon     1        NaN   2\n2    Mon     2       0.18   6\n..   ...   ...        ...  ..\n165  Sun    21       0.18  42\n166  Sun    22       0.18  32\n167  Sun    23       0.19  23\n\n[168 rows x 4 columns]\n\n\n\n(\n    so.Plot(taxi_day_hour, y='day', x='tip_ratio', color='n')\n    .add(so.Bar(width=.5))\n    .facet(\"hour\", wrap=6)\n    .layout(size=(8, 5.5))\n    .limit(x=(0.12, 0.23))\n    .theme({'ytick.labelsize': 9})\n)\n\n\n\n\n\n\n\n\n\n\n데이터 프레임들의 결합\n\nMerge\n\nmerge()를 사용하여 두 데이터 프레임을 join\nKey에 해당하는 변수들의 값이 매치되는 행들을 찾아 결합\n결합 방식: “inner”, “left”, “right”, “outer”\n\n\nInner join의 예:\n\nnyc_neighborhood.csv dataset\n\nnyc = pd.read_csv(\"data/nyc_neighborhood.csv\")\nnyc\n\n         borough  units  labor  carfree  density      pop\n0      Manhattan   5083   0.68     0.88    71.90  1628706\n1          Bronx   6120   0.59     0.71    33.70  1418207\n2       Brooklyn  10129   0.64     0.75    36.90  2559903\n3         Queens   6752   0.64     0.59    20.70  2253858\n4  Staten Island    582   0.62     0.34     8.30   476143\n\n\n\n(\n    taxi.merge(\n        nyc, left_on=\"pickup_borough\", right_on=\"borough\", how=\"left\"\n    ).head(3)\n)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay  ... tip_ratio_per  day hour duration  \\\n0   0.00  12.95  yellow  Card  ...          0.20  Sat   20     6.25   \n1   0.00   9.30  yellow  Cash  ...           NaN  Mon   16     7.08   \n2   0.00  14.16  yellow  Card  ...          0.20  Wed   17     7.40   \n\n     borough   units  labor carfree  density        pop  \n0  Manhattan 5083.00   0.68    0.88    71.90 1628706.00  \n1  Manhattan 5083.00   0.68    0.88    71.90 1628706.00  \n2  Manhattan 5083.00   0.68    0.88    71.90 1628706.00  \n\n[3 rows x 26 columns]\n\n\nMerge를 데이터 필터링에 사용할 수도 있음\n\ntaxi_day_hour\n\n     day  hour  tip_ratio   n\n0    Mon     0       0.19  13\n1    Mon     1        NaN   2\n2    Mon     2       0.18   6\n..   ...   ...        ...  ..\n165  Sun    21       0.18  42\n166  Sun    22       0.18  32\n167  Sun    23       0.19  23\n\n[168 rows x 4 columns]\n\n\n\ndays_top_ratio = (\n    taxi_day_hour\n    .query('n &gt; 10')\n    .nlargest(3, \"tip_ratio\")\n)\ndays_top_ratio\n\n    day  hour  tip_ratio   n\n71  Wed    23       0.23  39\n23  Mon    23       0.22  22\n24  Tue     0       0.22  13\n\n\n팁의 비율이 가장 높았던 3개의 (요일, 시간대) 조합에 해당하는 taxi 데이터셋을 추리려면\n\ntaxi.merge(days_top_ratio, on=[\"day\", \"hour\"])\n\n                pickup             dropoff  persons  distance  fare  tip  \\\n0  2019-03-25 23:05:54 2019-03-25 23:11:13        1      0.80  5.50 2.30   \n1  2019-03-27 23:10:02 2019-03-27 23:22:11        3      2.77 11.00 3.70   \n2  2019-03-20 23:19:55 2019-03-20 23:46:00        1      4.82 19.50 4.66   \n..                 ...                 ...      ...       ...   ...  ...   \n71 2019-03-18 23:48:38 2019-03-18 23:56:36        1      2.00  8.50  NaN   \n72 2019-03-06 23:32:50 2019-03-06 23:34:31        1      0.30  3.50 2.50   \n73 2019-03-26 00:06:16 2019-03-26 00:12:46        1      0.86  6.00  NaN   \n\n    tolls  total   color   pay  ... pickup_borough dropoff_borough total2  \\\n0    0.00  11.60  yellow  Card  ...      Manhattan       Manhattan   9.30   \n1    0.00  18.50  yellow  Card  ...      Manhattan       Manhattan  14.80   \n2    0.00  27.96  yellow  Card  ...      Manhattan       Manhattan  23.30   \n..    ...    ...     ...   ...  ...            ...             ...    ...   \n71   0.00   9.80   green  Cash  ...         Queens          Queens   9.80   \n72   0.00   7.30   green  Card  ...         Queens          Queens   4.80   \n73   0.00   7.30   green  Cash  ...      Manhattan       Manhattan   7.30   \n\n   tip_ratio_x  tip_ratio_per  day  hour duration  tip_ratio_y   n  \n0         0.25           0.25  Mon    23     5.32         0.22  22  \n1         0.25           0.08  Wed    23    12.15         0.23  39  \n2         0.20           0.20  Wed    23    26.08         0.23  39  \n..         ...            ...  ...   ...      ...          ...  ..  \n71         NaN            NaN  Mon    23     7.97         0.22  22  \n72        0.52           0.52  Wed    23     1.68         0.23  39  \n73         NaN            NaN  Tue     0     6.50         0.22  13  \n\n[74 rows x 22 columns]\n\n\n\n\nConcatenate\npd.concat([df1, df2, ...], axis=)\n행과 열의 index를 매치시켜 두 DataFrame/Series를 합침\n\ndf1 = pd.DataFrame(\n    np.arange(6).reshape(3, 2), index=[\"a\", \"b\", \"c\"], columns=[\"one\", \"two\"]\n)\ndf2 = pd.DataFrame(\n    5 + np.arange(4).reshape(2, 2), index=[\"a\", \"c\"], columns=[\"three\", \"four\"]\n)\n\n\n\n\n\n\n\n   one  two\na    0    1\nb    2    3\nc    4    5\n\n\n   three  four\na      5     6\nc      7     8\n\n\n\n\npd.concat([df1, df2], axis=1)\n\n   one  two  three  four\na    0    1   5.00  6.00\nb    2    3    NaN   NaN\nc    4    5   7.00  8.00\n\n\n\npd.concat([df1, df2])  # default: axis=0\n\n   one  two  three  four\na 0.00 1.00    NaN   NaN\nb 2.00 3.00    NaN   NaN\nc 4.00 5.00    NaN   NaN\na  NaN  NaN   5.00  6.00\nc  NaN  NaN   7.00  8.00",
    "crumbs": [
      "Exploratory Data Analysis",
      "Wrangling"
    ]
  },
  {
    "objectID": "contents/tree.html",
    "href": "contents/tree.html",
    "title": "Tree-based Models",
    "section": "",
    "text": "To begin with,"
  },
  {
    "objectID": "contents/statistics.html",
    "href": "contents/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n통계 분석은 크게 세 가지 주제로 나눌 수 있음.\n현대적 접근에서는 모호한 모집단에 대해 추론하기보다는 표본 내의 정보만으로 일반화(generalization)을 성취하고자 함.",
    "crumbs": [
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#simple-regressioncorrelation",
    "href": "contents/statistics.html#simple-regressioncorrelation",
    "title": "Statistics",
    "section": "Simple Regression/Correlation",
    "text": "Simple Regression/Correlation\n예측변수가 한 개인 경우,\n\n두 변수 간의 관계(association)을 파악: \\(Y=f(X)\\)\n\n선형인 경우 기울기를 의미\n\n그 관계의 크기(strength)를 측정\n\n\\(f\\)에 의해 \\(X\\)로 \\(Y\\)를 얼마나 정확히 예측할 수 있는가?\n\\(f\\)에 의해 \\(X\\)의 변량이 \\(Y\\)의 변량을 얼마나 설명할 수 있는가?\n\n\n\n\n\n\n\n\n\nPearson’s correlation coefficient: \\(r\\)\nLinear relationships을 측정\n\nx와 y의 선형적 연관성: [-1, 1]\n\nx로부터 y를 얼마나 정확히 예측가능한가?\nx와 y의 정보는 얼마나 중복(redundant)되는가?\n\n\n\n\n\n\n\n\n\nMultiple correlation coefficient: \\(R\\)\nExtented correlation: 예측치와 관측치의 pearson’s correlation\n\n\\(R\\)을 제곱한 \\(R^2\\)가 설명력의 정도를 나타냄\n\n\n\n\n\n\n\\(R\\): Multiple correlation coefficient\n\n\\(Y\\) 와 \\(\\widehat Y\\) 의 correlation 즉, Y와 회귀모형이 예측한 값의 (선형적) 상관 관계의 정도; 회귀모형의 예측의 정확성\n\n다시말하면, 예측변수들의 최적의 선형 조합과 Y의 상관 관계의 정도.\n\n\\(R^2\\): Coefficient of determination, 결정계수, 설명력\n\n(평면의) 선형모형에 의해 설명된 Y 변량의 비율:\n\n또는 예측변수들의 최적의 선형 조합에 의해 설명된 Y 변량의 비율.\n\n  즉, \\(\\displaystyle\\frac{V(\\widehat{Y})}{V(Y)}\\) 또는 \\(\\displaystyle 1 - \\frac{V(e)}{V(Y)}\\)\n\nAssociatiions과 그 strengths 비교\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n카테고리 변수에 대해서도 비슷하게 생각할 수 있음.\n이 경우, 두 그룹 간의 차이에 대한 효과의 크기를 말할 수 있고, \\(R^2\\) 이외에도 Cohen’s d로 표현할 수 있음.\n예를 들어, 결혼과 삶의 만족도 간의 관계(association)와 그 크기(strength)",
    "crumbs": [
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#multiple-regression",
    "href": "contents/statistics.html#multiple-regression",
    "title": "Statistics",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n예측변수가 2개 이상인 경우: 변수들 간의 진실한 관계를 분석\n미혼자에 대한 임금 차별이 있는가? 차별이 의미하는 바는 무엇인가?\n연령을 고려한 후에도 기혼자의 임금은 미혼자보다 높은가?\n여전히 높다면, 연령을 고려한 후 혹은 연령을 조정한 후(adjusted for age)의 차이는 얼마라고 봐야하는가?\n연령을 고려한 임금 차이를 조사하는 방법은 무엇이 있겠는가?; 연령별로 나누어 비교?\nData from the 1985 Current Population Survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n연령을 고려한 마라톤 기록?\n70세 노인과 20세 청년이 동일하게 2시간 30분의 기록을 세웠다면?\n\n“나이 차이가 큰 두 사람의 기록을 비교하는 것은 공평하지 않아”\n나이를 감안한 마라톤 실력?\n다시 말하면, 나이와는 무관한/독립적인 마라톤 능력에 대해 말하고자 함\n이는 동일한 나이의 사람들로만 제한해서 마라톤 기록을 비교하는 것이 공평한 능력의 비교라고 말하는 것과 같은 이치임\n\n\n\nSource: https://doi.org/10.1186/2052-1847-6-31",
    "crumbs": [
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#regression-analysis",
    "href": "contents/statistics.html#regression-analysis",
    "title": "Statistics",
    "section": "Regression analysis",
    "text": "Regression analysis\n예측 모형 vs. 인과 모형\n\n인과적 연관성을 탐구하고자 한다면 매우 신중한 접근을 요함\n\nSource: Cohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences (3rd ed.)\n교수의 연봉(salary)이 학위를 받은 후 지난 시간(time since Ph.D.)과 출판물의 수(pubs)에 의해 어떻게 영향을 받는가?\n\n\n\n\n\n\n\n\n\n\n\n\n\nData: c0301dt.csv\n\nacad0 = pd.read_csv(\"data/c0301dt.csv\")\nacad0.head(5)\n\n   time  pubs  salary\n0     3    18   51876\n1     6     3   54511\n2     3     2   53425\n3     8    17   61863\n4     9    11   52926\n\n\n\nfrom statsmodels.formula.api import ols\n\nmod1 = ols(\"salary ~ time\", data=acad0).fit()\nmod2 = ols(\"salary ~ pubs\", data=acad0).fit()\nmod3 = ols(\"salary ~ time + pubs\", data=acad0).fit()\n\n\n\n\n\n\n\nIntercept   43658.59\ntime         1224.39\ndtype: float64\n\n\nIntercept   46357.45\npubs          335.53\ndtype: float64\n\n\nIntercept   43082.39\ntime          982.87\npubs          121.80\ndtype: float64\n\n\n\n세 모형을 비교하면,\nModel 1: \\(\\widehat{salary} = \\$1,224\\:time + \\$43,659\\)\nModel 2 : \\(\\widehat{salary} = \\$336\\:pubs + \\$46,357\\)\nModel 3: \\(\\widehat{salary} = \\$983\\:time + \\$122\\:pubs + \\$43,082\\)\n\n연차(time)의 효과는 $1,224에서 $984로 낮아졌고,\n논문수(pubs)의 효과는 $336에서 $122로 낮아졌음.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n교수들의 연차와 그들이 쓴 논문 수는 깊이 연관되어 있으며 (r = 0.66), 두 변수의 redunancy가 각 변수들의 효과를 변화시킴.\n두 예측 변수의 산술적 합으로 연봉을 예측하므로 각 예측변수의 효과는 수정될 수 밖에 없음.\n수학적으로 보면, 각 예측변수의 기울기는 다른 예측변수의 값에 상관없이 일정하므로, 다른 예측변수들을 (임의의 값에) 고정시키는 효과를 가짐\n즉, 다른 변수와는 독립적인, 고유한 효과를 추정하게 됨\n\n각 회귀계수를 partial regression coefficient (부분 회귀 계수) 라고 부름.\n부분 회귀 계수의 첫번째 해석:\n\n만약 논문 수가 일정할 때, 예를 들어 10편의 논문을 쓴 경우만 봤을 때, 연차가 1년 늘 때마다 연봉은 $984 증가함; 평면의 선형모형을 가정했기에 이 관계는 논문 수에 상관없음.\n\n연차가 일정할 때, 예를 들어 연차가 12년차인 경우만 봤을 때, 논문이 1편 늘 때마다 연봉은 $336 증가함; 평면의 선형모형을 가정했기에 이 관계는 연차에 상관없음.\n\n이는 다른 변수를 고려 (통제, controlling for) 했을 때 혹은 다른 변수의 효과를 제거 (partial out) 했을 때, 각 변수의 고유한 효과를 의미함; holding constant, controlling for, partialing out, adjusted for, residualizing\n뒤집어 말하면, 연차만 고려했을때 연차가 1년 늘면 $1,224 연봉이 증가하는 효과는 연차가 늘 때 함께 늘어나는 논문 수의 효과가 함께 섞여 나온 효과라고 말할 수 있음.\n이는 인과관계에 있는 변수들의 진정한 효과를 찾는 것이 얼마나 어려운지를 보여줌\n부분 회귀 계수에 대한 두번째 해석\n\n다른 변수들이 partial out 된 후의 효과.\n\n실제로 $122는 연차로 (선형적으로) 예측/설명되지 않는 논문수(residuals)로 [연차로 예측/설명되지 않는] 연봉을 예측할 때의 기울기\n\n  \nDirect and Indirect Effects\n만약, 다음과 같은 인과모형을 세운다면,\n\n\n연차가 연봉에 미치는 효과가 두 경로로 나뉘어지고,\n연차 \\(\\rightarrow\\) 연봉: 직접효과 $983\n연차 \\(\\rightarrow\\) 논문 \\(\\rightarrow\\) 연봉: 간접효과 1.98 x $122 = $241.56\n두 효과를 더하면: $983 + $241.56 = $1224.56 = 논문수를 고려하지 않았을 때 연차의 효과\n\n즉, 연차가 1년 늘때 연봉이 $1224 증가하는 것은 연차 자체의 효과($983)와 논문의 증가에 따른 효과($241)가 합쳐져 나온 결과라고 말할 수 있음.\n\n이 때, 논문의 수가 연차와 연봉의 관계를 매개(mediate)한다고 표현.\n\n만약, 연차의 효과 $1224이 논문수를 고려했을 때 줄어든($983) 수준을 훨씬 넘어 통계적으로 유의하지 않을 정도로 0에 가까워진다면, 연차의 효과는 모두 논문의 효과를 거쳐 나타나는 것이라고 말할 수 있음. 이 때, 완전 매개 (fully mediate)한다고 표현함.\n\n이들는 인과관계의 기제/메커니즘의 일부를 설명해 줌.\n반대로, 만약 다음과 같이 논문의 효과가 거의 사라진다면, 논문의 효과는 가짜 효과, spurious effect라고 표현함. 이는 논문과 연봉 간의 관계가 보이는 이유는 연차라는 common cause가 연결하고 있기 때문임. 이를 confounding이라고 함.\nSpurious Relationships\n한편, 연차를 고려했을 때, 논문수(pubs)의 효과가 거의 사라진다면,\n논문수(pubs)와 연봉(salary)의 관계는 spurious한 관계라고 잠정적으로 말할 수 있음.\n연차(time)를 논문수와 연봉의 common cause 라고 말하며, confounding이 되어 논문수와 연봉의 인과관계는 실제로 없을 수 있음을 암시함.\n\nStrength of Associations\nCorrelations with salary\n\n\n\n\n\n\n\n\n\n\n\\(r\\) (simple)\n\\(pr\\) (partial)\n\\(sr\\) (semi-partial)\n\n\n\n\ntime\n0.71\n0.53\n0.43\n\n\npubs\n0.59\n0.23\n0.16",
    "crumbs": [
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#uncertainty",
    "href": "contents/statistics.html#uncertainty",
    "title": "Statistics",
    "section": "Uncertainty",
    "text": "Uncertainty\n관찰자가 관찰한 대상으로부터 얻은 결과를 관찰하지 않은 더 넓은 대상으로 일반화할 수 있는가?\n가령, 다음과 같이 150명에 대해 조사한 “연령이 임금에 미치는 효과”를 일반화 할 수 있는가?\n한 나라의 국민 전체?\n\nStatistical inference (통계적 추론)\n통계학의 추론(statistical inference)은 작은 샘플(sample)로부터 얻은 분석 결과를 바탕으로 모집단(population)이라고 부르는 전체에 대해 말하고자 하는 시도에서 비롯되었음\n\n농업 분야에서 시작; 비료/종자의 효과\n사람에게도 적용될 수 있는가?\n\n앞서 논의한 모든 내용은 “특정 샘플” 내에서 변수들 간의 관계에 대한 분석임.\n통계적 추론은 수많은 같은 수의 샘플들, 가령 N = 150인 즉, 150명으로 이루어진 샘플들을 반복적으로 관찰한다면 그 샘플들 간의 편차들이 어떠하겠는가에 대한 논의임.\n\n\n\n\n\n\n\nSource: The Truthful Art by Albert Cairo.\n\n\n\n\n\n\n\n샘플들로부터 나타나는 임금 차이 값의 분포 &gt;&gt; 남녀 임금 차이가 편차는 어떠한가?\n이 분포를 sampling distribution(표본 분포)이라고 부름\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n평균이 $2.27이고, 임금 차이 값들의 95%가 $1.47 ~ $3.04 범위에 있음을 알 수 있음.\n연구자가 관찰한 샘플로부터 연구자는 매우 큰 확신(95%)을 갖고 남녀의 시간당 임금의 차이는 1.47달러에서 3.04달러 사이에 있을 것이라고 말할 수 있음.\n\n\n비슷하게, 나이(age)와 시간당 임금(wage)의 true relationship에 대해서도\n샘플마다 age와 wage의 관계는 다르게 나타날 것임 (두번째 그림).\n\n예를 들어, 샘플들로부터 나타나는 기울기들의 분포를 살펴봄으로써 (세번째 그림): sampling distribution\n이 분포에 따르면 평균이 0.066이고, 기울기 값들의 95%가 0.005 ~ 0.140 범위에 있음을 알 수 있음.\n연구자가 관찰한 샘플로부터 연구자는 (age와 wage의 선형성을 가정한다면), 매우 큰 확신을 갖고 나이가 10세 늘때마다 시간당 임금의 증가율은 0.05에서 1.4달러 사이에 있을 것이라고 말할 수 있음.\n\n\n\n\n\n\n\n\nSource: The Truthful Art by Albert Cairo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis testing\nNull hypothesis(영가설)에 대한 테스트\n즉, coefficient가 0인지 아닌지에 대한 테스트\n\nfrom statsmodels.formula.api import ols\ncps = pd.read_csv('data/cps3.csv')\nmod = ols(\"wage ~ married + sex + age\", data=cps).fit()\nmod.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nwage\nR-squared:\n0.125\n\n\nModel:\nOLS\nAdj. R-squared:\n0.107\n\n\nMethod:\nLeast Squares\nF-statistic:\n6.956\n\n\nDate:\nTue, 26 Mar 2024\nProb (F-statistic):\n0.000208\n\n\nTime:\n23:01:35\nLog-Likelihood:\n-433.48\n\n\nNo. Observations:\n150\nAIC:\n875.0\n\n\nDf Residuals:\n146\nBIC:\n887.0\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4.4946\n1.349\n3.332\n0.001\n1.829\n7.161\n\n\nmarried[T.Single]\n-0.3592\n0.764\n-0.470\n0.639\n-1.870\n1.152\n\n\nsex[T.M]\n2.4337\n0.721\n3.374\n0.001\n1.008\n3.859\n\n\nage\n0.0880\n0.031\n2.834\n0.005\n0.027\n0.149\n\n\n\n\n\n\nOmnibus:\n31.384\nDurbin-Watson:\n2.127\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n45.207\n\n\nSkew:\n1.130\nProb(JB):\n1.53e-10\n\n\nKurtosis:\n4.458\nCond. No.\n153.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nt값에 대한 해석\n예측변수 \\(X_j\\) 에 대해서 모집단의 회귀계수 \\(b_j\\) 에 대한 표본 분포는 평균이 \\(b_j\\) 인 정규분포를 따르고, 표준편차, 즉 standard error는 근사적으로 다음과 같음.\n\\(\\displaystyle SE^2(b_j) = \\frac{{MS}_{residual}}{N \\cdot Var(X_j) \\cdot (1 - R^2_j)}, ~(df = N-k-1)\\)\n\n표본이 클수록\n평균 잔차가 작을수록\njth 예측변수의 값이 퍼져 있을수록\n다른 예측변수들로부터 jth 예측변수가 예측되지 못할수록; 즉 다른 변수들과 correlate되지 않을수록\n\n\n\n\n\n\n\nSource: Wikipedia, Student’s t-distribution\n\n정규 분포의 확률값\n\n\nSource: The Truthful Art by Albert Cairo",
    "crumbs": [
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/pandas.html",
    "href": "contents/pandas.html",
    "title": "NumPy and pandas",
    "section": "",
    "text": "파이썬의 모든 것은 객체(object)이며, 특정 클래스(class)의 인스턴스(instance)임\n인스턴스는 클래스에서 정의된 속성(attribute)과 메서드(method)를 전달 받음(inherited)\n\n\nx = \"Love me tender\"  # x: string object\n\nx는 str 클래스(빵의 틀)의 인스턴스(빵)가 되면서, str 클래스의 속성과 함수를 사용할 수 있게 됨\n\nx.upper()  # upper()라는 함수를 호출\n\n'LOVE ME TENDER'\n\n\n이는 다음과 같이 원래 str 클래스의 메서드를 사용하는 것과 동일함\n\nstr.upper(x)\n\n'LOVE ME TENDER'\n\n\n마찬가지로,\n\nx.count('e')  # 함수에 인자(argument)가 포함된 경우\n\n4\n\n\n\nstr.count(x, \"e\")\n\n4\n\n\n\n이렇게, 각 인스턴스가 가지는 함수를 호출해서 적용할 수 있는데\n이 때 그 함수를 메서드(method)라고 함\n이는 각 클레스에서 고유하게 정의된 함수들을 사용할 수 있게 함\n\n이 경우 str이라는 클래스에서 정의된 함수들을 사용할 수 있음\n\n파이썬의 고유한 함수들, 예를 들어\n\ntype(x)\n\nstr\n\n\n\nlen(x)  # 문자열의 길이\n\n14\n\n\npandas 패키지의 한 클래스를 살펴보면,\n\nimport pandas as pd\npd.DataFrame?\n\n\npd.DataFrame\n\npandas.core.frame.DataFrame\n\n\nDataFrame이라는 클래스가 정의되는데, 이는 사실 다음과 같은 폴더 위치에 있는 frame.py 마듈에서 정의된 클래스임\npd.core.frame.DataFrame\n\ndf = pd.DataFrame({'mango': [1, 2, 3], 'apple': [4, 5, 6]})\ndf\n\n   mango  apple\n0      1      4\n1      2      5\n2      3      6\n\n\n\n이 때, df는 DataFrame 클래스의 인스턴스(instance)가 되면서, DataFrame 클래스(class)에서 정의된 속성(attribute)과 함수를 사용할 수 있게 됨.\n\n이 함수를 메서드(method)라고 함\n\n\ndf.columns  # columns라는 속성을 추출\n\nIndex(['mango', 'apple'], dtype='object')\n\n\n\ndf.head(2)  # head()라는 함수를 호출\n\n   mango  apple\n0      1      4\n1      2      5\n\n\n\ndf.columns.sort_values()  # df.columns는 Index object이고, 이에 대한 sort_values()라는 함수를 호출\n\nIndex(['apple', 'mango'], dtype='object')",
    "crumbs": [
      "Python Basics",
      "NumPy & pandas"
    ]
  },
  {
    "objectID": "contents/pandas.html#python-objects",
    "href": "contents/pandas.html#python-objects",
    "title": "NumPy and pandas",
    "section": "",
    "text": "파이썬의 모든 것은 객체(object)이며, 특정 클래스(class)의 인스턴스(instance)임\n인스턴스는 클래스에서 정의된 속성(attribute)과 메서드(method)를 전달 받음(inherited)\n\n\nx = \"Love me tender\"  # x: string object\n\nx는 str 클래스(빵의 틀)의 인스턴스(빵)가 되면서, str 클래스의 속성과 함수를 사용할 수 있게 됨\n\nx.upper()  # upper()라는 함수를 호출\n\n'LOVE ME TENDER'\n\n\n이는 다음과 같이 원래 str 클래스의 메서드를 사용하는 것과 동일함\n\nstr.upper(x)\n\n'LOVE ME TENDER'\n\n\n마찬가지로,\n\nx.count('e')  # 함수에 인자(argument)가 포함된 경우\n\n4\n\n\n\nstr.count(x, \"e\")\n\n4\n\n\n\n이렇게, 각 인스턴스가 가지는 함수를 호출해서 적용할 수 있는데\n이 때 그 함수를 메서드(method)라고 함\n이는 각 클레스에서 고유하게 정의된 함수들을 사용할 수 있게 함\n\n이 경우 str이라는 클래스에서 정의된 함수들을 사용할 수 있음\n\n파이썬의 고유한 함수들, 예를 들어\n\ntype(x)\n\nstr\n\n\n\nlen(x)  # 문자열의 길이\n\n14\n\n\npandas 패키지의 한 클래스를 살펴보면,\n\nimport pandas as pd\npd.DataFrame?\n\n\npd.DataFrame\n\npandas.core.frame.DataFrame\n\n\nDataFrame이라는 클래스가 정의되는데, 이는 사실 다음과 같은 폴더 위치에 있는 frame.py 마듈에서 정의된 클래스임\npd.core.frame.DataFrame\n\ndf = pd.DataFrame({'mango': [1, 2, 3], 'apple': [4, 5, 6]})\ndf\n\n   mango  apple\n0      1      4\n1      2      5\n2      3      6\n\n\n\n이 때, df는 DataFrame 클래스의 인스턴스(instance)가 되면서, DataFrame 클래스(class)에서 정의된 속성(attribute)과 함수를 사용할 수 있게 됨.\n\n이 함수를 메서드(method)라고 함\n\n\ndf.columns  # columns라는 속성을 추출\n\nIndex(['mango', 'apple'], dtype='object')\n\n\n\ndf.head(2)  # head()라는 함수를 호출\n\n   mango  apple\n0      1      4\n1      2      5\n\n\n\ndf.columns.sort_values()  # df.columns는 Index object이고, 이에 대한 sort_values()라는 함수를 호출\n\nIndex(['apple', 'mango'], dtype='object')",
    "crumbs": [
      "Python Basics",
      "NumPy & pandas"
    ]
  },
  {
    "objectID": "contents/pandas.html#numpy",
    "href": "contents/pandas.html#numpy",
    "title": "NumPy and pandas",
    "section": "NumPy",
    "text": "NumPy\n\n수학적 symbolic 연산에 대한 구현이라고 볼 수 있으며,\n행렬(matrix) 또는 벡터(vector)를 ndarray (n-dimensional array)이라는 이름으로 구현함.\n\n사실상 정수(int)나 실수(float)의 한가지 타입으로 이루어짐.\n\n고차원의 arrays 가능\n\n\n\nSource: Medium.com\n가령, 다음과 같은 행렬 연산이 있다면,\n\\(\\begin{bmatrix}1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix}2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix}0 \\\\ 2 \\\\ 4 \\end{bmatrix}\\)\n\nimport numpy as np\n\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6]]) # 3x2 matrix\nX = np.array([[2],\n              [-1]]) # 2x1 matrix\n\nA @ X  # A * X : matrix multiplication\n\narray([[0],\n       [2],\n       [4]])\n\n\n\nA.dot(X)  # A @ X와 동일\n\narray([[0],\n       [2],\n       [4]])\n\n\n\nA + A # element-wise addition\n\narray([[ 2,  4],\n       [ 6,  8],\n       [10, 12]])\n\n\n\n2 * A - 1 # braodcasting\n\narray([[ 1,  3],\n       [ 5,  7],\n       [ 9, 11]])\n\n\n\nnp.exp(A) # element-wise\n\narray([[  2.72,   7.39],\n       [ 20.09,  54.6 ],\n       [148.41, 403.43]])\n\n\n\nPython vs. NumPy\n\na = 2**31 - 1\nprint(a)\nprint(a + 1)\n\n2147483647\n2147483648\n\n\n\na = np.array([2**31 - 1], dtype='int32')\nprint(a)\nprint(a + 1)\n\n[2147483647]\n[-2147483648]\n\n\n\nSource: Ch.4 in Python for Data Analysis (3e) by Wes McKinney",
    "crumbs": [
      "Python Basics",
      "NumPy & pandas"
    ]
  },
  {
    "objectID": "contents/pandas.html#pandas",
    "href": "contents/pandas.html#pandas",
    "title": "NumPy and pandas",
    "section": "pandas",
    "text": "pandas\nSeries & DataFrame\n\nSeries\n1개의 칼럼으로 이루어진 데이터 포멧: 1d numpy array에 labels을 부여한 것으로 볼 수 있음.\nDataFrame의 각 칼럼들을 Series로 이해할 수 있음.\n\nSource: Practical Data Science\n\n\nDataFrame\n각 칼럼들이 한 가지 데이터 타입으로 이루어진 tabular형태 (2차원)의 데이터 포맷\n\n각 칼럼은 기본적으로 한 가지 데이터 타입인 것이 이상적이나, 다른 타입이 섞여 있을 수 있음\nNumPy의 2차원 array의 각 칼럼에 labels을 부여한 것으로 볼 수도 있으나, 여러 다른 기능들이 추가됨\nNumPy의 경우 고차원의 array를 다룰 수 있음: ndarray\n\n고차원의 DataFrame과 비슷한 것은 xarray가 존재\n\nLabels와 index를 제외한 데이터 값은 거의 NumPy ndarray로 볼 수 있음\n(pandas.array 존재)\n\n\nSource: Practical Data Science\nNumPy의 ndarray &lt;-&gt; pandas의 DataFrame 상호 변환\n\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6]]) # 3x2 matrix\n\ndf = pd.DataFrame(A, columns=[\"A1\", \"A2\"])\ndf\n\n   A1  A2\n0   1   2\n1   3   4\n2   5   6\n\n\n\n# 데이터 값들은 NumPy array\ndf.values  # 함수 호출이 아니라 속성(attribute) 접근이므로 ()가 없음\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n한 개의 Column을 추출\n보통 Series로 반환됨\n\n\ns = df[\"A1\"] # A1 칼럼 선택\ns  # DataFrame의 column 이름이 Series의 name으로 전환\n\n# 0    1\n# 1    3\n# 2    5\n# Name: A1, dtype: int64\n\n\n\ndf\n\n#    A1  A2\n# 0   1   2\n# 1   3   4\n# 2   5   6\n\n\n\ntype(s)\n\npandas.core.series.Series\n\n\n\ns.values  # 또는 s.to_numpy(); Series의 값은 NumPy 1d array\n\narray([1, 3, 5])\n\n\n\ndf2 = df[[\"A1\"]]  # list로 들어가면 DataFrame으로 반환\ndf2\n\n   A1\n0   1\n1   3\n2   5\n\n\n\ntype(df2)\n\npandas.core.frame.DataFrame\n\n\n\n\nIndex objects\nframe = pd.DataFrame(np.arange(6).reshape((2, 3)),\n                     index=pd.Index([\"Ohio\", \"Colorado\"], name=\"state\"),\n                     columns=pd.Index([\"one\", \"two\", \"three\"], name=\"number\"))\nframe\n\n\n\nnumber    one  two  three\nstate                    \nOhio        0    1      2\nColorado    3    4      5\n\n\n\n\nframe.index  # 함수 호출이 아니라 속성(attribute) 접근이므로 ()가 없음\n\nIndex(['Ohio', 'Colorado'], dtype='object', name='state')\n\n\n\nframe.columns # columns도 index object\n\nIndex(['one', 'two', 'three'], dtype='object', name='number')\n\n\nIndex는 times series에 특화\n\nbike = pd.read_csv('data/day.csv', index_col='dteday', parse_dates=True)\nbike.head(3)\n\n            instant  season  yr  mnth  holiday  weekday  workingday  \\\ndteday                                                                \n2011-01-01        1       1   0     1        0        6           0   \n2011-01-02        2       1   0     1        0        0           0   \n2011-01-03        3       1   0     1        0        1           1   \n\n            weathersit  temp  atemp  hum  windspeed  casual  registered   cnt  \ndteday                                                                         \n2011-01-01           2  0.34   0.36 0.81       0.16     331         654   985  \n2011-01-02           2  0.36   0.35 0.70       0.25     131         670   801  \n2011-01-03           1  0.20   0.19 0.44       0.25     120        1229  1349  \n\n\n\nbike.plot(kind='line', y=['casual', 'registered'], figsize=(8, 4), title='Bike Sharing')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDataFrame의 연산\nNumPy의 ndarray들이 연산되는 방식과 동일하게 series나 DataFrame들의 연산 가능함\n\ndf + 2 * df\n\n   A1  A2\n0   3   6\n1   9  12\n2  15  18\n\n\n\nnp.log(df)\n\n    A1   A2\n0 0.00 0.69\n1 1.10 1.39\n2 1.61 1.79\n\n\n사실 연산은 index를 align해서 시행됨\n\n\n\n\n\n\nnumber    one  two  three\nstate                    \nOhio        0    1      2\nColorado    3    4      5\n\n\nnumber  one  two  three\nstate                  \nOhio      0    2      4\nFloria    6    8     10\n\n\n\nframe1 + frame2\n\n\n\nnumber    one  two  three\nstate                    \nColorado  NaN  NaN    NaN\nFloria    NaN  NaN    NaN\nOhio     0.00 3.00   6.00",
    "crumbs": [
      "Python Basics",
      "NumPy & pandas"
    ]
  },
  {
    "objectID": "contents/pandas.html#missing",
    "href": "contents/pandas.html#missing",
    "title": "NumPy and pandas",
    "section": "Missing",
    "text": "Missing\nNaN, NA, None\n\npandas에서는 missing을 명명하는데 R의 컨벤션을 따라 NA (not available)라 부름.\n\n대부분의 경우에서 NumPy object NaN(np.nan)을 NA을 나타내는데 사용됨.\n\nnp.nan은 실제로 floating-point의 특정 값으로 float64 데이터 타입임. Integer 또는 string type에서 약간 이상하게 작동될 수 있음.\n\nPython object인 None은 pandas에서 NA로 인식함.\n\n현재 NA라는 새로운 pandas object 실험 중임\n\nNA의 handling에 대해서는 교재 참고\n.dropna(), .fillna(), .isna(), .notna()\n\nMckinney’s: 7.1 Handling Missing Data,\nWorking with missing data\n\n\ns = pd.Series([1, 2, np.nan])\ns\n\n0   1.00\n1   2.00\n2    NaN\ndtype: float64\n\n\n\n# type을 변환: float -&gt; int\ns.astype(\"Int64\")\n\n0       1\n1       2\n2    &lt;NA&gt;\ndtype: Int64\n\n\n\ns = pd.Series([\"a\", \"b\", np.nan])\ns\n\n0      a\n1      b\n2    NaN\ndtype: object\n\n\n\n# type을 변환: object -&gt; string\ns.astype(\"string\")\n\n0       a\n1       b\n2    &lt;NA&gt;\ndtype: string\n\n\n\ns = pd.Series([1, 2, np.nan, None, pd.NA])\ns\n\n0       1\n1       2\n2     NaN\n3    None\n4    &lt;NA&gt;\ndtype: object\n\n\nMissing인지를 확인: .isna(), .notna()\n\ns.isna() # or s.isnull()\n\n0    False\n1    False\n2     True\n3     True\n4     True\ndtype: bool\n\n\n\ns.notna() # or s.notnull()\n\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n\n\npandas에서는 ExtensionDtype이라는 새로운 데이터 타입이 도입되었음.\n\ns2 = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\ns2.dtype  # date type 확인\n\nInt8Dtype()\n\n\n\nimport pyarrow as pa\ns2 = pd.Series([2, pd.NA], dtype=pd.ArrowDtype(pa.uint16()))\ns2.dtype\n\nuint16[pyarrow]\n\n\npandas dtypes 참고",
    "crumbs": [
      "Python Basics",
      "NumPy & pandas"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html",
    "href": "contents/mlsl-intro.html",
    "title": "Machine/Statistical Learning",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n\\(f\\): \\(X_1, X_2, ..., X_p\\)가 \\(Y\\)에 관해 제공하는 systematic information; unknown function \\(Y = f(X_1, X_2, ..., X_p) + \\epsilon\\),   \\(Y \\perp\\!\\!\\!\\perp \\epsilon\\)\n\\(\\epsilon\\): 불확실성의 소스들; reducible error & irreducible error\n(epistemic uncertainty & aleatory uncertainty)\n앞서 선형모형은 \\(f\\)를 \\(X\\)의 선형함수인 parametric model로 가정한 후 parameter를 추정하여 문제가 단순하였으나,\n이번에는 \\(f\\)의 형태를 가정하지 않고, \\(f\\) 자체를 추정하고자 하는 매우 광범위한 문제임.\n예를 들어,"
  },
  {
    "objectID": "contents/mlsl-intro.html#왜-분석하는가",
    "href": "contents/mlsl-intro.html#왜-분석하는가",
    "title": "Machine/Statistical Learning",
    "section": "왜 분석하는가?",
    "text": "왜 분석하는가?\nThe Occam’s Dilemma?\n\n예측(prediction)\n추론(inference)/해석(interpretation)\n\n정보 획득(information)의 관점에서: Leo Breiman 글 참고"
  },
  {
    "objectID": "contents/mlsl-intro.html#어떻게-f를-추정하는가",
    "href": "contents/mlsl-intro.html#어떻게-f를-추정하는가",
    "title": "Machine/Statistical Learning",
    "section": "어떻게 \\(f\\)를 추정하는가?",
    "text": "어떻게 \\(f\\)를 추정하는가?\n모수적(parametric) 접근\n\n모수적 접근: \\(f\\)의 형태를 가정하고, 그 형태에 대한 parameter를 추정\n가령, 선형성을 가정한 linear model: \\(f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\)\n모델을 관찰된 데이터에 fit(적합) 또는 train(훈련) 시켜, parameter(모수) 추정\n전형적으로 (ordinary) least squares 방법을 사용\n상대적으로 적은 수의 데이터로도 추정이 가능\n해석을 통해 변수 간의 관계 추론 용이\n\\(f\\)의 형태를 잘못 가정하면, 잘못된 결과를 낼 수 있음\n\n\n\n\n\n\n\n\n\n\n\n\nSource: p.21, An Introduction to Statistical Learning with Applications in Python\n\n비모수적(non-parametric) 접근\n\n\\(f\\)의 형태를 가정하지 않음\n보통, 예측의 정확성이 높도록, 즉 데이터와 최대한 가까운 매우 복잡한 형태의 \\(\\hat f\\)를 추구\n과적합(overfit)이 되지 않도록, 새로운 데이터에도 잘 일반화되도록 sweet spot을 찾아야 함\n매우 많은 데이터가 요구됨\n해석은 어려우나, \\(f\\)로부터 다양한 정보(information)을 추출하는 방법을 고안\n모수가 없을 수도, 있을 수 있으나, 해석 가능한 모수라고 보기 어려움\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: p.22, An Introduction to Statistical Learning with Applications in Python\n\nThe tradeoff between flexibility and interpretability\n\n\nSource: p.22, An Introduction to Statistical Learning with Applications in Python\n\n어떤 기준으로 이 tradeoff의 수준을 결정할 것인가?\n\n해석에 중심을 두고, 변수 간의 관계를 추론하고자 하는 경우: 모형이 틀릴 위험이 존재 &gt;&gt; bias가 높아짐\n예측의 정확성에 중심을 두는 경우: 과적합이 될 위험이 존재 &gt;&gt; variance가 높아짐\n이 둘이 항상 대치되는 것은 아님; 단순한 모형이 새로운 데이터에서 더 나은 예측 정확도를 가질 수도 있음!"
  },
  {
    "objectID": "contents/mlsl-intro.html#회귀regression-vs.-분류classification",
    "href": "contents/mlsl-intro.html#회귀regression-vs.-분류classification",
    "title": "Machine/Statistical Learning",
    "section": "회귀(regression) vs. 분류(classification)",
    "text": "회귀(regression) vs. 분류(classification)\n\n보통 \\(Y\\)가 연속형 변수인 경우, 회귀(regression) 문제로 다루고, \\(Y\\)가 범주형 변수인 경우, 분류(classification) 문제로 일컬어 짐.\n회귀모형을 확장하여 확률(연속값)을 구한 후, 이를 이용해 분류문제를 다룰 수 있음; ex. logistic regression\n예측 변수의 경우, 연속인지 범주형인지는 상관없음."
  },
  {
    "objectID": "contents/mlsl-intro.html#decision-theory",
    "href": "contents/mlsl-intro.html#decision-theory",
    "title": "Machine/Statistical Learning",
    "section": "Decision Theory",
    "text": "Decision Theory\n\nSource: Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H\n\n\\(Y = f(X) + \\epsilon\\)\n앞서 선형 회귀 모형에서 구한 것은, conditional probability \\(p(Y|X)\\)를 모델링 한 것이었음.\n이 분포를 Gaussian:\\(N(\\mu, \\sigma^2)\\)으로 가정하고, 분포의 평균과 표준편차를 likelihood가 최대가 되도록 추정하는 간단한 모델로 축소했음; predictive distribution\n\n\n분포 \\(p(Y|X)\\)를 통해 예측의 불확실성을 파악할 수 있음\n만약, 주어진 \\(X\\)에서 대해 예측값 \\(f(X)\\)을 하나 결정하는데, 실제 true 값을 t라고 하면,\n그 오차에 대해 발생되는 어떤 penalty 또는 cost를 정의; loss function\nloss function을 통해 “최적”의 예측값에 대한 기준을 제공\n\\(f(x) = E(Y|X=x)\\)로 예측하는 것이 많은 경우 적절하지만 일반적으로 그런 것은 아님\n\nLoss/Cost Function\nLoss function: \\(L(t, f(x))\\)\n\\(t\\): true value, \\(f\\): predicted value\n\n이 loss를 최소화하는 것이 목표\n실제 t를 모르기 때문에, 평균치인 expected loss를 최소화하는 것이 목표\n즉, \\(E(L) = \\displaystyle \\iint L(t, f(x))p(x, t) dt dy\\)를 최소화하도록 예측값(\\(f\\))을 결정\n\n각 \\(x\\)에 대해 \\(\\displaystyle \\int L(t, f(x))p(t|x)dt\\)를 최소화\n주어진 \\(x\\)값에 대한 \\(t\\)가 의미하는 바를 생각해볼 것!\n\n예를 들어, 다이아몬드의 무게로 가격을 예측한다면: 1 carat 다이아몬드의 true price?\n또는, 목소리(특정 frequency)로 성별을 예측한다면; 분류 문제의 경우 뒤에서 다룸\n\n\n\n\n\n\n\n\n\n\n\n Source: Language Log\n\n\n\n앞서 회귀 모형에서는 기본적으로 squared loss 사용;\n\n\\(L_2 = \\displaystyle(f(x) - t)^2\\)\n\n\\(E(L)\\)을 최소로 하는 함수: \\(f(x_0) = E(t|X=x_0)\\): regression function \n\n\\(L_1 = \\displaystyle |~f(x) - t~|\\)\n\n\\(E(L)\\)을 최소로 하는 함수: \\(f(x_0) = median(t|X=x_0)\\) \n\n\\(L_q = \\displaystyle |~f(x) - t~|^q\\)\n\n\n\nSource: Pattern Recognition and Machine Learning by Christopher M. Bishop\n\n예를 들어,\n\n\n\\(f(1.5) = E(Y|X=1.5)\\)인 \\(f\\)가 \\(E(L_2)\\)를 최소화하는 optimal한 함수임.\n즉, conditional mean \\(f(x) = E(Y |X = x)\\)는 \\(L_2\\) loss의 관점에서 최선의 함수이며, regression function이라고 부름.\n\n실제로는 \\(E(Y | X=x)\\)를 계산하는 것은 불가능하며, 이를 추정하기 위해 x의 근방에서 평균값을 취함.\n\n\\(\\hat f(x) = Ave(Y |X \\in N(x))\\)\n\n\n\n단, Predictor의 갯수와 관측치의 수에 따라 점차 효율성이 떨어짐; the curse of dimensionality\n\n예를 들어, \\(f(1.5) = E(Y|carat=1.5,~ cut=Fair,~ clarity=I1)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\\(f\\)의 smoothing에 대한 여러 접근이 있음\n\nregualization, spline, kernel 등\n\nThe Palmer Archipelago penguins 데이터셋의 예로 보면,\n\n\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n앞서 선형회귀모형을 사용한다면,\n\n관계를 선형이라고 전제하고,\nError가 Gaussian 분포를 따른다고 가정하고,\nLikelihood가 최대가 되도록 파라미터(기울기와 절편)를 구한 것임; maximum likelihood estimation\nsquared error를 최소화하는 것과 동일함.\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 600x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Visualizing a multivariate Gaussian\n\n\n\n\n\n\n\nExpected value\n\n\n\n\n\n\\(X: \\{a, b, a, a, b, c\\}\\)\nThe mean of \\(X=\\displaystyle \\frac{a + b + a + a + b + c}{6} = \\frac{3*a}{6} + \\frac{2*b}{6} + \\frac{1*c}{6}=a*\\frac{3}{6} + b*\\frac{2}{6} + c*\\frac{1}{6}\\)\n\\(~~~~~~~~~~~~~~~~~~~~~~~~~=a*p(X=a) + b*p(X=b) + c*(X=c)\\): weighted average\nExpectation: \\(E(X)=\\displaystyle \\sum x_i p(X=x_i)= \\sum xp(x)\\)\n연속값이라면, \\(E(X)=\\displaystyle \\sum \\Delta x * p(\\Delta x) = \\int x f(x) dx\\), where \\(f(x)\\) is the probability density function\n\\(p(\\Delta x) = f(x)\\Delta x\\)\n\n\n\n\nThe Bias–Variance Trade-off\n위의 논의는 특정 데이터셋에 의존할 수밖에 없는데, (참고: Bayesian의 경우 다른 접근)\n데이터셋마다 다른 \\(f\\)를 얻게 된다는 점을 감안했을 때, \\(f\\)의 변동성을 살펴보면,\n\\(L_2\\) loss의 경우,\n\\(h(x) := E(t|X=x)\\): optimal prediction of Y at any point x\n\\(E(L_2) = \\displaystyle \\iint \\left( f(x) - t \\right)^2 p(x, t) dxdt = \\iint \\left( f(x) - h(x) + h(x) - t \\right)^2 p(x, t) dxdt\\) \\(~~~~~~~~~~~ = \\displaystyle \\int \\left( f(x) - h(x) \\right)^2 p(x) dx + \\iint \\left( h(x) - t) \\right)^2p(x, t) dxdt\\)\n\n두 번째 항: optimal prediction \\(h(x)\\)가 true value와 얼마나 떨어져 있는가?(분산) - irreducible error\n첫 번째 항: \\(h(x)\\)와의 차이를 최소화하도록 \\(f(x)\\)를 선택하는데 하는데, 유한한 데이터셋에서는 그 간극이 존재\n\n이제, 분포 \\(p(t, x)\\)로부터 수많은 데이터셋들 \\(D_i\\)를 얻었다고 가정했을 때,\n특정 \\(x_0\\)에 대해서\n\n\n\\(E_D[\\left( f(x_0; D_i) - h(x_0)\\right)^2]=[E_D\\left( f(x_0; D_i)\\right) - h(x_0)]^2 + E_D[\\left\\{f(x_0;D_i) - E_D\\left(f(x_0;D_i)\\right)\\right\\}^2]\\)\n\\(~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ = bias^2 + variance\\)\nBias: 최적의 예측값(\\(h\\))에 비해, (여러 데이터로부터 얻은) “평균적인 예측값”이 얼마나 차이가 나는가?\n\n즉, 최적의 예측보다 평균적으로 얼마나 틀린 예측을 하는가?\nthe error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.\n\nVariance: 각 데이터로부터 얻은 예측값이 (여러 데이터부터 얻은) “평균적인 예측값”에서 얼마나 떨어져 있는가?(분산)\n\n즉, 데이터셋에 따라 \\(f(x_0;D_i)\\)가 얼마나 민감하게 변하는지 측정\n\n모든 \\(X\\)에 대한 평균값으로 이해하면,\nExpected loss \\(\\displaystyle E(L_2) = \\left( \\int (bias)^2 + \\int variance \\right) + noise\\)\nBias와 variance간에는 trade-off가 존재함\n\nFlexible한 모델의 경우 데이터에 더 잘 적합하여, variance가 높아짐. 즉 데이터마다 너무 다른 예측을 하게 됨\n\n데이터에 overfitting이 된다고 말할 수 있음.\n한편, 평균값이 실제값에 가까워져 bias가 줄어듦\n단, N이 증가하면(데이터 사이즈가 커지면), flexible한 모델의 variance가 줄어듦.\n\nRigid한 모델의 경우 데이터에 덜 적합하지만, bias가 높아짐. 즉, 평균적으로 더 틀린 예측을 하게 됨\n\n한편, 데이터에 덜 민감하여 variance가 줄어듦\n작은 데이터셋에서 더 유리\n\n이 둘의 적절한 균형을 갖는 모델이 최적의 예측 모델임\n\n예를 들어, 함수 \\(h(x) = sin(2\\pi x)\\)로부터 생성된 데이터셋(N=10)에 대해 다항식의 차수에 따른 flexibility의 변화에 따른 OLS 모델들을 비교하면,\n\n데이터셋의 사이즈가 커지면, (M=9인 경우)\n\n함수 \\(h(x) = sin(2\\pi x)\\)로부터 생성된 100개의 데이터셋(각 N = 25)에 대해 3가지 flexibility에 대한 모델들을 비교하면,\n\n\nSource: p.10, 12, 127, Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H"
  },
  {
    "objectID": "contents/mlsl-intro.html#classification",
    "href": "contents/mlsl-intro.html#classification",
    "title": "Machine/Statistical Learning",
    "section": "Classification",
    "text": "Classification\n\\(Y\\)가 범주형 변수인 경우, 즉 분류(classification) 문제인 경우\n\nY가 k개의 범주/클래스로 나뉘는 경우: \\(Y \\in \\{C_1, C_2, ..., C_k\\}\\)\n\n두 범주의 경우, 간단히 \\(Y \\in \\{0, 1\\}\\)\n\n\\(f(x_i): P(Y = C_k|X=x_i)\\)가 최대인 클래스 에 할당; \\(\\underset{k}{\\mathrm{argmax}}~ P(Y = C_k|X=x)\\)\n\nConditional class probabilities\nBayes classifier: 최대치\n\n이 \\(P(Y = C_k|X=x)\\)를 추정하기 위한 다양한 방식들이 존재\n\nK-nearest neighbors\nLogistic regression\nGeneralized additive models\nLinear/Quadratic discriminant analysis\nSupport vector machines\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n한 개의 예측변수로 예측한다면,\n가령 \\(X\\): bill_length_mm인 경우, 즉 펭귄의 부리 길이로만 두 종을 분류한다면,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuadratic discriminant analysis(QDA): 각각을 Gaussian으로 가정하고, 평균과 표준편차를 추정\n\n\n\n\n\n\n\n\n\n\nConfusion matrix; misclassifed된 관측치\n\n\nTruth      Adelie  Gentoo\nPredicted                \nAdelie        144       7\nGentoo          7     116\n\n\nLoss function; loss matrix\n\n\nTruth      Adelie  Gentoo\nPredicted                \nAdelie          0      10\nGentoo         30       0\n\n\nExpected loss:\n\\(E(L) = \\displaystyle \\sum_{i=1} \\sum_{j=1} L_{ij}P(y\\in C_i, \\hat y\\in C_j) = \\frac{1}{274}(144*0 + 7*10 + 30*30 + 116*0)\\)\n보통의 경우, 0-1 loss를 사용. 즉, misclassified된 관측치에 대한 비율; misclassification error rate\n\\(E(L) = \\displaystyle \\frac{1}{N} \\sum_{i=1}^N I(y_i \\neq \\hat y_i)\\) where \\(I\\) is the indicator function: 0 if \\(y_i = \\hat y_i\\), 1 otherwise\n잘못된 예측에 대한 비용이 다르다면, 이에 대응하는 expected loss를 기준으로 모델을 평가\n\n만약, 농작물에 대한 피해라고 가정하면,\n\nCosts: 농작물 피해, 펜스 설치비, 노동력 등\nBenefits: 수확물의 가치\n거짓 음성을 낮춰야 하는 경우: 예를 들어, 농작물의 작은 피해도 심각한 결과를 초래하는 경우\n거짓 양성을 낮춰야 하는 경우: 예를 들어, 농작물의 피해 예방을 위한 비용이 큰 경우\n\n만약, 와인 셀러가 와인의 품질(high:양성 vs. low:음성)을 성분들로 예측하는 모형을 만든다면, (in Stefanie Molin’s book)\n\nCosts: 높은 품질의 와인을 낮은 품질로 예측하면, 와인 품평가에게 신뢰를 잃을 수 있음\nBenefits: 낮은 품질의 와인을 높은 품질로 예측하면, 낮은 품질의 와인을 높은 가격에 팔아 수익으로 이어질 수 있음\n거짓 음성을 낮춰야 하는 경우: 예를 들어, 영세한 와이너리가 수익이 중요한 경우\n거짓 양성을 낮춰야 하는 경우: 예를 들어, 고품질의 와인을 생산하는 것으로 유명한 와이너리; 네임밸류를 유지하기 위해. 반면, 비싼 와인이 싸게 팔리는 것은 감당할 수 있음.\n\n확률을 정확히 예측하는 모형의 추구;\n\n확률값으로 communicate\nDecision maker는 당사자"
  },
  {
    "objectID": "contents/mlsl-intro.html#model-evaluation",
    "href": "contents/mlsl-intro.html#model-evaluation",
    "title": "Machine/Statistical Learning",
    "section": "Model evaluation",
    "text": "Model evaluation\n모델의 평가는 예측함수의 오차로 인한 손실(loss)이 최소인 것으로 기준을 둘 수 있으나,\n현실적으로는 모델은 특정 데이터셋, 혹은 특정 관찰값에 의존할 수밖에 없음.\n우리가 원하는 것은 관찰값에 대해서가 아닌 일반화된(generalized) 혹은 전체(population)에 대해 손실이 최소이기를 바라는 것임.\n\n전통적 통계에서는 모집단(population)이라는 것을 상정하여, 일반화된 모형을 추론하고자 했음.\n\n이를 위해 모집단에 대한 여러 가정들이 필요했으며, 이를 통해 모집단에 대한 추론(inference)을 얻었음.\n관측치를 최대한 모두 사용하여 모형을 추정하고, 모집단에 대해서 수학적인 수정을 거침.\nLikelihood의 관점에서 최선의 모형을 선택하고, 모집단에 대한 가정을 기반으로 불확성을 추론함.\n\n현대적인 관점에서는 가정없이 관찰된 데이터셋만으로 일반화할 수 있는 다른 접근 방식을 택함.\n\n데이터셋을 훈련셋(training set)과 테스트셋(test set)으로 나누어, 훈련셋으로 모델을 구축하고\n모형이 훈련과정에서 보지 못한 새로운 데이터셋인 테스트셋에서 얼마나 잘 작동하는지를 평가함.\n이는 모델의 일반화(generalization)가 얼마나 잘 되는지를 평가하는 접근이라고 볼 수 있음.\n모델이 특정 데이터셋에 따라 변하는 것(variance)을 고려하기 위해, 훈련셋을 다시 여러 개의 서브셋으로 나누어 교차검증(cross-validation)을 수행; 검증셋(validation set)\n\n\n\nResampling methods\nCross-Validation\n데이터셋을 훈련셋과 검증셋으로 나누어 보면; validation set approach\n예를 들어, auto dataset (ISLP 패키지)에서 연비(mpg)를 마력(horsepower)로 예측하는 모델을 만든다면,\n\n훈련셋과 검증셋을 어떻게 선택하는지에 따라 결과가 바뀜\n훈련셋의 양을 얼마나 선택하는지에 따라 결과가 바뀜: 데이터가 작을 수록 모형의 적절성이 낮아짐\n\n이를 해결하기 위해 교차검증(cross-validation)을 사용\n\n다수의 훈련셋과 검증셋을 생성하여, 평균값으로 모형의 성능을 평가\n다양한 변형이 존재\n\n\n\ncode\nfrom ISLP import load_data\nauto = load_data(\"Auto\")\nauto.head(3)\n\n\n    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n0 18.00          8        307.00         130    3504         12.00    70   \n1 15.00          8        350.00         165    3693         11.50    70   \n2 18.00          8        318.00         150    3436         11.00    70   \n\n   origin                       name  \n0       1  chevrolet chevelle malibu  \n1       1          buick skylark 320  \n2       1         plymouth satellite  \n\n\n\nfrom sklearn.model_selection import train_test_split\nauto_train, auto_valid = train_test_split(auto, test_size=.5, random_state=0)\n\n\ncode\np = (\n    so.Plot(auto_train, x='horsepower', y='mpg')\n    .add(so.Dots(color=\".5\"))\n)\np.show()\n\nfor i in range(1, 6):\n    p.add(so.Line(), so.PolyFit(i)).label(title=f\"M = {i}\").show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: p. 204, An Introduction to Statistical Learning with Applications in Python\n\n교차검증의 예\n\nk-fold cross-validation: 데이터셋을 k개의 서브셋으로 나누어, k번의 모형평가를 수행\nleave-one-out cross-validation(LOOCV): k=n인 경우\n\n거의 모든 데이터로부터 훈련\nk-fold 작업에서 발생하는 무작위성이 없음\n\nShuffle-split cross-validation: 데이터셋을 무작위로 섞어서 k번의 모형평가를 수행\n\nscikit-learn: Cross-validation 문서 참고\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n주로 k=5, k=10을 사용; 경험적으로 최적임\n\nk의 선택에 따라 bias와 variance의 trade-off가 달라짐\nk가 작을수록 variance가 높아지고, bias가 낮아짐\n\n\nk개의 훈련셋에서 각각에서 얻은 모델에 대해서 검증셋으로부터 모형의 평가치를 얻음: 주로 MSE(mean squared error)\nk-fold CV estimate: \\(\\displaystyle CV_{(k)} = \\frac{1}{k} \\sum_{i=1}^k MSE_i\\)\n분류의 경우, misclassification error rate: \\(\\displaystyle CV_{(k)} = \\frac{1}{k} \\sum_{i=1}^k \\frac{1}{n_i} \\sum_{j=1}^{n_i} I(y_j \\neq \\hat y_j)\\)\n\nfrom sklearn.model_selection import cross_validate, cross_val_score, KFold, ShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n\nmod_auto = make_pipeline(PolynomialFeatures(2), LinearRegression())  # 2차 다항함수\nmod_auto_cv = -cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # default scoring is R2\n\nprint(f\"10-fold Cross-validation \\nMSEs: {mod_auto_cv}, \\nAverage: {mod_auto_cv.mean()}\")\n\n10-fold Cross-validation \nMSEs: [15.98 15.84 18.39 25.88 18.71 23.58 23.58 23.83 11.19 14.88], \nAverage: 19.185331419374997\n\n\n\ncross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error') \n\narray([-15.98, -15.84, -18.39, -25.88, -18.71, -23.58, -23.58, -23.83,\n       -11.19, -14.88])\n\n\n\n\ncode\ncv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nmod_auto = make_pipeline(PolynomialFeatures(2), LinearRegression())  # 2차 다항함수\nmod_auto_cv = -cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # default scoring is R2\n\nprint(f\"Shuffle & Spllit Cross-validation with 20% test sets \\nMSEs: {mod_auto_cv}, \\nAverage: {mod_auto_cv.mean()}\")\n\n\nShuffle & Spllit Cross-validation with 20% test sets \nMSEs: [16.01 18.34 17.81 12.66 14.97 20.87 13.4  13.62 20.05 15.53], \nAverage: 16.326567350428938\n\n\n\ncode\nev_error1 = np.zeros([10, 10])\n\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n\nfor i, d in enumerate(range(1, 11)):\n    mod_auto = make_pipeline(PolynomialFeatures(d), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # cross_validate에 대한 wrapper\n    ev_error1[i, :] = -mod_auto_cv\n\n# plot\nplt.figure(figsize=(6, 4), dpi=60)\nfor i in range(10):\n    plt.plot(np.arange(1, 11), ev_error1[:, i])\n    plt.scatter(np.arange(1, 11), ev_error1[:, i], s=5)\n\nplt.xticks(np.arange(1, 11))\nplt.xlabel(\"Degree of Polynomial\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"10-fold Cross-validation \\nover 10 degrees of Polynomial\")\nplt.show()\n\n# Shuffle & Split Cross-validation\nev_error2 = np.zeros([10, 10])\ncv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nfor i, d in enumerate(range(1, 11)):\n    mod_auto = make_pipeline(PolynomialFeatures(d), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')\n    ev_error2[i, :] = -mod_auto_cv\n\n# plot\nplt.figure(figsize=(6, 4), dpi=60)\nfor i in range(10):\n    plt.plot(np.arange(1, 11), ev_error2[:, i])\n    plt.scatter(np.arange(1, 11), ev_error2[:, i], s=5)\n\nplt.xticks(np.arange(1, 11))\nplt.xlabel(\"Degree of Polynomial\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"10-split Shuffle & Split Cross-validation \\nwith 20% test sets over 10 degrees of Polynomial\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nev_error3 = np.zeros([10, 2])\n\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n\nfor i in range(10):\n    mod_auto = make_pipeline(PolynomialFeatures(i+1), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # cross_validate에 대한 wrapper\n    ev_error3[i, 0] = -mod_auto_cv.mean()\n\ncv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nfor i in range(10):\n    mod_auto = make_pipeline(PolynomialFeatures(i+1), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # cross_validate에 대한 wrapper\n    ev_error3[i, 1] = -mod_auto_cv.mean()\n\n# plot\nplt.figure(figsize=(6, 4), dpi=60)\nlabels = [\"10-fold CV\", \"10-split Shuffle & Split CV with 20% test set\"]\nfor i in range(2):\n    plt.scatter(np.arange(1, 11), ev_error3[:, i], s=5)\n    plt.plot(np.arange(1, 11), ev_error3[:, i], label=labels[i])\n\nplt.xticks(np.arange(1, 11))\nplt.xlabel(\"Degree of Polynomial\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"Average of 10-fold Cross-validation \\nover 10 degrees of Polynomial\")\nplt.ylim(15, 30)\nplt.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\nsklearn: Metrics and scoring\n교차검증을 통해 모델의 flexibility를 결정할 수 있음\n예를 들어, 아래의 3가지 형태의 true relationship에 대해,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft: Data simulated from f, shown in black. Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel.\n\n\n\n\nSource: pp. 29-32, An Introduction to Statistical Learning with Applications in Python\n\n\n보통 실제 test error rate보다 높게 나오나 최적의 flexibility의 위치는 유사함\n\n\n\nSource: p. 208, An Introduction to Statistical Learning with Applications in Python\n\n다음 절차를 통해, 어떤 클래스의 특정 모델을 선택 후 그 모델의 성능을 평가할 수 있음:\n\n모델 클래스를 선택: 예. linear regression\n데이터를 traing set과 test set으로 데이터셋을 나눈 후,\ntraining set으로 교차검증(sub-trainging & validation sets)을 통해 모델의 flexibility를 결정: 예. 다항식의 차수, tuning parameter 등\n최적의 flexibility를 선택한 후, 전체 training set으로 fitting한 모델을 선택 후\ntest set으로 모델의 성능을 평가\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n## Shuffle & Split\n# cv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nmod_auto = Pipeline([(\"poly\", PolynomialFeatures()), (\"lm\", LinearRegression())])\n\n# Grid Search\nparam_grid = {\"poly__degree\": np.arange(1, 11)}  # name__parameter\ngrid_search = GridSearchCV(mod_auto, param_grid, cv=cv, scoring='neg_mean_squared_error')\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('poly', PolynomialFeatures()),\n                                       ('lm', LinearRegression())]),\n             param_grid={'poly__degree': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('poly', PolynomialFeatures()),\n                                       ('lm', LinearRegression())]),\n             param_grid={'poly__degree': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n             scoring='neg_mean_squared_error') estimator: PipelinePipeline(steps=[('poly', PolynomialFeatures()), ('lm', LinearRegression())])  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures()  LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# Best parameter!\ngrid_search.best_params_\n\n{'poly__degree': 10}\n\n\n\npd.options.display.max_rows = 10\npd.DataFrame(\n    {\n        \"degree\": grid_search.cv_results_.get(\"param_poly__degree\").data,\n        \"mean_test_score\": -grid_search.cv_results_.get(\"mean_test_score\"),\n    }\n).sort_values(\"mean_test_score\")\n\n  degree  mean_test_score\n9     10            19.94\n5      6            20.15\n4      5            20.20\n8      9            20.28\n1      2            20.29\n2      3            20.37\n6      7            20.40\n7      8            20.49\n3      4            20.67\n0      1            24.82\n\n\n\nmod_auto_best = Pipeline([(\"poly\", PolynomialFeatures(2)), (\"lm\", LinearRegression())])\nmod_auto_best.fit(X_train, y_train)\n\nPipeline(steps=[('poly', PolynomialFeatures()), ('lm', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('poly', PolynomialFeatures()), ('lm', LinearRegression())])  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures()  LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# get root_mean_squared_error and median_absolute_error\nfrom sklearn.metrics import root_mean_squared_error\n\ny_pred = mod_auto_best.predict(X_test)\nroot_mean_squared_error(y_test, y_pred)\n\n4.001611502465339"
  },
  {
    "objectID": "contents/inspection.html",
    "href": "contents/inspection.html",
    "title": "Inspecting data",
    "section": "",
    "text": "# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Python Basics",
      "Data Inspection"
    ]
  },
  {
    "objectID": "contents/inspection.html#useful-method",
    "href": "contents/inspection.html#useful-method",
    "title": "Inspecting data",
    "section": "Useful method",
    "text": "Useful method\n.head(), .tail(), .sample()\n.info(), .describe(),\n.value_counts(),\n.sort_values(), .nlargest(), .nsmallest()\nData: Tips\n일정기간 한 웨이터가 얻은 팁에 대한 데이터\n\n# load a dataset\ntips = sns.load_dataset(\"tips\")\ntips\n\n     total_bill  tip     sex smoker   day    time  size\n0         16.99 1.01  Female     No   Sun  Dinner     2\n1         10.34 1.66    Male     No   Sun  Dinner     3\n2         21.01 3.50    Male     No   Sun  Dinner     3\n..          ...  ...     ...    ...   ...     ...   ...\n241       22.67 2.00    Male    Yes   Sat  Dinner     2\n242       17.82 1.75    Male     No   Sat  Dinner     2\n243       18.78 3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]\n\n\n\ntips.head(3)  # 앞 n개 나열, 기본값은 5\n\n   total_bill  tip     sex smoker  day    time  size\n0       16.99 1.01  Female     No  Sun  Dinner     2\n1       10.34 1.66    Male     No  Sun  Dinner     3\n2       21.01 3.50    Male     No  Sun  Dinner     3\n\n\n\ntips.sample(5)  # 무작위로 n개 표본 추출, 기본값은 1\n\n     total_bill  tip     sex smoker   day    time  size\n129       22.82 2.18    Male     No  Thur   Lunch     3\n30         9.55 1.45    Male     No   Sat  Dinner     2\n234       15.53 3.00    Male    Yes   Sat  Dinner     2\n215       12.90 1.10  Female    Yes   Sat  Dinner     2\n146       18.64 1.36  Female     No  Thur   Lunch     3\n\n\n\ntips.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  244 non-null    float64 \n 1   tip         244 non-null    float64 \n 2   sex         244 non-null    category\n 3   smoker      244 non-null    category\n 4   day         244 non-null    category\n 5   time        244 non-null    category\n 6   size        244 non-null    int64   \ndtypes: category(4), float64(2), int64(1)\nmemory usage: 7.4 KB\n\n\n\ntips.describe()  # numerical type만 나열\n\n       total_bill    tip   size\ncount      244.00 244.00 244.00\nmean        19.79   3.00   2.57\nstd          8.90   1.38   0.95\n...           ...    ...    ...\n50%         17.80   2.90   2.00\n75%         24.13   3.56   3.00\nmax         50.81  10.00   6.00\n\n[8 rows x 3 columns]\n\n\n\ntips.describe(include=\"all\")  # all types 나열\n\n        total_bill    tip   sex smoker  day    time   size\ncount       244.00 244.00   244    244  244     244 244.00\nunique         NaN    NaN     2      2    4       2    NaN\ntop            NaN    NaN  Male     No  Sat  Dinner    NaN\n...            ...    ...   ...    ...  ...     ...    ...\n50%          17.80   2.90   NaN    NaN  NaN     NaN   2.00\n75%          24.13   3.56   NaN    NaN  NaN     NaN   3.00\nmax          50.81  10.00   NaN    NaN  NaN     NaN   6.00\n\n[11 rows x 7 columns]\n\n\n\ntips.describe(include=\"category\")\n\n         sex smoker  day    time\ncount    244    244  244     244\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq     157    151   87     176\n\n\n\ns1 = tips.value_counts(\"day\") # \"day\" 칼럼에 대한 각 카테고리별 counts\ns2 = tips.value_counts(\"day\", sort=False) # default: sort is true\ns3 = tips.value_counts(\"day\", ascending=True) # default: ascending is False\ns4 = tips.value_counts(\"day\", normalize=True) # 카테고리별 비율\ns5 = tips.value_counts([\"sex\", \"smoker\"]) # \"sex\", \"smoker\" 칼럼에 대한 유니크한 카테고리별 counts\n\n\n\n\n\n\n\n\n\nday\nSat     87\nSun     76\nThur    62\nFri     19\nName: count, dtype: int64\n\n\n(a) s1\n\n\n\n\n\n\n\n\nday\nThur    62\nFri     19\nSat     87\nSun     76\nName: count, dtype: int64\n\n\n(b) s2\n\n\n\n\n\n\n\n\n\n\nday\nFri     19\nThur    62\nSun     76\nSat     87\nName: count, dtype: int64\n\n\n(c) s3\n\n\n\n\n\n\n\n\nday\nSat    0.36\nSun    0.31\nThur   0.25\nFri    0.08\nName: proportion, dtype: float64\n\n\n(d) s4\n\n\n\n\n\n\n\n\n\n\nsex     smoker\nMale    No        97\n        Yes       60\nFemale  No        54\n        Yes       33\nName: count, dtype: int64\n\n\n(e) s5\n\n\n\n\n\n\n\nFigure 1: value_count()의 arguments\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n.value_count()의 결과는 Series이며 그 이름은 ‘count’ 또는 ’proportion’임 (pandas 2.0)\nMissing(NA)을 count하지 않으나 dropna=False을 이용해 나타낼 수 있음\ntips.value_counts(\"day\", dropna=False)\nSeries에 대해서도 적용되며, DataFrame으로 컬럼을 선택해 적용할 수 있음\ntips[\"day\"].value_counts()  # tips[\"day\"]: Series object\ntips[[\"sex\", \"smoker\"]].value_counts()\n\n\n\nData: palmerpenguins\n\n# load a dataset\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen           39.10          18.70             181.00   \n1  Adelie  Torgersen           39.50          17.40             186.00   \n2  Adelie  Torgersen           40.30          18.00             195.00   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen           36.70          19.30             193.00   \n\n   body_mass_g     sex  \n0      3750.00    Male  \n1      3800.00  Female  \n2      3250.00  Female  \n3          NaN     NaN  \n4      3450.00  Female  \n\n\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\npenguins.describe(include=\"object\")\n\n\n\n       species  island   sex\ncount      344     344   333\nunique       3       3     2\ntop     Adelie  Biscoe  Male\nfreq       152     168   168\n\n\n\n\npenguins.value_counts([\"island\", \"species\"])\n\nisland     species  \nBiscoe     Gentoo       124\nDream      Chinstrap     68\n           Adelie        56\nTorgersen  Adelie        52\nBiscoe     Adelie        44\nName: count, dtype: int64\n\n\n\npenguins.value_counts([\"sex\", \"species\"], dropna=False) # NA은 기본적으로 생략\n\nsex     species  \nFemale  Adelie       73\nMale    Adelie       73\n        Gentoo       61\n                     ..\n        Chinstrap    34\nNaN     Adelie        6\n        Gentoo        5\nName: count, Length: 8, dtype: int64\n\n\n\ntips.sort_values(\"tip\", ascending=False)\n\n     total_bill   tip     sex smoker  day    time  size\n170       50.81 10.00    Male    Yes  Sat  Dinner     3\n212       48.33  9.00    Male     No  Sat  Dinner     4\n23        39.42  7.58    Male     No  Sat  Dinner     4\n..          ...   ...     ...    ...  ...     ...   ...\n111        7.25  1.00  Female     No  Sat  Dinner     1\n67         3.07  1.00  Female    Yes  Sat  Dinner     1\n92         5.75  1.00  Female    Yes  Fri  Dinner     2\n\n[244 rows x 7 columns]\n\n\n\ntips.sort_values([\"size\", \"tip\"], ascending=[False, True])\n\n     total_bill  tip     sex smoker   day    time  size\n125       29.80 4.20  Female     No  Thur   Lunch     6\n143       27.05 5.00  Female     No  Thur   Lunch     6\n156       48.17 5.00    Male     No   Sun  Dinner     6\n..          ...  ...     ...    ...   ...     ...   ...\n111        7.25 1.00  Female     No   Sat  Dinner     1\n82        10.07 1.83  Female     No  Thur   Lunch     1\n222        8.58 1.92    Male    Yes   Fri   Lunch     1\n\n[244 rows x 7 columns]\n\n\n\ntips.nlargest(3, \"tip\")  # 다수의 동등 순위가 있을 때 처리: keep=\"first\", \"last\", \"all\"\n\n     total_bill   tip   sex smoker  day    time  size\n170       50.81 10.00  Male    Yes  Sat  Dinner     3\n212       48.33  9.00  Male     No  Sat  Dinner     4\n23        39.42  7.58  Male     No  Sat  Dinner     4",
    "crumbs": [
      "Python Basics",
      "Data Inspection"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균 / sk.cho@snu.ac.kr\n면담 시간: 수업 후\n수업 시간: 월, 수 3:00 ~ 4:30PM\nWebsite: dgds101.modellings.art\n출석: 링크\n과제: Notice\n질문: Communicate/Ask",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-정보",
    "href": "index.html#강의-정보",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균 / sk.cho@snu.ac.kr\n면담 시간: 수업 후\n수업 시간: 월, 수 3:00 ~ 4:30PM\nWebsite: dgds101.modellings.art\n출석: 링크\n과제: Notice\n질문: Communicate/Ask",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-개요",
    "href": "index.html#강의-개요",
    "title": "Welcome",
    "section": "강의 개요",
    "text": "강의 개요\n데이터 분석은 오랜 역사를 거쳐 통계학의 영역에서 발전해왔고, 양적연구를 기반으로하는 여러 분야에서 핵심적인 역할을 한 반면, 인공지능의 하위 분야로 연구되어온 기계학습은 방대한 데이터와 더불어 최근에 그 유용성이 크게 부각되면서 이 두 분야는 데이터 사이언스라는 큰 틀에서 통합되고 있습니다. 이러한 광범위한 주제에 대해 각 기법의 핵심적 아이디어와 응용 예시에 초점을 맞추고, 더 세부적인 주제들을 탐구하기 위한 초석을 제공하고자 합니다. 또한 구체적인 예들을 직접 코딩하여 어느 정도 데이터 분석 기술의 기초를 갖추도록 과제를 통해 학습할 기회도 제공됩니다.\n\n전통적 통계와 기계 학습에서 추구하는 바를 이해하고,\n\n데이터로부터 패턴과 의미를 추론하는 방식을 이해하며,\n\n기계/통계적 학습의 응용 가능성에 대해 파악합니다.\n\n\n교재\n\nPython Data Science Handbook by Jake VanderPlas: code on GitHub\n\n\n\n참고도서\n\nAn Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani, Taylor: code on GitHub\nPython for Data Analysis (3e) by Wes McKinney: code on Github\n3판 번역서: 파이썬 라이브러리를 활용한 데이터 분석",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#수업-활동",
    "href": "index.html#수업-활동",
    "title": "Welcome",
    "section": "수업 활동",
    "text": "수업 활동\n출석 (10%), 일반과제 (20%), 중간고사 (35%), 기말고사 (35%)\n코딩 실습 설문",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#수업-계획",
    "href": "index.html#수업-계획",
    "title": "Welcome",
    "section": "수업 계획",
    "text": "수업 계획\n1주. 데이터 사이언스 소개\n2주. 데이터 분석의 두 문화 1: 전통적 통계\n3주. 데이터 분석의 두 문화 2: 기계 학습\n4주. 파이썬 기초\n5주. 탐색적 분석 및 시각화\n6주. 선형 모형(linear model) 소개\n7주. 선형 모형의 활용: 해석과 의미의 추론\n8주. 중간고사\n9주. 기계 학습 소개\n10주. 분류(classification) 1: 로지스틱(logistic) 모형\n11주. 분류(classification) 2: 서포트 벡터 머신\n12주. Tree-based 모형\n13주. 딥러닝\n14주. 비지도 학습(unsupervised learning)\n15주. 기말고사",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "contents/setup.html",
    "href": "contents/setup.html",
    "title": "Python 설정",
    "section": "",
    "text": "데이터 사이언스를 위한 Python 개발 환경\n몇 가지 선택지",
    "crumbs": [
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#클라우드-환경",
    "href": "contents/setup.html#클라우드-환경",
    "title": "Python 설정",
    "section": "클라우드 환경",
    "text": "클라우드 환경\nColab\n\n사용법: Colab Welcome\n클라우드 환경 vs. 구글 드라이브 mount\nColab AI assistant\n구글 드라이브의 데이터셋을 import: pd.read_csv(\"drive/MyDrive/...\")",
    "crumbs": [
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#로컬-환경",
    "href": "contents/setup.html#로컬-환경",
    "title": "Python 설정",
    "section": "로컬 환경",
    "text": "로컬 환경\nPython과 Conda Package Manager\nConda Cheatsheet: 기본적인 conda 명령어 요약\n\nMiniconda 설치\nAnaconda보다는 기본 패키지들이 미리 설치되지 않는 miniconda를 추천: miniconda install page\n\nWindows 경우: 설치시 물어보는 “add Miniconda to your PATH variable” 옵션을 켜고 설치할 것\n\nShell 사용에 대해서는 아래 Command Line Tool 참고\n\nWindows 경우: Anaconda의 응용 프로그램으로 등록된 Anaconda Powershell Prompt를 이용\nMac의 경우: 기본 terminal을 이용\n커서 앞에 (base)가 보이면 conda가 설치된 것\n\n# Terminal (Mac) or Miniconda Powershell Prompt (Windows)\n\n(base)&gt; conda info # 콘다 정보 \n(base)&gt; conda update conda # 콘다 업데이트\n\n\nConda Environment\nconda/user guide\n환경 생성: miniconda에서 자체 제공하는 환경 (다른 가상환경 툴인 pyenv나 venv도 있음)\n(base)&gt; conda create --name myenv  # --name 대신 -n으로 축약 가능\n\n# 특정 버전의 파이썬과 함께 설치시\n(base)&gt; conda create --name myenv python=3.12\n환경 확인\n(base)&gt; conda env list\n# conda environments:\n#\n# base         */.../miniconda3\n# myenv         /.../miniconda3/envs/myenv\n환경 제거\n(base)&gt; conda env remove --name myenv\n환경 activate/deactivate\n(base)&gt; conda activate myenv\n(myenv)&gt; conda deactivate\n특정 환경 안의 파이썬 버전 확인\n(myenv)&gt; python --version\n\n\n환경(activated) 내에서 패키지 설치 및 제거\n\n\n\n\n\n\n패키지 repository(channel) 선택\n\n\n\n\n\nconda/managing channels\n다음을 통해 .condarc 환경파일에 configuration 추가\n(base)&gt; conda config --add channels conda-forge\n(base)&gt; conda config --set channel_priority strict  # 채널 순으로 검색, 버전 순이 아니고\n# 개별적으로 채널을 선택해서 install하려면 (특정 환경에 설치하려면 아래 conda environment 참조)\n(base)&gt; conda install scipy --channel conda-forge\n\n# pakcage가 있는 채널들\n(base)&gt; conda search scipy\n\n\n\n# 특정 환경을 activate한 후\n\n# Python을 update하거나 다른 버전을 설치하려면, 가령 3.12으로 업데이트 하려면\n(myenv)&gt; conda install python=3.12  # python update\n\n# 패키지 설치\n(myenv)&gt; conda install &lt;package name1&gt; &lt;package name2&gt; ...\n# 특정한 채널, conda-forge 통한 설치: --channel 대신 -c로 축약 가능\n(myenv)&gt; conda install --channel conda-forge &lt;package name&gt;\n\n# 제거\n(myenv)&gt; conda remove &lt;package name1&gt; &lt;package name2&gt; ...\n\n# 업데이트\n(myenv)&gt; conda update &lt;package name1&gt; &lt;package name2&gt; ...\n(myenv)&gt; conda update --all  # all packages\n\n# 패키지 리스트 확인\n(myenv)&gt; conda list\n환경 밖에서 특정 환경 안에 설치하려면 환경 이름 추가\n(base)&gt; conda install --name myenv &lt;package name1&gt;  # --name 대신 -n으로 축약 가능\npip을 이용한 패키지 설치: conda repository에 없는 패키지들을 설치하는 경우. 충돌의 우려 있음\n(myenv)&gt; pip install &lt;package name1&gt; &lt;package name2&gt; ...\n수업에 필요한 기본 패키지 설치\n# 수업에 필요한 기본 패키지 설치\n(myenv)&gt; conda install jupyter numpy pandas matplotlib seaborn\n\n\n\n\n\n\nCommand Line Tools\n\n\n\n\n\n\nMac의 경우: 기본 terminal을 이용하되 기본 zsh shell 대신 다음 Oh-My-Zsh을 추천\nOh-My-Zsh!: 링크\n\n\nWindows의 경우: Windows Terminal 추천\n\n설치 링크는 구글링…\n명령프롬프트(CMD) vs. Powershell\nPowershell에서 conda를 사용하기 위해서는 몇 가지 설정 필요: 블로그 링크\n잘 안될 경우, conda 설치시 함께 설치되는 응용프로그램 콘다 powershell을 이용",
    "crumbs": [
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#sec-vscode",
    "href": "contents/setup.html#sec-vscode",
    "title": "Python 설정",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\nVS Code 설치\n\n개인마다 선호하는 text editor가 있으나 본 수업에서는 VS Code로 진행: download and install here\n\n\n\nExtensions\n\nPython\nPython Extension Pack 중\n\nIntelliCode\nPython Environment Manager\n\nPylance: 문법 체크, 자동완성, …\nDocs View\n\n안 보일시, 설정에서 language server를 default(Pylance)에서 Jedi로 바꾸면 해결\n\nCopilot…\n\n\n\nPreferences\n\nThemes\nFont, font size (notebook, markup, output)\n\n\n\nShortcuts\nShow Command Palette: ctrl(cmd) + shift + p, 또는 F1\nCell 안과 밖에서 다르게 작동\n\nundo / redo : ctrl(cmd) + z / ctrl(cmd) + shift + z\nmove: alt(option) + arrow up/down\ncopy : alt(option) + shift + arrow up/down\n\n코드 실행 방식 3가지: ctrl/shift/alt(option) + enter\nHelp: Keyboard shortcuts reference의 Basic editing 참고\n\n\n그 외\n\ninteractive mode\nexport\ndocs view: sticky mode\nvariables viewer, data viewer\nformatter: “Black formatter”\nsnippets: 구글링…\n\n\n\nVS Code내에서 terminal 사용\nTerminal: Select Default Profile에서 선택\n\nMac: zsh\nWindows: powershell",
    "crumbs": [
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#jupyter-notebooklab",
    "href": "contents/setup.html#jupyter-notebooklab",
    "title": "Python 설정",
    "section": "Jupyter Notebook/Lab",
    "text": "Jupyter Notebook/Lab\n\n\n\n\n\n\n콘다 환경 등록\n\n\n\n\n\n새로 만든 환경을 등록해줘야 함. 환경을 activate한 상태에서\n(myenv)&gt; ipython kernel install --user --name=myenv\n환경을 삭제해도 등록시킨 kernel 이름은 삭제되지 않으니 직접 삭제.\n등록된 커널 리스트를 확인\n(myenv)&gt; jupyter kernelspec list\n커널 삭제\n(myenv)&gt; jupyter kernelspec remove myenv\n\n\n\nJupyter Notebook 또는 lab 실행\n\nAnaconda 응용 프로그램을 이용해 실행하거나,\n쉘에서 실행하려면,\n\n# jupytet notebook\n(base)&gt; jupyter notebook\n\n# jupyter lab\n(base)&gt; jupyter lab\n등록한 커널을 선택 후 시작\n커널을 종료하려면, 쉘에서 Ctrl-C 두 번",
    "crumbs": [
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#python-packages-modules-functions",
    "href": "contents/setup.html#python-packages-modules-functions",
    "title": "Python 설정",
    "section": "Python Packages, Modules, Functions",
    "text": "Python Packages, Modules, Functions\nJupyter notebook 파일을 생성: filename.ipynb\n\nimport numpy as np\nimport pandas as pd\nfrom numpy.linalg import inv\n\n\n\n\n\n\n\n두 개 이상의 함수/모듈을 import하거나 as로 별칭 지정 가능\n\n\n\n\n\nfrom numpy.linalg import inv as inverse\nfrom numpy.linalg import inv, det\nfrom numpy.linalg import inv as inverse, det as determinant\n\n\n\nnp.linalg?  # 함수에 대한 도움말\nNumPy 패키지(package)의 linalg 모듈(module)\nnp.linalg  # linalg.py 파이썬 스트립트 파일\nlinalg 모듈 파일 안에 def으로 정의된 함수\n\nnp.linalg.inv  # 모듈 안에서 def으로 정의된 함수 inv()\n\n&lt;function numpy.linalg.inv(a)&gt;\n\n\nnp.linalg.inv?  # 함수에 대한 도움말\n예를 들어, 행렬의 역행렬을 구하는 함수 inv를 사용하려면\n\nrng = np.random.default_rng(123)  # random number generator\nx = rng.standard_normal((3, 3))  # 3x3 matrix from standard normal distribution\nx\n\narray([[-0.98912135, -0.36778665,  1.28792526],\n       [ 0.19397442,  0.9202309 ,  0.57710379],\n       [-0.63646365,  0.54195222, -0.31659545]])\n\n\n어떻게 import하느냐에 따라 다른 방식으로 사용\n\ninv(x)  # inverse matrix of x\n\n# inv 함수를 따로 import하지 않은 경우, numpy의 linalg 모듈을 통해 사용\nnp.linalg.inv(x)  # same as above\n\narray([[-0.37762186,  0.36352647, -0.87353193],\n       [-0.19121277,  0.70815019,  0.51298399],\n       [ 0.4318268 ,  0.4814099 , -0.52437858]])\n\n\n주로 모듈 이름을 함께 쓰는 것이 관례인데,\n\n이는 코드의 가독성을 높이고,\n사용자 정의 함수와의 충돌을 방지하기 위함\n\n모듈 안에 정의된 함수들을 확인하려면: dir() 함수\n\ndir(np.linalg)  # 모듈 안에 정의된 함수들\n\n['LinAlgError',\n '__all__',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__path__',\n '__spec__',\n '_umath_linalg',\n 'cholesky',\n 'cond',\n 'det',\n 'eig',\n 'eigh',\n 'eigvals',\n 'eigvalsh',\n 'inv',\n 'linalg',\n 'lstsq',\n 'matrix_power',\n 'matrix_rank',\n 'multi_dot',\n 'norm',\n 'pinv',\n 'qr',\n 'slogdet',\n 'solve',\n 'svd',\n 'tensorinv',\n 'tensorsolve',\n 'test']",
    "crumbs": [
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/notice.html",
    "href": "contents/notice.html",
    "title": "Notice",
    "section": "",
    "text": "4.21(일) 23:59까지 제출\n\nNYC Taxi 데이터셋을 이용하여 시각화를 포함한 탐색적 분석 연습; Explore, Wrangle 파트 참고\n.ipynb 파일로 eclass에 제출",
    "crumbs": [
      "Notice"
    ]
  },
  {
    "objectID": "contents/notice.html#과제",
    "href": "contents/notice.html#과제",
    "title": "Notice",
    "section": "",
    "text": "4.21(일) 23:59까지 제출\n\nNYC Taxi 데이터셋을 이용하여 시각화를 포함한 탐색적 분석 연습; Explore, Wrangle 파트 참고\n.ipynb 파일로 eclass에 제출",
    "crumbs": [
      "Notice"
    ]
  },
  {
    "objectID": "contents/notice.html#중간고사",
    "href": "contents/notice.html#중간고사",
    "title": "Notice",
    "section": "중간고사",
    "text": "중간고사",
    "crumbs": [
      "Notice"
    ]
  },
  {
    "objectID": "contents/notice.html#기말고사",
    "href": "contents/notice.html#기말고사",
    "title": "Notice",
    "section": "기말고사",
    "text": "기말고사",
    "crumbs": [
      "Notice"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "contents/intro.html",
    "href": "contents/intro.html",
    "title": "개관",
    "section": "",
    "text": "다양한 소스들로부터 데이터 생성: 전지구적 개인과 환경에 대한 상세한 정보 발생\n인터넷 & 통신 (SNS, 사진, 위치, 장소, 유동인구, 상품거래, 물류)\n사물인터넷 (IoT), CCTV\n스마트 팩토리, 파밍\n게놈프로젝트, 생체정보, 의료/보건: 인류, 실시간\n\n23andMe, Theranos\n\n과학적 발견: 물리법칙의 발견, 약물의 합성, 생체 내 상호작용의 메커니즘 규명\n자율주행차량: 내부, 외부\n금융정보 및 흐름\n사회 지표 활용: 고용, 직업, 연봉, 만족도 조사, 취약 계층, 우울\n생성형 인공지능: 기계의 정보 생산\nAI companion: 개인 내면에 대한 정보\n\n영화 Her (2013), Sony’s robotics dog ‘Aibo’",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#미래-데이터의-중요성",
    "href": "contents/intro.html#미래-데이터의-중요성",
    "title": "개관",
    "section": "",
    "text": "다양한 소스들로부터 데이터 생성: 전지구적 개인과 환경에 대한 상세한 정보 발생\n인터넷 & 통신 (SNS, 사진, 위치, 장소, 유동인구, 상품거래, 물류)\n사물인터넷 (IoT), CCTV\n스마트 팩토리, 파밍\n게놈프로젝트, 생체정보, 의료/보건: 인류, 실시간\n\n23andMe, Theranos\n\n과학적 발견: 물리법칙의 발견, 약물의 합성, 생체 내 상호작용의 메커니즘 규명\n자율주행차량: 내부, 외부\n금융정보 및 흐름\n사회 지표 활용: 고용, 직업, 연봉, 만족도 조사, 취약 계층, 우울\n생성형 인공지능: 기계의 정보 생산\nAI companion: 개인 내면에 대한 정보\n\n영화 Her (2013), Sony’s robotics dog ‘Aibo’",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#데이터-사이언스의-응용-사례",
    "href": "contents/intro.html#데이터-사이언스의-응용-사례",
    "title": "개관",
    "section": "데이터 사이언스의 응용 사례",
    "text": "데이터 사이언스의 응용 사례\n\n영업 및 마케팅\n\n웹사이트에서 고객의 구매 행동, 소셜 미디어의 댓글을 추적 고객의 선호도를 파악\n월마트의 경우,\n\n수 십년 넘게 매장의 재고 수준을 최적화했고, 2004년, 몇 주 전에 발생한 허리케인의 판매 데이터를 분석하여 “딸기 팝타트”를 재입고\n소셜 미디어 트렌드 및 신용카드 활동을 분석하여 신제품 출시 및 고객 경험 개인화/최적화\n\n추천 시스템을 통해 사용자 취향에 맞는 제품을 추천, 틈새 상품의 판매도 촉진\n\n\n\n공공기관\n\n미국의 경우, 정부 주도의 데이터 과학 이니셔티브 발족; 특히, 보건 분야에 큰 투자\n\nPrecision Medicine Initiative (2015)\n\n인간 게놈 시퀀싱과 데이터 과학을 결합하여 개별 환자를 위한 약물을 설계\n백만 명 이상의 자원봉사자로부터 환경, 라이프스타일, 생물학적 데이터를 수집하여 정밀 의학을 위한 세계 최대의 데이터를 구축\n\n\n\n도시 운영 및 설계\n\n스마트 시티: 환경, 에너지, 교통 흐름을 추적, 분석, 제어\n장기적 도시 계획 수립에 정보를 축적\n\n치안 및 범죄 예측\n\nPolice Data Initiative\n\n범죄 다발 지역과 재범률을 예측\n시민 단체들의 비판도 존재\n\n시카고 경찰; 1주일 이내의 범죄 예측\n비판: Event-level prediction of urban crime reveals a signature of enforcement bias in US cities. Nature human behaviour\n\n각종 보험료 산정\n\n과거의 데이터를 분석하여, 보험금을 지급할 확률을 계산하고 보험료를 산정\n\n\n\n\n스포츠\n\nMoneyball: The Art of Winning an Unfair Game\n\n야구에서 전통적으로 강조되던 도루, 타점, 타율의 통계보다 출루율과 장타율이 더 나은 척도였음\n“저평가된” 선수, 승리에 기여하는 능력에 비해 낮은 급여를 받는 선수를 찾아 영입\n\nSabermetrics: sciecne of baseball\n데이터 분석을 통해 시장에서 어떤 조직이 우위를 점할 수 있는 방법을 제시\n적절한 속성을 찾는 것의 중요성",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#사회적-파장",
    "href": "contents/intro.html#사회적-파장",
    "title": "개관",
    "section": "사회적 파장",
    "text": "사회적 파장\n\n유토피아 vs. 디스토피아\n\n초연결성, 투명성 vs. 완전한 감시와 통제\n개인화된 서비스 vs. 설득/유혹/조작\n개별성/자율성 vs. 피동적/비주체적\n기계와의 교감 vs. 인간관계의 소외, 현실과의 단절\n정보와 인간에 대한 신뢰 vs. 사회적 연대, 문명 붕괴\n자연과의 조화 vs. 생태계의 파괴\n\n\n\n\n\n\n\n\n\n\n\nBrave New World, 1932\n\n\n\n\n\n\n\nThe Technological Society, 1954\n\n\n\n\n\n\n\nSapiens, Homo Deus, 21 Lessons for the 21st Century by Yuval Noah Harari\n\n\n\n\n\nYuval Noah Harari: An Urgent Warning They Hope You Ignore.\n\nThe Social Dilemma (2020)",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#data-science",
    "href": "contents/intro.html#data-science",
    "title": "개관",
    "section": "Data Science",
    "text": "Data Science\n\nArtificial intelligence (인공 지능)\nMachine learning (기계 학습)\nDeep learning (심층 학습)\nData mining (데이터 마이닝)\nStatistical Learning (통계적 학습)\n\n\n\n\n소프트웨어 개발\n데이터에 기반한 분석 위해 작동하도록 프로그래밍을 하여 운영되도록 하는 일\n주로 전통적인 컴퓨터 사이언스의 커리큘럼에 의해 트레이닝\n\n유튜브의 영상 추천\n페이스북의 친구 매칭\n스팸메일 필터링\n자율주행\n\n\n\n\n\n\n데이터 분석\n하나의 구체적인 질문에 답하고자 함\n다양한 소스의 정제되는 않은 데이터를 통합하거나 가공하는 기술이 요구\n\nDNA의 분석을 통해 특정 질병의 발병 인자를 탐색\n유동인구와 매출을 분석해 상권을 분석\n어떤 정책의 유효성을 분석에 정책결정에 공헌\n교통 흐름의 지연이 어떻게 발생하는지를 분석, 해결책 제시",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#skills",
    "href": "contents/intro.html#skills",
    "title": "개관",
    "section": "Skills",
    "text": "Skills\n\n\nDomain knowledge\n\n해결하려는 문제에 대한 이해없이 단순한 알고리즘만으로 “one size fits all”은 효과적이지 않음\n추상화된 현실에 대한 모형은 수많은 가정/사전 지식(prior knowledge)을 전제하고 있음.\n각 분야의 전문 지식은 데이터가 발생되는 과정, 데이터의 특성, 데이터의 의미를 이해하는데 필수적\n\nEthics\n\n데이터를 합법적이고 적절하게 사용하려면 규정을 이해하고, 자신의 업무에 미치는 영향과 사회에 미치는 파급력 대한 윤리적 이해가 필요\n\n배출(exhaust) 데이터: 어떤 목적을 가진 데이터 수집 프로세스로부터 얻어진 부산물\n\n소셜 미디어: 사용자가 다른 사람들과 소통할 수 있도록 도움\n\n공유된 이미지, 블로그 게시물, 트윗, 좋아요 등으로부터\n누가/얼마나 많이 보았는지/좋아요/리트윗을 했는지 등을 수집\n\n아마존 웹사이트: 다양한 물건을 편리하게 구매할 수 있도록 도움\n\n사용자가 장바구니에 어떤 품목을 담았는지, 사이트에 얼마나 오래 머물렀는지, 어떤 다른 품목을 보았는지 등을 수집\n\n메타데이터(metadata)\n통화 내역만으로 많은 민감한 정보을 유추할 수 있음\n\n알코올 중독자 모임, 이혼 전문 변호사, 성병 전문 병원 등\n\n\n한편, 서비스와 마케팅을 타겟팅할 수 있는 잠재력\n\n\nWrangling\n\n데이터 소스는 다양한 형식으로 존재\n통합, 정리, 변환, 정규화 등의 작업이 요구\ndata munging, data wrangling, data cleaning, data preparation, data preprocessing 등으로 불림\n\nDatabase & computer science\n\n수집된 데이터가 저장되고, 가공/추출된 데이터의 재저장 등 데이터베이스와의 소통할 수 있는 기술\n다양해지고 방대해진 빅데이터를 저장/배포하기 위한 도구를 활용\nML 모델을 이해하고 개발하여 제품의 출시, 분석, 백엔드 애플리케이션에 통합할 수 있는 기술 등\n\nVisualisation\n\n작업 프로세스의 모든 과정에 관여\n\n데이터를 탐색하거나,\n데이터의 의미를 효과적으로 전달\n\n\nStatistics & Probability\n\n데이터 과학 프로세스 전반에 걸쳐 사용됨\n\n초기 수집과 조사\n다양한 모델과 분석의 결과를 해석\n의사결정에 활용\n\n\nMachine Learning\n\n데이터로부터 패턴을 찾기 위한 다양한 알고리즘을 사용\n응용 측면에서는\n\n수많은 알고리즘에 대해 가정, 특성, 용도, 결과의 의미, 적용가능한 유형의 데이터 등\n해결할 문제와 데이터에 가장 적합한 알고리즘을 파악\n\n\nCommunication\n\n데이터에 담긴 스토리를 효과적으로 전달하는 능력\n분석을 통해 얻은 인사이트, 조직 내 목적에 어떻게 부합하는지, 조직의 기능에 미칠 수 있는 영향 등",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#응용비즈니스에서-정형적인-절차",
    "href": "contents/intro.html#응용비즈니스에서-정형적인-절차",
    "title": "개관",
    "section": "응용/비즈니스에서 정형적인 절차",
    "text": "응용/비즈니스에서 정형적인 절차\nPhases of the CRISP-DM (CRoss-Industry Standard Process for Data Mining)\nsource: Chapman et al., 2000\n\nGeneric tasks of the CRISP-DM reference model\n\n비즈니스의 이해와 데이터의 이해\n\n프로젝트의 목표를 정의하고, 비즈니스 문제를 이해하는 것\n어떤 데이터를 수집하는 것이 유용한지, 어떤 데이터가 수집 가능한지 등을 탐색\n\n데이터 준비와 모델링\n\n노이즈와 비정형화된 데이터를 정제하고, 모델링을 위한 데이터를 준비\n데이터로부터 의미있는 패턴(signal vs. noise)과 통찰을 찾기 위해 다양한 모델을 검토하고 실행\n\n모델 평가와 배포\n\n모델링 성능을 평가하고 개선, 모델을 배포\n실제 환경에서는 훈련/평가을 위해 사용된 데이터가 보진 못한 새로운 데이터에 적용됨으로 모델의 성능을 지속적으로 모니터링\n\n데이터 질의 중요성\n\n2016년 데이터 과학자를 대상으로 한 설문조사(CrowdFlower report, 2016)\n데이터 준비(데이터 수집, 클린닝)에 79%의 시간이 소요\n프로젝트의 초점이 명확하고, 그에 맞는 올바른 데이터가 수집되었는지, 모델이 프로젝트의 목표에 잘 부응하는지 중요!\nGarbage in, garbage out\n\n\n\nSource: Cleaning Big Data",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#표준-비즈니스-영역에서의-데이터-사이언스-작업",
    "href": "contents/intro.html#표준-비즈니스-영역에서의-데이터-사이언스-작업",
    "title": "개관",
    "section": "표준 비즈니스 영역에서의 데이터 사이언스 작업",
    "text": "표준 비즈니스 영역에서의 데이터 사이언스 작업\nSource: Data Science (The MIT Press Essential Knowledge Series), 2018, by John D. Kelleher & Brendan Tierney\n\nClustering\nAnomaly detection\nAssociation-rule mining\nPrediction: classification & regression\n\n\nClustering\nWho Are Our Customers?\n\n\n클러스터링을 통해 타깃 고객을 더 세분화된 군집으로 분류하여 마케팅 캠페인의 타겟을 명확히 정의할 수 있음\n\nMeta S. Brown (2014)의 보고서에 따르면,\n\nSoccer Moms? \n\n어린이집에 다니는 어린 자녀를 둔 전업주부\n고등학생 자녀와 함께 파트타임으로 일하는 엄마\n음식과 건강에 관심이 많지만 자녀가 없는 여성\n\n\n\n클러스터링을 통해 얻은 고객 세그먼트에 페르소나를 부여\n각 특성에 맞는 캠패인 전략을 수립\n\n작고 집중된 고객 클러스터를 발견\n많은 매출을 창출하는 고객이 포함된 클러스터에 집중\n\n\n\n  Source: Introduction to Statistical Learning by James et al.\n\n\n\n클러스터링을 위해 사용할 수 있는 속성들: 어떤 속성을 포함하고 어떤 속성을 제외할지 결정하는 것이 중요!\n\n인구통계학적 정보(연령, 성별 등)\n위치(우편번호, 시골 또는 도시 주소 등)\n거래 정보(예: 고객이 어떤 제품이나 서비스를 추구했는지)\n기업이 고객으로부터 창출하는 수익\n고객이 된 지 얼마나 되었는지\n로열티 카드 회원인지\n제품을 반품하거나 서비스에 대해 불만을 제기한 적이 있는지 등\n\n\n\n\n\n\n\n프로젝트의 데이터 이해 단계에서 탐색 도구로 자주 사용됨\n추가 지원이 필요하거나 다른 학습 접근 방식을 선호하는 학생 그룹을 식별\n생물 정보학에서 마이크로어레이 분석에서 유전자 서열을 분석\n\n\n\nAnomaly detection\nIs This Fraud?\n\n잠재적인 사기, 특히 금융 거래 행위를 식별하고 조사\n\n예를 들어, 비정상적인 위치에서 발생한 거래\n비정상적으로 많은 금액이 포함된 거래\n\n어떤 면에서 클러스터링과 반대 개념\n\n클러스터링: 유사한 인스턴스 그룹을 식별\n이상 징후 탐지: 특별한 인스턴스를 식별\n\n이상 징후는 드물다는 그 고유한 특징으로 인해 식별이 어려움\n여러 가지 모델을 결합: 서로 다른 모델이 서로 다른 유형의 이상 징후를 포착\n\n예를 들어, 4개의 모델 중 3~4개 모델에서 거래가 사기성 거래로 식별되는 경우\n\n다양한 분야에서 활용\n\n금융기관: 잠재적 사기 또는 자금 세탁 사례로 추가 조사가 필요한 금융 거래를 식별\n보험기관: 회사의 일반적인 청구와 일치하지 않는 청구를 식별\n사이버 보안: 해킹 가능성, 직원의 비정상적인 행동을 탐지하여 네트워크 침입을 식별\n의료 분야: 의료 기록의 이상 징후를 식별하여 질병을 진단\n사물 인터넷: 데이터를 모니터링하고 비정상적인 센서 이벤트의 발생을 감지, 조치\n\n\n\nSource: The Hundred-Page Machine Learning Book, 2019 by Andriy Burkov\n\n\nAssociation-Rule Mining\nDo You Want Fries with That?\n\n고객에게 다른 관련 제품이나 보완 제품, 혹은 잊어 있었던 제품을 제안\n\n예를 들어, 슈퍼마켓에서 핫도그를 구매한 고객은 케첩과 맥주도 함께 구매할 가능성이 높음.\n이에 맞춰 매장은 제품 레이아웃을 계획할 수 있음\n온라인 마켓의 경우, 웹사이트의 배열, 추천, 광고 등을 설계\n즉, 제품 간 연관성을 이해하고 교차 판매를 촉진\n\n연관 규칙 마이닝은 데이터 세트의 속성(또는 열) 간의 관계를 살펴보는 데 중점을 둠: 속성 간의 상관관계\n위의 경우, 고객의 장바구니 품목을 추적\nIF {핫도그, 케첩}, THEN {맥주}\n\n연관성 규칙의 신뢰도가 75%라면 고객이 핫도그와 케첩을 모두 구매한 경우 75%에서 맥주도 함께 구매했음을 의미\n인구통계학적 정보를 연관성 분석에 포함하여 마케팅 및 타겟팅 광고에 활용\n\n특히, 구매 기록 정보는 없는 경우\n\nIF 성별(남성) & 나이(35세 미만)& {핫도그, 케첩}, THEN {맥주}\n장바구니 분석을 통해 다음과 같은 질문에 답을 탐색\n\n마케팅 캠페인이 효과가 있었는지,\n이 고객의 구매 패턴에 변화가 있었는지,\n고객에게 중요한 인생 이벤트가 있었는지,\n제품 위치가 구매 행동에 영향을 미치는지,\n신제품으로 누구를 타깃팅해야 하는지 등\n\n구매 경향의 시간적 요소를 더하면\n\n적절한 시기에 (재)구매를 추천\n유지보수, 부품 교체 일정\n\n다양한 영역에서도 유용함\n\n통신: 회사의 다양한 서비스를 패키지로 묶는 방법을 설계\n보험: 상품과 보험금 청구 사이에 연관성을 파악\n의료: 기존 치료법과 새로운 치료법 및 의약품 간에 상호 작용이 있는지 확인\n\n추천 시스템(recommnder system)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Machine Learning Class (2016.7.8) from Microsoft Research by Chris Bishop, YouTube\n\n\nClassification (Prediction)\nChurn or No Churn(고객 이탈), That Is the Question\n\n행동 성향 모델링의 목표: 마케팅 응답, 서비스 탈퇴 등 다양한 행동 예측\n휴대폰 서비스 회사의 고객 유지 필요성: 신규 고객 유치 비용 대비 기존 고객 유지 비용의 상대적 높음\n이탈 가능성이 높은 고객 식별의 중요성: 유지 비용 최소화 및 이탈 예측을 통한 효율적인 혜택 제공 필요\n이탈 예측의 의미와 활용: 서비스 이탈 예측을 통해 고객 이탈 가능성을 예측하고 효율적인 대응 가능\n다양한 산업의 이탈 예측 활용: 통신, 유틸리티, 은행, 보험 등에서의 이탈 예측을 통한 비즈니스 전략 개발 및 운영 향상\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이해보다는 예측에 초점을 두는 deep learning\n\nImage recognition\nSpeech recognition\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression (Prediction)\nHow Much Will It Cost?\n\n앞서 분류는 범주형 속성의 값을 추정하는 반며, 회귀는 연속적인 값을 추정\n전통적인 통계적 모형의 근간\n예를 들어, 주택의 “가격”을 예측하는 경우\n\n주택의 크기, 방의 개수, 층수, 해당 지역의 평균 주택 가격, 해당 지역의 평균 주택 크기 등의 속성을 포함\n\n자동차의 “가격”을 예측하려며\n\n자동차의 연식, 주행 거리, 엔진 크기, 자동차 제조사, 문 개수 등의 속성을 포함",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/pollr.html",
    "href": "contents/pollr.html",
    "title": "설문",
    "section": "",
    "text": "설문 링크",
    "crumbs": [
      "설문"
    ]
  },
  {
    "objectID": "contents/two-cultures.html",
    "href": "contents/two-cultures.html",
    "title": "Two Cultures",
    "section": "",
    "text": "오랜동안 여러 분야에서 각자의 방식을 개발\nComputer Science\nStatistics\nBiostatistics\nEconomics\nEpidemiology\nPolitical Science\nEngineering\n\n\n\n서로 다른 용어를 쓰기도 하며, 그 분야에서 필요로하는 방식에 초점을 맞춤.\n\n서로 의사소통이 거의 없었음.\n\nData Science라는 이름하에 통합되어가는 과정 중\n\n\n\n\n컴퓨터 사이언스의 경우, 데이터로부터 패턴을 찾아 주로 분류나 예측을 위한 이론과 툴들이 개발되는 반면,\n과학자들은 예측보다는, 변수들 간의 진정한 관계 혹은 인과 관계를 탐구\n현재 이 둘은 소위 cross-fertilization을 지향하며 같이 발전, 통합되어가고 있음.",
    "crumbs": [
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#the-data-modeling-culture",
    "href": "contents/two-cultures.html#the-data-modeling-culture",
    "title": "Two Cultures",
    "section": "The Data Modeling Culture",
    "text": "The Data Modeling Culture\n\nY = f(X, random noise, parameters)\n\n모델의 타당도(validation): goodness-of-fit 테스트와 잔차의 검토\n통계학자의 98%\n\n\nParameter Estimation & Uncertainty\n\n현재 관찰된 데이터는 어떤 모집단(population)으로부터 (독립적으로) 발생된 표본(sample)이라고 가정\n\n변수들 간의 관계성(relationships)을 파악하기 위해, 데이터 모델링은 현상에 대한 모델을 사전 설정(assumptions)하고,\n\n예를 들어, 선형관계를 가정: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e\\)\n\n변수들의 값이 어떻게 발생하는지(generated)에 대한 가정을 세우고,\n\n예를 들어, Gaussian, Binormial distribution, …\n\n데이터와 가장 적합한(best fitted) 특정 모델을 선택 (즉, 파라미터 \\(\\beta\\)를 추정)\n\n즉, 위의 선형 함수가 X, Y의 1) 관계를 나타내고, 2) 예측을 위해 사용됨\n노이즈(noise)로부터 시그널(signal)을 분리: “true relationships”\n\n그 파라미터의 불확실성(uncertainty)을 추정\n\n모집단에 대한 추정이므로 불확실성이 존재\n\n\n예를 들어,\n\n\n\n\nSource: Suicide rates for girls are rising. Are smartphones to blame?\n\n\n \n\nSource: Racial differences in homicide rates are poorly explained by economics\n\n\n\n\n\n\n\n\n\n\n\nSource: Jiang, W., Lavergne, E., Kurita, Y., Todate, K., Kasai, A., Fuji, T., & Yamashita, Y. (2019). Age determination and growth pattern of temperate seabass Lateolabrax japonicus in Tango Bay and Sendai Bay, Japan. Fisheries science, 85, 81-98.\n\n단순한 선형 모델의 예\n다이아몬드 가격에 대한 예측 모델\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n가정들:\nX와 Y의 간의 true relationship: \\(Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\nNoise/residual의 분포: \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\n\nMean function: \\(E(Y | X = x_i) = \\beta_0 + \\beta_1 x_i\\)\nVariance function: \\(Var(Y | X = x_i) = \\sigma^2\\)\nDistribution: \\((Y | X = x_i) \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\)\n\n파라미터의 추정 및 불확실성:\n\n\\(\\hat{\\beta}_0 = 0.4, \\hat{\\beta}_1 = 5123, \\hat{\\sigma} = 300\\)\n95%의 확률로 \\(\\beta_0 \\in (0.3, 0.5), \\beta_1 \\in (4823, 5423)\\)\n\\(\\widehat{price}_i = 0.4 + 5123 \\cdot carat\\) : “평균적으로 다이아몬드가 1 carat 커질 때마다 $5123 비싸짐”\n노이즈로부터 관계에 관한 시그널을 추출한 것으로 볼 수 있음\n\n\n\n데이터 모델의 한계\n\n과연 가정한 모델이 자연/현상을 잘 모방하고 있는가?\n\n\nhas at its heart the belief that a statistician, by imagination and by looking at the data, can invent a reasonably good parametric class of models for a complex mechanism devised by nature.\n핵심은 통계학자가 상상력을 발휘하고 데이터를 살펴보면 자연이 고안한 복잡한 메커니즘에 대해 합리적으로 좋은 파라메트릭 클래스의 모델을 발명할 수 있다는 믿음입니다. (번역 by DeepL)\n\n\nThe belief in the infallibility of data models was almost religious. It is a strange phenomenon—once a model is made, then it becomes truth and the conclusions from it are infallible.\n데이터 모델의 무오류성에 대한 믿음은 거의 종교에 가까웠습니다. 일단 모델이 만들어지면 그것이 진리가 되고 그 모델에서 도출된 결론은 오류가 없다고 믿는 이상한 현상입니다. (번역 by DeepL)\n\n\n모델이 데이터에 얼마나 잘 맞는지에 대한 논의가 거의 없음\n\n특히, 주로 예-아니오로 답하는 적합도 테스트를 통해 모델의 적합성을 판단\n\n주로 톡창적인 확률 모형을 찾는데 주력\n모델을 데이터에 맞출 때 결론은 자연의 메커니즘이 아니라 모델의 메커니즘에 관한 것임\n모델이 자연을 제대로 모방하지 못하면 결론이 잘못될 수 있음\n\n데이터 모델의 다양성\n\n데이터 모델링의 가장 큰 장점: 입력 변수와 응답 간의 관계를 간단하고 이해하기 쉬운 그림으로 표현 가능\n하지만, 모델이 데이터에 동일하게 적합한 여러 모델이 존재\n\n적합성(goodness-of-fit)에 대한 기준이 통계적 검정을 통한 예-아니오로 판별\n\n관찰한 데이터로만 모델의 파라미터를 추정하기 때문에, 과적합이 발생하며, 새로운 데이터에 대한 예측 정확도가 떨어질 수 있음\n편향되지 않은 예측 정확도 추정치를 얻으려면 교차 검증(cross-validation)을 사용하거나 테스트 집합(test set)을 따로 보관할 필요가 있음\n\n모형의 가정에 위배\n\n일반적으로 의료 데이터, 재무 데이터와 같이 복합 시스템에서 생성된 데이터에 단순한 파라메트릭 모델을 적용하면 알고리즘 모델에 비해 정확성과 정보가 손실될 수 있음\n데이터의 발생 메커니즘(확률 분포)에 대한 가정이 성립되기 어려움\n모델을 가정하기보다는 데이터와 실제로 처한 문제로부터 해결책을 찾아 갈 필요가 있음.\n\n\n“If all a man has is a hammer, then every problem looks like a nail.”\n\n\n\n\n\n\n\nBayesian Models\n\n\n\n\n\n불확실성에 대한 가정에 대한 다른 접근\n        \n\nSource: McElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.",
    "crumbs": [
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#the-algorithmic-modeling-culture",
    "href": "contents/two-cultures.html#the-algorithmic-modeling-culture",
    "title": "Two Cultures",
    "section": "The Algorithmic Modeling Culture",
    "text": "The Algorithmic Modeling Culture\n\n자연이 복잡하고 신비하며 적어도 부분적으로는 알 수 없는 블랙박스에서 데이터를 생성한다고 가정\n데이터로부터 반응 y를 예측을 하기 위해 x에 작용하는 알고리즘 함수 f(x)를 찾고자 함\n\n가정: 데이터가 어떤 분포로부터 독립적으로 발생(i.i.d. Independent & identically distributed)\n모델의 타당도 지표: 예측 정확도 (predictive accuracy)\n통계학자의 2%\n\n예를 들어, 야구 선수의 연봉을 예측하기 위한 결정 트리 모델 (regression tree)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n다양한 방식의 여러 결정 트리 모델을 생성 후\n이들을 결합(aggregating)하여 평균을 내어 예측 정확도를 높일 수 있음\n\n새로운 연구 커뮤니티\n\n젊은 컴퓨터 과학자, 물리학자, 엔지니어와 나이든 몇 명의 통계학자 등이 주도\n1980년대 초 리처드 올슨의 연구를 시작으로 의료 데이터 분석에 조금씩 진출하기 시작\n1980년대 중반에 두 가지 강력한 새 알고리즘, 즉 신경망(neural network)과 의사 결정 트리(tree)가 등장\n1990년대 통계학에서도 smoothing spline 알고리즘, cross validation을 사용한 데이터에 대한 적용에 관한 연구가 존재\n1990년대 중반에는 Vapnik의 서포트 벡터 머신(support vector machine)이 등장\n예측 정확도를 목표로, 음성 인식, 이미지 인식, 비선형 시계열 예측, 필기 인식, 금융 시장 예측 등 데이터 모델을 적용할 수 없는 것이 분명한 복잡한 예측 문제를 해결\n\n\n\n머신 러닝 분야로부터의 레슨\n좋은 모델의 다양성 (multiplicity)\n\n예측도가 비슷한 전혀 다른 모델이 존재할 수 있는데\n이 모델들을 결합(aggregating)하면 예측 정확도를 높일 수 있으며, 단일한 모델로 환원할 수 있음\n\n단순성 대 정확성 (simplicity vs. accuracy)\nThe Occam’s Dilemma\n\n예측에 있어 정확성과 단순성(해석 가능성)은 상충됨\n\n정확도를 높이려면 더 복잡한 예측 방법이 요구\n단순하고 해석 가능한 함수는 예측력이 높지 못함\n\n예측 정확도를 먼저 추구한 후 그 이유를 이해하는 것이 더 낫다고 제안\n목표 지향적인 통계적 관점에서 보면 오컴의 딜레마는 존재하지 않음\n\n차원의 저주 (the curse of dimensionality)\nDigging It Out in Small Pieces\n\n전통적으로 변수가 많을수록 좋지 않다고 여겼으나,\nTree나 neural network에서는 변수가 많은 것이 문제가 되지 않고, 오히려 작은 정보들이 추가됨\n\n예를 들어, 30개의 예측 변수로부터 4차항들을 추가하면 약 40,000개의 새로운 변수가 생성됨\n이들의 정보는 분류에 도움이 되어 예측 정확도를 높일 수 있음\n\n\n\n   \n\n\n블랙박스로부터의 정보 추출\nThe goal is not interpretability, but accurate information.\n\n“정확성(accuracy)과 해석 가능성(interpretability) 중 하나를 선택해야 한다면, 그들은 해석 가능성을 택할 것입니다.  정확성과 해석 가능성 사이의 선택으로 질문을 구성하는 것은 통계 분석의 목표가 무엇인지에 대한 잘못된 해석입니다.  모델의 핵심은 응답(Y)과 예측 변수(X) 간의 관계에 대한 유용한 정보를 얻는 것입니다.  해석 가능성은 정보를 얻는 한 가지 방법입니다.  그러나 예측 변수와 응답 변수 간의 관계에 대한 신뢰할 수 있는 정보를 제공하기 위해 모델이 반드시 단순할 필요는 없으며, 데이터 모델일 필요도 없습니다.” (번역 by DeepL)\n\n\n예측 정확도가 높을수록 기본 데이터 메커니즘에 대한 더 신뢰할 수 있는 정보가 내재함\n예측 정확도가 낮으면 의심스러운 결론을 내릴 수 있음\n알고리즘 모델은 데이터 모델보다 예측 정확도가 더 높으며, 기본 메커니즘에 대한 더 나은 정보를 제공할 수 있음.\n\n예를 들어,\n\n의료 데이터와 같이 변수가 데이터에 비해 상대적으로 매우 많은 경우, 더 신뢰만한 변수들의 중요도를 추출할 수 있었음\n클러스터링과 같은 유사한 패턴을 보이는 군집들을 발견할 수 있었음\n유전자 분석처럼 데이터 모델을 생각하기 어려운 곳에 적용 가능; 머신러닝은 변수가 많을수록 좋으며, 과적합하지 않음",
    "crumbs": [
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#결론",
    "href": "contents/two-cultures.html#결론",
    "title": "Two Cultures",
    "section": "결론",
    "text": "결론\n\n통계의 목표는 데이터를 사용하여 예측하고 기본 데이터 메커니즘에 대한 정보를 얻는 것입니다. 데이터와 관련된 문제를 해결하기 위해 어떤 종류의 모델을 사용해야 하는지는 석판에 적혀 있지 않습니다. 제 입장을 분명히 말씀드리자면, 저는 데이터 모델 자체를 반대하는 것이 아닙니다. 어떤 상황에서는 데이터 모델이 문제를 해결하는 가장 적절한 방법일 수 있습니다. 하지만 문제와 데이터에 중점을 두어야 합니다. (번역 by DeepL)\n\n\n\nThe goals in statistics are to use data to predict and to get information about the underlying data mechanism. Nowhere is it written on a stone tablet what kind of model should be used to solve problems involving data. To make myposition clear, I am not against data models per se. In some situations they are the most appropriate wayto solve the problem. But the emphasis needs to be on the problem and on the data.",
    "crumbs": [
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#올바른-모델의-필요성",
    "href": "contents/two-cultures.html#올바른-모델의-필요성",
    "title": "Two Cultures",
    "section": "올바른 모델의 필요성",
    "text": "올바른 모델의 필요성\n\n\n천체의 움직임에 대한 모델\n프톨레마이오스(CE 100년 출생, 이집트)의 지동설 모델\n\n행성의 움직임에 수학적 모델은 매우 정확했으며, 천 년 넘게 활용되었음\n적절한 위치에 충분한 에피사이클을 배치하면 행성의 움직임을 매우 정확하게 예측할 수 있음\n\n\n\n\nSource: McElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.\n\n\n\n\n\nMaya Astronomy\n\n천체의 운행에 대한 정교한 계산법 개발\n예를 들어, 일식과 월식, 계절의 변화를 예측\n\n\n\n\n\n\n\nSource: YouTube, Maya Astronomy and Mathematics\n\n\n\n과학적 발견의 프로세스\n\n추측 &gt; 모델링 &gt; 관측/실험\n계산된 결과(예측)와 실제 결과(관측)의 비교를 통해 모델의 타당성을 판단\n결코 모델이 참인지 확신할 수 없음: 즉 true model은 존재하지 않을 수 있음\n\n뉴튼 역학 -&gt; 상대성 이론에 의해 수정\n\n모델이 틀린지에 대해서는 확신할 수 있음\n그럼에도 불구하고, 모델이 없는 과학은 위험할 수 있음\n\n두 문화의 결합\n원자의 움직임에 대한 슈뢰딩거 방정식\n\\(\\displaystyle \\left( -\\frac{{\\hbar^2}}{{2m}} \\frac{{1}}{{r^2}} \\frac{{\\partial}}{{\\partial r}} \\left( r^2 \\frac{{\\partial}}{{\\partial r}} \\right) - \\frac{{\\hbar^2}}{{2m r^2}} \\left( \\frac{{\\partial^2}}{{\\partial \\theta^2}} + \\cot \\theta \\frac{{\\partial}}{{\\partial \\theta}} + \\frac{{1}}{{\\sin^2 \\theta}} \\frac{{\\partial^2}}{{\\partial \\phi^2}} \\right) - \\frac{{k e^2}}{{r}} \\right) \\psi(r,\\theta,\\phi) = E \\psi(r,\\theta,\\phi)\\)\n\n이 방정식을 이용해 synthetic data를 생성: 현실에 대한 simulation\n이 데이터를 이용해 머신러닝 모델을 학습: 물질의 속성에 대해 학습\n그 모델을 이용해 많은 새로운 후보 물질들 중에 유용한 것을 매우 빠르게 걸러낼 수 있음\n예측 모형의 다양한 활용 가능성을 보여줌",
    "crumbs": [
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#causal-revolution",
    "href": "contents/two-cultures.html#causal-revolution",
    "title": "Two Cultures",
    "section": "Causal Revolution",
    "text": "Causal Revolution\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n\nAssociation\n\n관찰을 기반으로 규칙성 발견하고 예측\n올빼미가 쥐의 움직임을 관찰하고 잠시 후 쥐가 어디에 있을지를 파악\n컴퓨터 바둑 프로그램이 수백만 개의 바둑 게임 데이터베이스를 연구하여 어떤 수와 승률이 높은지 알아내는 것\n하나의 이벤트를 관찰하면 다른 이벤트를 관찰할 가능성이 달라진다면, 하나의 이벤트가 다른 이벤트와 연관되어 있다고 말할 수 있음\n“치약을 구매한 고객이 치실도 구매할 가능성이 얼마나 되는가?”; \\(P(치실 ~| 치약~)\\)\n통계의 핵심: 상관관계, 회귀\n올빼미는 쥐가 왜 항상 A 지점에서 B 지점으로 가는지 이해하지 못해도 훌륭한 사냥꾼이 될 수 있음\n위스키 한 병을 들고 있는 보행자가 경적을 울릴 때 다르게 반응할 가능성이 있다는 것을 기계가 스스로 파악할 수 있는가?\n\nAssociation 단계의 한계: 유연성과 적응성의 부족\n\n\n\n\nIntervention\n\n관찰을 넘어, 세상에 대한 개입\n“치약 가격을 두 배로 올리면 치실 판매량은 어떻게 될까?”\n데이터에는 없는 새로운 종류의 지식을 요구\n통계학의 언어로는 이 질문을 표현하는 것조차 불충분함\n수동적으로 수집된 데이터만으로는 이러한 질문에 대답할 수 없음\n\n과거의 데이터를 이용하면?\n과거에 가격이 두 배 비쌌을 때, 치실 판매량으로 추론?\n이전에 가격이 두 배 비쌌을 때, 다른 이유가 있었을 수 있음\n\n전통적으로 실험을 통해 해결\n정확한 인과 관계 모델이 있으면 관찰 데이터만으로도 가능; \\(P(치실 ~| ~do(치약~))\\)\n사실, 일상 생활에서 항상 개입을 수행: 어떻게(How) 하면 두통이 사라질까?\n\n\n\nCounterfactuals\n\n두통이 사라졌다면 왜(Why) 그럴까?\n약을 먹지 않았어도 두통이 사라졌을까?: 가상의 세계 (counterfactual world)\n“현재 치약을 구매한 고객이 가격을 두 배로 올려도 여전히 치약을 구매할 확률은 얼마인가?”\n우리는 현실 세계(고객이 현재 가격으로 치약을 구매했다는 것을 알고 있는)와 가상의 세계(가격이 두 배 높은 경우)와 비교\n보이는 세계  볼 수 있는 새로운 세계  볼 수 없는 세계(보이는 것과 모순)\n이를 위해서는 “이론” 또는 “자연의 법칙”이라고 볼 수 있는 근본적인 인과 과정의 모델이 필요\n\n전형적인 인과적 질문들\n\n\n\nHow effective is a given treatment in preventing a disease?\nWas it the new tax break that caused our sales to go up? Or our marketing campaign?\nWhat is the annual health-care costs attributed to obesity?\nCan hiring records prove an employer guilty of sex discrimination?\nI am about to quit my job, will I regret it?\n\n\n번역 by DeepL\n\n특정 치료법이 질병 예방에 얼마나 효과적일까요?\n새로운 세금 감면 혜택이 매출 상승의 원인이었을까요? 아니면 마케팅 캠페인 때문이었나요?\n비만으로 인한 연간 의료 비용은 얼마인가요?\n채용 기록으로 고용주의 성차별을 입증할 수 있나요?\n직장을 그만두려고 하는데 후회하게 될까요?",
    "crumbs": [
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#dag",
    "href": "contents/two-cultures.html#dag",
    "title": "Two Cultures",
    "section": "DAG",
    "text": "DAG\nDirected acyclic graph (DAG): 인과 관계 다이어그램\n\n\nSource: Causality: Models, Reasoning, and Inference (2000) by Judea Pearl",
    "crumbs": [
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#confounding",
    "href": "contents/two-cultures.html#confounding",
    "title": "Two Cultures",
    "section": "Confounding",
    "text": "Confounding\n\n\n신발을 신고 잠든 다음날 두통이 생긴다면?\nFork:Common Cause\n\nSource: Introduction to Causal Inference (ICI) by Brady Neal\n\n운동능력이 뛰어나면 지능이 떨어지는가?\nColider: Common Effect \n\n\n\n\n\n\n\n\n\nConfounding\n\n\n\n일반적으로, 표면적으로 드러난 변수간의 관계가 숨겨진 다른 변수들(lurking third variables)에 의해 매개되어 있어 진실한 관계가 아닌 경우, confounding 혹은 confounder가 존재한다고 함.\n극단적이지만 이해하지 쉬운 예로는\n\n초등학생 발 사이즈 → 독해력\n\n머리 길이 → 우울증\n\n\nSimpson’s paradox\n아래 첫번째 그림은 집단 전체에 대한 플랏이고, 두번째 그림은 나이대별로 나누어 본 플랏\n전체 집단을 보면 운동을 많이 할수록 콜레스테롤이 증가하는 것으로 보이나,\n나이대별로 보면, 상식적으로 운동이 긍정적 효과가 나타남.\n왜 그렇게 나타나는가?\n\nSource: The book of why by Judea Pearl\n\n\nCOVID-27\nSource: Introduction to Causal Inference (ICI) by Brady Neal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n학생들의 과제는 성적에 영향을 주는가?\nSource: National Education Longitudinal Study of 1988 (NELS:88)",
    "crumbs": [
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/eda.html",
    "href": "contents/eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n다음 네 가지의 시각화 패키지를 사용해서 그 차이를 확인해 볼 것임.\nMatplotlib 방식\n두 가지 interface를 제공하는데, 혼동을 야기함\npandas/seaborn 방식",
    "crumbs": [
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/eda.html#위도-경도-값의-활용",
    "href": "contents/eda.html#위도-경도-값의-활용",
    "title": "Exploratory Data Analysis",
    "section": "위도, 경도 값의 활용",
    "text": "위도, 경도 값의 활용\n\n\n\n\n\n\nShow Matplotlib styles\n\n\n\nplt.style.available\n\n\n\n# set the style\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\nlat, lon = housing['latitude'], housing['longitude']\n\n## MATLAB 스타일\n# figure() 함수를 직접 호출\nplt.figure(figsize=(7, 5)) # create a plot figure, figsize는 생략가능\n\n# scatter() 함수를 직접 호출\nplt.scatter(x=lon, y=lat, label=None, edgecolors=\"w\", linewidths=.4, alpha=0.3)\n\n# set the labels\nplt.xlabel('longitude')\nplt.ylabel('latitude')\nplt.axis('equal') # set the aspect of the plot to be equal\n\nplt.show()\n\n\n\n\n\n\n\n\n\n## 객체 방식\n# figure, axes라는 객체를 생성 후 메서드를 호출\nfig, ax = plt.subplots(figsize=(5, 3)) \n\n# ax의 메서드인 .scatter로 그래프를 그림\nax.scatter(x=lon, y=lat, label=None, edgecolors=\"w\", linewidths=.4, alpha=0.3)\n\n# ax의 메서드인 .set_xlabel, .set_ylabel로 라벨을 지정\nax.set_xlabel('longitude')\nax.set_ylabel('latitude')\nax.axis('equal')  # set the aspect of the plot to be equal\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# pandas의 plot 메서드를 사용하는 방식\nhousing.plot.scatter(x=\"longitude\", y=\"latitude\", alpha=0.3)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n# 다음과 동일함\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.3)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n\n\n\npandas가 제공하는 plots\n\n‘line’ : line plot (default)\n‘bar’ : vertical bar plot\n‘barh’ : horizontal bar plot\n‘hist’ : histogram\n‘box’ : boxplot\n‘kde’ : Kernel Density Estimation plot\n‘density’ : same as ‘kde’\n‘area’ : area plot\n‘pie’ : pie plot\n‘scatter’ : scatter plot (DataFrame only)\n‘hexbin’ : hexbin plot (DataFrame only)\n\n\n# NEAR OCEAN에 해당하는 부분만 시각화\nhousing2 = housing.query('ocean_proximity == \"NEAR OCEAN\"')\n\nhousing2.plot.scatter(x=\"longitude\", y=\"latitude\", alpha=0.3, figsize=(7, 5))\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n\n\n# Seaborn을 사용하는 방식\nplt.figure(figsize=(7, 5))\nsns.scatterplot(housing, x=\"longitude\", y=\"latitude\", hue=\"ocean_proximity\", alpha=0.5)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n\n\n\n  The San Francisco Bay Area\n\n\n\n\n집값과의 관계를 보기 위해, 집값을 컬러에 매핑하면,\n\nhousing.plot.scatter(\n    x=\"longitude\",\n    y=\"latitude\",\n    s=housing[\"population\"] / 100,  # point size\n    c=\"median_house_value\",  # color\n    alpha=0.3,  # transparency\n    cmap=\"flare\",  # color map\n    figsize=(7, 5),\n)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Annotation 추가\n\n\n\n\n\n아래 코드를 추가하여 도시 이름을 표시\npath = \"https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks_v1/data/california_cities.csv\"\ncities = pd.read_csv(path)\n\npopular_cities = cities.query('population_total &gt; 400000')\nlat, lon, names = popular_cities['latd'], popular_cities['longd'], popular_cities[\"city\"]\n\nplt.scatter(lon, lat, c=\"w\", alpha=1)\nfor name, lat, lon in zip(names, lat, lon):\n    plt.annotate(name, (lon, lat), xytext=(5, 5), textcoords=\"offset points\", color=\"k\")\n\n\n\n\n\n\n\n\n\nColor 사용에 관한 체계적 가이드\n\n\n\nChoosing color pallettes from Seaborn website",
    "crumbs": [
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/eda.html#데이터의-분포",
    "href": "contents/eda.html#데이터의-분포",
    "title": "Exploratory Data Analysis",
    "section": "데이터의 분포",
    "text": "데이터의 분포\nHistogram, density plot, boxplot\n\n# pandas의 DataFrame 메서드인 hist()를 사용\nhousing.hist(bins=50, figsize=(9, 6))\nplt.show()\n\n\n\n\n\n\n\n\n\n# density plot\nhousing.plot.density(bw_method=0.2, subplots=True, layout=(3, 3), sharex=False, sharey=False, figsize=(9, 6))\nplt.show()\n\n\n\n\n\n\n\n\n\n# Using matplotlib\nfig, ax = plt.subplots(3, 3, figsize=(9, 6))\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n\nfor i in range(3):\n    for j in range(3):\n        ax[i, j].hist(housing.iloc[:, i * 3 + j], bins=30)\n        ax[i, j].set_title(housing.columns[i * 3 + j])        \n\n\n\n\n\n\n\n\n\n# 한 변수의 각 레벨/카테고리별로 그리기, using pandas\nhousing.plot.hist(column=[\"median_house_value\"], by=\"ocean_proximity\", sharey=False, sharex=True, figsize=(6, 8), bins=50)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\n\n\n\n\n\n\nsource: R for Data Science\n\n\n\n\n\n# Using pandas\nhousing.plot.box(column=\"median_house_value\", by=\"ocean_proximity\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Using seaborn\nplt.figure(figsize=(9, 5))\nsns.boxplot(housing, x=\"ocean_proximity\", y=\"median_house_value\", hue=\"median_age_cat\", fill=False, gap=.2)\nplt.show()",
    "crumbs": [
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/eda.html#두-연속-변수간의-관계",
    "href": "contents/eda.html#두-연속-변수간의-관계",
    "title": "Exploratory Data Analysis",
    "section": "두 연속 변수간의 관계",
    "text": "두 연속 변수간의 관계\n\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"] / housing[\"households\"]\nhousing[\"bedrooms_per_household\"] = housing[\"total_bedrooms\"] / housing[\"households\"]\nhousing[\"people_per_household\"] = housing[\"population\"] / housing[\"households\"]\n\n또는 assign()를 사용\n\nhousing.assign(\n    rooms_per_household = lambda x: x[\"total_rooms\"] / x[\"households\"],\n    bedrooms_per_household = lambda x: x[\"total_bedrooms\"] / x[\"households\"],\n    people_per_household = lambda x: x[\"population\"] / x[\"households\"]\n).head(1)\n\n   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88               41.00       880.00          129.00   \n\n   population  households  median_income  median_house_value ocean_proximity  \\\n0      322.00      126.00           8.33           452600.00        NEAR BAY   \n\n  median_age_cat  rooms_per_household  bedrooms_per_household  \\\n0          40-52                 6.98                    1.02   \n\n   people_per_household  \n0                  2.56  \n\n\n\nhousing.value_counts(\"median_house_value\").sort_index()\n\nmedian_house_value\n14999.00       4\n17500.00       1\n22500.00       4\n25000.00       1\n            ... \n499000.00      1\n499100.00      1\n500000.00     27\n500001.00    965\nName: count, Length: 3842, dtype: int64\n\n\n\n# median_house_value &lt; 500001 값으로 필터링\nhousing = housing.query('median_house_value &lt; 500001')\n\n\nxvar = \"rooms_per_household\"\nyvar = \"median_house_value\"\n\n# matplotlib의 객체 방식\nfig, ax = plt.subplots()\nhousing.plot.scatter(ax=ax, x=xvar, y=yvar, alpha=0.1, figsize=(7, 5))\n\n# fitted line of natural spline: 아래 노트 참고\nnspline_fit = nspline(housing, xvar, yvar, df_n=15).sort_values(xvar)\nnspline_fit.plot.line(ax=ax, x=xvar, y=yvar, c=\".3\", figsize=(7, 5))\n\nplt.xlim(0, 10)\nplt.ylim(0, 500000)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNatural spline fit\n\n\n\n\n\ndef nspline(df, x, y, df_n=5):\n    from statsmodels.formula.api import ols\n\n    df = df[[x, y]].dropna()\n    formula = f\"{y} ~ cr({x}, df={df_n})\"\n    df[y] = ols(formula, data=df).fit().fittedvalues\n\n    return df\n\n\n\n해변에 가까운 정도(ocean_proximity) 따라 나누어 보면,\n\n# divide plots by ocean_proximity\nfig, ax = plt.subplots(1, 4, figsize=(12, 3))\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n\ntypes = ['NEAR OCEAN', '&lt;1H OCEAN', 'NEAR BAY', 'INLAND']\nfor i, op in enumerate(types):\n\n    df = housing.query(f'ocean_proximity == \"{op}\"')\n    df.plot.scatter(ax=ax[i], x=xvar, y=yvar, alpha=0.1)\n\n    # fitted line of natural spline\n    nspline_fit = nspline(df, xvar, yvar, df_n=15).sort_values(xvar)\n    nspline_fit.plot.line(ax=ax[i], x=xvar, y=yvar, c=\".3\")\n    \n    ax[i].set_title(op)\n    ax[i].set_xlim(1, 12)\n    ax[i].set_ylim(0, 500000)\n\nplt.show()\n\n\n\n\n\n\n\n\nseaborn.object 방식\n\n(\n    so.Plot(housing, x='rooms_per_household', y='median_house_value')\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet('ocean_proximity')\n    .limit(x=(1, 12), y=(0, 500000))\n    .layout(size=(8.9, 3))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n범주형 변수의 순서 할당\n\n\n\npd.Categorical을 사용하여 범주형 변수의 순서를 지정할 수 있음.\nhousing[\"ocean_proximity\"] = pd.Categorical(\n    housing[\"ocean_proximity\"],\n    categories=[\"NEAR BAY\", \"NEAR OCEAN\", \"&lt;1H OCEAN\", \"INLAND\"],\n    ordered=True,\n)\n\n\n해변에 가까운 정도(ocean_proximity)와 집의 연령(median_age_cat)에 따라 나누어 보면,\n\n(\n    so.Plot(\n        housing.query('ocean_proximity != \"ISLAND\"'),\n        x=\"rooms_per_household\",\n        y=\"median_house_value\",\n    )\n    .add(so.Dots(alpha=0.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet(col=\"ocean_proximity\", row=\"median_age_cat\")\n    .limit(x=(1, 12), y=(0, 500000))\n    .layout(size=(8, 8))\n)\n\n\n\n\n\n\n\n\n해안에 가까운 정도(ocean_proximity)가 고정되어 있을 때, 그 안에서 여전히\n경도(longitude)가 작을수록, 집값(median_house_value)이 변화하는지 살펴보면,\n\n(\n    so.Plot(\n        housing.query('ocean_proximity != \"ISLAND\"'),\n        x='longitude',\n        y='median_house_value')\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet(\"ocean_proximity\")\n    .layout(size=(8.9, 3))\n    .share(x=False)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nPopulation과의 관계가 있을까?\n\n\nShow the code\nhousing2 = housing.copy()\nhousing2[\"ocean_proximity\"] = (\n    housing\n    .query('ocean_proximity not in [\"ISLAND\", \"INLAND\"]')[\"ocean_proximity\"]\n    .cat.remove_unused_categories()\n)\n\n(\n    so.Plot(housing2, x='longitude', y='population')\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet(\"ocean_proximity\")\n    .layout(size=(8.9, 3))\n    .share(x=False)\n    .limit(y=(0, 3000))\n)\n\n\n\n\n\n\n\n\n\nPanelized spline fit: pyGAM 참고",
    "crumbs": [
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/logistic.html",
    "href": "contents/logistic.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Load Packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\nfrom sbcustom import *\n\n# statistics\nimport statsmodels.api as sm\nOptions\n# pandas options\npd.set_option(\"mode.copy_on_write\", True)\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 8\n\n# Numpy options\nnp.set_printoptions(precision = 2, suppress=True)"
  },
  {
    "objectID": "contents/logistic.html#emperical-probability-odds",
    "href": "contents/logistic.html#emperical-probability-odds",
    "title": "Logistic Regression",
    "section": "Emperical probability/ Odds",
    "text": "Emperical probability/ Odds\n이제 y를 직접 예측하기보다, 확률을 예측하는 방식을 취하면,\n\n각 두께를 가진 나무들의 개수 (m)와\n그 중에 태풍으로 죽은 나무의 수 (died)를 고려하면,\n나무의 두께에 따라 죽은 나무 수의 비율 (p= died/m)을 계산할 수 있음. 이를 emperical probability라고 함.\n사실, 이 p는 binary response (0, 1)의 conditional mean(평균)인데,\n통계적으로 표현하면 \\(E(Y|d=d_i)\\)이며 선형모형의 mean function를 제공. (모형은 variance function과 함께 설정됨)\n\n\nblowbs_bn = blowbs.groupby(\"d\")[\"y\"].agg([(\"died\", \"sum\"), (\"m\", \"count\"), (\"p\", \"mean\")]).reset_index()\n# blowbs_bn = blowbs_bn.assign(\n#     p = (blowbs_bn.died + 0.1)/ (blowbs_bn.m + 0.2),  # 확률이 0이거나 1이 되는 것을 피하기 위해 작은 값을 추가할 수 있음.\n# )\nblowbs_bn\n\n       d  died   m    p\n0   5.00     7  90 0.08\n1   6.00     7  92 0.08\n2   7.00    18  91 0.20\n3   8.00    13  70 0.19\n..   ...   ...  ..  ...\n21 27.00     1   1 1.00\n22 28.00     2   2 1.00\n23 29.00     1   2 0.50\n24 32.00     1   1 1.00\n\n[25 rows x 4 columns]\n\n\n\n\ncode\n(\n    so.Plot(blowbs_bn, x='d', y='p')\n    .add(so.Dots(color=\"orangered\"), pointsize='m')\n    .add(so.Dots(alpha=.3), so.Jitter(y=.1), x=blowbs.d, y=blowbs.y)\n    .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(), so.PolyFit(1))\n    .limit(y=(-0.2, 1.2))\n    .layout(size=(8, 5))\n    .label(title='Emperical Probability')\n)\n\n\n\n\n\n\n\n\n\n\nOLS estimate으로도 충분한가?\n1차보다는 고차 다항함수로 fit한다면?\n\n우선 d를 log2 변환해서 살펴보면,\n\n\ncode\nblowbs_bn[\"log2d\"] = np.log2(blowbs_bn[\"d\"])\n(\n    so.Plot(blowbs_bn, x='log2d', y='p')\n    .add(so.Dots(color=\"orangered\"), pointsize='m')\n    .add(so.Dots(alpha=.3), so.Jitter(y=.1), x=blowbs.log2d, y=blowbs.y)\n    .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(), so.PolyFit(1))\n    .limit(y=(-0.2, 1.2))\n    .layout(size=(8, 5))\n    .label(title='Emperical Probability')\n)\n\n\n\n\n\n\n\n\n\n만약, 이 proportion p (= died/m)를 log2d로 예측하는 모형을 만들어, 그 잔차를 살펴보면,\n\n\ncode\nmod_prob = ols('p ~ log2d', data = blowbs_bn).fit()\nmod_prob_plot = blowbs_bn.assign(resid = mod_prob.resid)\n\n(\n    so.Plot(mod_prob_plot, x='log2d', y='resid')\n    .add(so.Dot())\n    .add(so.Line(color=\".3\", linestyle=\":\"), so.Agg(lambda x: 0))\n    .label(title='Residual Plot')\n)\n\n\n\n\n\n\n\n\n\n위에서 살펴본 OLS의 문제들 즉,\n\n잔차에 패턴이 보인다는 것은 충분히 좋은 모형이 아니라는 것을 의미하고,\n예측값이 확률을 의미하지 못할 수 있음.\n잔차의 분산이 x값에 따라 달라져 모집단에 대한 추론을 어렵게 함.\n\n이런 문제들을 해결하고 예측값이 분명한 “확률”의 의미를 품도록 여러 방식이 제시되는데 주로 사용되는 것이 logistic regression임.\n\n\n\n\n\n\nImportant\n\n\n\nBinary outcome을 예측하는 모형은 binary 값을 예측하는 것이 아니고, 확률 값을 예측하는 것임.\n이후에 이를 이용해 binary outcome을 예측.\n\n예를 들어, 두께가 5cm인 (특정 종의) 나무가 태풍에 쓰러질 확률(true probability)을 파악하고자 함.\n이 때, 관측값은 5cm인 나무 중 쓰러진 나무의 “비율”이고, 이 관측치들로부터 true probability를 추정하고자 함.\n\nOdds의 정의: 실패할 확률 대비 성공할 확률의 비율\n\\(\\displaystyle odds = \\frac{p}{1-p}\\)\n예를 들어, 5cm 두께의 나무는 90그루 중 7그루가 죽었으므로 83그루는 살았음.\n즉, 죽음:생존 = 7:83 \\(\\approx\\) 1:12 이고 odds = 7/83 = 0.084; 생존할 가능성 대비 죽을 가능성이 8.4%임.\n확률로 표현하면, \\(odds = \\frac{\\frac{7}{90}}{1 - \\frac{7}{90}} = \\frac{7}{90-7} = \\frac{7}{83}\\)\n확률과 odds, logit(log odds)의 관계\n\n\n\n\nblowbs_bn = blowbs_bn.assign(odds = lambda x: x.p / (1 - x.p))\n# p = 1인 경우 odds가 무한대가 되므로 편의상 inf 값을 50으로 대체\nblowbs_bn[\"odds\"] = blowbs_bn[\"odds\"].apply(lambda x: 50 if x == np.inf else x)\nblowbs_bn\n\n       d  died   m    p  log2d  odds\n0   5.00     7  90 0.08   2.32  0.08\n1   6.00     7  92 0.08   2.58  0.08\n2   7.00    18  91 0.20   2.81  0.25\n3   8.00    13  70 0.19   3.00  0.23\n..   ...   ...  ..  ...    ...   ...\n21 27.00     1   1 1.00   4.75 50.00\n22 28.00     2   2 1.00   4.81 50.00\n23 29.00     1   2 0.50   4.86  1.00\n24 32.00     1   1 1.00   5.00 50.00\n\n[25 rows x 6 columns]\n\n\n이 odds를 선형모형으로 나무두께로 예측하려고 하는 것인데, 예를 들어,\n\\(\\widehat{odds} = b_0 + b_1 \\cdot log_{2}(d)\\)\nodds 값의 범위는 (0, \\(\\infty\\))이므로, 선형모형으로 fit하는 것이 적절하지 않음.\n한편, odds를 log 변환하면, 그 범위는 (\\(-\\infty\\), \\(\\infty\\))가 되어 선형모형으로 fit하는 것이 적절해짐.\n\\(\\displaystyle logit(p):= log(odds) = log\\left(\\frac{p}{1-p}\\right)\\)\n이 때, 예측모형은 \\(\\displaystyle log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right) = b_0 + b_1 \\cdot log_{2}(d)\\)\n즉, logit은 예측변수 \\(x\\) 와 선형적으로 연결되는데 반해, 확률 \\(p\\) 는 예측변수 \\(x\\) 와 비선형적 관계를 맺음\n\n\n\n\n\n\n\n\n\nlog odds값을 구해보면,\n\nblowbs_bn = blowbs_bn.assign(log_odds = lambda x: np.log(x.odds))\nblowbs_bn\n\n       d  died   m    p  log2d  odds  log_odds\n0   5.00     7  90 0.08   2.32  0.08     -2.47\n1   6.00     7  92 0.08   2.58  0.08     -2.50\n2   7.00    18  91 0.20   2.81  0.25     -1.40\n3   8.00    13  70 0.19   3.00  0.23     -1.48\n..   ...   ...  ..  ...    ...   ...       ...\n21 27.00     1   1 1.00   4.75 50.00      3.91\n22 28.00     2   2 1.00   4.81 50.00      3.91\n23 29.00     1   2 0.50   4.86  1.00      0.00\n24 32.00     1   1 1.00   5.00 50.00      3.91\n\n[25 rows x 7 columns]\n\n\n정리하면, 아래와 같은 관측값들과 emperical probability/log odds을 함께 살펴보면,\n\n\ncode\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.p, size=blowbs_bn.m, sizes=(20, 200), ax=ax)\n\ndef jitter(values, j):\n    return values + np.random.normal(0, j, values.shape)\n\nsns.scatterplot(x=blowbs.log2d, y=jitter(blowbs.y, 0.02), alpha=.3, ax=ax)\n\nfor i, row in blowbs_bn.iterrows():\n    ax.annotate(f\"{row.died:n}/{row.m:n}\", xy=(row.log2d, row.p), xytext=(row.log2d, row.p-0.05), size=9)\n\nax.set_xticks(blowbs_bn.log2d.unique())\nax.set_xticklabels(blowbs_bn.log2d.unique().round(2))\nax.tick_params(axis='x', rotation=45)\nax.set_title(\"Emperical Probability\")\nax.legend_.remove()\n\n\n\n\n\n\n\n\n\nLogit 값(red)을 추가해서 그리면,\n\n\ncode\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.p, size=blowbs_bn.m, sizes=(20, 200), ax=ax, legend=False)\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.log_odds, size=blowbs_bn.m, sizes=(20, 200), color=\"red\", ax=ax)\n\ndef jitter(values, j):\n    return values + np.random.normal(0, j, values.shape)\n\nsns.scatterplot(x=blowbs.log2d, y=jitter(blowbs.y, 0.02), alpha=.3, ax=ax)\n\nfor i, row in blowbs_bn.iterrows():\n    ax.annotate(f\"logit{row.died:n}/{row.m:n}\", xy=(row.log2d, row.log_odds), xytext=(row.log2d, row.log_odds-0.4), size=9)\n\nax.set_xticks(blowbs_bn.log2d.unique())\nax.set_xticklabels(blowbs_bn.log2d.unique().round(2))\nax.tick_params(axis='x', rotation=45)\nax.set_ylabel(\"P & Logit\")\nax.set_title(\"Emperical Probability and Logit\")\nax.legend_.remove()\n\n# # polyfit 5\n# x = np.linspace(blowbs_bn.log2d.min(), blowbs_bn.log2d.max(), 100)\n# y = np.polyval(np.polyfit(blowbs_bn.log2d, blowbs_bn.log_odds, 5), x)\n# sns.lineplot(x=x, y=y, ax=ax, color=\".6\")\n\n# # polyfit 1\n# y = np.polyval(np.polyfit(blowbs_bn.log2d, blowbs_bn.log_odds, 1), x)\n# sns.lineplot(x=x, y=y, ax=ax, color=\".6\")\n\n\n\n\n\n\n\n\n\n위의 logit값을 선형모형으로 예측하는 모형: \\(\\displaystyle log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right) = b_0 + b_1 \\cdot log_{2}(d)\\)\n파라미터 \\(b_0, b_1\\)의 추정은 잔차들의 제곱의 합을 최소로 하는 OLS 방식은 부적절하며, 대신에 Maximum Likelihood Estimation을 사용함.\n\n아이디어는 관측치가 전체적으로 관찰될 likelihood가 최대가 되도록 \\(b_0, b_1\\)을 선택하는 것임\n이를 위해서 적절한 확률모형을 결합시켜야 함.\n기본적으로 선택하는 확률모형은 binominal distribution (이항분포)임.\n이항분포의 평균과 표준편차는 확률 p와 n에 의해 바뀜.\n관찰값은 이항분포로부터 발생했다고 가정함으로써, 실제 관찰값들의 randomness(잔차들)을 모형에 반영할 수 있음.\n\n참고: 이항분포 (Y: 사건의 횟수)\n\n\nblowbs_bn.head(3)\n\n     d  died   m    p  log2d  odds  log_odds\n0 5.00     7  90 0.08   2.32  0.08     -2.47\n1 6.00     7  92 0.08   2.58  0.08     -2.50\n2 7.00    18  91 0.20   2.81  0.25     -1.40\n\n\n\n각 likelihood는 관측치들이 모두 독립적으로 발생했다고 가정했을 때의 확률값이고,\n모든 데이터가 관찰될 likelihood:\nLik = \\(_{90}C_7 \\cdot p_1^7 \\cdot (1-p_1)^{83} \\cdot _{92}C_7 \\cdot p_2^7 \\cdot (1-p_2)^{85} \\cdot _{91}C_{18} \\cdot p_3^{18} \\cdot (1-p_3)^{73} \\cdots\\)\n이 때, 모형을 예측변수 x의 1차 다항함수로 fit한다면,   \\(\\displaystyle log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\cdot x\\) 인데,\n변형하면   \\(\\displaystyle p = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot x)}}\\)\nLik \\(\\propto\\) \\(\\displaystyle \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.32)}}\\right)^7 \\cdot \\left(1 - \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.32)}}\\right)^{83} \\cdot \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.58)}}\\right)^7 \\cdot \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.58)}}\\right)^{85} \\cdots\\)\n이 Likelihood를 최대가 되도록 \\(\\beta_0, \\beta_1\\)의 추정치를 찾는 것이 Maximum Likelihood Estimation임.\n\nimport statsmodels.formula.api as smf\n\n# Binomial response\nmod_bn = smf.glm('died + I(m-died) ~ log2d', data=blowbs_bn, family=sm.families.Binomial()).fit()\n\n# Binary response: 동일한 분석\nmod = smf.logit('y ~ log2d', data=blowbs).fit() # 아래와 동일하나 glm과 다른 정보를 제공\nmod = smf.glm('y ~ log2d', data=blowbs, family=sm.families.Binomial()).fit()\nprint(mod.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.499165\n         Iterations 6\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                  659\nModel:                            GLM   Df Residuals:                      657\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -328.95\nDate:                Mon, 11 Dec 2023   Deviance:                       657.90\nTime:                        01:39:36   Pearson chi2:                     679.\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.2599\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -7.8162      0.628    -12.437      0.000      -9.048      -6.584\nlog2d          2.2408      0.190     11.773      0.000       1.868       2.614\n==============================================================================\n\n\nModel deviance:\n\\(D_k = -2[log(likelihood_k) - log(likelihood_{perfect})]\\)\n\nPerfect model의 likelihood: 1\n\n\n# deviance for an intercept only model\nmod.null_deviance\n\n856.2073760911842\n\n\n\nPseudo R-squared: \\(\\displaystyle \\frac{Null~Deviance - Deviance}{Null~Deviance}\\)\nCox-Snell’s Pseudo R-squared\nNagelkerke’s Pseudo R-squared\n\n위 fitted model의 예측값들: fitted line\n\n\ncode\nfig, ax = plt.subplots(1, 1, figsize=(9, 5))\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.p, size=blowbs_bn.m, sizes=(20, 200), ax=ax, legend=False)\n\ndef jitter(values, j):\n    return values + np.random.normal(0, j, values.shape)\n\nsns.scatterplot(x=blowbs.log2d, y=jitter(blowbs.y, 0.02), alpha=.3, ax=ax)\n\nfor i, row in blowbs_bn.iterrows():\n    ax.annotate(f\"{row.died:n}/{row.m:n}\", xy=(row.log2d, row.p), xytext=(row.log2d, row.p-0.05), size=9)\n\nax.set_xticks(blowbs_bn.log2d.unique())\nax.set_xticklabels(blowbs_bn.log2d.unique().round(2))\n# x-axis with 45 degree rotation\nax.tick_params(axis='x', rotation=45)\n\n# fitted line\nsns.lineplot(x=blowbs.log2d, y=mod.fittedvalues, ax=ax, color=\".6\")\n\nplt.show()"
  },
  {
    "objectID": "contents/logistic.html#예측값",
    "href": "contents/logistic.html#예측값",
    "title": "Logistic Regression",
    "section": "예측값",
    "text": "예측값\nLogistic regression에서는 세 가지 타입의 예측값들이 있음.\n\nPredicted probability:   \\(\\displaystyle \\hat{p} = \\frac{1}{1+e^{-(b_0 + b_1 \\cdot x)}}\\)\nOdds:   \\(\\displaystyle odds = \\frac{\\hat{p}}{1 - \\hat{p}} = e^{b_0 + b_1 \\cdot x}\\)\nLog odds:   \\(\\displaystyle logit = b_0 + b_1 \\cdot x\\)\n\n\n\ncode\nblowbs_bn_pred = blowbs_bn.assign(\n    pred_prob = mod_bn.fittedvalues,\n    pred_odds = lambda x: x.pred_prob / (1 - x.pred_prob),\n    pred_logit = lambda x: np.log(x.pred_odds)\n)\nblowbs_bn_pred.iloc[:, 1:]\n\n\n    died   m    p  log2d  odds  log_odds  pred_prob  pred_odds  pred_logit\n0      7  90 0.08   2.32  0.08     -2.47       0.07       0.07       -2.61\n1      7  92 0.08   2.58  0.08     -2.50       0.12       0.13       -2.02\n2     18  91 0.20   2.81  0.25     -1.40       0.18       0.22       -1.53\n3     13  70 0.19   3.00  0.23     -1.48       0.25       0.33       -1.09\n..   ...  ..  ...    ...   ...       ...        ...        ...         ...\n21     1   1 1.00   4.75 50.00      3.91       0.94      17.09        2.84\n22     2   2 1.00   4.81 50.00      3.91       0.95      19.22        2.96\n23     1   2 0.50   4.86  1.00      0.00       0.96      21.53        3.07\n24     1   1 1.00   5.00 50.00      3.91       0.97      29.59        3.39\n\n[25 rows x 9 columns]\n\n\n예를 들어, 두께(log2d)가 3인 나무의 경우,\n\nProbabiliy: 태풍에 쓰러질 확률은 25%로 예측되며,\nOdds: 태풍에 쓰러지지 않을 가능성 대비 쓰러질 가능성의 비율은 1:0.33이므로 대략 3:1로 예측됨. 즉 다시 말하면, 1 그루가 쓰러진다면 3 그루는 쓰러지지 않을 것으로 예측함.\n\nOdds가 1이면 event의 확률(p)이 0.5\nOdds가 1보다 작으면 event의 확률(p)이 0.5보다 작고,\nOdds가 1보다 크면 event의 확률(p)이 0.5보다 큼.\n\nLog odds (logit): 확률 p의 [0, 1]의 값을 무한한 값으로 늘려 linearly fit할 수 있게 함."
  },
  {
    "objectID": "contents/logistic.html#모형의-파라미터-해석",
    "href": "contents/logistic.html#모형의-파라미터-해석",
    "title": "Logistic Regression",
    "section": "모형의 파라미터 해석",
    "text": "모형의 파라미터 해석\nOdds의 비율 (odds ratio, OR)을 통해 해석\n\n\\(\\displaystyle odds: \\frac{\\hat{p}}{1 - \\hat{p}} = e^{b_0 + b_1 \\cdot x}=e^{b_0}\\cdot e^{b_1 \\cdot x}\\)  로부터\n\\(x\\)가 1 증가할 때 odds의 비율: \\(\\displaystyle odds ~ratio: \\frac{\\frac{\\hat{p_2}}{1 - \\hat{p_2}}}{\\frac{\\hat{p_1}}{1 - \\hat{p_1}}}  = e^{b_1 \\cdot (x+1) - b_1 \\cdot x} = e^{b_1}\\)\n즉, \\(x\\)가 1 증가하면, odds가 “몇 배”로 증가하는지를 나타냄.\n따라서, odds ratio가 1보다 크면 (\\(b_1\\)이 양수) \\(x\\)가 1 증가할 때, event의 odds가 커지며,\nodds ratio가 1보다 작으면 (\\(b_1\\)이 음수) event의 odds가 줄어듬.\n\n위의 경우, \\(\\displaystyle \\widehat{odds} = e^{-7.82 + 2.24 \\cdot log_2(d)}\\) 이므로 odds ratio = \\(e^{2.24} = 9.4\\)\n\n해석하면, 나무의 두께가 (log2 scale로) 1 늘어남 (2배 증가)에 따라 태풍에 나무가 쓰러질 odds가 9.4배 증가함\n다시 말하면, 나무의 두께가 (log2 scale로) 1 늘어남 (2배 증가)에 따라 태풍에 나무가 쓰러지지 않을 가능성 대비 쓰러질 가능성이 9.4배 증가함.\n나무의 두께 (원래 d)로 말하면, 두께가 10% 두꺼워지면, \\(e^{2.24 \\cdot log_2(1.1)}=1.36\\) 배, 즉 odds가 36% 증가함.\n\n\\(b_0\\): d = 1일 때의 odds: \\(\\displaystyle e^{-7.82 + 2.34 \\cdot 0} = e^{-7.82} = 0.004\\), 즉 두께가 1cm 일 때 태풍에 나무가 쓰러지지 않을 가능성 대비 쓰러질 가능성은 0.004임."
  },
  {
    "objectID": "contents/logistic.html#모형의-예측력",
    "href": "contents/logistic.html#모형의-예측력",
    "title": "Logistic Regression",
    "section": "모형의 예측력",
    "text": "모형의 예측력\n\n확률 예측에 대한 정확성 (evaluating predicted probability)\nBinary outcome에 대한 예측 정확성 (evaluating predicted class)\n\n\nEvaluattion of predicted probability\n\n\n\n\n\n\nImportant\n\n\n\n이제 이 모형이 좋은 모형인지 살펴보기 위해 residual, 잔차를 살펴볼 수 있는가?\nBinomial version\n\nPearson residual: \\(\\displaystyle \\frac{actual ~ count ~ - predicted ~ count}{SD ~ of ~ count} = \\frac{y_i - m_i \\hat{p}_i}{\\sqrt{m_i \\hat{p}_i (1-\\hat{p}_i)}}\\)\nDeviance residual: \\(\\displaystyle sign(y_i - m_i\\hat{p}_i) \\sqrt{2[y_i log(\\frac{y_i}{m_i\\hat{p}_i}) + (m_i-y_i) log(\\frac{m_i - y_i}{m_i-m_i\\hat{p}_i})]}\\)\n\nBinary version\n\nPearson residual: \\(\\displaystyle \\frac{y_i - \\hat{p}_i}{\\sqrt{\\hat{p}_i (1-\\hat{p}_i)}}\\)\nDeviance residual: \\(\\displaystyle sign(y_i - \\hat{p}_i) \\sqrt{-2[-y_i log(\\hat{p}_i) - (1-y_i) log(1-\\hat{p}_i)]}\\)\n\nDeviance를 이용해 OLS에서의 \\(R^2\\)와 비슷한 개념을 구성\n\nCox-Snell \\(R^2\\)\nNagelkerke \\(R^2\\)\n\n“Coefficient of discrimination” (Tjur, 2009): average \\(\\hat{p}\\) when \\(y=1\\) - average \\(\\hat{p}\\) when \\(y=0\\)\n\n\nBinomial response에 대해 residual들을 살펴보면,\nsmf.glm('died + I(m-died) ~ log2d', data=blowbs_bn, family=sm.families.Binomial())\n\nblowbs_bn_resid = blowbs_bn.assign(\n    prob_pred = mod_bn.fittedvalues,\n    died_pred = lambda x: x.prob_pred * x.m,\n    count_resid = lambda x: x.died - x.died_pred,\n    pearson_resid = mod_bn.resid_pearson,\n    deviance_resid = mod_bn.resid_deviance,\n)\nblowbs_bn_resid\n\n       d  died   m    p  log2d  odds  log_odds  prob_pred  died_pred  \\\n0   5.00     7  90 0.08   2.32  0.08     -2.47       0.07       6.15   \n1   6.00     7  92 0.08   2.58  0.08     -2.50       0.12      10.74   \n2   7.00    18  91 0.20   2.81  0.25     -1.40       0.18      16.26   \n3   8.00    13  70 0.19   3.00  0.23     -1.48       0.25      17.56   \n..   ...   ...  ..  ...    ...   ...       ...        ...        ...   \n21 27.00     1   1 1.00   4.75 50.00      3.91       0.94       0.94   \n22 28.00     2   2 1.00   4.81 50.00      3.91       0.95       1.90   \n23 29.00     1   2 0.50   4.86  1.00      0.00       0.96       1.91   \n24 32.00     1   1 1.00   5.00 50.00      3.91       0.97       0.97   \n\n    count_resid  pearson_resid  deviance_resid  \n0          0.85           0.36            0.35  \n1         -3.74          -1.21           -1.29  \n2          1.74           0.48            0.47  \n3         -4.56          -1.26           -1.30  \n..          ...            ...             ...  \n21         0.06           0.24            0.34  \n22         0.10           0.32            0.45  \n23        -0.91          -3.13           -1.88  \n24         0.03           0.18            0.26  \n\n[25 rows x 12 columns]\n\n\n\n(\n    so.Plot(blowbs_bn_resid, x='log2d', y='pearson_resid')\n    .add(so.Dot())\n    .add(so.Dot(color=\"orangered\", alpha=.5), y = 'deviance_resid')\n    .label(y=\"Pearson / Deviance Residuals\")\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(blowbs_bn_resid, x='prob_pred', y='p',)\n    .add(so.Dot())\n    .add(so.Line(color=\".6\"), y = 'prob_pred') # y = x line\n    .label(x=\"Predicted Probability\", y=\"Emperical Probability\")\n)\n\n\n\n\n\n\n\n\nBinary response에 대해 residual들을 살펴보면,\nsmf.glm('y ~ log2d', data=blowbs, family=sm.families.Binomial())\n\nblowbs_resid = blowbs.assign(\n    prob_pred = mod.fittedvalues,\n    prob_resid = mod.resid_response,\n    pearson_resid = mod.resid_pearson,\n    deviance_resid = mod.resid_deviance\n)\nblowbs_resid.sort_values(\"d\")\n\n         d    s  y           spp  log2d  prob_pred  prob_resid  pearson_resid  \\\n2102  5.00 0.45  0  black spruce   2.32       0.07       -0.07          -0.27   \n724   5.00 0.18  0  black spruce   2.32       0.07       -0.07          -0.27   \n723   5.00 0.18  1  black spruce   2.32       0.07        0.93           3.69   \n1687  5.00 0.37  0  black spruce   2.32       0.07       -0.07          -0.27   \n...    ...  ... ..           ...    ...        ...         ...            ...   \n2634 28.00 0.57  1  black spruce   4.81       0.95        0.05           0.23   \n1784 29.00 0.38  1  black spruce   4.86       0.96        0.04           0.22   \n1079 29.00 0.25  0  black spruce   4.86       0.96       -0.96          -4.64   \n3455 32.00 0.80  1  black spruce   5.00       0.97        0.03           0.18   \n\n      deviance_resid  \n2102           -0.38  \n724            -0.38  \n723             2.32  \n1687           -0.38  \n...              ...  \n2634            0.32  \n1784            0.30  \n1079           -2.50  \n3455            0.26  \n\n[659 rows x 9 columns]\n\n\n\n(\n    so.Plot(blowbs_resid, x='log2d')\n    .pair(y=[\"prob_resid\", \"pearson_resid\", \"deviance_resid\"])\n    .add(so.Dots(alpha=.3), so.Jitter(y=.5))\n    .add(so.Line(), so.PolyFit(5))\n    .layout(size=(5, 8))\n)\n\n\n\n\n\n\n\n\nEmperical probability vs. predicted probability\n\n\n\n\n\n\ncode\n\n\n\n\n\nblowbs_resid[\"y2\"] = blowbs_resid.y.map({0: \"survived(y=0)\", 1: \"died(y=1)\"})\n(\n    so.Plot(blowbs_resid, x='prob_pred')\n    .add(so.Bars(), so.Hist())\n    .facet(\"y2\")\n    .label(x=\"Predicted Probability\", y=\"Count\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nblowbs_resid[\"prob_cat\"] = (\n    pd.cut(\n        blowbs_resid.prob_pred,\n        bins=np.arange(0, 11) * 0.1,\n        include_lowest=True,\n        labels=False,  # labels=False: 0, 1, 2, ... 9,\n    )\n    * 0.1 + 0.05  # 각 percentile의 중앙값을 만듦\n)\n\nblowbs_resid[[\"prob_pred\", \"prob_cat\"]]\n#       prob_pred  prob_cat\n# 17         0.33      0.35\n# 24         0.48      0.45\n# 25         0.33      0.35\n# 50         0.33      0.35\n# ...         ...       ...\n# 3636       0.07      0.05\n# 3646       0.33      0.35\n# 3647       0.79      0.75\n# 3661       0.25      0.25\n\n# 각 percentile에서 실제 event(y=1)의 관찰 비율을 계산\nprob_plot = blowbs_resid.groupby(\"prob_cat\")[\"y\"].mean().reset_index()\n\n(\n    so.Plot(prob_plot, x='prob_cat', y='y')\n    .add(so.Dot())\n    .add(so.Line(color=\".6\"), y = 'prob_cat') # y = x line\n    .label(x=\"Predicted Probability\", y=\"Observed Probability\")\n)\n\n\n\nCalibration plot\n\n\n\n\n\n\n\n\n\n\n\nEvaluation of predicted class\n\n\n\n\n\n\nImportant\n\n\n\n예측된 확률을 기반으로 binary outcome으로 예측하여, 모형의 예측력을 평가\n\n예측된 확률값에 대해 임계치를 정하여, 예를 들어 0.5보다 크면 1, 0.5보다 작으면 0으로 분류하여, 이 binary 예측값과 실제값을 비교하여, 예측력을 평가\nConfusion matrix\nROC curve\nROC area (AUC) = Concordance ratio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted\n\n\n\n\n\nActual\nsurvied\ndied\n\n\nsurvied\n377\n49\n\n\ndied\n107\n126\n\n\n\n\n\n\n\n\n\nPredicted\n\n\n\n\n\nActual\nsurvied\ndied\n\n\nsurvied\nTrue Negative\nFalse Positive\n\n\ndied\nFalse Negative\nTrue Positive\n\n\n\n\n\n\n\nmod_lg = smf.logit('y ~ log2d', data=blowbs).fit() # logit has .pred_table() method\n\nOptimization terminated successfully.\n         Current function value: 0.499165\n         Iterations 6\n\n\n\n# accuracy rate with threshold 0.3\ncm = mod_lg.pred_table(0.3) # confusion matrix\ndisplay(cm)\n(cm[0, 0] + cm[1, 1]) / cm.sum()\n\narray([[298., 128.],\n       [ 45., 188.]])\n\n\n0.7374810318664643\n\n\n\n# accuracy rate with threshold 1: no information\ncm = mod_lg.pred_table(1) # confusion matrix\ndisplay(cm)\n(cm[0, 0] + cm[1, 1]) / cm.sum()\n\narray([[426.,   0.],\n       [233.,   0.]])\n\n\n0.6464339908952959\n\n\n옳은 예측의 비율\n\nTrue positive rate (TPR) | sensitivity: \\(\\displaystyle P(\\hat{y}=1 ~| ~y = 1)\\)\n\npresicion: \\(\\displaystyle P(y=1 ~| ~\\hat{y}=1)\\)\n\nTrue negative rate (TNR) | specificity: \\(\\displaystyle P(\\hat{y}=0 ~| ~y = 0)\\)\n\n틀린 예측의 비율\n\n거짓 양성, False positive rate (FPR): \\(\\displaystyle P(\\hat{y}=1 ~| ~y = 0)\\) = 1 - TNR\n거짓 음성, False negative rate (FNR): \\(\\displaystyle P(\\hat{y}=0 ~| ~y = 1)\\) = 1 - TPR\n\nReceiver operating characteristic (ROC) curve\n예측된 확률에 대한 임계치를 조정함에 따라 옳은 예측과 틀린 예측의 비율이 어떻게 달라지는지 살펴봄으로써 임계치를 설정하는데 도움을 줌\n\n\n\n\n\n\ncode\n\n\n\n\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(blowbs_resid.y, blowbs_resid.prob_pred)\nroc = pd.DataFrame(\n    {\n        \"thresholds\": thresholds.round(2),\n        \"False Pos\": fpr,\n        \"sensitivity(True Pos)\": tpr,\n        \"specificity(True Neg)\": 1 - fpr,\n    }\n)\nroc.sort_values(\"thresholds\").head(10)\n\n\n\n\n\n    thresholds  False Pos  sensitivity(True Pos)  specificity(True Neg)\n25        0.07       1.00                   1.00                   0.00\n24        0.12       0.81                   0.97                   0.19\n23        0.18       0.61                   0.94                   0.39\n22        0.25       0.43                   0.86                   0.57\n21        0.33       0.30                   0.81                   0.70\n20        0.41       0.22                   0.74                   0.78\n19        0.48       0.15                   0.64                   0.85\n18        0.55       0.12                   0.54                   0.88\n17        0.62       0.09                   0.48                   0.91\n16        0.67       0.08                   0.38                   0.92\n\n\n\n\n\n\n\n\n\n\n\nAUC: Area Under the Curve = concordance index\n\nAUC: 각 specificity값에 대한 sensitivity의 합 {{&lt; bi arrow-right &gt;}} 모형 예측력에 대한 전반적 평가\nConcordance index(c): 모든 Y의 쌍, 예를 들어 \\((0_i, 1_j)\\)에 대해서 해당하는 예측된 확률의 크기가 \\(p_i &lt; p_j\\) 인 비율, 즉 순서가 맞는(concordance) 비율\n잘못된 예측에 대한 비용이 다르다면, 특정 임계치를 선택\n\n\nfrom sklearn.metrics import roc_auc_score, auc\nroc_auc = roc_auc_score(blowbs_resid.y, blowbs_resid.prob_pred)\nroc_auc  # auc(fpr, tpr)\n\n0.8161306897177054\n\n\nClassifier로서 전반적인 모형의 성능 vs. 특정 임계치에서의 모형의 성능 vs. 확률모형\n\n잘못된 예측에 대한 비용이 다르다면, 임계치를 조정하여, 잘못된 예측에 대한 비용을 줄일 수 있음\n만약, 농작물에 대한 피해라고 가정하면,\n\nCosts: 농작물 피해, 펜스 설치비, 노동력 등\nBenefits: 수확물의 가치\n거짓 음성을 낮춰야 하는 경우: 예를 들어, 농작물의 작은 피해도 심각한 결과를 초래하는 경우\n거짓 양성을 낮춰야 하는 경우: 예를 들어, 농작물의 피해 예방을 위한 비용이 큰 경우\n\n만약, 와인 셀러가 와인의 품질(high:양성 vs. low:음성)을 성분들로 예측하는 모형을 만든다면, (in Stefanie Molin’s book)\n\nCosts: 높은 품질의 와인을 낮은 품질로 예측하면, 와인 품평가에게 신뢰를 잃을 수 있음\nBenefits: 낮은 품질의 와인을 높은 품질로 예측하면, 낮은 품질의 와인을 높은 가격에 팔아 수익으로 이어질 수 있음\n거짓 음성을 낮춰야 하는 경우: 예를 들어, 영세한 와이너리가 수익이 중요한 경우\n거짓 양성을 낮춰야 하는 경우: 예를 들어, 고품질의 와인을 생산하는 것으로 유명한 와이너리; 네임밸류를 유지하기 위해. 반면, 비싼 와인이 싸게 팔리는 것은 감당할 수 있음.\n\n확률을 정확히 예측하는 모형의 추구;\n\n확률값으로 communicate\nDecision maker는 당사자"
  },
  {
    "objectID": "contents/logistic.html#logistic-regression-정리",
    "href": "contents/logistic.html#logistic-regression-정리",
    "title": "Logistic Regression",
    "section": "Logistic Regression 정리",
    "text": "Logistic Regression 정리\nBinary response에 대해 그들의 conditional 평균인 확률 \\(p(x_i)\\)를 추정\nDistribution: \\((Y_i | X = x_i) \\sim Bin(m_i, p(x_i))\\) 의 분포가 binomial distribution을 따름. 따라서 \\(Y_i\\)의 평균과 분산은 \\(m_i\\)와 \\(p(x_i)\\)에 따라 결정되어 변함.\n\n나무의 두께(d)를 가진 m개의 나무들에 대해 태풍에 쓰러지는 나무의 수(count)는 이항분포 \\(Bin(m, p)\\) 를 따름.\n\nLinear predictors: 확률 \\(p(x_i)\\)는 link function (위에서는 logit)를 통해 예측변수들의 선형 결합으로 결정됨.\n\n\\(\\displaystyle log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right) = b_0 + b_1 \\cdot X_1 + b_2 \\cdot X_2\\)\nProbit link function으로 쓸 수도 있음: \\(\\displaystyle \\Phi^{-1}(\\hat{p}) = b_0 + b_1 \\cdot X_1 + b_2 \\cdot X_2\\)\n\n\\(\\Phi\\): cumulative standard normal distribution function\n\n\n\n\n\n\n\n\n\n\n\n위의 아이디어는 generalized linear model (GLM)로 확장되는데,\n\n\\((Y_i | X = x_i)\\)의 분포가 exponential family distribution을 따름\n\\(Y\\)는 예측변수들의 선형 결합으로만 결정되며,\nLink function을 통해 \\(Y_i\\)의 평균을 예측함. 즉, \\(g(E(Y | X=x_i)) = \\beta_0 + \\beta_1 \\cdot x_i\\)"
  },
  {
    "objectID": "contents/logistic.html#a.-blowdown",
    "href": "contents/logistic.html#a.-blowdown",
    "title": "Logistic Regression",
    "section": "A. Blowdown",
    "text": "A. Blowdown\nSource: p.274, Applied Linear Regression (4e) by Sanford Weisberg\n위에서 다룬 blowdown 데이터셋에서 black spruce라는 종에 대해서만 살펴봤는데, 이번에는 black spruce와 aspen 두 종의 나무를 모두 포함한 데이터로 비슷한 모형을 세워 분석해보세요.\n\n예측변수에 나무의 종을 나타내는 spp를 추가하고,\n나무의 두께(d)에 따라 태풍에 쓰러질(y) 확률이 두 종에서 큰 차이를 보이는지 살펴보고,\n그렇다면, 이를 interaction term으로 모형에 추가하여 분석해보세요.\n\n나무에 따른 확률 예측값을 그려보고 비교해보세요.\n나무에 따라, 나무가 10% 두꺼워지면(d) 쓰러질 odds ratio(OR)가 얼마나 증가하는지 파라미터를 통해 구해보고,\n모형의 예측력에 대해서도 1) 확률 측면, 2) binary class 측면에서 살펴보세요.\n\n\n\nblow2 = blowdown.query('spp in [\"black spruce\", \"aspen\"]')\nblow2[\"d\"] = np.floor(blow2[\"d\"])\nblow2[\"log2d\"] = np.log2(blow2[\"d\"])\nblow2\n\n         d    s  y           spp  log2d\n17    9.00 0.02  0  black spruce   3.17\n24   11.00 0.03  0  black spruce   3.46\n25    9.00 0.03  0  black spruce   3.17\n50    9.00 0.03  0  black spruce   3.17\n...    ...  ... ..           ...    ...\n3640 29.00 0.94  1         aspen   4.86\n3646  9.00 0.94  1  black spruce   3.17\n3647 17.00 0.94  1  black spruce   4.09\n3661  8.00 0.98  1  black spruce   3.00\n\n[1095 rows x 5 columns]\n\n\n\nblow2_bn = blow2.groupby([\"log2d\", \"spp\"])[\"y\"].agg([(\"died\", \"sum\"), (\"m\", \"count\"), (\"p\", \"mean\")]).reset_index()\nblow2_bn\n\n    log2d           spp  died   m    p\n0    2.32         aspen     0   2 0.00\n1    2.32  black spruce     7  90 0.08\n2    2.58         aspen     1   2 0.50\n3    2.58  black spruce     7  92 0.08\n..    ...           ...   ...  ..  ...\n66   5.61         aspen     1   1 1.00\n67   5.67         aspen     0   1 0.00\n68   5.75         aspen     1   1 1.00\n69   5.81         aspen     0   1 0.00\n\n[70 rows x 5 columns]"
  },
  {
    "objectID": "contents/logistic.html#b.-the-u.s.-youth-risk-behavior-surveillance-system",
    "href": "contents/logistic.html#b.-the-u.s.-youth-risk-behavior-surveillance-system",
    "title": "Logistic Regression",
    "section": "B. The U.S. Youth Risk Behavior Surveillance System",
    "text": "B. The U.S. Youth Risk Behavior Surveillance System\nSource: p.172, Beyond Multiple Linear Regression, by Paul Roback, Julie Legler.\n다음에서는 체중감량을 하고자하는 학생들의 의도에 영향을 줄 수 있는 3가지 요소들을 탐색합니다.\n\n성별에 따라 (sex)\n자신의 비만 정도(BMI)에 따라 (bmipct)\nTV에 노출되는 시간이 많을수록 (media)\n\n\nA sample of 500 teens from data collected in 2009 through the U.S. Youth Risk Behavior Surveillance System (YRBSS) [Centers for Disease Control and Prevention, 2009]. The YRBSS is an annual national school-based survey conducted by the Centers for Disease Control and Prevention (CDC) and state, territorial, and local education and health agencies and tribal governments.\n\nData: diet.csv\nlose_wt_01\nQ66. Which of the following are you trying to do about your weight?\nA. Lose weight (1)\nB. Gain weight (0)\nC. Stay the same weight (0)\nD. I am not trying to do anything about my weight (0)\nmedia\nQ81. On an average school day, how many hours do you watch TV?\nA. I do not watch TV on an average school day (0)\nB. Less than 1 hour per day (0.5)\nC. 1 hour per day (1)\nD. 2 hours per day (2)\nE. 3 hours per day (3)\nF. 4 hours per day (4)\nG. 5 or more hours per day (5)\nbmipct\nThe percentile for a given BMI for members of the same sex.\n\ndiet = pd.read_csv(\"data/diet.csv\")\ndiet\n\n            lose_wt  lose_wt_01     sex  media  bmipct\n0       Lose weight           1    Male   3.00      98\n1    No weight loss           0  Female   1.00      41\n2    No weight loss           0    Male   3.00       6\n3    No weight loss           0    Male   3.00      41\n..              ...         ...     ...    ...     ...\n441  No weight loss           0  Female   0.50      43\n442  No weight loss           0    Male   3.00      40\n443     Lose weight           1  Female   1.00      39\n444  No weight loss           0    Male   2.00      34\n\n[445 rows x 5 columns]\n\n\n\n아래 플랏들처럼 탐색적 분석을 수행하고,\nlose_wt_01을 response로 하는 logistic regression 모형을 세워 분석하고,\n모형의 파라미터를 해석한 후\n모형의 예측력을 파악하기 위해 위에서 다룬 몇 가지 방식으로 평가하세요.\n\n\n\n\n\n\n\n\n\n\n\nbmipct를 10개의 percentile 구간으로 나누어 (즉, 10%씩 10개 구간) emperical logit값을 구해 그려볼 것\npd.qcut을 이용\nLog가 0이 안되도록 적당히 조정하거나 값을 제거할 것.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n티비 시청시간(media)을 discrete하게 보고 각 시간 구간에서 emperical logit을 구해 살펴볼 것\n\n\n\n\n\n\n\n\n\n\n다음 모형에 대해\nlose_wt_01 ~ sex + bmipct + media\n\n모형의 파라미터를 해석해보고,\n모형의 예측력에 대해서도 1) 확률 측면, 2) binary class 측면에서 살펴보세요."
  },
  {
    "objectID": "contents/model-basic.html",
    "href": "contents/model-basic.html",
    "title": "Model Basics",
    "section": "",
    "text": "Source: R for Data Science by Wickham & Grolemund\n\n\n모델의 목표는 데이터 세트에 대한 간단한 저차원 요약을 제공하는 것입니다. 이상적으로, 모델은 진정한 ‘신호’(즉, 관심 있는 현상에 의해 생성된 패턴)를 포착하고 ‘노이즈’(즉, 관심 없는 임의의 변동)는 무시합니다. (번역 by DeepL)\n\n\nThe goal of a model is to provide a simple low-dimensional summary of a dataset. Ideally, the model will capture true “signals” (i.e. patterns generated by the phenomenon of interest), and ignore “noise” (i.e. random variation that you’re not interested in).\n\n이상적으로, 모형(model)이 현상으로부터 노이즈가 제거된 진정한 신호를 잡아내 주기를 기대.\n물리 법칙: 물리적 세계에 대한 모델\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n예를 들어, 캐럿과 가격의 진정한 관계를 모델로 표현\nLinear model: \\(y = ax + b + \\epsilon\\),   \\(\\epsilon\\): errors\n\n\\(price = a * carat + b + \\epsilon\\)\n\n로그 변환: \\(log(price) = a * log(carat) + b + \\epsilon\\)\n\n\\(\\epsilon\\): Gaussian 분포로 가정, 즉 \\(\\epsilon \\sim N(0, \\sigma^2)\\)\n\n\\(E(y|x_i) = a * x_i + b\\)   (\\(E\\): expectation, 기대값)\n\n\\(x_i\\)에 대해서 conditional mean이 선형함수로 결정됨을 가정\n\n\nErrors은 노이즈?\n\nReducible error: 모형이 잡아내지 못한 신호; 영향을 미치지만 측정하지 않은 변수가 존재\nIrreducible error:\n\n측정 오차 (measurement error): ex. 성별, 젠더, 키, 온도, 강수량, 지능, 불쾌지수, …\nrandom processes\n\n물리적 세계의 불확실성: stochastic vs. deterministic world\n \n\n예를 들어, 동전을 4번 던질 때,\n\nH, H, T, H\nH, T, T, H\nT, T, H, H\nT, H, T, H\nT, T, T, H\n…\n\n보통 Gaussian 분포를 이루거나 가정: ex. 측정 오차들, 동전 앞면의 개수들, 키, 몸무게, IQ, …\n\n그 외에 자연스럽게 나타나는 분포들이 있음; Binomial, Poisson, Exponential, Gamma, Beta, …\n\n\n불확실성(uncertainty)\n\n예측은 정확할 수 없으며, 이 불확실성이 error로 표현되며,\n확률(probability)의 개념으로 모형의 일부로 포함되어 예측하는데 중요한 요소로 활용됨.\n\nGaussian/Normal distribution\n\n랜덤한 값들의 합/평균들이 나타내는 분포\n\n측정 오차의 분포(error distribution)\n다양한 힘들의 상호작용으로 인한 분포\n\n분산이 유한한 분포 중에 정보 엔트로피가 최대(maximum entropy)인 분포\n중심극한정리(Central Limit Theorem)\n\n\n\n\n\n\n\n1000 people, each 16 steps tossing a coin: \n [[-1  1 -1 ... -1  1 -1]\n [ 1 -1 -1 ... -1  1  1]\n [-1  1 -1 ... -1  1 -1]\n ...\n [-1  1 -1 ... -1  1 -1]\n [-1  1 -1 ... -1  1 -1]\n [-1 -1  1 ... -1 -1 -1]]\n\n\n\n\n\n\n\n\n\n\n\n\nCoin tossing widget code\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nnp.random.seed(0)\ndef plot_Gaussin(n=0, ax=None):\n    N = 200\n    num_people, num_rounds = N, n\n\n    if n == 0:\n        toss = np.zeros(N).reshape(N, -1)\n    else:\n        toss = np.random.choice([-1, 1], size=(num_people, num_rounds))\n    toss_sum = toss.sum(axis=1)\n\n    ax = ax or plt.gca()\n    ax.set_title(f\"{N} people, each tossing a fair coin {n} times\")\n\n    df = pd.DataFrame({\"TossSum\": toss_sum})\n    df2 = df.value_counts().reset_index()\n    x = df2[\"TossSum\"].values.astype(int)\n    y = df2[\"count\"].values\n\n    if n &lt; 10:\n        ax.bar(x, y, color=\"#1f77b4\", alpha=.6, width=.8)\n        ax.set_xlim(-10, 10)\n        ax.set_xticks(x)\n    else:\n        ax.bar(x, y, color=\"#1f77b4\", alpha=.6)\n        ax.set_xticks(x)\n\n    for i, v in enumerate(y):\n        ax.text(x[i], v + 0.5, str(v), ha=\"center\", color=\"r\")\n\nfrom ipywidgets import interact, fixed\ninteract(plot_Gaussin,  n=(0, 100), ax=fixed(None))",
    "crumbs": [
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#a-simple-model",
    "href": "contents/model-basic.html#a-simple-model",
    "title": "Model Basics",
    "section": "A simple model",
    "text": "A simple model\nData: sim1.csv\n\nsim1 = pd.read_csv(\"data/sim1.csv\")\n\n\n\n\n\n\n\n     x     y\n0    1  4.20\n1    1  7.51\n2    1  2.13\n..  ..   ...\n27  10 24.97\n28  10 23.35\n29  10 21.98\n\n[30 rows x 2 columns]\n\n\n\n\n\n\n\n패턴: 강한 선형 관계\n선형 모델 family/class인 \\(y = \\beta_0 + \\beta_1 x\\)을 세운 후\n무수히 많은 \\(\\beta_0, \\beta_1\\)의 값들 중 위 데이터에 가장 가까운 값을 찾음\n그 예로, 임의로 250개의 선형 모델을 그려보면,\n\n\n\n\n\n\n\n\n\n\n\n이 선형모델 중 데이터에 가장 가까운 모델을 찾고자 하는데, 이를 위해서는 데이터와 모델과의 거리를 정의해야 함.\n  \\(d =|~data - model~|\\)\n예) 모델과 데이터의 수직 거리(residuals)의 총체\n\nModel 1.1: \\(y = 1.5x+7\\)의 경우, 이 모델이 예측하는 값들\n\n\narray([ 8.5,  8.5,  8.5, 10. , 10. , 10. , 11.5, 11.5, 11.5, 13. , 13. ,\n       13. , 14.5, 14.5, 14.5, 16. , 16. , 16. , 17.5, 17.5, 17.5, 19. ,\n       19. , 19. , 20.5, 20.5, 20.5, 22. , 22. , 22. ])\n\n\n이 때, 관측치(\\(Y_i\\))와 예측치(\\(\\hat{Y}_i\\))의 차이, \\(Y_i - \\hat{Y}_i\\)를 잔차(residuals) 또는 예측 오차(errors)라고 함\n\n\n     x     y  pred  resid | e\n0    1  4.20  8.50      -4.30\n1    1  7.51  8.50      -0.99\n2    1  2.13  8.50      -6.37\n..  ..   ...   ...        ...\n27  10 24.97 22.00       2.97\n28  10 23.35 22.00       1.35\n29  10 21.98 22.00      -0.02\n\n[30 rows x 4 columns]\n\n\n\n\n\n\n\n\nRMSE = \\(\\displaystyle\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{e^2}}\\) = 2.67\n\n\nMAE = \\(\\displaystyle\\frac{1}{n} \\sum_{i=1}^{n}|~e~|\\) = 1.43\n\n\n\n\n\n\n\n\n\nModel evaluation\n\n\n\nError functions\n\nRoot-mean-squared error: \\(RMSE = \\displaystyle\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{(y_i -\\hat y_i)^2}}\\)\n\nMean absolute error: \\(MAE = \\displaystyle\\frac{1}{n} \\sum_{i=1}^{n}{|~y_i -\\hat y_i~|}\\) : 극단값들에 덜 민감함\n\n\n\n즉, 데이터셋 sim1과 model 1.1 과의 거리를 RMSE로 정의하면, \\(d=|~sim1 -model1~| = 2.67\\)\n위의 250개의 모델에 대해 각각 거리를 구하면\n\n\n       b0    b1  dist\n0   21.79 -2.92 17.42\n1   -2.83 -0.57 22.83\n2   -6.39  2.16 10.26\n..    ...   ...   ...\n247  0.51  4.19 10.38\n248 27.94 -0.84 11.59\n249 27.93  2.45 25.99\n\n[250 rows x 3 columns]\n\n\n이 중 제일 좋은 모델(dist가 최소) 10개의 모델을 그리면,\n\n\n\n\n\n\n\n\n\n\n250개의 모델 중 10개의 모델을 다음과 같은 \\((\\beta_0, \\beta_1)\\) 평면으로 살펴보면, 즉, model space에서 살펴보면\n\n오렌지 색은 위에서 구한 10 best models\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Introduction to Statistical Learning by James et al.\n\n점차 촘촘한 간격으로 grid search를 하면서 거리를 최소로 하는 모델을 찾아가는 것이고, 실제로는 Newton-Raphson search를 통해 최소값을 구하는 알고리즘을 통해 구할 수 있음.\n즉, 거리를 최소로 하는 \\(\\beta_0\\), \\(\\beta_1\\)를 찾으면,\n\nfrom scipy.optimize import minimize\nminimize(measure_distance, [0, 0], args=(sim1)).x\n\narray([4.22, 2.05])\n\n\n\n\n\n\n\n\n\n\n\n이렇게 squared error가 최소가 되도록 추정하는 것을 ordinary least squares(OLS) estimattion라고 함.\n실제로는 위에서 처럼 grid search를 하지 않고, closed-form solution을 통해 바로 구할 수 있음.",
    "crumbs": [
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#maximum-likelihood-estimation",
    "href": "contents/model-basic.html#maximum-likelihood-estimation",
    "title": "Model Basics",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\n데이터가 발생된 것으로 가정하는 분포를 고려했을 때,\n어떨때 주어진 데이터가 관측될 확률/가능도(likelihood)가 최대가 되겠는가로 접근하는 방식으로,\nX, Y의 관계와 확률분포를 함께 고려함.\n\n선형관계라면, 즉 \\(E(Y|X=x_i) = \\beta_0 + \\beta_1x_i\\)   (\\(E\\): expected value, 기대값)\n분포가 Gaussian이라면, 즉 \\(Y|(X=x_i) \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\)   (\\(\\sigma\\): 표준편차)\n\n\nLikelihood \\(L = \\displaystyle\\prod_{i=1}^{n}{P_i}\\)   (관측치들 독립일 때, product rule에 의해)\n분포가 Gaussian이라면(평균: \\(\\mu\\), 표준편차: \\(\\sigma\\)), 즉 \\(f(t) = \\displaystyle\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(t-\\mu)^2}{2\\sigma^2}\\right)\\)라면\n\\(L = \\displaystyle\\prod_{i=1}^{n}{f(y_i, x_i)} = \\displaystyle\\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)}\\)\n이 때, 이 likelihood를 최대화하는 \\(\\beta_0, \\beta_1, \\sigma\\)를 찾는 것이 목표이며,\n이처럼 분포가 Gaussian라면, OLS estimation과 동일한 값을 얻음. (단, \\(\\sigma\\)는 bias가 존재)\n다른 분포를 가지더라도 동일하게 적용할 수 있음!\n\n즉, likelihood의 관점에서 주어진 데이터에 가장 근접하도록(likelihood가 최대가 되는) “분포의 구조”를 얻는 과정임\n\n여러 편의를 위해, log likelihood를 최대화함.\n\n\n\n\n\n\nLog likelihood\n\n\n\n\n\n다음 두가지를 고려하면,\n\\(log(x*y) = log(x) + log(y)\\)\n\\(e^x * e^y = e^{x+y}\\)\n\\(log(L) = \\displaystyle\\sum_{i=1}^{n}{log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)\\right)} = \\displaystyle\\sum_{i=1}^{n}{-log(\\sqrt{2\\pi\\sigma^2}) - \\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}}\\)\n두 번째 항이 앞서 정의한 squared error와 동일함\n\n\n\n앞서와 마찬가지로 여러 \\(\\beta_0, \\beta_1\\)값을 대입해서 log likelihood를 최대화하는 값을 찾아보면,\n\n\ncreate a grid for b0, b1\n# create a grid for b0: (-20, 40), b1:(-5, 5)\nnp.random.seed(123)\nb0 = np.linspace(-20, 40, 100)\nb1 = np.linspace(-5, 5, 100)\n\n# meshgrid\nb0, b1 = np.meshgrid(b0, b1)\n\nmodels = pd.DataFrame(dict(b0=b0.ravel(), b1=b1.ravel()))\nmodels\n\n\n         b0    b1\n0    -20.00 -5.00\n1    -19.39 -5.00\n2    -18.79 -5.00\n...     ...   ...\n9997  38.79  5.00\n9998  39.39  5.00\n9999  40.00  5.00\n\n[10000 rows x 2 columns]\n\n\n표준편차(\\(\\sigma\\))는 고정하고(2.2), - log likelihood 값을 구해 정렬하면,\n(참고: 마찬가지로 likelihood를 최대화하는 \\(\\sigma\\)값을 찾을 수 있음)\n\n\ncalculate the likelihood\nfrom scipy.stats import norm  # normal distribution\n\ndef likelihood(b, data):\n    mu = b[0] + b[1] * data[\"x\"]\n    sigma = 2.2\n    return -np.sum(np.log(norm.pdf(data[\"y\"], mu, sigma)))\n\nmodels[\"-log likelihood\"] = models.apply(lambda x: likelihood(x, sim1), axis=1)\nmodels.sort_values(\"-log likelihood\")\n\n\n         b0    b1  -log likelihood\n7040   4.24  2.07            65.32\n6941   4.85  1.97            65.53\n7139   3.64  2.17            65.65\n...     ...   ...              ...\n406  -16.36 -4.60              inf\n209  -14.55 -4.80              inf\n0    -20.00 -5.00              inf\n\n[10000 rows x 3 columns]\n\n\nscipy의 minimize 함수를 이용해서 최소값을 구해보면,\n\nfrom scipy.optimize import minimize\nminimize(likelihood, [0, 0], args=(sim1)).x\n\narray([4.22, 2.05])\n\n\n\nfrom statsmodels.formula.api import ols\n\nmod = ols('y ~ x', data=sim1).fit()\ndisplay(mod.summary().tables[0], mod.summary().tables[1])\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.885\n\n\nModel:\nOLS\nAdj. R-squared:\n0.880\n\n\nMethod:\nLeast Squares\nF-statistic:\n214.7\n\n\nDate:\nThu, 04 Apr 2024\nProb (F-statistic):\n1.17e-14\n\n\nTime:\n05:56:18\nLog-Likelihood:\n-65.226\n\n\nNo. Observations:\n30\nAIC:\n134.5\n\n\nDf Residuals:\n28\nBIC:\n137.3\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4.2208\n0.869\n4.858\n0.000\n2.441\n6.001\n\n\nx\n2.0515\n0.140\n14.651\n0.000\n1.765\n2.338\n\n\n\n\n\n\n\nMaximum likelihood estimation은 다양한 분포의 데이터에 대해서도 적용할 수 있음.\n예를 들어, 다음과 같은 전형적인 Gaussian이 아닌 분포에 대해서도 적용할 수 있음.\nNon-constant variance(왼쪽), Poisson distribution(오른쪽)",
    "crumbs": [
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#uncertainty",
    "href": "contents/model-basic.html#uncertainty",
    "title": "Model Basics",
    "section": "Uncertainty",
    "text": "Uncertainty\n관찰된 데이터(표본, sample)로부터 모집단(population)에 대한 정보를 추론할 때, 불확실성이 존재함.\n이는 새로운 데이터에 대한 예측의 불확실성 혹은 일반화(generalization)에 대한 문제와 동일함.\n예를 들어, 과거 병원 기록으로 새로운 환자에 대한 진단을 내리는 경우\n\n이 환자의 고유한 상태로부터 오는 불확실성: 측정된 부분(measured) + 측정되지 않은 부분(unmeasured)\n과거 기록을 통한 진단의 정확성/true relationship에 대한 불확실성\n\n불확실성에 대한 종류\n\n파라미터 값에 대한 불확실성: confidence interval\n\n데이터가 많을 수록\nX가 넓게 분포할 수록\n\n특정 값에 대한 불확실성\n\n평균값(\\(E(Y|X_i)\\))에 대한 불확실성: confidence interval\n예측값(\\(f(X_i)\\))에 대한 불확실성: prediciton interval\n\n\n전통적으로는 분포에 대한 가정으로부터 이론적으로 불확실성을 추론했으나,\n현대적인 접근으로 resampling 방식의 bootstrapping이나 sample을 traing/test set으로 나누는 cross-validation 등을 통해 시뮬레이션을 통해 불확실성을 추정할 수 있음\nBayesian 방식에서는 분포에 대한 가정없이, 불확실성에 대한 분포 자체(posterior predictive distribution)에 대해 엄밀히 추정하는 방식도 있음\n반대로, machine learning에서는 특정 action 혹은 decision-making을 하는 것이 중요한 경우가 많아, 불확실성에 대한 고려가 적은 경향이 있음\n\n\ncalculate confidence intervals\nsim1_new = pd.DataFrame({\"x\": [0.5, 1.5, 4.5, 7.5, 10.5]})\nmod = ols('y ~ x', data=sim1).fit()\npredictions = mod.get_prediction(sim1_new).summary_frame(.1)\n\nplt.fill_between(sim1_new[\"x\"], predictions['obs_ci_lower'], predictions['obs_ci_upper'], alpha=.1, label='90% Prediction interval')\nplt.fill_between(sim1_new[\"x\"], predictions['mean_ci_lower'], predictions['mean_ci_upper'], alpha=.5, label='90% Confidence interval')\nplt.scatter(sim1[\"x\"], sim1[\"y\"], label='Observed', marker='o', color='.6')\nplt.scatter(sim1_new[\"x\"], sim1_new[\"x\"]*0, label='New data', marker='x', color='.2')\nplt.plot(sim1_new[\"x\"], predictions['mean'], label='Regression line')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(frameon=False)\nsns.despine()\nplt.ylim(-0.5, 30)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n0.50\n5.25\n0.81\n3.87\n6.62\n1.26\n9.24\n\n\n1\n1.50\n7.30\n0.69\n6.13\n8.47\n3.37\n11.22\n\n\n2\n4.50\n13.45\n0.43\n12.73\n14.18\n9.64\n17.27\n\n\n3\n7.50\n19.61\n0.49\n18.77\n20.44\n15.77\n23.45\n\n\n4\n10.50\n25.76\n0.81\n24.39\n27.14\n21.77\n29.75\n\n\n\n\n\n\n\n\n\n아래는 bootstrapping을 통한 실제 표본을 재추출하는 방식이나, 이론적으로 표본 분포(sampling distribution)에 대한 이론을 통해 closed form으로 얻을 수 있음.\n\n\n\n\n\n\n\nSource: The Truthful Art by Albert Cairo.",
    "crumbs": [
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#predictive-accuracy",
    "href": "contents/model-basic.html#predictive-accuracy",
    "title": "Model Basics",
    "section": "Predictive Accuracy",
    "text": "Predictive Accuracy\n전통적인 모형에서는\n\n샘플에 대해서 계산된 값은 실제보다 overestimate 되는 경향이 있으므로,\n이를 보정하는 방식으로 계산: adjusted, shrunken\n\n현대적인 방식으로는\n\n샘플을 training/test set으로 나누어서, test set에 대한 예측값을 계산하고, 이를 통해 예측의 정확성을 평가함\n비슷하게, resampling 방식의 bootstrapping을 통해 해결\n\n주로 사용되는 지표들\n\n\\(RMSE = \\displaystyle\\sqrt{{\\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}}{{e^2}}}}\\)\n\\(MAE = \\displaystyle\\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}}|~e~|\\)\n\\(R^2 = 1 - \\displaystyle\\frac{\\frac{1}{n} \\sum_{i=1}^{n}{(e-0)^2}}{\\frac{1}{n} \\sum_{i=1}^{n}{(Y-\\overline{Y})^2}} = 1 - \\frac{V(e)}{V(Y)}\\)\n\n전통적으로 X가 Y를 얼마나 잘 “설명”해주는지에 대한 지표로서 \\(R^2\\)를 사용함\n\n\ncalculate RMSE, MAE, R2\nfrom statsmodels.tools.eval_measures import rmse, meanabs\nypred = mod.predict(sim1)\ny = sim1[\"y\"]\n\nprint(f\"RMSE = {rmse(y, ypred):.2f} \\nMAE = {meanabs(y, ypred):.2f} \\nR-squared = {mod.rsquared:.2f}\")\n\n\nRMSE = 2.13 \nMAE = 1.71 \nR-squared = 0.88",
    "crumbs": [
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#predictions-the-pattern-that-the-model-has-captured",
    "href": "contents/model-basic.html#predictions-the-pattern-that-the-model-has-captured",
    "title": "Model Basics",
    "section": "Predictions: the pattern that the model has captured",
    "text": "Predictions: the pattern that the model has captured\n우선, 예측 변수들의 데이터 값을 커버하는 grid를 구성\n\nsim1\n\n     x     y\n0    1  4.20\n1    1  7.51\n2    1  2.13\n..  ..   ...\n27  10 24.97\n28  10 23.35\n29  10 21.98\n\n[30 rows x 2 columns]\n\n\n\n# create a grid for the range of x sim1: new data\ngrid = pd.DataFrame(dict(x=np.linspace(sim1.x.min(), sim1.x.max(), 10)))\n\n모델에 grid를 입력하여 prediction값을 추가\n\n# a model for sim1\nfrom statsmodels.formula.api import ols\nsim1_mod = ols(\"y ~ x\", data=sim1).fit()\n\ngrid[\"pred\"] = sim1_mod.predict(grid) # column 이름이 매치되어야 함\ngrid\n\n       x  pred\n0   1.00  6.27\n1   2.00  8.32\n2   3.00 10.38\n..   ...   ...\n7   8.00 20.63\n8   9.00 22.68\n9  10.00 24.74\n\n[10 rows x 2 columns]\n\n\nprediction을 시각화\n\n\nShow the code\n(\n    so.Plot(sim1, x='x', y='y')\n    .add(so.Dot(color=\".8\"))\n    .add(so.Line(marker=\".\", pointsize=10), x=grid.x, y=grid.pred)  # prediction!\n    .layout(size=(4.5, 3.5))\n    .scale(x=so.Continuous().tick(at=grid.x))\n)",
    "crumbs": [
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#residuals-what-the-model-has-missed.",
    "href": "contents/model-basic.html#residuals-what-the-model-has-missed.",
    "title": "Model Basics",
    "section": "Residuals: what the model has missed.",
    "text": "Residuals: what the model has missed.\n\\(e = Y - \\hat{Y}\\) : 관측값 - 예측값\n\nsim1[\"resid\"] = sim1_mod.resid  # Y - Y_hat\n\n\nsim1\n\n     x     y  fitted  resid\n0    1  4.20    6.27  -2.07\n1    1  7.51    6.27   1.24\n2    1  2.13    6.27  -4.15\n..  ..   ...     ...    ...\n27  10 24.97   24.74   0.23\n28  10 23.35   24.74  -1.39\n29  10 21.98   24.74  -2.76\n\n[30 rows x 4 columns]\n\n\n우선, residuals의 분포를 시각화해서 살펴보면,\n\nsim1[\"resid\"].hist(bins=20)\nplt.show()\n\n\n\n\n\n\n\n\n예측 변수와 residuals의 관계를 시각화해서 보면,\n\n(\n    so.Plot(sim1, x='x', y='resid')\n    .add(so.Dot())\n    .add(so.Line(), so.PolyFit(5))\n    .layout(size=(5, 4))\n)\n\n\n\n\n\n\n\n\n위의 residuals은 특별한 패턴을 보이지 않아야 모델이 데이터의 패턴을 잘 잡아낸 것으로 판단할 수 있음.\n아래는 원래 데이터와 일차 선형 모형에 대한 예측값의 관계를 시각화한 것\n\n\n\n\n\n\n\n\n\nResiduals에 패턴이 보이는 경우",
    "crumbs": [
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#two-continuous",
    "href": "contents/model-basic.html#two-continuous",
    "title": "Model Basics",
    "section": "Two continuous",
    "text": "Two continuous\n두 연속변수가 서로 상호작용하는 경우: not additive, but multiplicative\n\n각각의 효과가 더해지는 것을 넘어서서 서로의 효과를 증폭시키거나 감소시키는 경우\n강수량과 풍속이 함께 항공편의 지연을 가중시키는 경우\n운동량과 식사량이 함께 체중 감량에 영향을 미치는 경우\n\n\n\nShow the code\nnp.random.seed(123)\nx1 = np.random.uniform(0, 10, 200)\nx2 = 2*x1 - 1 + np.random.normal(0, 12, 200)\ny = x1 + x2 + x1*x2 + np.random.normal(0, 50, 200)\ndf = pd.DataFrame(dict(precip=x1, wind=x2, delay=y))\ndf\n\n\n     precip   wind  delay\n0      6.96   4.04  95.70\n1      2.86   5.60  31.23\n2      2.27   8.37 -30.97\n..      ...    ...    ...\n197    7.45  16.38 186.67\n198    4.73 -18.56 -96.90\n199    1.22  -5.63 -22.95\n\n[200 rows x 3 columns]\n\n\n\n# additive model\nmod1 = ols('delay ~ precip + wind', data=df).fit()\n\n# interaction model\nmod2 = ols('delay ~ precip + wind + precip:wind', data=df).fit()\n\nmod2: y ~ x1 + x2 + x1:x2는 \\(\\hat{y} = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\) 로 변환되고,\n변형하면, \\(\\hat{y} = a_0 + a_1x_1 + (a_2 + a_3x_1)x_2\\)",
    "crumbs": [
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#continuous-and-categorical",
    "href": "contents/model-basic.html#continuous-and-categorical",
    "title": "Model Basics",
    "section": "Continuous and Categorical",
    "text": "Continuous and Categorical\n연속변수와 범주형 변수가 서로 상호작용하는 경우\n\n운동량이 건강에 미치는 효과: 혼자 vs. 단체\n\nData: sim3.csv\n\nsim3 = pd.read_csv(\"data/sim3.csv\")\nsim3\n\n     x1 x2  rep     y  sd\n0     1  a    1 -0.57   2\n1     1  a    2  1.18   2\n2     1  a    3  2.24   2\n..   .. ..  ...   ...  ..\n117  10  d    1  6.56   2\n118  10  d    2  5.06   2\n119  10  d    3  5.14   2\n\n[120 rows x 5 columns]\n\n\n\n\nCode\n(\n    so.Plot(sim3, x='x1', y='y', color='x2')\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(), so.PolyFit(5), color=None)\n)\n\n\n\n\n\n\n\n\n\n두 가지 모델로 fit할 수 있음\n\nmod1 = ols('y ~ x1 + x2', data=sim3).fit()\nmod2 = ols('y ~ x1 * x2', data=sim3).fit() # 같은 의미 'y ~ x1 + x2 + x1:x2'\n\nformula y ~ x1 * x2는 \\(\\hat{y} = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\)로 변환됨\n\n하지만, 여기서는 x2가 범주형 변수라 dummy-coding후 적용됨.\nDesign matrix를 확인해 보면,\n\ny, X = dmatrices(\"y ~ x1 + x2\", data=sim3, return_type=\"dataframe\")\nX.iloc[:, 1:]\n\n     x2[T.b]  x2[T.c]  x2[T.d]    x1\n0       0.00     0.00     0.00  1.00\n1       0.00     0.00     0.00  1.00\n2       0.00     0.00     0.00  1.00\n..       ...      ...      ...   ...\n117     0.00     0.00     1.00 10.00\n118     0.00     0.00     1.00 10.00\n119     0.00     0.00     1.00 10.00\n\n[120 rows x 4 columns]\n\n\n\ny, X = dmatrices(\"y ~ x1 * x2\", data=sim3, return_type=\"dataframe\")\nX.iloc[:, 1:]\n\n     x2[T.b]  x2[T.c]  x2[T.d]    x1  x1:x2[T.b]  x1:x2[T.c]  x1:x2[T.d]\n0       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n1       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n2       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n..       ...      ...      ...   ...         ...         ...         ...\n117     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n118     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n119     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n\n[120 rows x 7 columns]\n\n\n\ngrid = sim3.value_counts([\"x1\", \"x2\"]).reset_index().drop(columns=\"count\")\ngrid[\"mod1\"] =  mod1.predict(grid)\ngrid[\"mod2\"] =  mod2.predict(grid)\ngrid_long = grid.melt(id_vars=[\"x1\", \"x2\"], var_name=\"model\", value_name=\"pred\")\ngrid_full = grid_long.merge(sim3[[\"x1\", \"x2\", \"y\"]])\n\n\ngrid_full\n\n     x1 x2 model  pred     y\n0     1  a  mod1  1.67 -0.57\n1     1  a  mod1  1.67  1.18\n2     1  a  mod1  1.67  2.24\n..   .. ..   ...   ...   ...\n237  10  d  mod2  3.98  6.56\n238  10  d  mod2  3.98  5.06\n239  10  d  mod2  3.98  5.14\n\n[240 rows x 5 columns]\n\n\n\n\nCode\n(\n    so.Plot(grid_full, x=\"x1\", y=\"y\", color=\"x2\")\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(), y=\"pred\")\n    .facet(\"model\")\n    .layout(size=(8, 5))\n)\n\n\n\n\n\n\n\n\n\n\ninteraction이 없는 모형 mod1의 경우, 네 범주에 대해 기울기가 동일하고 절편의 차이만 존재\ninteraction이 있는 모형 mod2의 경우, 네 범주에 대해 기울기가 다르고 절편도 다름\n\n\n\\(y = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\)에서 \\(x_1x_2\\)항이 기울기를 변할 수 있도록 해줌 \\(y = a_0 + a_2x_2 + (a_1 + a_3x_2)x_1\\)으로 변형하면, \\(x_1\\)의 기울기는 \\(a_1 + a_3 x_2\\)\n\n\n\n\n\n\n\nFitted models\n\n\n\n\n\nmod1 = ols('y ~ x1 + x2', data=sim3).fit()\nmod1.params\n# Intercept    1.87\n# x2[T.b]      2.89\n# x2[T.c]      4.81\n# x2[T.d]      2.36\n# x1          -0.20\n\nmod2 = ols('y ~ x1 * x2', data=sim3).fit() # 같은 의미 'y ~ x1 + x2 + x1:x2'\nmod2.params\n# Intercept     1.30\n# x2[T.b]       7.07\n# x2[T.c]       4.43\n# x2[T.d]       0.83\n# x1           -0.09\n# x1:x2[T.b]   -0.76\n# x1:x2[T.c]    0.07\n# x1:x2[T.d]    0.28\n\n\n\n두 모형을 비교하여 중 더 나은 모형을 선택하기 위해, residuals을 차이를 살펴보면,\n\nsim3[\"mod1\"] = mod1.resid\nsim3[\"mod2\"] = mod2.resid\n\nsim3_long = sim3.melt(\n    id_vars=[\"x1\", \"x2\"],\n    value_vars=[\"mod1\", \"mod2\"],\n    var_name=\"model\",\n    value_name=\"resid\",\n)\nsim3_long\n\n     x1 x2 model  resid\n0     1  a  mod1  -2.25\n1     1  a  mod1  -0.49\n2     1  a  mod1   0.56\n3     1  b  mod1   2.87\n..   .. ..   ...    ...\n236  10  c  mod2  -0.64\n237  10  d  mod2   2.59\n238  10  d  mod2   1.08\n239  10  d  mod2   1.16\n\n[240 rows x 4 columns]\n\n\n\n\nCode\n(\n    so.Plot(sim3_long, x=\"x1\", y=\"resid\", color=\"x2\")\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(linestyle=\":\", color=\".5\"), so.Agg(lambda x: 0))\n    .facet(\"x2\", \"model\")\n    .layout(size=(9, 6))\n    .scale(color=\"Set2\")\n)\n\n\n\n\n\n\n\n\n\n\n둘 중 어떤 모델이 더 나은지에 대한 정확한 통계적 비교가 가능하나 (잔차의 제곱의 평균인 RMSE나 잔차의 절대값의 평균인 MAE 등)\n여기서는 직관적으로 어느 모델이 데이터의 패턴을 더 잘 잡아냈는지를 평가하는 것으로 충분\n잔차를 직접 들여다봄으로써, 어느 부분에서 어떻게 예측이 잘 되었는지, 잘 안 되었는지를 면밀히 검사할 수 있음\ninteraction 항이 있는 모형이 더 나은 모형\n\nSaratogaHouses 데이터에서 가령, livingArea와 centralAir의 interaction을 살펴보면,\n\nhouses = sm.datasets.get_rdataset(\"SaratogaHouses\", \"mosaicData\").data\n(\n    so.Plot(houses, x='livingArea', y='price')\n    .add(so.Dots(color='.6'))\n    .add(so.Line(color=\"orangered\"), so.PolyFit(1))\n    .facet(\"centralAir\")\n    .label(title=\"Central Air: {}\".format)\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\nmod1 = ols('price ~ livingArea + centralAir', data=houses).fit()\nmod2 = ols('price ~ livingArea * centralAir', data=houses).fit()\n\ndisplay(mod1.params, mod2.params)\n\nIntercept           14144.05\ncentralAir[T.Yes]   28450.58\nlivingArea            106.76\ndtype: float64\n\n\nIntercept                       44977.64\ncentralAir[T.Yes]              -53225.75\nlivingArea                         87.72\nlivingArea:centralAir[T.Yes]       44.61\ndtype: float64\n\n\nR-squared 비교\n\ndisplay(mod1.rsquared, mod2.rsquared)\n\n0.5253223149339137\n\n\n0.5430362101820772",
    "crumbs": [
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/regularization.html",
    "href": "contents/regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n\n\n앞서 flexibility를 조정하기 위해 구체적으로 특정 모델을 선택했다면,\n좀 더 알고리즘적으로 optimal한 bias-variance trade-off를 찾는 방식을 알아봄.\n함수 \\(h(x) = sin(2\\pi x)\\)로부터 생성된 데이터셋(N=10)에 대해 다항식의 차수에 따른 flexibility의 변화에 따른 OLS 모델들을 비교하면,\n\n\n\nSource: pp. 10-12, Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H\n\nShrinkage\n\n모든 예측변수를 포함시킨 후\n추정 계수가 너무 커지지 않도록 제약을 가하는 방법\n이를 shrinkage/regularization이라고 하며, 이는 variance를 줄이는 효과가 있음.\nShrinkage 종류에 따라 일부 계수는 정확히 0으로 추정될 수 있음; 변수 선택도 수행됨.\n\nOLS estimation에서는 \\(\\displaystyle RSS = \\sum_{i=1}^{n} e^2\\),   \\((e = y - \\hat{y})\\) 를 최소화시켰으나,\n다음과 같이 shrinkage penalty를 추가하여 최소화시키는 방법을 사용함.\n\nRidge regression (L2 regularization): \\(\\displaystyle \\sum_{j=1}^{p} e^2 + \\lambda\\sum_{j=1}^{p} \\beta_j^2\\)\nLasso regression (L1 regularization): \\(\\displaystyle \\sum_{j=1}^{p} e^2 + \\lambda\\sum_{j=1}^{p} |\\beta_j|\\)\nElastic net regression: ridge와 lasso regression을 결합\n\n\n\\(\\lambda \\geq 0\\)는 shrinkage penalty의 크기를 결정하는 tuning parameter(hyperparameter)\n\n절편에 대해서는 shrinkage penalty를 적용하지 않음\n\\(\\lambda = 0\\)이면 OLS와 동일\n\\(\\lambda\\)가 커질 수록 penalty의 상대적 중요도가 높아져 파라미터의 크기가 줄어듦; \\(\\beta_j \\rightarrow 0\\) as \\(\\lambda \\rightarrow \\infty\\) (null model)\n적절한 \\(\\lambda\\)를 찾기 위해 cross-validation을 사용함\n각 X와 Y간의 관계에 대해서만 shrink하길 기대\nSubset Selection 이 접근 방식에는 응답과 관련이 있다고 생각되는 p 예측 변수의 하위 집합을 식별하는 것입니다. 그런 다음 축소된 변수 집합에 최소 제곱을 사용하여 모델을 맞춥니다.\nDimension Reduction 이 접근 방식은 예측값을 M 차원으로 투영하는 것을 포함합니다. 예측자를 M차원 하위 공간으로 투영하는 것입니다(여기서 M&lt;p). 이는 변수의 다양한 선형 조합 또는 투영을 M개 계산하여 이루어집니다. 그런 다음 이러한 M개의 예측을 최소 제곱으로 선형 회귀 모델을 맞추기 위한 예측자로 사용합니다.",
    "crumbs": [
      "Machine Learning",
      "Regularization"
    ]
  },
  {
    "objectID": "contents/subsetting.html",
    "href": "contents/subsetting.html",
    "title": "Subsetting",
    "section": "",
    "text": "Load Packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nDataFrame의 일부를 선택하는 subsetting의 방식에 여러 가지 있음\nflights.head(3)\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1    517.00             515       2.00    830.00   \n1  2013      1    1    533.00             529       4.00    850.00   \n2  2013      1    1    542.00             540       2.00    923.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n0             819      11.00      UA    1545  N14228    EWR  IAH    227.00  \n1             830      20.00      UA    1714  N24211    LGA  IAH    227.00  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00",
    "crumbs": [
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#bracket",
    "href": "contents/subsetting.html#bracket",
    "title": "Subsetting",
    "section": "Bracket [ ]",
    "text": "Bracket [ ]\nBracket안에 labels이 있는 경우 columns을 select\n\nA single string: Series로 반환\n\nA list of a single string: DataFrame으로 반환\n\nA list of strings\n\n\nflights['dest']  # return as a Series\n\n0         IAH\n1         IAH\n         ... \n336774    CLE\n336775    RDU\nName: dest, Length: 336776, dtype: object\n\n\n\nflights[['dest']]  # return as a DataFrame\n\n       dest\n0       IAH\n1       IAH\n...     ...\n336774  CLE\n336775  RDU\n\n[336776 rows x 1 columns]\n\n\n\nflights[['origin', 'dest']]\n\n       origin dest\n0         EWR  IAH\n1         LGA  IAH\n...       ...  ...\n336774    LGA  CLE\n336775    LGA  RDU\n\n[336776 rows x 2 columns]\n\n\nBracket안에 numbers가 있는 경우 rows를 select: position-based\n\nSlicing만 허용\nFirst index는 포함, last index는 제외\n[1, 5, 8]과 같이 특정 rows를 선택하는 것은 허용안됨\n\n\nflights[2:5]\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n3  2013      1    1    544.00             545      -1.00   1004.00   \n4  2013      1    1    554.00             600      -6.00    812.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00  \n3            1022     -18.00      B6     725  N804JB    JFK  BQN    183.00  \n4             837     -25.00      DL     461  N668DN    LGA  ATL    116.00  \n\n\n 만약, 아래와 같이 index가 number일 때 out of order가 된 경우에도 row position으로 적용됨\n\n\n   origin dest  arr_delay\n42    LGA  DFW      48.00\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n33    EWR  MSP      29.00\n\n\n\ndf_outoforder[2:4]\n\n   origin dest  arr_delay\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n\n\n Chaining with brackets\n\nflights[['origin', 'dest']][2:5]\n# 순서 바꿔어도 동일: flights[2:5][['origin', 'dest']]\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL",
    "crumbs": [
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#dot-notation-.",
    "href": "contents/subsetting.html#dot-notation-.",
    "title": "Subsetting",
    "section": "Dot notation .",
    "text": "Dot notation .\n편리하나 주의해서 사용할 필요가 있음\n\n\n\n\n\n\nNote\n\n\n\n\nspace 또는 . 이 있는 변수명 사용 불가\nmethods와 동일한 이름의 변수명 사용 불가: 예) 변수명이 count인 경우 df.count는 df의 method로 인식\n새로운 변수를 만들어 값을 assgin할 수 없음: 예) df.new_var = 1 불가; 대신 df[\"new_var\"] = 1\n만약, 다음과 같이 변수을 지정했을 때 vars_names=[\"origin\", \"dest\"],\n\ndf[vars_names]는 \"orign\"과 \"dest\" columns을 선택\ndf.vars_names는 vars_names이라는 이름의 column을 의미\n\n\n\n\n\nflights.dest  # flihgts[\"dest\"]와 동일\n\n0         IAH\n1         IAH\n         ... \n336774    CLE\n336775    RDU\nName: dest, Length: 336776, dtype: object",
    "crumbs": [
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#loc-.iloc",
    "href": "contents/subsetting.html#loc-.iloc",
    "title": "Subsetting",
    "section": ".loc & .iloc",
    "text": ".loc & .iloc\n각각 location, integer location의 약자\ndf.(i)loc[row_indexer, column_indexer]\n\n.loc: label-based indexing\n\nIndex가 number인 경우도 label로 처리\nSlicing의 경우 first, last index 모두 inclusive\n\n\nflights.loc[2:5, ['origin', 'dest']]  # 2:5는 index의 label, not position\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL\n5    EWR  ORD\n\n\n다음과 같이 index가 labels인 경우는 혼동의 염려 없음\n\n\n       origin dest\nred       JFK  MIA\nblue      JFK  BQN\ngreen     LGA  ATL\nyellow    EWR  ORD\n\n\n\ndf_labels.loc[\"blue\":\"green\", :]\n\n      origin dest\nblue     JFK  BQN\ngreen    LGA  ATL\n\n\n하지만, index가 number인 경우는 혼동이 있음\n앞서 본 예에서처럼 index가 out of order인 경우 loc은 다르게 작동\n\n\n   origin dest  arr_delay\n42    LGA  DFW      48.00\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n33    EWR  MSP      29.00\n\n\n\ndf_outoforder.loc[2:14, :]  # position 아님\n\n   origin dest  arr_delay\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n\n\n\ndf_outoforder.loc[[25, 33], :]  # slicing이 아닌 특정 index 선택\n\n   origin dest  arr_delay\n25    EWR  ORD      32.00\n33    EWR  MSP      29.00\n\n\n\nflights.loc[2:5, 'dest']  # returns as a Series\n\n2    MIA\n3    BQN\n4    ATL\n5    ORD\nName: dest, dtype: object\n\n\n\nflights.loc[2:5, ['dest']]  # return as a DataFrame\n\n  dest\n2  MIA\n3  BQN\n4  ATL\n5  ORD\n\n\n\n\n\n\n\n\nTip\n\n\n\n생략 표시\nflights.loc[2:5, :]  # ':' means all\nflights.loc[2:5]\nflights.loc[2:5, ]  # flights.loc[ , ['dest', 'origin']]은 에러\n\n\n\n# select a single row\nflights.loc[2, :]  # returns as a Series, column names as its index\n\nyear         2013\nmonth           1\n            ...  \ndest          MIA\nair_time   160.00\nName: 2, Length: 15, dtype: object\n\n\n\n# select a single row\nflights.loc[[2], :]  # returns as a DataFrame\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00  \n\n\n\n\n\n.iloc: position-based indexing\n\nSlicing의 경우 as usual: first index는 inclusive, last index는 exclusive\n\n\nflights.iloc[2:5, 12:14]  # 2:5는 index의 position, last index는 미포함\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL\n\n\n\nflights.iloc[2:5, 12]  # return as a Series\n\n2    JFK\n3    JFK\n4    LGA\nName: origin, dtype: object\n\n\n\nflights.iloc[2:5, :]\n# 다음 모두 가능\n# flights.iloc[2:5]\n# flights.iloc[2:5, ]\n\n# flights.iloc[, 2:5]는 에러\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n3  2013      1    1    544.00             545      -1.00   1004.00   \n4  2013      1    1    554.00             600      -6.00    812.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00  \n3            1022     -18.00      B6     725  N804JB    JFK  BQN    183.00  \n4             837     -25.00      DL     461  N668DN    LGA  ATL    116.00  \n\n\n\nflights.iloc[2:5, [12]]  # return as a DataFrame\n\n  origin\n2    JFK\n3    JFK\n4    LGA\n\n\n\nflights.iloc[[2, 5, 7], 12:14]  # 특정 위치의 rows 선택\n\n  origin dest\n2    JFK  MIA\n5    EWR  ORD\n7    LGA  IAD\n\n\n\n\n\n\n\n\nNote\n\n\n\n단 하나의 scalar 값을 추출할 때, 빠른 처리를 하는 다음을 사용할 수 있음\n.at[i, j], .iat[i, j]",
    "crumbs": [
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#series의-indexing",
    "href": "contents/subsetting.html#series의-indexing",
    "title": "Subsetting",
    "section": "Series의 indexing",
    "text": "Series의 indexing\nDataFrame과 같은 방식으로 이해\nIndex가 numbers인 경우\n\n\n42    DFW\n2     MIA\n25    ORD\n14    DFW\n33    MSP\nName: dest, dtype: object\n\n\n\ns.loc[25:14]\n\n25    ORD\n14    DFW\nName: dest, dtype: object\n\n\n\ns.iloc[2:4]\n\n25    ORD\n14    DFW\nName: dest, dtype: object\n\n\n\ns[:3]\n\n42    DFW\n2     MIA\n25    ORD\nName: dest, dtype: object\n\n\n\n\n\n\n\n\nNote\n\n\n\n다음과 같은 경우 혼동스러움\ns[3] # 3번째? label 3?\n#&gt; errors occur\n\n\n Index가 lables인 경우 다음과 같이 편리하게 subsetting 가능\n\n\nred       MIA\nblue      BQN\ngreen     ATL\nyellow    ORD\nName: dest, dtype: object\n\n\n\ns[\"red\":\"green\"]\n\nred      MIA\nblue     BQN\ngreen    ATL\nName: dest, dtype: object\n\n\n\ns[[\"red\", \"green\"]]\n\nred      MIA\ngreen    ATL\nName: dest, dtype: object",
    "crumbs": [
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#boolean-indexing",
    "href": "contents/subsetting.html#boolean-indexing",
    "title": "Subsetting",
    "section": "Boolean indexing",
    "text": "Boolean indexing\n\nBracket [ ] 이나 loc을 이용\niloc은 적용 안됨\n\n\nBracket [ ]\n\nnp.random.seed(123)\nflights_6 = flights[:100][[\"dep_delay\", \"arr_delay\", \"origin\", \"dest\"]].sample(6)\nflights_6\n\n    dep_delay  arr_delay origin dest\n8       -3.00      -8.00    JFK  MCO\n70       9.00      20.00    LGA  ORD\n..        ...        ...    ...  ...\n63      -2.00       2.00    JFK  LAX\n0        2.00      11.00    EWR  IAH\n\n[6 rows x 4 columns]\n\n\n\nflights_6[flights_6[\"dep_delay\"] &lt; 0]\n\n    dep_delay  arr_delay origin dest\n8       -3.00      -8.00    JFK  MCO\n82      -1.00     -26.00    JFK  SFO\n63      -2.00       2.00    JFK  LAX\n\n\n\nidx = flights_6[\"dep_delay\"] &lt; 0\nidx # bool type의 Series\n\n8      True\n70    False\n      ...  \n63     True\n0     False\nName: dep_delay, Length: 6, dtype: bool\n\n\n\n# Select a column with the boolean indexing\nflights_6[idx][\"dest\"]\n\n8     MCO\n82    SFO\n63    LAX\nName: dest, dtype: object\n\n\n\n\n\n\n\n\nNote\n\n\n\n사실, boolean indexing을 할때, DataFrame/Series의 index와 match함\n대부분 염려하지 않아도 되나 다음과 같은 결과 참고\n# Reset index\nidx_reset = idx.reset_index(drop=True)\n# 0     True\n# 1    False\n# 2     True\n# 3    False\n# 4     True\n# 5    False\n# Name: dep_delay, dtype: bool\n\nflights_6[idx_reset][\"dest\"]\n#&gt; IndexingError: Unalignable boolean Series provided as indexer \n#&gt; (index of the boolean Series and of the indexed object do not match)\n\n# Index가 없는 numpy array로 boolean indexing을 하는 경우 문제없음\nflights_6[idx_reset.to_numpy()][\"dest\"]\n# 8     MCO\n# 82    SFO\n# 63    LAX\n# Name: dest, dtype: object\n\n\n\nbool_idx = flights_6[[\"dep_delay\", \"arr_delay\"]] &gt; 0\nbool_idx\n\n    dep_delay  arr_delay\n8       False      False\n70       True       True\n..        ...        ...\n63      False       True\n0        True       True\n\n[6 rows x 2 columns]\n\n\n\nidx_any = bool_idx.any(axis=1)\nidx_any\n\n8     False\n70     True\n      ...  \n63     True\n0      True\nLength: 6, dtype: bool\n\n\n\nbool_idx.all(axis=1)\n\n8     False\n70     True\n      ...  \n63    False\n0      True\nLength: 6, dtype: bool\n\n\n\n\nnp.where() 활용\nnp.where(boolean condition, value if True, value if False)\n\nflights_6[\"delayed\"] = np.where(idx, \"delayed\", \"on-time\")\nflights_6\n\n    dep_delay  arr_delay origin dest  delayed\n8       -3.00      -8.00    JFK  MCO  delayed\n70       9.00      20.00    LGA  ORD  on-time\n..        ...        ...    ...  ...      ...\n63      -2.00       2.00    JFK  LAX  delayed\n0        2.00      11.00    EWR  IAH  on-time\n\n[6 rows x 5 columns]\n\n\n\nnp.where(flights_6[\"dest\"].str.startswith(\"S\"), \"S\", \"T\")  # str method: \"S\"로 시작하는지 여부\n\narray(['T', 'T', 'S', 'S', 'T', 'T'], dtype='&lt;U1')\n\n\n\nflights_6[\"dest_S\"] = np.where(flights_6[\"dest\"].str.startswith(\"S\"), \"S\", \"T\")\nflights_6\n\n    dep_delay  arr_delay origin dest  delayed dest_S\n8       -3.00      -8.00    JFK  MCO  delayed      T\n70       9.00      20.00    LGA  ORD  on-time      T\n..        ...        ...    ...  ...      ...    ...\n63      -2.00       2.00    JFK  LAX  delayed      T\n0        2.00      11.00    EWR  IAH  on-time      T\n\n[6 rows x 6 columns]\n\n\n\n\nloc\n\nflights_6.loc[idx, \"dest\"]  # flights_6[idx][\"dest\"]과 동일\n\n8     MCO\n82    SFO\n63    LAX\nName: dest, dtype: object\n\n\n만약 column 이름에 “time”을 포함하는 columns만 선택하고자 하면\n\nSeries/Index object는 str method 존재\nstr.contains(), str.startswith(), str.endswith()\n자세한 사항은 7.4 String Manipulation/String Functions in pandas by Wes McKinney\n\n\ncols = flights.columns.str.contains(\"time\")  # str method: \"time\"을 포함하는지 여부\ncols\n\narray([False, False, False,  True,  True, False,  True,  True, False,\n       False, False, False, False, False,  True])\n\n\n\n# Columns 쪽으로 boolean indexing\nflights.loc[:, cols]\n\n        dep_time  sched_dep_time  arr_time  sched_arr_time  air_time\n0         517.00             515    830.00             819    227.00\n1         533.00             529    850.00             830    227.00\n...          ...             ...       ...             ...       ...\n336774       NaN            1159       NaN            1344       NaN\n336775       NaN             840       NaN            1020       NaN\n\n[336776 rows x 5 columns]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nChained indexing으로 값을 assign하는 경우 copy vs. view 경고 메세지\nflights[flights[\"arr_delay\"] &lt; 0][\"arr_delay\"] = 0\n/var/folders/mp/vcywncl97ml2q4c_5k2r573m0000gn/T/ipykernel_96692/3780864177.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n 경고가 제시하는데로 .loc을 이용하여 assign\nflights.loc[flights[\"arr_delay\"] &lt; 0, \"arr_delay\"] = 0",
    "crumbs": [
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#summary",
    "href": "contents/subsetting.html#summary",
    "title": "Subsetting",
    "section": "Summary",
    "text": "Summary\n\nBracket [ ]의 경우\n\n간단히 columns을 선택하고자 할때 column labels: df[[\"var1\", \"var2\"]]\n간단히 rows를 선택하고자 할때 numerical indexing: df[:10]\n\nDot-notation은\n\npandas의 methods와 중복된 이름을 피하고,\nassignment의 왼편에는 사용을 피할 것\n\n가능하면 분명한 loc 또는 iloc을 사용\n\nloc[:, [\"var1\", \"var2\"]]는 df[[\"var1\", \"var2\"]]과 동일\niloc[:10, :]은 df[:10]와 동일\nloc의 경우, index가 숫자라 할지라도 label로 처리됨\nloc은 iloc과는 다른게 slicing(:)에서 first, last index 모두 inclusive\n\nBoolean indexing의 경우\n\nBracket [ ]: df[bool_idx]\nloc: df.loc[bool_idx, :]\niloc 불가\n\nAssignment를 할때는,\n\nchained indexing을 피하고: df[:5][\"dest\"]\nloc or iloc 사용:\n\ndf.loc[:4, \"dest\"]: index가 0부터 정렬되어 있다고 가정했을 때, slicing에서 위치 하나 차이남\ndf.iloc[:5, 13]: “dest”의 column 위치 13\n\n\n한 개의 column 혹은 row을 선택하면 Series로 반환: df[\"var1\"] 또는 df.loc[2, :]\n\n\n\n\n\n\n\nNote\n\n\n\nNumpy의 indexing에 대해서는 교재 참고\nCh.4/Basic Indexing and Slicing in Python Data Analysis by Wes McKinney",
    "crumbs": [
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/vis-intro.html",
    "href": "contents/vis-intro.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Python의 시각화 라이브러리는 다양하게 개발되어지고 있으며, 각기 특성이 달라 하나로만 쓰기 어려운 상황임\n\n\nMatplotlib\n\n가장 오래된 Python과 잘 통합된 널리 사용되는 라이브러리\n거의 가능한 모든 플랏을 그릴 수 있음\n한편, 디테일한 부분을 모두 specify해야 함으로써 많은 코딩이 요구되며, interactive 또는 web graphs에 취약함\n\npandas\n\nMatplotlib로 구현된 DataFrame의 method로 간략하게 시각화가 가능하며, 빠르게 데이터를 들여다볼 수 있음\n\nSeaborn & the seaborn.objects interface\n\nMatplotlib 위에 개발된 간결한 문법의 high-level 언어\nDecalative: 변수들이 어떤 시각화 속성과 위치를 지니는지만 specify\n“Grammer of graphics”라는 시각화 문법에 충실하고자 the seaborn.objects로 새롭게 변화 중\n\n\n\n\nAltair\n\n“Grammer of graphics”를 충실히 따라 설계됨\n각 plot이 이미지가 아닌 data + specification으로 이루어짐: 이미지가 저장되지 않고, 브라우저에서 이미지로 complie되어 생성됨\nWeb-based interactive 시각화인 D3에 그 모체를 두며, Vega/Vega-Lite로부터 파생됨\njavascript-based로 interactive 시각화에 용이하나 Python과 연계가 부족한 부분이 있고, 지원/커뮤니티가 미흡함\n\nBokeh\nPlotly\n다양한 언어(R, Python, Julia)을 지원하며, 기업 수준의 상용화 제품들도 있으며, 지원군 많음\n\nJake VanderPlas의 2017년 발표 자료 중: The Python Visualization Landscape\n\nSource: Jake VanderPlas - The Python Visualization Landscape PyCon 2017",
    "crumbs": [
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#대표적-도구들",
    "href": "contents/vis-intro.html#대표적-도구들",
    "title": "Data Visualization",
    "section": "",
    "text": "Matplotlib\n\n가장 오래된 Python과 잘 통합된 널리 사용되는 라이브러리\n거의 가능한 모든 플랏을 그릴 수 있음\n한편, 디테일한 부분을 모두 specify해야 함으로써 많은 코딩이 요구되며, interactive 또는 web graphs에 취약함\n\npandas\n\nMatplotlib로 구현된 DataFrame의 method로 간략하게 시각화가 가능하며, 빠르게 데이터를 들여다볼 수 있음\n\nSeaborn & the seaborn.objects interface\n\nMatplotlib 위에 개발된 간결한 문법의 high-level 언어\nDecalative: 변수들이 어떤 시각화 속성과 위치를 지니는지만 specify\n“Grammer of graphics”라는 시각화 문법에 충실하고자 the seaborn.objects로 새롭게 변화 중\n\n\n\n\nAltair\n\n“Grammer of graphics”를 충실히 따라 설계됨\n각 plot이 이미지가 아닌 data + specification으로 이루어짐: 이미지가 저장되지 않고, 브라우저에서 이미지로 complie되어 생성됨\nWeb-based interactive 시각화인 D3에 그 모체를 두며, Vega/Vega-Lite로부터 파생됨\njavascript-based로 interactive 시각화에 용이하나 Python과 연계가 부족한 부분이 있고, 지원/커뮤니티가 미흡함\n\nBokeh\nPlotly\n다양한 언어(R, Python, Julia)을 지원하며, 기업 수준의 상용화 제품들도 있으며, 지원군 많음\n\nJake VanderPlas의 2017년 발표 자료 중: The Python Visualization Landscape\n\nSource: Jake VanderPlas - The Python Visualization Landscape PyCon 2017",
    "crumbs": [
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#the-grammer-of-graphics",
    "href": "contents/vis-intro.html#the-grammer-of-graphics",
    "title": "Data Visualization",
    "section": "The Grammer of Graphics",
    "text": "The Grammer of Graphics\nA coherent system for describing and building graphs\nSource: Fundamentals of Data Visualization by Claus O. Wilke\nAesthetics and types of data\n\n데이터의 값을 특정 aesthetics에 mapping\n\n데이터의 타입은 다음과 같이 나누어짐\n\ncontinuous / discrete\nquatitative / qualitative\ncategorical unordered (nominal) / categorical ordered (ordinal)\n\n성별, 지역 / 등급, 랭킹\nordinal: 등간격을 가정\n\n퀄리티 good, fair, poor는 등간격이라고 봐야하는가?\n랭킹은?\n선호도 1, 2, …, 8; continuous?\n임금 구간?\n\n\n\n데이터 타입에 따라 좀 더 적절한 aesthetic mapping이 있으며,\n같은 정보를 품고 있는 시각화라도 더 적절한 representation이 존재\nBertin’s Semiology of Graphics (1967)\nLevels of organization\n\nSource: Jake VanderPlas’ presentation at PyCon 2018\n\nCase 1\n예를 들어, 다음과 같이 1) 지역별로 2) 날짜에 따른 3) 온도의 변화를 나타낸다면,\n즉, x축의 위치에 날짜 정보를, y축의 위치에 온도 정보를, 색깔에 지역 정보를 할당했음.\n\n한편, 아래는 x축의 위치에 압축된 날짜 정보를, y축의 위치에 지역 정보를, 색깔에 압축된 온도 정보를 할당했음.\n\n\n\nCase 2\n다음은 GDP, mortality, population, region의 네 정보를 다른 방식으로 mapping한 결과임.",
    "crumbs": [
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#탐색적-exploratory-vs.-정보전달-communicative",
    "href": "contents/vis-intro.html#탐색적-exploratory-vs.-정보전달-communicative",
    "title": "Data Visualization",
    "section": "탐색적 (Exploratory) vs. 정보전달 (Communicative)",
    "text": "탐색적 (Exploratory) vs. 정보전달 (Communicative)\n\n분석도구: 현미경, 연장도구\n강점이자 약점",
    "crumbs": [
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#interative-plots",
    "href": "contents/vis-intro.html#interative-plots",
    "title": "Data Visualization",
    "section": "Interative Plots",
    "text": "Interative Plots\nAltair\n\n\n\n\n\n\n\nPlotly",
    "crumbs": [
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/notice.html#중간고사-4.29-월",
    "href": "contents/notice.html#중간고사-4.29-월",
    "title": "Notice",
    "section": "중간고사 (4.29 월)",
    "text": "중간고사 (4.29 월)\n\nOverview\nTwo Cultures\nLinear Models\n파이썬 코드는 포함되지 않음.",
    "crumbs": [
      "Notice"
    ]
  },
  {
    "objectID": "contents/shrinkage.html",
    "href": "contents/shrinkage.html",
    "title": "Shrinkage",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 5, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n\n\n\nfrom ISLP import load_data\nfrom ISLP.models import ModelSpec\nfrom functools import partial\n\nfrom statsmodels.api import OLS\nimport sklearn.model_selection as skm\nimport sklearn.linear_model as skl\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.pipeline import Pipeline\nfrom ISLP.models import sklearn_selected, sklearn_selection_path\n\n\nHitters = load_data('Hitters')\nHitters = Hitters.dropna()\nHitters.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 263 entries, 1 to 321\nData columns (total 20 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   AtBat      263 non-null    int64   \n 1   Hits       263 non-null    int64   \n 2   HmRun      263 non-null    int64   \n 3   Runs       263 non-null    int64   \n 4   RBI        263 non-null    int64   \n 5   Walks      263 non-null    int64   \n 6   Years      263 non-null    int64   \n 7   CAtBat     263 non-null    int64   \n 8   CHits      263 non-null    int64   \n 9   CHmRun     263 non-null    int64   \n 10  CRuns      263 non-null    int64   \n 11  CRBI       263 non-null    int64   \n 12  CWalks     263 non-null    int64   \n 13  League     263 non-null    category\n 14  Division   263 non-null    category\n 15  PutOuts    263 non-null    int64   \n 16  Assists    263 non-null    int64   \n 17  Errors     263 non-null    int64   \n 18  Salary     263 non-null    float64 \n 19  NewLeague  263 non-null    category\ndtypes: category(3), float64(1), int64(16)\nmemory usage: 38.1 KB\n\n\n\nMS = ModelSpec(Hitters.columns.drop(['Salary']), intercept=False)\ny = Hitters['Salary']\nX = MS.fit_transform(Hitters)\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nnp.logspace(-2, 8, 10)\n\narray([1.00000e-02, 1.29155e-01, 1.66810e+00, 2.15443e+01, 2.78256e+02,\n       3.59381e+03, 4.64159e+04, 5.99484e+05, 7.74264e+06, 1.00000e+08])\n\n\n\nfrom sklearn.linear_model import Ridge\n\n# standardize X\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# create an array of alpha values\nlambdas = np.logspace(-2, 10, 100) / y.std()  \n\nridge_coefs = []\nfor alpha in lambdas:\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_scaled, y)\n    ridge_coefs.append(ridge.coef_)\nridge_coefs = np.array(ridge_coefs)\n\n\nridge_coefs = pd.DataFrame(ridge_coefs, columns=X.columns, index=lambdas)\nridge_coefs.index.name = 'lambda'\nridge_coefs\n\n                   AtBat        Hits      HmRun       Runs        RBI  \\\nlambda                                                                  \n2.216712e-05 -291.094162  337.829131  37.853033 -60.571234 -26.994245   \n2.930362e-05 -291.094036  337.828697  37.852775 -60.570833 -26.994007   \n3.873766e-05 -291.093868  337.828124  37.852433 -60.570303 -26.993693   \n...                  ...         ...        ...        ...        ...   \n1.268484e+07    0.003685    0.004095   0.003202   0.003919   0.004195   \n1.676861e+07    0.002788    0.003098   0.002422   0.002965   0.003174   \n2.216712e+07    0.002109    0.002343   0.001832   0.002243   0.002401   \n\n                   Walks      Years      CAtBat      CHits     CHmRun  \\\nlambda                                                                  \n2.216712e-05  135.073570 -16.694157 -391.033663  86.691953 -14.178832   \n2.930362e-05  135.073465 -16.694414 -391.032055  86.693349 -14.177901   \n3.873766e-05  135.073326 -16.694753 -391.029931  86.695195 -14.176671   \n...                  ...        ...         ...        ...        ...   \n1.268484e+07    0.004143   0.003740    0.004911   0.005124   0.004900   \n1.676861e+07    0.003134   0.002829    0.003715   0.003876   0.003707   \n2.216712e+07    0.002371   0.002140    0.002810   0.002932   0.002804   \n\n                   CRuns        CRBI      CWalks  League[N]  Division[W]  \\\nlambda                                                                     \n2.216712e-05  480.740064  260.684702 -213.891103  31.248776   -58.414130   \n2.930362e-05  480.737788  260.683034 -213.890731  31.248781   -58.414151   \n3.873766e-05  480.734779  260.680828 -213.890240  31.248787   -58.414180   \n...                  ...         ...         ...        ...          ...   \n1.268484e+07    0.005252    0.005292    0.004572  -0.000133    -0.001797   \n1.676861e+07    0.003973    0.004003    0.003459  -0.000101    -0.001359   \n2.216712e+07    0.003006    0.003029    0.002616  -0.000076    -0.001028   \n\n                PutOuts    Assists     Errors  NewLeague[N]  \nlambda                                                       \n2.216712e-05  78.761297  53.732322 -22.160934    -12.348893  \n2.930362e-05  78.761297  53.732268 -22.160957    -12.348919  \n3.873766e-05  78.761297  53.732197 -22.160987    -12.348954  \n...                 ...        ...        ...           ...  \n1.268484e+07   0.002805   0.000237  -0.000050     -0.000026  \n1.676861e+07   0.002122   0.000180  -0.000038     -0.000020  \n2.216712e+07   0.001605   0.000136  -0.000029     -0.000015  \n\n[100 rows x 19 columns]\n\n\n\nridge_coefs.plot(figsize=(8, 6))\nplt.xlabel('$\\lambda$')\nplt.ylabel('Standardized coefficients')\nplt.xscale('log')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\npd.options.display.max_rows = 0\nbeta_hat = ridge_coefs.iloc[50, :]\nprint(f\"lambda: {lambdas[50]}, \\nbeta hats: \\n{beta_hat}\")\n\nlambda: 25.48679637046264, \nbeta hats: \nAtBat          -60.554231\nHits            94.445543\nHmRun          -11.652839\nRuns            29.082098\nRBI             20.558595\nWalks           61.290878\nYears          -32.326762\nCAtBat          11.255280\nCHits           72.074056\nCHmRun          52.153738\nCRuns           76.011242\nCRBI            73.044427\nCWalks         -45.183088\nLeague[N]       23.712255\nDivision[W]    -59.725489\nPutOuts         70.812650\nAssists         18.834114\nErrors         -22.285540\nNewLeague[N]    -5.619252\nName: 25.48679637046264, dtype: float64\n\n\n\n\n\n\n\n\nUsing Pipeline\n\n\n\n\n\n파이프라인을 사용해, 표준화와 Ridge 회귀를 함께 묶을 수 있음\nfrom sklearn.pipeline import Pipeline\n\nridge = skl.Ridge(alpha=25.486)\nscaler = StandardScaler()  # standardize\n\nridge_scaled = Pipeline([('scaler', scaler), ('ridge', ridge)])  \n\nridge_scaled.fit(X, y)\nridge.coef_\n\n# array([-60.5563 ,  94.44708, -11.65294,  29.08199,  20.55851,  61.29151,\n#        -32.32745,  11.25465,  72.07487,  52.154  ,  76.01231,  73.0451 ,\n#        -45.18461,  23.71244, -59.72562,  70.81285,  18.83444, -22.28569,\n#         -5.61946])\n\n\n\n\nEstimating Test Error of Ridge Regression\n\nridge4 = skl.Ridge(alpha=25)\nscaler = StandardScaler()  # standardize\n\nridge_scaled = Pipeline([('scaler', scaler), ('ridge', ridge4)])  \n\nridge_scaled.fit(X, y)\n\nPipeline(steps=[('scaler', StandardScaler()), ('ridge', Ridge(alpha=25))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()), ('ridge', Ridge(alpha=25))])  StandardScaler?Documentation for StandardScalerStandardScaler()  Ridge?Documentation for RidgeRidge(alpha=25) \n\n\n\nridge4.coef_\n\narray([-61.83616,  95.40164, -11.71174,  29.01527,  20.50437,  61.68053,\n       -32.74913,  10.86313,  72.57968,  52.3169 ,  76.67548,  73.45853,\n       -46.12262,  23.82647, -59.80705,  70.93771,  19.03332, -22.37589,\n        -5.75064])\n\n\n\nridge_scaled.named_steps['ridge'].coef_\n\narray([-61.83616,  95.40164, -11.71174,  29.01527,  20.50437,  61.68053,\n       -32.74913,  10.86313,  72.57968,  52.3169 ,  76.67548,  73.45853,\n       -46.12262,  23.82647, -59.80705,  70.93771,  19.03332, -22.37589,\n        -5.75064])\n\n\n\ncv = skm.ShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n\n\nresults = skm.cross_validate(ridge_scaled, X, y, scoring=\"neg_mean_squared_error\", cv=cv)\n-results[\"test_score\"]\n\narray([126726.7882])\n\n\n\nridge.coef_\n\narray([-61.83616,  95.40164, -11.71174,  29.01527,  20.50437,  61.68053,\n       -32.74913,  10.86313,  72.57968,  52.3169 ,  76.67548,  73.45853,\n       -46.12262,  23.82647, -59.80705,  70.93771,  19.03332, -22.37589,\n        -5.75064])\n\n\n\nridge2 = skl.Ridge(alpha=25)\nresults2 = skm.cross_validate(ridge2, X, y, scoring=\"neg_mean_squared_error\", cv=cv)\n-results2[\"test_score\"]\n\narray([134313.38247])\n\n\n\nridge2.fit(X_scaled, y).coef_\n\narray([-61.83616,  95.40164, -11.71174,  29.01527,  20.50437,  61.68053,\n       -32.74913,  10.86313,  72.57968,  52.3169 ,  76.67548,  73.45853,\n       -46.12262,  23.82647, -59.80705,  70.93771,  19.03332, -22.37589,\n        -5.75064])\n\n\n\nridge2.fit(X, y).coef_\n\narray([ -2.05716,   7.61961,   3.74274,  -2.26527,  -0.74349,   6.16906,\n        -2.98581,  -0.17189,   0.0973 ,  -0.21095,   1.51935,   0.81737,\n        -0.80669,  27.1358 , -83.08738,   0.2854 ,   0.3757 ,  -3.14053,\n         4.96431])\n\n\n\nresults = skm.cross_validate(ridge, X, y, scoring=\"neg_mean_squared_error\", cv=cv)\n-results[\"test_score\"]\n\narray([134313.38247])"
  }
]