[
  {
    "objectID": "contents/setup.html",
    "href": "contents/setup.html",
    "title": "Python 설정",
    "section": "",
    "text": "데이터 사이언스를 위한 Python 개발 환경\n몇 가지 선택지",
    "crumbs": [
      "Trees",
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#클라우드-환경",
    "href": "contents/setup.html#클라우드-환경",
    "title": "Python 설정",
    "section": "클라우드 환경",
    "text": "클라우드 환경\nColab\n\n사용법: Colab Welcome\n클라우드 환경 vs. 구글 드라이브 mount\nColab AI assistant\n구글 드라이브의 데이터셋을 import:\n\n\npd.read_csv(\"drive/MyDrive/...\")\n\n\n패키지 업데이트\n\n!pip install --upgrade pandas numpy seaborn matplotlib statsmodels scikit-learn",
    "crumbs": [
      "Trees",
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#로컬-환경",
    "href": "contents/setup.html#로컬-환경",
    "title": "Python 설정",
    "section": "로컬 환경",
    "text": "로컬 환경\nPython과 Conda Package Manager\nConda Cheatsheet: 기본적인 conda 명령어 요약\n\nMiniconda 설치\nAnaconda보다는 기본 패키지들이 미리 설치되지 않는 miniconda를 추천: miniconda install page\n\nWindows 경우: 설치시 물어보는 “add Miniconda to your PATH variable” 옵션을 켜고 설치할 것\n\nShell 사용에 대해서는 아래 Command Line Tool 참고\n\nWindows 경우: Anaconda의 응용 프로그램으로 등록된 Anaconda Powershell Prompt를 이용\nMac의 경우: 기본 terminal을 이용\n커서 앞에 (base)가 보이면 conda가 설치된 것\n\n# Terminal (Mac) or Miniconda Powershell Prompt (Windows)\n\n(base)&gt; conda info # 콘다 정보 \n(base)&gt; conda update conda # 콘다 업데이트\n\n\nConda Environment\nconda/user guide\n환경 생성: miniconda에서 자체 제공하는 환경 (다른 가상환경 툴인 pyenv나 venv도 있음)\n(base)&gt; conda create --name myenv  # --name 대신 -n으로 축약 가능\n\n# 특정 버전의 파이썬과 함께 설치시\n(base)&gt; conda create --name myenv python=3.12\n환경 확인\n(base)&gt; conda env list\n# conda environments:\n#\n# base         */.../miniconda3\n# myenv         /.../miniconda3/envs/myenv\n환경 제거\n(base)&gt; conda env remove --name myenv\n환경 activate/deactivate\n(base)&gt; conda activate myenv\n(myenv)&gt; conda deactivate\n특정 환경 안의 파이썬 버전 확인\n(myenv)&gt; python --version\n\n\n환경(activated) 내에서 패키지 설치 및 제거\n\n\n\n\n\n\n패키지 repository(channel) 선택\n\n\n\n\n\nconda/managing channels\n다음을 통해 .condarc 환경파일에 configuration 추가\n(base)&gt; conda config --add channels conda-forge\n(base)&gt; conda config --set channel_priority strict  # 채널 순으로 검색, 버전 순이 아니고\n# 개별적으로 채널을 선택해서 install하려면 (특정 환경에 설치하려면 아래 conda environment 참조)\n(base)&gt; conda install scipy --channel conda-forge\n\n# pakcage가 있는 채널들\n(base)&gt; conda search scipy\n\n\n\n# 특정 환경을 activate한 후\n\n# Python을 update하거나 다른 버전을 설치하려면, 가령 3.12으로 업데이트 하려면\n(myenv)&gt; conda install python=3.12  # python update\n\n# 패키지 설치\n(myenv)&gt; conda install &lt;package name1&gt; &lt;package name2&gt; ...\n# 특정한 채널, conda-forge 통한 설치: --channel 대신 -c로 축약 가능\n(myenv)&gt; conda install --channel conda-forge &lt;package name&gt;\n\n# 제거\n(myenv)&gt; conda remove &lt;package name1&gt; &lt;package name2&gt; ...\n\n# 업데이트\n(myenv)&gt; conda update &lt;package name1&gt; &lt;package name2&gt; ...\n(myenv)&gt; conda update --all  # all packages\n\n# 패키지 리스트 확인\n(myenv)&gt; conda list\n환경 밖에서 특정 환경 안에 설치하려면 환경 이름 추가\n(base)&gt; conda install --name myenv &lt;package name1&gt;  # --name 대신 -n으로 축약 가능\npip을 이용한 패키지 설치: conda repository에 없는 패키지들을 설치하는 경우. 충돌의 우려 있음\n(myenv)&gt; pip install &lt;package name1&gt; &lt;package name2&gt; ...\n수업에 필요한 기본 패키지 설치\n# 수업에 필요한 기본 패키지 설치\n(myenv)&gt; conda install jupyter numpy pandas matplotlib seaborn scikit-learn statsmodels\n\n\n\n\n\n\nCommand Line Tools\n\n\n\n\n\n\nMac의 경우: 기본 terminal을 이용하되 기본 zsh shell 대신 다음 Oh-My-Zsh을 추천\nOh-My-Zsh!: 링크\n\n\nWindows의 경우: Windows Terminal 추천\n\n설치 링크는 구글링…\n명령프롬프트(CMD) vs. Powershell\nPowershell에서 conda를 사용하기 위해서는 몇 가지 설정 필요: 블로그 링크\n잘 안될 경우, conda 설치시 함께 설치되는 응용프로그램 콘다 powershell을 이용",
    "crumbs": [
      "Trees",
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#sec-vscode",
    "href": "contents/setup.html#sec-vscode",
    "title": "Python 설정",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\nVS Code 설치\n\n개인마다 선호하는 text editor가 있으나 본 수업에서는 VS Code로 진행: download and install here\n\n\n\nExtensions\n\nPython\nPython Extension Pack 중\n\nIntelliCode\nPython Environment Manager\n\nPylance: 문법 체크, 자동완성, …\nDocs View\n\n안 보일시, 설정에서 language server를 default(Pylance)에서 Jedi로 바꾸면 해결\n\nCopilot…\n\n\n\nPreferences\n\nThemes\nFont, font size (notebook, markup, output)\n\n\n\nShortcuts\nShow Command Palette: ctrl(cmd) + shift + p, 또는 F1\nCell 안과 밖에서 다르게 작동\n\nundo / redo : ctrl(cmd) + z / ctrl(cmd) + shift + z\nmove: alt(option) + arrow up/down\ncopy : alt(option) + shift + arrow up/down\n\n코드 실행 방식 3가지: ctrl/shift/alt(option) + enter\nHelp: Keyboard shortcuts reference의 Basic editing 참고\n\n\n그 외\n\ninteractive mode\nexport\ndocs view: sticky mode\nvariables viewer, data viewer\nformatter: “Black formatter”\nsnippets: 구글링…\n\n\n\nVS Code내에서 terminal 사용\nTerminal: Select Default Profile에서 선택\n\nMac: zsh\nWindows: powershell",
    "crumbs": [
      "Trees",
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#jupyter-notebooklab",
    "href": "contents/setup.html#jupyter-notebooklab",
    "title": "Python 설정",
    "section": "Jupyter Notebook/Lab",
    "text": "Jupyter Notebook/Lab\n\n\n\n\n\n\n콘다 환경 등록\n\n\n\n\n\n새로 만든 환경을 등록해줘야 함. 환경을 activate한 상태에서\n(myenv)&gt; ipython kernel install --user --name=myenv\n환경을 삭제해도 등록시킨 kernel 이름은 삭제되지 않으니 직접 삭제.\n등록된 커널 리스트를 확인\n(myenv)&gt; jupyter kernelspec list\n커널 삭제\n(myenv)&gt; jupyter kernelspec remove myenv\n\n\n\nJupyter Notebook 또는 lab 실행\n\nAnaconda 응용 프로그램을 이용해 실행하거나,\n쉘에서 실행하려면,\n\n# jupytet notebook\n(base)&gt; jupyter notebook\n\n# jupyter lab\n(base)&gt; jupyter lab\n등록한 커널을 선택 후 시작\n커널을 종료하려면, 쉘에서 Ctrl-C 두 번",
    "crumbs": [
      "Trees",
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/setup.html#python-packages-modules-functions",
    "href": "contents/setup.html#python-packages-modules-functions",
    "title": "Python 설정",
    "section": "Python Packages, Modules, Functions",
    "text": "Python Packages, Modules, Functions\nJupyter notebook 파일을 생성: filename.ipynb\n\nimport numpy as np\nimport pandas as pd\nfrom numpy.linalg import inv\n\n\n\n\n\n\n\n두 개 이상의 함수/모듈을 import하거나 as로 별칭 지정 가능\n\n\n\n\n\nfrom numpy.linalg import inv as inverse\nfrom numpy.linalg import inv, det\nfrom numpy.linalg import inv as inverse, det as determinant\n\n\n\nnp.linalg?  # 함수에 대한 도움말\nNumPy 패키지(package)의 linalg 모듈(module)\nnp.linalg  # linalg.py 파이썬 스트립트 파일\nlinalg 모듈 파일 안에 def으로 정의된 함수\n\nnp.linalg.inv  # 모듈 안에서 def으로 정의된 함수 inv()\n\nnp.linalg.inv?  # 함수에 대한 도움말\n예를 들어, 행렬의 역행렬을 구하는 함수 inv를 사용하려면\n\nrng = np.random.default_rng(123)  # random number generator\nx = rng.standard_normal((3, 3))  # 3x3 matrix from standard normal distribution\nx\n\n어떻게 import하느냐에 따라 다른 방식으로 사용\n\ninv(x)  # inverse matrix of x\n\n# inv 함수를 따로 import하지 않은 경우, numpy의 linalg 모듈을 통해 사용\nnp.linalg.inv(x)  # same as above\n\n주로 모듈 이름을 함께 쓰는 것이 관례인데,\n\n이는 코드의 가독성을 높이고,\n사용자 정의 함수와의 충돌을 방지하기 위함\n\n모듈 안에 정의된 함수들을 확인하려면: dir() 함수\n\ndir(np.linalg)  # 모듈 안에 정의된 함수들",
    "crumbs": [
      "Trees",
      "Introduction",
      "Python Setup"
    ]
  },
  {
    "objectID": "contents/two-cultures.html",
    "href": "contents/two-cultures.html",
    "title": "Two Cultures",
    "section": "",
    "text": "오랜동안 여러 분야에서 각자의 방식을 개발\nComputer Science\nStatistics\nBiostatistics\nEconomics\nEpidemiology\nPolitical Science\nEngineering\n\n\n\n서로 다른 용어를 쓰기도 하며, 그 분야에서 필요로하는 방식에 초점을 맞춤.\n\n서로 의사소통이 거의 없었음.\n\nData Science라는 이름하에 통합되어가는 과정 중\n\n\n\n\n컴퓨터 사이언스의 경우, 데이터에 존재하는 패턴을 학습하여 분류를 하거나 예측을 위한 이론과 툴들이 개발되는 반면,\n과학자들은 예측보다는, 변수들 간의 진정한 관계 혹은 인과 관계를 탐구\n현재 이 둘은 소위 cross-fertilization을 지향하며 같이 발전, 통합되어가고 있음.",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#the-data-modeling-culture",
    "href": "contents/two-cultures.html#the-data-modeling-culture",
    "title": "Two Cultures",
    "section": "The Data Modeling Culture",
    "text": "The Data Modeling Culture\n\n\\(Y\\) = \\(f(X, \\text{random noise, parameters})\\)\n\n모델의 타당도(validation): goodness-of-fit 테스트와 잔차의 검토\n통계학자의 98%\n\n\nParameter Estimation & Uncertainty\n\n현재 관찰된 데이터는 어떤 모집단(population)으로부터 (독립적으로) 발생된 표본(sample)이라고 가정\n\n변수들 간의 관계성(relationships)을 파악하기 위해, 데이터 모델링은 현상에 대한 모델을 사전 설정(assumptions)하고,\n\n예를 들어, 선형관계를 가정: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e\\)\n\n변수들의 값이 어떻게 발생하는지(generated)에 대한 가정을 세우고,\n\n예를 들어, Gaussian, Binormial distribution, …\n\n데이터와 가장 적합한(best fitted) 특정 모델을 선택 (즉, 파라미터 \\(\\beta s\\)를 추정)\n\n즉, 위의 선형 함수가 X, Y의 1) 관계를 나타내고, 2) 예측을 위해 사용됨\n노이즈(noise)로부터 시그널(signal)을 분리: “true relationships”\n\n그 파라미터의 불확실성(uncertainty)을 추정\n\n모집단에 대한 추정이므로 불확실성이 존재\n\n\n예를 들어,\n\n\n\n\nSource: Kids Who Get Smartphones Earlier Become Adults With Worse Mental Health\n\n\n\n\nSource: Racial differences in homicide rates are poorly explained by economics\n\n\n\n\n\n\n\n\n\n\n\nSource: Jiang, W., Lavergne, E., Kurita, Y., Todate, K., Kasai, A., Fuji, T., & Yamashita, Y. (2019). Age determination and growth pattern of temperate seabass Lateolabrax japonicus in Tango Bay and Sendai Bay, Japan. Fisheries science, 85, 81-98.\n\n단순한 선형 모델의 예\n다이아몬드 가격에 대한 예측 모델\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n가정들:\nX와 Y의 간의 true relationship: \\(Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\)\nNoise/residual의 분포: \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\n\nMean function: \\(E(Y | X = x_i) = \\beta_0 + \\beta_1 x_i\\)\nVariance function: \\(Var(Y | X = x_i) = \\sigma^2\\)\nDistribution: \\((Y | X = x_i) \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\)\n\n파라미터의 추정 및 불확실성:\n\n\\(\\hat{\\beta}_0 = 0.4, \\hat{\\beta}_1 = 5123, \\hat{\\sigma} = 300\\)\n95%의 확률로 \\(\\beta_0 \\in (0.3, 0.5), \\beta_1 \\in (4823, 5423)\\)\n\\(\\widehat{price}_i = 0.4 + 5123 \\cdot carat\\) : “평균적으로 다이아몬드가 1 carat 커질 때마다 $5123 비싸짐”\n노이즈로부터 관계에 관한 시그널을 추출한 것으로 볼 수 있음\n\n\n\n데이터 모델의 한계\n\n과연 가정한 모델이 자연/현상을 잘 모방하고 있는가?\n\n\nhas at its heart the belief that a statistician, by imagination and by looking at the data, can invent a reasonably good parametric class of models for a complex mechanism devised by nature.\n핵심은 통계학자가 상상력을 발휘하고 데이터를 살펴보면 자연이 고안한 복잡한 메커니즘에 대해 합리적으로 좋은 파라메트릭 클래스의 모델을 발명할 수 있다는 믿음입니다. (번역 by DeepL)\n\n\nThe belief in the infallibility of data models was almost religious. It is a strange phenomenon—once a model is made, then it becomes truth and the conclusions from it are infallible.\n데이터 모델의 무오류성에 대한 믿음은 거의 종교에 가까웠습니다. 일단 모델이 만들어지면 그것이 진리가 되고 그 모델에서 도출된 결론은 오류가 없다고 믿는 이상한 현상입니다. (번역 by DeepL)\n\n\n모델이 데이터에 얼마나 잘 맞는지에 대한 논의가 거의 없음\n\n특히, 주로 예-아니오로 답하는 적합도 테스트를 통해 모델의 적합성을 판단\n\n주로 독창적인 확률 모형을 찾는데 주력\n(가정한) 모델을 데이터에 맞출 때 결론은 자연의 메커니즘이 아니라 모델의 메커니즘에 관한 것이 됨\n모델이 자연을 제대로 모방하지 못하면 결론이 잘못될 수 있음\n\n데이터 모델의 다양성\n\n데이터 모델링의 가장 큰 장점: 입력 변수와 응답 간의 관계를 간단하고 이해하기 쉬운 그림으로 표현 가능\n하지만, 데이터에 동일하게 적합한 여러 모델이 존재\n\n적합성(goodness-of-fit)에 대한 기준이 통계적 검정을 통한 예-아니오로 판별\n\n관찰한 데이터로만 모델의 파라미터를 추정하기 때문에, 과적합이 발생하며, 새로운 데이터에 대한 예측 정확도가 떨어질 수 있음\n편향되지 않은 예측 정확도 추정치를 얻으려면 교차 검증(cross-validation)을 사용하거나 일부 데이터를 테스트 집합(test set)으로 따로 떼어 놓을 필요가 있음\n\n모형의 가정에 위배\n\n일반적으로 의료 데이터, 재무 데이터와 같이 복합 시스템에서 생성된 데이터에 단순한 파라메트릭 모델을 적용하면 알고리즘 모델에 비해 정확성과 정보가 손실될 수 있음\n현실에서 데이터의 발생 메커니즘(확률 분포)에 대한 가정이 성립되기 어려움\n모델을 가정하기보다는 데이터와 실제로 처한 문제로부터 해결책을 찾아 갈 필요가 있음.\n\n\n“If all a man has is a hammer, then every problem looks like a nail.”\n\n\n\n\n\n\n\nBayesian Models\n\n\n\n\n\n불확실성에 대한 가정에 대한 다른 접근\n        \n\nSource: McElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#the-algorithmic-modeling-culture",
    "href": "contents/two-cultures.html#the-algorithmic-modeling-culture",
    "title": "Two Cultures",
    "section": "The Algorithmic Modeling Culture",
    "text": "The Algorithmic Modeling Culture\n\n자연의 복잡하고 신비하며 적어도 부분적으로는 알 수 없는 블랙박스에서부터 데이터가 생성된다고 가정\n데이터로부터 반응 \\(Y\\)를 예측을 하기 위해 \\(X\\)에 작용하는 알고리즘 함수 \\(f(X)\\)를 찾고자 함\n\n가정: 데이터가 어떤 분포로부터 독립적으로 발생(i.i.d. Independent & identically distributed)\n모델의 타당도 지표: 예측 정확도 (predictive accuracy)\n통계학자의 2%\n\n예를 들어, 야구 선수의 연봉을 예측하기 위한 결정 트리 모델 (regression tree)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n다양한 방식의 여러 결정 트리 모델을 생성 후\n이들을 결합(aggregating)하여 평균을 내어 예측 정확도를 높일 수 있음\n\n새로운 연구 커뮤니티\n\n젊은 컴퓨터 과학자, 물리학자, 엔지니어와 나이든 몇 명의 통계학자 등이 주도\n1980년대 초 리처드 올슨의 연구를 시작으로 의료 데이터 분석에 조금씩 진출하기 시작\n1980년대 중반에 두 가지 강력한 새 알고리즘, 즉 신경망(neural network)과 의사 결정 트리(tree)가 등장\n1990년대 통계학에서도 smoothing spline 알고리즘, cross validation을 사용한 데이터에 대한 적용에 관한 연구가 존재\n1990년대 중반에는 Vapnik의 서포트 벡터 머신(support vector machine)이 등장\n예측 정확도를 목표로, 음성 인식, 이미지 인식, 비선형 시계열 예측, 필기 인식, 금융 시장 예측 등 데이터 모델을 적용할 수 없는 것이 분명한 복잡한 예측 문제를 해결\n\n\n\n머신 러닝 분야로부터의 레슨\n좋은 모델의 다양성 (multiplicity)\n\n예측도가 비슷한 전혀 다른 모델이 존재할 수 있는데\n이 모델들을 결합(aggregating)하면 예측 정확도를 높일 수 있으며, 단일한 모델로 환원할 수 있음\n\n단순성 대 정확성 (simplicity vs. accuracy)\nThe Occam’s Dilemma\n\n예측에 있어 정확성과 단순성(해석 가능성)은 상충됨\n\n정확도를 높이려면 더 복잡한 예측 방법이 요구\n단순하고 해석 가능한 함수는 예측력이 높지 못함\n\n예측 정확도를 먼저 추구한 후 그 이유를 이해하는 것이 더 낫다고 제안\n목표 지향적인 관점에서 보면 오컴의 딜레마는 존재하지 않음\n\n차원의 저주 (the curse of dimensionality)\nDigging It Out in Small Pieces\n\n전통적으로 변수가 많을수록 좋지 않다고 여겼으나,\nTree나 neural network에서는 변수가 많은 것이 문제가 되지 않고, 오히려 작은 정보들이 추가됨\n\n예를 들어, 30개의 예측 변수로부터 4차항들을 추가하면 약 40,000개의 새로운 변수가 생성됨\n이들의 정보는 분류에 도움이 되어 예측 정확도를 높일 수 있음\n\n\n\n   \n\n\n블랙박스로부터의 정보 추출\nThe goal is not interpretability, but accurate information.\n\n“정확성(accuracy)과 해석 가능성(interpretability) 중 하나를 선택해야 한다면, 그들은 해석 가능성을 택할 것입니다.  정확성과 해석 가능성 사이의 선택으로 질문을 구성하는 것은 통계 분석의 목표가 무엇인지에 대한 잘못된 해석입니다.  모델의 핵심은 응답(Y)과 예측 변수(X) 간의 관계에 대한 유용한 정보를 얻는 것입니다.  해석 가능성은 정보를 얻는 한 가지 방법입니다.  그러나 예측 변수와 응답 변수 간의 관계에 대한 신뢰할 수 있는 정보를 제공하기 위해 모델이 반드시 단순할 필요는 없으며, 데이터 모델일 필요도 없습니다.” (번역 by DeepL)\n\n\n예측 정확도가 높을수록 기저에 있는 데이터 메커니즘에 대한 더 신뢰할 수 있는 정보가 내재함\n예측 정확도가 낮으면 의심스러운 결론을 내릴 수 있음\n알고리즘 모델은 데이터 모델보다 예측 정확도가 더 높으며, 기본 메커니즘에 대한 더 나은 정보를 제공할 수 있음.\n\n예를 들어,\n\n의료 데이터와 같이 변수가 데이터에 비해 상대적으로 매우 많은 경우, 더 신뢰만한 변수들의 중요도를 추출할 수 있었음\n클러스터링과 같은 유사한 패턴을 보이는 군집들을 발견할 수 있었음\n유전자 분석처럼 데이터 모델을 생각하기 어려운 곳에 적용 가능; 머신러닝은 변수가 많을수록 좋으며, 과적합하지 않음",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#결론",
    "href": "contents/two-cultures.html#결론",
    "title": "Two Cultures",
    "section": "결론",
    "text": "결론\n\n통계의 목표는 데이터를 사용하여 예측하고 기저에 있는 데이터 메커니즘에 대한 정보를 얻는 것입니다. 데이터와 관련된 문제를 해결하기 위해 어떤 종류의 모델을 사용해야 하는지는 석판에 적혀 있지 않습니다. 제 입장을 분명히 말씀드리자면, 저는 데이터 모델 자체를 반대하는 것이 아닙니다. 어떤 상황에서는 데이터 모델이 문제를 해결하는 가장 적절한 방법일 수 있습니다. 하지만 문제와 데이터에 중점을 두어야 합니다. (번역 by DeepL)\n\n\n\nThe goals in statistics are to use data to predict and to get information about the underlying data mechanism. Nowhere is it written on a stone tablet what kind of model should be used to solve problems involving data. To make myposition clear, I am not against data models per se. In some situations they are the most appropriate wayto solve the problem. But the emphasis needs to be on the problem and on the data.",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#올바른-모델의-필요성",
    "href": "contents/two-cultures.html#올바른-모델의-필요성",
    "title": "Two Cultures",
    "section": "올바른 모델의 필요성",
    "text": "올바른 모델의 필요성\n\n\n천체의 움직임에 대한 모델\n프톨레마이오스(CE 100년 출생, 이집트)의 천동설 모델\n\n행성의 움직임에 대한 수학적 모델은 매우 정확했으며, 천 년 넘게 활용되었음\n적절한 위치에 충분한 에피사이클을 배치하면 행성의 움직임을 매우 정확하게 예측할 수 있음\n\n\n\n\nSource: McElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.\n\n\n\n\n\nMaya Astronomy\n\n천체의 운행에 대한 정교한 계산법 개발\n예를 들어, 일식과 월식, 계절의 변화를 예측\n\n\n\n\n\n\n\nSource: YouTube, Maya Astronomy and Mathematics\n\n\n\n과학적 발견의 프로세스\n\n추측 &gt; 모델링 &gt; 관측/실험\n계산된 결과(예측)와 실제 결과(관측)의 비교를 통해 모델의 타당성을 판단\n결코 모델이 참인지 확신할 수 없음: 즉 true model은 존재하지 않을 수 있음; 예. 뉴튼 역학은 상대성 이론에 의해 수정\n모델이 틀린지에 대해서는 확신할 수 있음\n그럼에도 불구하고, 모델이 없는 과학은 위험할 수 있음\n\n두 문화의 결합\n원자의 움직임에 대한 슈뢰딩거 방정식\n\\(\\displaystyle \\left( -\\frac{{\\hbar^2}}{{2m}} \\frac{{1}}{{r^2}} \\frac{{\\partial}}{{\\partial r}} \\left( r^2 \\frac{{\\partial}}{{\\partial r}} \\right) - \\frac{{\\hbar^2}}{{2m r^2}} \\left( \\frac{{\\partial^2}}{{\\partial \\theta^2}} + \\cot \\theta \\frac{{\\partial}}{{\\partial \\theta}} + \\frac{{1}}{{\\sin^2 \\theta}} \\frac{{\\partial^2}}{{\\partial \\phi^2}} \\right) - \\frac{{k e^2}}{{r}} \\right) \\psi(r,\\theta,\\phi) = E \\psi(r,\\theta,\\phi)\\)\n\n이 방정식을 이용해 synthetic data를 생성: 현실에 대한 simulation\n이 데이터를 이용해 머신러닝 모델을 학습: 물질의 속성에 대해 학습\n그 모델을 이용해 많은 새로운 후보 물질들 중에 유용한 것을 매우 빠르게 걸러낼 수 있음\n예측 모형의 다양한 활용 가능성을 보여줌",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#causal-revolution",
    "href": "contents/two-cultures.html#causal-revolution",
    "title": "Two Cultures",
    "section": "Causal Revolution",
    "text": "Causal Revolution\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n사람들은 어떻게 인과성에 대한 지식을 얻게 되는가?\n어떤 경험 패턴이 이 연관성을 “인과적”이라고 확신시키는가?\n\n\n\nAssociation\n\n관찰을 기반으로 규칙성 발견하고 예측\n올빼미가 쥐의 움직임을 관찰하고 잠시 후 쥐가 어디에 있을지를 파악\n컴퓨터 바둑 프로그램이 수백만 개의 바둑 게임 데이터베이스를 연구하여 어떤 수와 승률이 높은지 알아내는 것\n하나의 이벤트를 관찰하면 다른 이벤트를 관찰할 가능성이 달라진다면, 하나의 이벤트가 다른 이벤트와 연관되어 있다고 말할 수 있음\n“치약을 구매한 고객이 치실도 구매할 가능성이 얼마나 되는가?”; \\(P(치실 ~| 치약~)\\)\n통계의 핵심: 상관관계, 회귀\n올빼미는 쥐가 왜 항상 A 지점에서 B 지점으로 가는지 이해하지 못해도 훌륭한 사냥꾼이 될 수 있음\n위스키 한 병을 들고 있는 보행자가 경적을 울릴 때 다르게 반응할 가능성이 있다는 것을 기계가 스스로 파악할 수 있는가?\n\nAssociation 단계의 한계: 유연성과 적응성의 부족\n\n\n\n\nIntervention\n\n관찰을 넘어, 세상에 대한 개입\n“치약 가격을 두 배로 올리면 치실 판매량은 어떻게 될까?”\n데이터에는 없는 새로운 종류의 지식을 요구\n통계학의 언어로는 이 질문을 표현하는 것조차 불충분함\n수동적으로 수집된 데이터만으로는 이러한 질문에 대답할 수 없음\n\n과거의 데이터를 이용하면?\n과거에 가격이 두 배 비쌌을 때, 치실 판매량으로 추론?\n이전에 가격이 두 배 비쌌을 때, 다른 이유가 있었을 수 있음\n\n전통적으로 실험을 통해 해결\n정확한 인과 관계 모델이 있으면 관찰 데이터만으로도 가능; \\(P(치실 ~| ~do(치약~))\\)\n사실, 일상 생활에서 항상 개입을 수행: 어떻게(How) 하면 두통이 사라질까?\n\n\n\nCounterfactuals\n\n두통이 사라졌다면 왜(Why) 그럴까?\n약을 먹지 않았어도 두통이 사라졌을까?: 가상의 세계 (counterfactual world)\n“현재 치약을 구매한 고객이 가격을 두 배로 올려도 여전히 치약을 구매할 확률은 얼마인가?”\n우리는 현실 세계(고객이 현재 가격으로 치약을 구매했다는 것을 알고 있는)와 가상의 세계(가격이 두 배 높은 경우)와 비교\n보이는 세계  볼 수 있는 새로운 세계  볼 수 없는 세계(보이는 것과 모순)\n이를 위해서는 “이론” 또는 “자연의 법칙”이라고 볼 수 있는 근본적인 인과 과정의 모델이 필요\n\n\n인과 모델은 실제 개입(intervention) 없이도 가상의 상황에서 어떻게 되리라고 예측할 수 있는 추론(reasoning)의 능력을 가능하게 함: deep understanding\n달이 사라진다면?\n\n전형적인 인과적 질문들\n\n\n\nHow effective is a given treatment in preventing a disease?\nWas it the new tax break that caused our sales to go up? Or our marketing campaign?\nWhat is the annual health-care costs attributed to obesity?\nCan hiring records prove an employer guilty of sex discrimination?\nI am about to quit my job, will I regret it?\n\n\n\n특정 치료법이 질병 예방에 얼마나 효과적일까요?\n새로운 세금 감면 혜택이 매출 상승의 원인이었을까요? 아니면 마케팅 캠페인 때문이었나요?\n비만으로 인한 연간 의료 비용은 얼마인가요?\n채용 기록으로 고용주의 성차별을 입증할 수 있나요?\n직장을 그만두려고 하는데 후회하게 될까요?",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#dag",
    "href": "contents/two-cultures.html#dag",
    "title": "Two Cultures",
    "section": "DAG",
    "text": "DAG\nDirected acyclic graph (DAG): 인과 관계 다이어그램\n\n\nSource: Causality: Models, Reasoning, and Inference (2000) by Judea Pearl\n\n관찰(seeing)과 개입(doing)에 대한 수학적 기술(algebra)\n\n\n\n\n\n\n\n\n\n관찰(seeing)\n\n개입(doing)\n\n\n\n\n\n잔디가 젖었다는 것을 보았을 때(see) 비가 내렸을 가능성은 얼마인가?\n\n잔디를 젖게 하면(doing) 비가 내릴 가능성은 얼마인가?\n\n\n\n\\(P(~rain~ | ~wet~)\\)\n\n\\(P( ~rain~ | ~do(wet)~)\\)\n\n\n\n\n개입(intervention)의 과학을 위한 인과를 표현하기 위한 새로운 수학적 언어가 요구됨!\n인과 효과: 개입했을 때의 변화량으로 정의\n\n물리학의 법칙도 소위 “방정식”으로 표현되었음; 방정식의 대칭성\n\n\\(F = m*a\\)\n\\(a = \\frac{F}{m}\\)\n\\(m = \\frac{F}{a}\\)\n\n통계학은 확률이라는 수학적 언어로 표현되어 개입의 효과를 표현할 방법이 없음\n\n“잔디가 젖어 있으면, 비가 왔다”와 “이 스프링클러를 켜면, 잔디가 젖을 것이다”라는 정보가 주어지면,\n컴퓨터는 “이 스프링클러를 켜면, 비가 왔다”라고 결론 내릴 것임.",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#confounding",
    "href": "contents/two-cultures.html#confounding",
    "title": "Two Cultures",
    "section": "Confounding",
    "text": "Confounding\n\n\n신발을 신고 잠든 다음날 두통이 생긴다면?\nFork:Common Cause\n\n\nSource: Introduction to Causal Inference (ICI) by Brady Neal\n\n\n미모가 뛰어나면 연기력이 떨어지는가?\nCollider: Common Effect \n\n\n\n\n\n\n\n\n\nConfounding\n\n\n\n일반적으로, 표면적으로 드러난 변수간의 관계가 숨겨진 다른 변수들(lurking third variables)에 의해 매개되어 있어 진실한 관계가 아닌 경우, confounding 혹은 confounder가 존재한다고 함.\n극단적이지만 이해하지 쉬운 예로는\n\n초등학생 발 사이즈 → 독해력\n\n머리 길이 → 우울증\n\n\nSimpson’s paradox\n아래 첫번째 그림은 집단 전체에 대한 플랏이고, 두번째 그림은 나이대별로 나누어 본 플랏\n전체 집단을 보면 운동을 많이 할수록 콜레스테롤이 증가하는 것으로 보이나,\n나이대별로 보면, 상식적으로 운동이 긍정적 효과가 나타남.\n왜 그렇게 나타나는가?\n\nSource: The book of why by Judea Pearl\n\n\nCOVID-27\nSource: Introduction to Causal Inference (ICI) by Brady Neal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n코딩 기술이 뛰어나면 협업능력이 떨어지는가?\n어느 회사에서 지원자의 코딩 능력과 협업 능력을 1점부터 5점까지 정량화하여,\n총점 8점 이상을 받은 지원자를 모두 채용한다고 했을 때,\n\n\n학생들의 과제는 성적에 영향을 주는가?\nSource: National Education Longitudinal Study of 1988 (NELS:88)",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/intro.html",
    "href": "contents/intro.html",
    "title": "개관",
    "section": "",
    "text": "다양한 소스들로부터 데이터 생성: 전지구적 개인과 환경에 대한 상세한 정보 발생\n\n인터넷 & 통신 (SNS, 사진, 위치, 장소, 유동인구, 상품거래, 물류)\n사물인터넷 (IoT), 로봇, 웨어러블 기기\nCCTV\n스마트 팩토리, 파밍\n게놈프로젝트, 생체정보, 의료/보건: 인류, 실시간\n\n23andMe, Theranos\n\n과학적 발견: 과학적 정보와 데이터의 공유; 생물학, 천문학, …\n\n물리법칙의 발견, 약물의 합성, 생체 내 분자의 상호작용의 메커니즘 규명; 알파폴드\n\n자율주행차량: 내부, 외부\n금융정보 및 흐름\n사회 지표 활용: 고용, 직업, 연봉, 만족도 조사, 취약 계층, 우울\n생성형 인공지능: 기계의 정보 생산\nAI companion: 개인 내면에 대한 정보\n\n영화 Her (2013), Sony’s robotics dog ‘Aibo’",
    "crumbs": [
      "Trees",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#미래-데이터의-중요성",
    "href": "contents/intro.html#미래-데이터의-중요성",
    "title": "개관",
    "section": "",
    "text": "다양한 소스들로부터 데이터 생성: 전지구적 개인과 환경에 대한 상세한 정보 발생\n\n인터넷 & 통신 (SNS, 사진, 위치, 장소, 유동인구, 상품거래, 물류)\n사물인터넷 (IoT), 로봇, 웨어러블 기기\nCCTV\n스마트 팩토리, 파밍\n게놈프로젝트, 생체정보, 의료/보건: 인류, 실시간\n\n23andMe, Theranos\n\n과학적 발견: 과학적 정보와 데이터의 공유; 생물학, 천문학, …\n\n물리법칙의 발견, 약물의 합성, 생체 내 분자의 상호작용의 메커니즘 규명; 알파폴드\n\n자율주행차량: 내부, 외부\n금융정보 및 흐름\n사회 지표 활용: 고용, 직업, 연봉, 만족도 조사, 취약 계층, 우울\n생성형 인공지능: 기계의 정보 생산\nAI companion: 개인 내면에 대한 정보\n\n영화 Her (2013), Sony’s robotics dog ‘Aibo’",
    "crumbs": [
      "Trees",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#데이터-사이언스의-응용-사례",
    "href": "contents/intro.html#데이터-사이언스의-응용-사례",
    "title": "개관",
    "section": "데이터 사이언스의 응용 사례",
    "text": "데이터 사이언스의 응용 사례\nHow AI could empower any business by Andrew Ng\nSource: Data Science (The MIT Press Essential Knowledge Series), 2018, by John D. Kelleher & Brendan Tierney\n\n영업 및 마케팅\n\n웹사이트에서 고객의 구매 행동, 소셜 미디어의 댓글을 추적 고객의 선호도를 파악\n월마트의 경우,\n\n수 십년 넘게 매장의 재고 수준을 최적화했고, 2004년, 몇 주 전에 발생한 허리케인의 판매 데이터를 분석하여 “딸기 팝타트”를 재입고\n소셜 미디어 트렌드 및 신용카드 활동을 분석하여 신제품 출시 및 고객 경험 개인화/최적화\n\n추천 시스템을 통해 사용자 취향에 맞는 제품을 추천, 틈새 상품의 판매도 촉진\n\n\n\n공공기관\n\n미국의 경우, 정부 주도의 데이터 과학 이니셔티브 발족; 특히, 보건 분야에 큰 투자\n\nPrecision Medicine Initiative (2015)\n\n인간 게놈 시퀀싱과 데이터 과학을 결합하여 개별 환자를 위한 약물을 설계\n백만 명 이상의 자원봉사자로부터 환경, 라이프스타일, 생물학적 데이터를 수집하여 정밀 의학을 위한 세계 최대의 데이터를 구축\n\n\n\n도시 운영 및 설계\n\n스마트 시티: 환경, 에너지, 교통 흐름을 추적, 분석, 제어\n장기적 도시 계획 수립에 정보를 축적\n\n치안 및 범죄 예측\n\nPolice Data Initiative\n\n범죄 다발 지역과 재범률을 예측\n시민 단체들의 비판도 존재\n\n시카고 경찰; 1주일 이내의 범죄 예측\n비판: Event-level prediction of urban crime reveals a signature of enforcement bias in US cities. Nature human behaviour\n\n각종 보험료 산정\n\n과거의 데이터를 분석하여, 보험금을 지급할 확률을 계산하고 보험료를 산정\n\n\n\n\n스포츠\n\nMoneyball: The Art of Winning an Unfair Game\n\n야구에서 전통적으로 강조되던 도루, 타점, 타율의 통계보다 출루율과 장타율이 더 나은 척도였음\n“저평가된” 선수, 승리에 기여하는 능력에 비해 낮은 급여를 받는 선수를 찾아 영입\n\nSabermetrics: sciecne of baseball\n데이터 분석을 통해 시장에서 어떤 조직이 우위를 점할 수 있는 방법을 제시\n적절한 속성을 찾는 것의 중요성",
    "crumbs": [
      "Trees",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#사회적-파장",
    "href": "contents/intro.html#사회적-파장",
    "title": "개관",
    "section": "사회적 파장",
    "text": "사회적 파장\n\n유토피아 vs. 디스토피아\n\n초연결성, 투명성 vs. 완전한 감시와 통제\n개인화된 서비스 vs. 설득/유혹/조작\n개별성/자율성 vs. 피동적/비주체적\n기계와의 교감 vs. 인간관계의 소외, 현실과의 단절\n정보와 인간에 대한 신뢰 약화와 사회적 연대, 문명 붕괴\n자연과의 조화 vs. 생태계의 파괴\n\n우울한 사회지표들\n\n\n\n\n\n\n\n\n\n\nBrave New World, 1932\n\n\n\n\n\n\n\nThe Technological Society, 1954\n\n\n\n\n\n\n\nSapiens, Homo Deus, 21 Lessons for the 21st Century by Yuval Noah Harari\n\n\n\n\n\n\n\n\n\n\n\nYuval Noah Harari: An Urgent Warning They Hope You Ignore.\n\n\n\nThe Social Dilemma (2020)\nNetflix documentary",
    "crumbs": [
      "Trees",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#data-science",
    "href": "contents/intro.html#data-science",
    "title": "개관",
    "section": "Data Science",
    "text": "Data Science\n\nArtificial intelligence (인공 지능)\nMachine learning (기계 학습)\nDeep learning (심층 학습)\nData mining (데이터 마이닝)\nStatistical Learning (통계적 학습)\n\n\n\n\n소프트웨어 개발\n데이터에 기반한 분석 위해 작동하도록 프로그래밍을 하여 운영되도록 하는 일\n주로 전통적인 컴퓨터 사이언스의 커리큘럼에 의해 트레이닝\n\n유튜브의 영상 추천\n페이스북의 친구 매칭\n스팸메일 필터링\n자율주행\n\n\n\n\n\n\n데이터 분석\n하나의 구체적인 질문에 답하고자 함\n다양한 소스의 정제되는 않은 데이터를 통합하거나 가공하는 기술이 요구\n\nDNA의 분석을 통해 특정 질병의 발병 인자를 탐색\n유동인구와 매출을 분석해 상권을 분석\n어떤 정책의 유효성을 분석에 정책결정에 공헌\n교통 흐름의 지연이 어떻게 발생하는지를 분석, 해결책 제시",
    "crumbs": [
      "Trees",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#skills",
    "href": "contents/intro.html#skills",
    "title": "개관",
    "section": "Skills",
    "text": "Skills\n\n\nDomain knowledge\n\n해결하려는 문제에 대한 이해없이 단순한 알고리즘만으로 “one size fits all”은 효과적이지 않음\n추상화된 현실에 대한 모형은 수많은 가정/사전 지식(prior knowledge)을 전제하고 있음.\n각 분야의 전문 지식은 데이터가 발생되는 과정, 데이터의 특성, 데이터의 의미를 이해하는데 필수적\n\nEthics\n\n데이터를 합법적이고 적절하게 사용하려면 규정을 이해하고, 자신의 업무에 미치는 영향과 사회에 미치는 파급력 대한 윤리적 이해가 필요\n\n배출(exhaust) 데이터: 어떤 목적을 위해 데이터를 얻는 과정에서 얻어지는 부산물\n\n소셜 미디어: 사용자가 다른 사람들과 소통할 수 있도록 도움\n\n공유된 이미지, 블로그 게시물, 트윗, 좋아요 등으로부터\n누가/얼마나 많이 보았는지/좋아요/리트윗을 했는지 등을 수집\n\n아마존 웹사이트: 다양한 물건을 편리하게 구매할 수 있도록 도움\n\n사용자가 장바구니에 어떤 품목을 담았는지, 사이트에 얼마나 오래 머물렀는지, 어떤 다른 품목을 보았는지 등을 수집\n\n메타데이터(metadata)\n통화 내역만으로 많은 민감한 정보을 유추할 수 있음\n\n알코올 중독자 모임, 이혼 전문 변호사, 성병 전문 병원 등\n\n\n한편, 서비스와 마케팅을 타겟팅할 수 있는 잠재력\n\n\nWrangling\n\n데이터 소스는 다양한 형식으로 존재\n통합, 정리, 변환, 정규화 등의 작업이 요구\ndata munging, data wrangling, data cleaning, data preparation, data preprocessing 등으로 불림\n\nDatabase & computer science\n\n수집된 데이터가 저장되고, 가공/추출된 데이터의 재저장 등 데이터베이스와의 소통할 수 있는 기술\n다양해지고 방대해진 빅데이터를 저장/배포하기 위한 도구를 활용 능력\nML 모델을 이해하고 개발하여 제품의 출시, 분석, 백엔드 애플리케이션에 통합할 수 있는 기술 등\n\nVisualisation\n\n작업 프로세스의 모든 과정에 관여\n\n데이터를 탐색하거나,\n데이터의 의미를 효과적으로 전달\n\n\nStatistics & Probability\n\n데이터 과학 프로세스 전반에 걸쳐 사용됨\n\n초기 수집과 조사\n다양한 모델과 분석의 결과를 해석\n의사결정에 활용\n\n\nMachine Learning\n\n데이터로부터 패턴을 찾기 위한 다양한 알고리즘을 사용\n응용 측면에서는\n\n수많은 알고리즘에 대해 가정, 특성, 용도, 결과의 의미, 적용가능한 유형의 데이터 등을 파악\n해결할 문제와 데이터에 가장 적합한 알고리즘을 파악\n\n\nCommunication\n\n데이터에 담긴 스토리를 효과적으로 전달하는 능력\n분석을 통해 얻은 인사이트, 조직 내 목적에 어떻게 부합하는지, 조직의 기능에 미칠 수 있는 영향 등을 파악",
    "crumbs": [
      "Trees",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#응용비즈니스에서-정형적인-절차",
    "href": "contents/intro.html#응용비즈니스에서-정형적인-절차",
    "title": "개관",
    "section": "응용/비즈니스에서 정형적인 절차",
    "text": "응용/비즈니스에서 정형적인 절차\nPhases of the CRISP-DM (CRoss-Industry Standard Process for Data Mining)\nsource: Chapman et al., 2000\n\nGeneric tasks of the CRISP-DM reference model\n\n비즈니스의 이해와 데이터의 이해\n\n프로젝트의 목표를 정의하고, 비즈니스 문제를 이해하는 것\n어떤 데이터를 수집하는 것이 유용한지, 어떤 데이터가 수집 가능한지 등을 탐색\n\n데이터 준비와 모델링\n\n노이즈와 비정형화된 데이터를 정제하고, 모델링을 위한 데이터를 준비\n데이터로부터 의미있는 패턴(signal vs. noise)과 통찰을 찾기 위해 다양한 모델을 검토하고 실행\n\n모델 평가와 배포\n\n모델링 성능을 평가하고 개선, 모델을 배포\n실제 환경에서는 훈련/평가을 위해 사용된 데이터가 보진 못한 새로운 데이터에 적용됨으로 모델의 성능을 지속적으로 모니터링\n\n데이터 질의 중요성\n\n2016년 데이터 과학자를 대상으로 한 설문조사(CrowdFlower report, 2016)\n데이터 준비(데이터 수집, 클린닝)에 79%의 시간이 소요\n프로젝트의 초점이 명확하고, 그에 맞는 올바른 데이터가 수집되었는지, 모델이 프로젝트의 목표에 잘 부응하는지 중요!\nGarbage in, garbage out\n\n\n\nSource: Cleaning Big Data",
    "crumbs": [
      "Trees",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "contents/intro.html#표준-비즈니스-영역에서의-데이터-사이언스-작업",
    "href": "contents/intro.html#표준-비즈니스-영역에서의-데이터-사이언스-작업",
    "title": "개관",
    "section": "표준 비즈니스 영역에서의 데이터 사이언스 작업",
    "text": "표준 비즈니스 영역에서의 데이터 사이언스 작업\nSource: Data Science (The MIT Press Essential Knowledge Series), 2018, by John D. Kelleher & Brendan Tierney\n\nClustering\nAnomaly detection\nAssociation-rule mining\nPrediction: classification & regression\n\n\nClustering\nWho Are Our Customers?\n\n\n클러스터링을 통해 타깃 고객을 더 세분화된 군집으로 분류하여 마케팅 캠페인의 타겟을 명확히 정의할 수 있음\n\nMeta S. Brown (2014)의 보고서에 따르면,\n\nSoccer Moms? \n\n어린이집에 다니는 어린 자녀를 둔 전업주부\n고등학생 자녀와 함께 파트타임으로 일하는 엄마\n음식과 건강에 관심이 많지만 자녀가 없는 여성\n\n\n\n클러스터링을 통해 얻은 고객 세그먼트에 페르소나를 부여\n각 특성에 맞는 캠패인 전략을 수립\n\n작고 집중된 고객 클러스터를 발견\n많은 매출을 창출하는 고객이 포함된 클러스터에 집중\n\n\n\n  Source: Introduction to Statistical Learning by James et al.\n\n\n\n클러스터링을 위해 사용할 수 있는 속성들: 어떤 속성을 포함하고 어떤 속성을 제외할지 결정하는 것이 중요!\n\n인구통계학적 정보(연령, 성별 등)\n위치(우편번호, 시골 또는 도시 주소 등)\n거래 정보(예: 고객이 어떤 제품이나 서비스를 추구했는지)\n고객이 된 지 얼마나 되었는지\n로열티 카드 회원인지\n제품을 반품하거나 서비스에 대해 불만을 제기한 적이 있는지 등\n\n\n\n\n\n\n\n프로젝트의 데이터 이해 단계에서 탐색 도구로 자주 사용됨\n구체적인 예로,\n\n추가 지원이 필요하거나 다른 학습 접근 방식을 선호하는 학생 그룹을 식별\n생물 정보학에서 마이크로어레이 분석에서 유전자 서열을 분석\n\n\n\n\nAnomaly detection\nIs This Fraud?\n\n잠재적인 사기, 특히 금융 거래 행위를 식별하고 조사\n\n예를 들어, 비정상적인 위치에서 발생한 거래\n비정상적으로 많은 금액이 포함된 거래\n\n어떤 면에서 클러스터링과 반대 개념\n\n클러스터링: 유사한 인스턴스 그룹을 식별\n이상 징후 탐지: 특별한 인스턴스를 식별\n\n이상 징후는 드물다는 그 고유한 특징으로 인해 식별이 어려움\n여러 가지 모델을 결합: 서로 다른 모델이 서로 다른 유형의 이상 징후를 포착\n\n예를 들어, 4개의 모델 중 3~4개 모델에서 거래가 사기성 거래로 식별되는 경우\n\n다양한 분야에서 활용\n\n금융기관: 잠재적 사기 또는 자금 세탁 사례로 추가 조사가 필요한 금융 거래를 식별\n보험기관: 회사의 일반적인 청구와 일치하지 않는 청구를 식별\n사이버 보안: 해킹 가능성, 직원의 비정상적인 행동을 탐지하여 네트워크 침입을 식별\n의료 분야: 의료 기록의 이상 징후를 식별하여 질병을 진단\n사물 인터넷: 데이터를 모니터링하고 비정상적인 센서 이벤트의 발생을 감지, 조치\n\n\n\nSource: The Hundred-Page Machine Learning Book, 2019 by Andriy Burkov\n\n\nAssociation-Rule Mining\nDo You Want Fries with That?\n\n고객에게 다른 관련 제품이나 보완 제품, 혹은 잊어 있었던 제품을 제안\n\n예를 들어, 슈퍼마켓에서 핫도그를 구매한 고객은 케첩과 맥주도 함께 구매할 가능성이 높음.\n이에 맞춰 매장은 제품 레이아웃을 계획할 수 있음\n온라인 마켓의 경우, 웹사이트의 배열, 추천, 광고 등을 설계\n즉, 제품 간 연관성을 이해하고 교차 판매를 촉진\n\n연관 규칙 마이닝은 데이터 세트의 속성(또는 열) 간의 관계를 살펴보는 데 중점을 둠: 속성 간의 상관관계\n위의 경우, 고객의 장바구니 품목을 추적\nIF {핫도그, 케첩}, THEN {맥주}\n\n연관성 규칙의 신뢰도가 75%라면 고객이 핫도그와 케첩을 모두 구매한 경우 75%에서 맥주도 함께 구매했음을 의미\n인구통계학적 정보를 연관성 분석에 포함하여 마케팅 및 타겟팅 광고에 활용\n\n특히, 구매 기록 정보가 없는 경우\nIF 성별(남성) & 나이(35세 미만) & {핫도그, 케첩}, THEN {맥주}\n\n장바구니 분석을 통해 다음과 같은 질문에 답을 탐색\n\n마케팅 캠페인이 효과가 있었는지,\n이 고객의 구매 패턴에 변화가 있었는지,\n고객에게 중요한 인생 이벤트가 있었는지,\n제품 위치가 구매 행동에 영향을 미치는지,\n신제품으로 누구를 타깃팅해야 하는지 등\n\n구매 경향의 시간적 요소를 더하면\n\n적절한 시기에 (재)구매를 추천\n유지보수, 부품 교체 일정\n\n다양한 영역에서도 유용함\n\n통신: 회사의 다양한 서비스를 패키지로 묶는 방법을 설계\n보험: 상품과 보험금 청구 사이에 연관성을 파악\n의료: 기존 치료법과 새로운 치료법 및 의약품 간에 상호 작용이 있는지 확인\n\n추천 시스템(recommnder system)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Machine Learning Class (2016.7.8) from Microsoft Research by Chris Bishop, YouTube\n\n\nClassification (Prediction)\nChurn or No Churn(고객 이탈), That Is the Question\n\n개인의 행동 성향에 대한 모델링이 목표: 예, 광고 마케팅에 대한 반응, 서비스 탈퇴 등 다양한 행동 예측\n\n휴대폰 서비스 회사의 고객 유지 필요성: 기존 고객 유지 비용 대비 신규 고객 유치 비용이 상대적 높음\n이탈 가능성이 높은 고객 식별의 중요성: 유지 비용 최소화 및 이탈 예측을 통한 효율적인 혜택 제공 필요\n이탈 예측의 의미와 활용: 서비스 이탈 예측을 통해 고객 이탈 가능성을 예측하고 효율적인 대응 가능\n\n다양한 산업의 이탈 예측에 활용: 통신, 유틸리티, 은행, 보험 등에서의 이탈 예측을 통한 비즈니스 전략 개발 및 운영 향상\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이해보다는 예측에 초점을 두는 deep learning\n\nImage recognition\nSpeech recognition\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression (Prediction)\nHow Much Will It Cost?\n\n앞서, 분류는 범주형 속성의 값을 추정하는 반면, 회귀는 연속적인 값을 추정\n전통적인 통계적 모형의 근간\n예를 들어, 주택의 “가격”을 예측하는 경우\n\n주택의 크기, 방의 개수, 층수, 해당 지역의 평균 주택 가격, 해당 지역의 평균 주택 크기 등의 속성을 포함\n\n자동차의 “가격”을 예측하려면\n\n자동차의 연식, 주행 거리, 엔진 크기, 자동차 제조사, 문 개수 등의 속성을 포함",
    "crumbs": [
      "Trees",
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "contents/vis-intro.html",
    "href": "contents/vis-intro.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Python의 시각화 라이브러리는 다양하게 개발되어지고 있으며, 각기 특성이 달라 하나로만 쓰기 어려운 상황임\n\n\nMatplotlib\n\n가장 오래된 Python과 잘 통합된 널리 사용되는 라이브러리\n거의 가능한 모든 플랏을 그릴 수 있음\n한편, 디테일한 부분을 모두 specify해야 함으로써 많은 코딩이 요구되며, interactive 또는 web graphs에 취약함\n\npandas\n\nMatplotlib로 구현된 DataFrame의 method로 간략하게 시각화가 가능하며, 빠르게 데이터를 들여다볼 수 있음\n\nSeaborn & the seaborn.objects interface\n\nMatplotlib 위에 개발된 간결한 문법의 high-level 언어\nDecalative: 변수들이 어떤 시각화 속성과 위치를 지니는지만 specify\n“Grammer of graphics”라는 시각화 문법에 충실하고자 the seaborn.objects로 새롭게 변화 중\n\n\n\n\nAltair\n\n“Grammer of graphics”를 충실히 따라 설계됨\n각 plot이 이미지가 아닌 data + specification으로 이루어짐: 이미지가 저장되지 않고, 브라우저에서 이미지로 complie되어 생성됨\nWeb-based interactive 시각화인 D3에 그 모체를 두며, Vega/Vega-Lite로부터 파생됨\njavascript-based로 interactive 시각화에 용이하나 Python과 연계가 부족한 부분이 있고, 지원/커뮤니티가 미흡함\n\nBokeh\nPlotly\n다양한 언어(R, Python, Julia)을 지원하며, 기업 수준의 상용화 제품들도 있으며, 지원군 많음\n\nJake VanderPlas의 2017년 발표 자료 중: The Python Visualization Landscape\n\nSource: Jake VanderPlas - The Python Visualization Landscape PyCon 2017",
    "crumbs": [
      "Trees",
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#대표적-도구들",
    "href": "contents/vis-intro.html#대표적-도구들",
    "title": "Data Visualization",
    "section": "",
    "text": "Matplotlib\n\n가장 오래된 Python과 잘 통합된 널리 사용되는 라이브러리\n거의 가능한 모든 플랏을 그릴 수 있음\n한편, 디테일한 부분을 모두 specify해야 함으로써 많은 코딩이 요구되며, interactive 또는 web graphs에 취약함\n\npandas\n\nMatplotlib로 구현된 DataFrame의 method로 간략하게 시각화가 가능하며, 빠르게 데이터를 들여다볼 수 있음\n\nSeaborn & the seaborn.objects interface\n\nMatplotlib 위에 개발된 간결한 문법의 high-level 언어\nDecalative: 변수들이 어떤 시각화 속성과 위치를 지니는지만 specify\n“Grammer of graphics”라는 시각화 문법에 충실하고자 the seaborn.objects로 새롭게 변화 중\n\n\n\n\nAltair\n\n“Grammer of graphics”를 충실히 따라 설계됨\n각 plot이 이미지가 아닌 data + specification으로 이루어짐: 이미지가 저장되지 않고, 브라우저에서 이미지로 complie되어 생성됨\nWeb-based interactive 시각화인 D3에 그 모체를 두며, Vega/Vega-Lite로부터 파생됨\njavascript-based로 interactive 시각화에 용이하나 Python과 연계가 부족한 부분이 있고, 지원/커뮤니티가 미흡함\n\nBokeh\nPlotly\n다양한 언어(R, Python, Julia)을 지원하며, 기업 수준의 상용화 제품들도 있으며, 지원군 많음\n\nJake VanderPlas의 2017년 발표 자료 중: The Python Visualization Landscape\n\nSource: Jake VanderPlas - The Python Visualization Landscape PyCon 2017",
    "crumbs": [
      "Trees",
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#the-grammer-of-graphics",
    "href": "contents/vis-intro.html#the-grammer-of-graphics",
    "title": "Data Visualization",
    "section": "The Grammer of Graphics",
    "text": "The Grammer of Graphics\nA coherent system for describing and building graphs\nSource: Fundamentals of Data Visualization by Claus O. Wilke\nAesthetics and types of data\n\n데이터의 값을 특정 aesthetics에 mapping\n\n데이터의 타입은 다음과 같이 나누어짐\n\ncontinuous / discrete\nquatitative / qualitative\ncategorical unordered (nominal) / categorical ordered (ordinal)\n\n성별, 지역 / 등급, 랭킹\nordinal: 등간격을 가정\n\n퀄리티 good, fair, poor는 등간격이라고 봐야하는가?\n랭킹은?\n선호도 1, 2, …, 8; continuous?\n임금 구간?\n\n\n\n데이터 타입에 따라 좀 더 적절한 aesthetic mapping이 있으며,\n같은 정보를 품고 있는 시각화라도 더 적절한 representation이 존재\nBertin’s Semiology of Graphics (1967)\nLevels of organization\n\nSource: Jake VanderPlas’ presentation at PyCon 2018\n\nCase 1\n예를 들어, 다음과 같이 1) 지역별로 2) 날짜에 따른 3) 온도의 변화를 나타낸다면,\n즉, x축의 위치에 날짜 정보를, y축의 위치에 온도 정보를, 색깔에 지역 정보를 할당했음.\n\n한편, 아래는 x축의 위치에 압축된 날짜 정보를, y축의 위치에 지역 정보를, 색깔에 압축된 온도 정보를 할당했음.\n\n\n\nCase 2\n다음은 GDP, mortality, population, region의 네 정보를 다른 방식으로 mapping한 결과임.",
    "crumbs": [
      "Trees",
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#탐색적-exploratory-vs.-정보전달-communicative",
    "href": "contents/vis-intro.html#탐색적-exploratory-vs.-정보전달-communicative",
    "title": "Data Visualization",
    "section": "탐색적 (Exploratory) vs. 정보전달 (Communicative)",
    "text": "탐색적 (Exploratory) vs. 정보전달 (Communicative)\n\n분석도구: 현미경, 연장도구\n강점이자 약점",
    "crumbs": [
      "Trees",
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/vis-intro.html#interative-plots",
    "href": "contents/vis-intro.html#interative-plots",
    "title": "Data Visualization",
    "section": "Interative Plots",
    "text": "Interative Plots\nAltair\n\n\n\n\n\n\n\nPlotly",
    "crumbs": [
      "Trees",
      "Exploratory Data Analysis",
      "Visualize"
    ]
  },
  {
    "objectID": "contents/subsetting.html",
    "href": "contents/subsetting.html",
    "title": "Subsetting",
    "section": "",
    "text": "Load Packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nDataFrame의 일부를 선택하는 subsetting의 방식에 여러 가지 있음\nflights.head(3)\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1    517.00             515       2.00    830.00   \n1  2013      1    1    533.00             529       4.00    850.00   \n2  2013      1    1    542.00             540       2.00    923.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n0             819      11.00      UA    1545  N14228    EWR  IAH    227.00  \n1             830      20.00      UA    1714  N24211    LGA  IAH    227.00  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00",
    "crumbs": [
      "Trees",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#bracket",
    "href": "contents/subsetting.html#bracket",
    "title": "Subsetting",
    "section": "Bracket [ ]",
    "text": "Bracket [ ]\nBracket안에 labels이 있는 경우 columns을 select\n\nA single string: Series로 반환\n\nA list of a single string: DataFrame으로 반환\n\nA list of strings\n\n\nflights['dest']  # return as a Series\n\n0         IAH\n1         IAH\n         ... \n336774    CLE\n336775    RDU\nName: dest, Length: 336776, dtype: object\n\n\n\nflights[['dest']]  # return as a DataFrame\n\n       dest\n0       IAH\n1       IAH\n...     ...\n336774  CLE\n336775  RDU\n\n[336776 rows x 1 columns]\n\n\n\nflights[['origin', 'dest']]\n\n       origin dest\n0         EWR  IAH\n1         LGA  IAH\n...       ...  ...\n336774    LGA  CLE\n336775    LGA  RDU\n\n[336776 rows x 2 columns]\n\n\nBracket안에 numbers가 있는 경우 rows를 select: position-based\n\nSlicing만 허용\nFirst index는 포함, last index는 제외\n[1, 5, 8]과 같이 특정 rows를 선택하는 것은 허용안됨\n\n\nflights[2:5]\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n3  2013      1    1    544.00             545      -1.00   1004.00   \n4  2013      1    1    554.00             600      -6.00    812.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00  \n3            1022     -18.00      B6     725  N804JB    JFK  BQN    183.00  \n4             837     -25.00      DL     461  N668DN    LGA  ATL    116.00  \n\n\n 만약, 아래와 같이 index가 number일 때 out of order가 된 경우에도 row position으로 적용됨\n\n\n   origin dest  arr_delay\n42    LGA  DFW      48.00\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n33    EWR  MSP      29.00\n\n\n\ndf_outoforder[2:4]\n\n   origin dest  arr_delay\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n\n\n Chaining with brackets\n\nflights[['origin', 'dest']][2:5]\n# 순서 바꿔어도 동일: flights[2:5][['origin', 'dest']]\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL",
    "crumbs": [
      "Trees",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#dot-notation-.",
    "href": "contents/subsetting.html#dot-notation-.",
    "title": "Subsetting",
    "section": "Dot notation .",
    "text": "Dot notation .\n편리하나 주의해서 사용할 필요가 있음\n\n\n\n\n\n\nNote\n\n\n\n\nspace 또는 . 이 있는 변수명 사용 불가\nmethods와 동일한 이름의 변수명 사용 불가: 예) 변수명이 count인 경우 df.count는 df의 method로 인식\n새로운 변수를 만들어 값을 assgin할 수 없음: 예) df.new_var = 1 불가; 대신 df[\"new_var\"] = 1\n만약, 다음과 같이 변수을 지정했을 때 vars_names=[\"origin\", \"dest\"],\n\ndf[vars_names]는 \"orign\"과 \"dest\" columns을 선택\ndf.vars_names는 vars_names이라는 이름의 column을 의미\n\n\n\n\n\nflights.dest  # flihgts[\"dest\"]와 동일\n\n0         IAH\n1         IAH\n         ... \n336774    CLE\n336775    RDU\nName: dest, Length: 336776, dtype: object",
    "crumbs": [
      "Trees",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#loc-.iloc",
    "href": "contents/subsetting.html#loc-.iloc",
    "title": "Subsetting",
    "section": ".loc & .iloc",
    "text": ".loc & .iloc\n각각 location, integer location의 약자\ndf.(i)loc[row_indexer, column_indexer]\n\n.loc: label-based indexing\n\nIndex가 number인 경우도 label로 처리\nSlicing의 경우 first, last index 모두 inclusive\n\n\nflights.loc[2:5, ['origin', 'dest']]  # 2:5는 index의 label, not position\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL\n5    EWR  ORD\n\n\n다음과 같이 index가 labels인 경우는 혼동의 염려 없음\n\n\n       origin dest\nred       JFK  MIA\nblue      JFK  BQN\ngreen     LGA  ATL\nyellow    EWR  ORD\n\n\n\ndf_labels.loc[\"blue\":\"green\", :]\n\n      origin dest\nblue     JFK  BQN\ngreen    LGA  ATL\n\n\n하지만, index가 number인 경우는 혼동이 있음\n앞서 본 예에서처럼 index가 out of order인 경우 loc은 다르게 작동\n\n\n   origin dest  arr_delay\n42    LGA  DFW      48.00\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n33    EWR  MSP      29.00\n\n\n\ndf_outoforder.loc[2:14, :]  # position 아님\n\n   origin dest  arr_delay\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n\n\n\ndf_outoforder.loc[[25, 33], :]  # slicing이 아닌 특정 index 선택\n\n   origin dest  arr_delay\n25    EWR  ORD      32.00\n33    EWR  MSP      29.00\n\n\n\nflights.loc[2:5, 'dest']  # returns as a Series\n\n2    MIA\n3    BQN\n4    ATL\n5    ORD\nName: dest, dtype: object\n\n\n\nflights.loc[2:5, ['dest']]  # return as a DataFrame\n\n  dest\n2  MIA\n3  BQN\n4  ATL\n5  ORD\n\n\n\n\n\n\n\n\nTip\n\n\n\n생략 표시\nflights.loc[2:5, :]  # ':' means all\nflights.loc[2:5]\nflights.loc[2:5, ]  # flights.loc[ , ['dest', 'origin']]은 에러\n\n\n\n# select a single row\nflights.loc[2, :]  # returns as a Series, column names as its index\n\nyear         2013\nmonth           1\n            ...  \ndest          MIA\nair_time   160.00\nName: 2, Length: 15, dtype: object\n\n\n\n# select a single row\nflights.loc[[2], :]  # returns as a DataFrame\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00  \n\n\n\n\n\n.iloc: position-based indexing\n\nSlicing의 경우 as usual: first index는 inclusive, last index는 exclusive\n\n\nflights.iloc[2:5, 12:14]  # 2:5는 index의 position, last index는 미포함\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL\n\n\n\nflights.iloc[2:5, 12]  # return as a Series\n\n2    JFK\n3    JFK\n4    LGA\nName: origin, dtype: object\n\n\n\nflights.iloc[2:5, :]\n# 다음 모두 가능\n# flights.iloc[2:5]\n# flights.iloc[2:5, ]\n\n# flights.iloc[, 2:5]는 에러\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n3  2013      1    1    544.00             545      -1.00   1004.00   \n4  2013      1    1    554.00             600      -6.00    812.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00  \n3            1022     -18.00      B6     725  N804JB    JFK  BQN    183.00  \n4             837     -25.00      DL     461  N668DN    LGA  ATL    116.00  \n\n\n\nflights.iloc[2:5, [12]]  # return as a DataFrame\n\n  origin\n2    JFK\n3    JFK\n4    LGA\n\n\n\nflights.iloc[[2, 5, 7], 12:14]  # 특정 위치의 rows 선택\n\n  origin dest\n2    JFK  MIA\n5    EWR  ORD\n7    LGA  IAD\n\n\n\n\n\n\n\n\nNote\n\n\n\n단 하나의 scalar 값을 추출할 때, 빠른 처리를 하는 다음을 사용할 수 있음\n.at[i, j], .iat[i, j]",
    "crumbs": [
      "Trees",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#series의-indexing",
    "href": "contents/subsetting.html#series의-indexing",
    "title": "Subsetting",
    "section": "Series의 indexing",
    "text": "Series의 indexing\nDataFrame과 같은 방식으로 이해\nIndex가 numbers인 경우\n\n\n42    DFW\n2     MIA\n25    ORD\n14    DFW\n33    MSP\nName: dest, dtype: object\n\n\n\ns.loc[25:14]\n\n25    ORD\n14    DFW\nName: dest, dtype: object\n\n\n\ns.iloc[2:4]\n\n25    ORD\n14    DFW\nName: dest, dtype: object\n\n\n\ns[:3]\n\n42    DFW\n2     MIA\n25    ORD\nName: dest, dtype: object\n\n\n\n\n\n\n\n\nNote\n\n\n\n다음과 같은 경우 혼동스러움\ns[3] # 3번째? label 3?\n#&gt; errors occur\n\n\n Index가 lables인 경우 다음과 같이 편리하게 subsetting 가능\n\n\nred       MIA\nblue      BQN\ngreen     ATL\nyellow    ORD\nName: dest, dtype: object\n\n\n\ns[\"red\":\"green\"]\n\nred      MIA\nblue     BQN\ngreen    ATL\nName: dest, dtype: object\n\n\n\ns[[\"red\", \"green\"]]\n\nred      MIA\ngreen    ATL\nName: dest, dtype: object",
    "crumbs": [
      "Trees",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#boolean-indexing",
    "href": "contents/subsetting.html#boolean-indexing",
    "title": "Subsetting",
    "section": "Boolean indexing",
    "text": "Boolean indexing\n\nBracket [ ] 이나 loc을 이용\niloc은 적용 안됨\n\n\nBracket [ ]\n\nnp.random.seed(123)\nflights_6 = flights[:100][[\"dep_delay\", \"arr_delay\", \"origin\", \"dest\"]].sample(6)\nflights_6\n\n    dep_delay  arr_delay origin dest\n8       -3.00      -8.00    JFK  MCO\n70       9.00      20.00    LGA  ORD\n..        ...        ...    ...  ...\n63      -2.00       2.00    JFK  LAX\n0        2.00      11.00    EWR  IAH\n\n[6 rows x 4 columns]\n\n\n\nflights_6[flights_6[\"dep_delay\"] &lt; 0]\n\n    dep_delay  arr_delay origin dest\n8       -3.00      -8.00    JFK  MCO\n82      -1.00     -26.00    JFK  SFO\n63      -2.00       2.00    JFK  LAX\n\n\n\nidx = flights_6[\"dep_delay\"] &lt; 0\nidx # bool type의 Series\n\n8      True\n70    False\n      ...  \n63     True\n0     False\nName: dep_delay, Length: 6, dtype: bool\n\n\n\n# Select a column with the boolean indexing\nflights_6[idx][\"dest\"]\n\n8     MCO\n82    SFO\n63    LAX\nName: dest, dtype: object\n\n\n\n\n\n\n\n\nNote\n\n\n\n사실, boolean indexing을 할때, DataFrame/Series의 index와 match함\n대부분 염려하지 않아도 되나 다음과 같은 결과 참고\n# Reset index\nidx_reset = idx.reset_index(drop=True)\n# 0     True\n# 1    False\n# 2     True\n# 3    False\n# 4     True\n# 5    False\n# Name: dep_delay, dtype: bool\n\nflights_6[idx_reset][\"dest\"]\n#&gt; IndexingError: Unalignable boolean Series provided as indexer \n#&gt; (index of the boolean Series and of the indexed object do not match)\n\n# Index가 없는 numpy array로 boolean indexing을 하는 경우 문제없음\nflights_6[idx_reset.to_numpy()][\"dest\"]\n# 8     MCO\n# 82    SFO\n# 63    LAX\n# Name: dest, dtype: object\n\n\n\nbool_idx = flights_6[[\"dep_delay\", \"arr_delay\"]] &gt; 0\nbool_idx\n\n    dep_delay  arr_delay\n8       False      False\n70       True       True\n..        ...        ...\n63      False       True\n0        True       True\n\n[6 rows x 2 columns]\n\n\n\nidx_any = bool_idx.any(axis=1)\nidx_any\n\n8     False\n70     True\n      ...  \n63     True\n0      True\nLength: 6, dtype: bool\n\n\n\nbool_idx.all(axis=1)\n\n8     False\n70     True\n      ...  \n63    False\n0      True\nLength: 6, dtype: bool\n\n\n\n\nnp.where() 활용\nnp.where(boolean condition, value if True, value if False)\n\nflights_6[\"delayed\"] = np.where(idx, \"delayed\", \"on-time\")\nflights_6\n\n    dep_delay  arr_delay origin dest  delayed\n8       -3.00      -8.00    JFK  MCO  delayed\n70       9.00      20.00    LGA  ORD  on-time\n..        ...        ...    ...  ...      ...\n63      -2.00       2.00    JFK  LAX  delayed\n0        2.00      11.00    EWR  IAH  on-time\n\n[6 rows x 5 columns]\n\n\n\nnp.where(flights_6[\"dest\"].str.startswith(\"S\"), \"S\", \"T\")  # str method: \"S\"로 시작하는지 여부\n\narray(['T', 'T', 'S', 'S', 'T', 'T'], dtype='&lt;U1')\n\n\n\nflights_6[\"dest_S\"] = np.where(flights_6[\"dest\"].str.startswith(\"S\"), \"S\", \"T\")\nflights_6\n\n    dep_delay  arr_delay origin dest  delayed dest_S\n8       -3.00      -8.00    JFK  MCO  delayed      T\n70       9.00      20.00    LGA  ORD  on-time      T\n..        ...        ...    ...  ...      ...    ...\n63      -2.00       2.00    JFK  LAX  delayed      T\n0        2.00      11.00    EWR  IAH  on-time      T\n\n[6 rows x 6 columns]\n\n\n\n\nloc\n\nflights_6.loc[idx, \"dest\"]  # flights_6[idx][\"dest\"]과 동일\n\n8     MCO\n82    SFO\n63    LAX\nName: dest, dtype: object\n\n\n만약 column 이름에 “time”을 포함하는 columns만 선택하고자 하면\n\nSeries/Index object는 str method 존재\nstr.contains(), str.startswith(), str.endswith()\n자세한 사항은 7.4 String Manipulation/String Functions in pandas by Wes McKinney\n\n\ncols = flights.columns.str.contains(\"time\")  # str method: \"time\"을 포함하는지 여부\ncols\n\narray([False, False, False,  True,  True, False,  True,  True, False,\n       False, False, False, False, False,  True])\n\n\n\n# Columns 쪽으로 boolean indexing\nflights.loc[:, cols]\n\n        dep_time  sched_dep_time  arr_time  sched_arr_time  air_time\n0         517.00             515    830.00             819    227.00\n1         533.00             529    850.00             830    227.00\n...          ...             ...       ...             ...       ...\n336774       NaN            1159       NaN            1344       NaN\n336775       NaN             840       NaN            1020       NaN\n\n[336776 rows x 5 columns]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nChained indexing으로 값을 assign하는 경우 copy vs. view 경고 메세지\nflights[flights[\"arr_delay\"] &lt; 0][\"arr_delay\"] = 0\n/var/folders/mp/vcywncl97ml2q4c_5k2r573m0000gn/T/ipykernel_96692/3780864177.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n 경고가 제시하는데로 .loc을 이용하여 assign\nflights.loc[flights[\"arr_delay\"] &lt; 0, \"arr_delay\"] = 0",
    "crumbs": [
      "Trees",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/subsetting.html#summary",
    "href": "contents/subsetting.html#summary",
    "title": "Subsetting",
    "section": "Summary",
    "text": "Summary\n\nBracket [ ]의 경우\n\n간단히 columns을 선택하고자 할때 column labels: df[[\"var1\", \"var2\"]]\n간단히 rows를 선택하고자 할때 numerical indexing: df[:10]\n\nDot-notation은\n\npandas의 methods와 중복된 이름을 피하고,\nassignment의 왼편에는 사용을 피할 것\n\n가능하면 분명한 loc 또는 iloc을 사용\n\nloc[:, [\"var1\", \"var2\"]]는 df[[\"var1\", \"var2\"]]과 동일\niloc[:10, :]은 df[:10]와 동일\nloc의 경우, index가 숫자라 할지라도 label로 처리됨\nloc은 iloc과는 다른게 slicing(:)에서 first, last index 모두 inclusive\n\nBoolean indexing의 경우\n\nBracket [ ]: df[bool_idx]\nloc: df.loc[bool_idx, :]\niloc 불가\n\nAssignment를 할때는,\n\nchained indexing을 피하고: df[:5][\"dest\"]\nloc or iloc 사용:\n\ndf.loc[:4, \"dest\"]: index가 0부터 정렬되어 있다고 가정했을 때, slicing에서 위치 하나 차이남\ndf.iloc[:5, 13]: “dest”의 column 위치 13\n\n\n한 개의 column 혹은 row을 선택하면 Series로 반환: df[\"var1\"] 또는 df.loc[2, :]\n\n\n\n\n\n\n\nNote\n\n\n\nNumpy의 indexing에 대해서는 교재 참고\nCh.4/Basic Indexing and Slicing in Python Data Analysis by Wes McKinney",
    "crumbs": [
      "Trees",
      "Python Basics",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/shrinkage.html",
    "href": "contents/shrinkage.html",
    "title": "Shrinkage Implementations",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 5, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nfrom ISLP import load_data\nfrom ISLP.models import ModelSpec\n\nimport sklearn.model_selection as skm\nimport sklearn.linear_model as sklm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import root_mean_squared_error, r2_score\nMajor League Baseball Data from the 1986 and 1987 seasons\nHitters = load_data('Hitters') # from ISLP\nHitters = Hitters.dropna()\nHitters\n\n     AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  \\\n1      315    81      7    24   38     39     14    3449    835      69   \n2      479   130     18    66   72     76      3    1624    457      63   \n3      496   141     20    65   78     37     11    5628   1575     225   \n..     ...   ...    ...   ...  ...    ...    ...     ...    ...     ...   \n319    475   126      3    61   43     52      6    1700    433       7   \n320    573   144      9    85   60     78      8    3198    857      97   \n321    631   170      9    77   44     31     11    4908   1457      30   \n\n     CRuns  CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary  \\\n1      321   414     375      N        W      632       43      10   475.0   \n2      224   266     263      A        W      880       82      14   480.0   \n3      828   838     354      N        E      200       11       3   500.0   \n..     ...   ...     ...    ...      ...      ...      ...     ...     ...   \n319    217    93     146      A        W       37      113       7   385.0   \n320    470   420     332      A        E     1314      131      12   960.0   \n321    775   357     249      A        W      408        4       3  1000.0   \n\n    NewLeague  \n1           N  \n2           A  \n3           N  \n..        ...  \n319         A  \n320         A  \n321         A  \n\n[263 rows x 20 columns]",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Regularization",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/shrinkage.html#ridge-regression",
    "href": "contents/shrinkage.html#ridge-regression",
    "title": "Shrinkage Implementations",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nimport sklearn.linear_model as sklm\nfrom sklearn.preprocessing import StandardScaler\n\n# standardize X\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# create an array of alpha values\nlambdas = np.logspace(-2, 10, 100) / y.std()  \n\nridge_coefs = []\nfor alpha in lambdas:\n    ridge = sklm.Ridge(alpha=alpha)\n    ridge.fit(X_scaled, y)\n    ridge_coefs.append(ridge.coef_)\nridge_coefs = np.array(ridge_coefs)\n\n\nridge_coefs = pd.DataFrame(ridge_coefs, columns=X.columns, index=lambdas)\nridge_coefs.index.name = 'lambda'\nridge_coefs.head(3)\n\n               AtBat        Hits      HmRun       Runs        RBI       Walks  \\\nlambda                                                                          \n0.000022 -291.094162  337.829131  37.853033 -60.571234 -26.994245  135.073570   \n0.000029 -291.094036  337.828697  37.852775 -60.570833 -26.994007  135.073465   \n0.000039 -291.093868  337.828124  37.852433 -60.570303 -26.993693  135.073326   \n\n              Years      CAtBat      CHits     CHmRun       CRuns        CRBI  \\\nlambda                                                                          \n0.000022 -16.694157 -391.033663  86.691953 -14.178832  480.740064  260.684702   \n0.000029 -16.694414 -391.032055  86.693349 -14.177901  480.737788  260.683034   \n0.000039 -16.694753 -391.029931  86.695195 -14.176671  480.734779  260.680828   \n\n              CWalks    PutOuts    Assists     Errors   League_N  Division_W  \\\nlambda                                                                         \n0.000022 -213.891103  78.761297  53.732322 -22.160934  31.248776  -58.414130   \n0.000029 -213.890731  78.761297  53.732268 -22.160957  31.248781  -58.414151   \n0.000039 -213.890240  78.761297  53.732197 -22.160987  31.248787  -58.414180   \n\n          NewLeague_N  \nlambda                 \n0.000022   -12.348893  \n0.000029   -12.348919  \n0.000039   -12.348954  \n\n\n\nridge_coefs.plot(figsize=(8, 6))\nplt.xlabel('$\\lambda$')\nplt.ylabel('Standardized coefficients')\nplt.xscale('log')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\\(\\lambda\\)값 중 하나를 살펴보면, 가령 50번째 \\(\\lambda\\)값과 그에 대응하는 회귀계수들은\n\npd.options.display.max_rows = 0\nbeta_hat = ridge_coefs.iloc[50, :]\nprint(f\"lambda: {lambdas[50]}, \\nbeta hats: \\n{beta_hat}\")\n\nlambda: 25.48679637046264, \nbeta hats: \nAtBat         -60.554231\nHits           94.445543\nHmRun         -11.652839\nRuns           29.082098\nRBI            20.558595\nWalks          61.290878\nYears         -32.326762\nCAtBat         11.255280\nCHits          72.074056\nCHmRun         52.153738\nCRuns          76.011242\nCRBI           73.044427\nCWalks        -45.183088\nPutOuts        70.812650\nAssists        18.834114\nErrors        -22.285540\nLeague_N       23.712255\nDivision_W    -59.725489\nNewLeague_N    -5.619252\nName: 25.48679637046264, dtype: float64\n\n\n\nEstimating Test Error of Ridge Regression\n한 예로, \\(\\lambda = 25\\)로 Ridge regression을 수행하면,\n\nfrom sklearn.pipeline import Pipeline\n\nridge = sklm.Ridge(alpha=25)\nscaler = StandardScaler()  # standardize\n\nridge_scaled = Pipeline([('scaler', scaler), ('ridge', ridge)])  \nridge_scaled.fit(X, y)\n\nPipeline(steps=[('scaler', StandardScaler()), ('ridge', Ridge(alpha=25))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()), ('ridge', Ridge(alpha=25))])  StandardScaler?Documentation for StandardScalerStandardScaler()  Ridge?Documentation for RidgeRidge(alpha=25) \n\n\n\n\n\n\n\n\nUsing Pipeline\n\n\n\n파이프라인을 사용해, scikit-learn의 여러 class들을(예. estimator, tranformer) 순서대로 적용되도록 묶을 수 있음\nTuple의 list로 입력: (name, estimator)\nridge_scaled = Pipeline([('scaler', scaler), ('ridge', ridge)])  \nridge_scaled.fit(X, y)\n이름을 지정하지 않고 간략히 처리하는 경우, Pipeline 대신 make_pipeline을 사용할 수 있음\n이 경우, 이름은 자동으로 생성됨\nfrom sklearn.pipeline import make_pipeline\n\nridge_scaled = make_pipeline(scaler, ridge)\nridge_scaled.fit(X, y)\n\n\n각 변수의 파라미터 추정치\n\nridge.coef_\n\narray([-61.83616,  95.40164, -11.71174,  29.01527,  20.50437,  61.68053,\n       -32.74913,  10.86313,  72.57968,  52.3169 ,  76.67548,  73.45853,\n       -46.12262,  70.93771,  19.03332, -22.37589,  23.82647, -59.80705,\n        -5.75064])\n\n\n이제, 최적의 \\(\\lambda\\)를 찾기 위해 cross-validation을 수행하는데,\n기준이 되는 대표적 metric은 MSE(mean squared error) 또는 \\(R^2\\)\n다른 metric에 대해서는 sklearn.metric 참고\n\nimport sklearn.model_selection as skm\n\nridge = sklm.Ridge()\nscaler = StandardScaler()\nridge_scaled = Pipeline([('scaler', scaler), ('ridge', ridge)])  \n\nK = 5\nkfold = skm.KFold(K, shuffle=True, random_state=0)\nparam_grid = {'ridge__alpha': lambdas}  # ridge: the name of the step in the pipeline, alpha: the parameter name\n\ngrid = skm.GridSearchCV(ridge_scaled, \n                        param_grid,\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\ngrid.fit(X, y)\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('ridge', Ridge())]),\n             param_grid={'ridge__alpha': array([       0.00002,        0.00003,        0.00004,        0.00005,\n              0.00007,        0.00009,        0.00012,        0.00016,\n              0.00021,        0.00027,        0.00036,        0.00048,\n              0.00063,        0.00083,        0.0011 ,        0.00146,\n              0.00193,        0.00255,        0.00337,        0.00445,\n              0.00589,        0.00778,        0.010...\n          36126.87535,    47757.60309,    63132.74068,    83457.76772,\n         110326.25731,   145844.81929,   192798.26791,   254867.9637 ,\n         336920.44865,   445389.00483,   588778.05255,   778329.93498,\n        1028906.36814,  1360153.66596,  1798043.09927,  2376907.15964,\n        3142131.38041,  4153712.76566,  5490963.82383,  7258731.02346,\n        9595615.22556, 12684838.61151, 16768610.12221, 22167115.72313])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('ridge', Ridge())]),\n             param_grid={'ridge__alpha': array([       0.00002,        0.00003,        0.00004,        0.00005,\n              0.00007,        0.00009,        0.00012,        0.00016,\n              0.00021,        0.00027,        0.00036,        0.00048,\n              0.00063,        0.00083,        0.0011 ,        0.00146,\n              0.00193,        0.00255,        0.00337,        0.00445,\n              0.00589,        0.00778,        0.010...\n          36126.87535,    47757.60309,    63132.74068,    83457.76772,\n         110326.25731,   145844.81929,   192798.26791,   254867.9637 ,\n         336920.44865,   445389.00483,   588778.05255,   778329.93498,\n        1028906.36814,  1360153.66596,  1798043.09927,  2376907.15964,\n        3142131.38041,  4153712.76566,  5490963.82383,  7258731.02346,\n        9595615.22556, 12684838.61151, 16768610.12221, 22167115.72313])},\n             scoring='neg_mean_squared_error') best_estimator_: PipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', Ridge(alpha=np.float64(2.732865634209876)))])  StandardScaler?Documentation for StandardScalerStandardScaler()  Ridge?Documentation for RidgeRidge(alpha=np.float64(2.732865634209876)) \n\n\n\ngrid.best_params_\n\n{'ridge__alpha': np.float64(2.732865634209876)}\n\n\n\ngrid.best_estimator_\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', Ridge(alpha=np.float64(2.732865634209876)))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge', Ridge(alpha=np.float64(2.732865634209876)))])  StandardScaler?Documentation for StandardScalerStandardScaler()  Ridge?Documentation for RidgeRidge(alpha=np.float64(2.732865634209876)) \n\n\n\ngrid.best_estimator_.fit(X, y).named_steps['ridge'].coef_\n\narray([-230.65706,  247.37135,    5.00202,   -6.18649,    2.11026,\n        111.2861 ,  -49.87498, -118.09574,  123.57104,   56.01087,\n        222.83363,  121.82137, -155.27562,   77.90012,   41.12019,\n        -24.88459,   30.4831 ,  -61.48008,  -13.76634])\n\n\n\nplt.figure(figsize=(8, 6), dpi=70)\nplt.errorbar(\n    lambdas,\n    -grid.cv_results_[\"mean_test_score\"],\n    yerr=grid.cv_results_[\"std_test_score\"] / np.sqrt(K),\n)\nplt.axvline(grid.best_params_[\"ridge__alpha\"], color=\".5\", linestyle=\":\")\nplt.xlabel(\"$\\lambda$\")\nplt.ylabel(\"Cross-validated MSE\")\nplt.xscale(\"log\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR-squared 값으로 비교하면\n\n\n\n\n\nR-squared 값으로 비교하면, (default 값)\ngrid = skm.GridSearchCV(ridge_scaled, \n                        param_grid,\n                        cv=kfold,\n                        scoring='r2') # default\ngrid.fit(X, y)\n위와 마찬가지로 plot을 그리면,\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCross-validation이 통합되어 있는 간편한 함수: RidgeCV, LassoCV, ElasticNetCV\n\n\n\n# suppress warnings for ElasticNet\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nkfold = skm.KFold(5, shuffle=True, random_state=0)\n\n# RidgeCV 함수에 mse_path가 없으므로, 대신 ElasticNetCV를 사용\nridgeCV = sklm.ElasticNetCV(alphas=lambdas, l1_ratio=0, cv=kfold)  # l1_ratio=0: L2 regularization, l1_ratio=1: L1 regularization\npipeCV = Pipeline(steps=[(\"scaler\", scaler), (\"ridge\", ridgeCV)])\npipeCV.fit(X, y)\n\n\nridgeCV.alpha_\n\nnp.float64(0.01360153665961598)\n\n\n\nridgeCV.coef_\n\narray([-213.94939,  229.32917,    1.36088,    0.44074,    5.24782,\n        106.26368,  -51.59113,  -92.22839,  120.087  ,   58.1399 ,\n        197.63691,  114.42331, -144.621  ,   77.54784,   38.9252 ,\n        -25.15106,   30.2249 ,  -61.76237,  -13.56158])\n\n\n\n\n\n\n\n\n\\(\\lambda\\)에 따른 변화를 보려면\n\n\n\ntuned_ridge = pipeCV.named_steps[\"ridge\"]\n\nplt.figure(figsize=(8, 6), dpi=40)\nplt.errorbar(\n    lambdas[::-1],\n    tuned_ridge.mse_path_.mean(axis=1),\n    yerr=tuned_ridge.mse_path_.std(axis=1) / np.sqrt(K),\n)\nplt.axvline(tuned_ridge.alpha_, c=\".5\", ls=\":\")\nplt.xlabel(\"$\\lambda$\")\nplt.ylabel(\"Cross-validated MSE\")\nplt.xscale(\"log\")\nplt.show()\n\n\n\n\n\nEvaluating Test Error of Cross-Validated Ridge\n앞서 적절한 \\(\\lambda\\)를 찾기 위해 cross-validation을 활용하는 방법을 살펴보았음.\n타당한 “test” error를 구하기 위해서는 처음에 training set만을 이용해 \\(\\lambda\\)를 찾은 후,\ntest set을 이용해 error를 구해야 함.\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.5, random_state=123)\n\n# standardize X\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Fit the model on the training set\nkfold = skm.KFold(n_splits=5, shuffle=True, random_state=2)\nridgeCV = sklm.RidgeCV(alphas=lambdas, cv=kfold)\n\nridgeCV.fit(X_train_scaled, y_train)\n\nRidgeCV(alphas=array([275.03283, 256.49654, 239.20953, 223.08761, 208.05225, 194.03023,\n       180.95324, 168.7576 , 157.3839 , 146.77675, 136.88449, 127.65893,\n       119.05515, 111.03123, 103.5481 ,  96.5693 ,  90.06085,  83.99105,\n        78.33034,  73.05113,  68.12773,  63.53615,  59.25403,  55.2605 ,\n        51.53613,  48.06277,  44.8235 ,  41.80255,  38.98519,  36.35772,\n        33.90733,  31.62209,  29.49087,  27.50328,  25.64965,  23.9...\n         4.18025,   3.89852,   3.63577,   3.39073,   3.16221,   2.94909,\n         2.75033,   2.56497,   2.3921 ,   2.23088,   2.08052,   1.9403 ,\n         1.80953,   1.68758,   1.57384,   1.46777,   1.36884,   1.27659,\n         1.19055,   1.11031,   1.03548,   0.96569,   0.90061,   0.83991,\n         0.7833 ,   0.73051,   0.68128,   0.63536,   0.59254,   0.55261,\n         0.51536,   0.48063,   0.44823,   0.41803,   0.38985,   0.36358,\n         0.33907,   0.31622,   0.29491,   0.27503]),\n        cv=KFold(n_splits=5, random_state=2, shuffle=True))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RidgeCV?Documentation for RidgeCViFittedRidgeCV(alphas=array([275.03283, 256.49654, 239.20953, 223.08761, 208.05225, 194.03023,\n       180.95324, 168.7576 , 157.3839 , 146.77675, 136.88449, 127.65893,\n       119.05515, 111.03123, 103.5481 ,  96.5693 ,  90.06085,  83.99105,\n        78.33034,  73.05113,  68.12773,  63.53615,  59.25403,  55.2605 ,\n        51.53613,  48.06277,  44.8235 ,  41.80255,  38.98519,  36.35772,\n        33.90733,  31.62209,  29.49087,  27.50328,  25.64965,  23.9...\n         4.18025,   3.89852,   3.63577,   3.39073,   3.16221,   2.94909,\n         2.75033,   2.56497,   2.3921 ,   2.23088,   2.08052,   1.9403 ,\n         1.80953,   1.68758,   1.57384,   1.46777,   1.36884,   1.27659,\n         1.19055,   1.11031,   1.03548,   0.96569,   0.90061,   0.83991,\n         0.7833 ,   0.73051,   0.68128,   0.63536,   0.59254,   0.55261,\n         0.51536,   0.48063,   0.44823,   0.41803,   0.38985,   0.36358,\n         0.33907,   0.31622,   0.29491,   0.27503]),\n        cv=KFold(n_splits=5, random_state=2, shuffle=True)) \n\n\n\nfrom sklearn.metrics import root_mean_squared_error, r2_score\n\n# Calculate the test RMSE and R-squared using the test set\ndef print_metrics(y, x):\n    print(f\"Test RMSE = {root_mean_squared_error(y, x):.2f}\")\n    print(f\"Test R-squared = {r2_score(y, x):.2f}\")\n\nprint_metrics(y_test, ridgeCV.predict(X_test_scaled))\n\nTest RMSE = 330.19\nTest R-squared = 0.34\n\n\n\n\n\n\n\n\nShuffleSplit을 이용한 trick\n\n\n\n\n\nShuffleSplit으로 1번 교차검증을 하는 방식으로, training set과 test set을 나누는 방법\nouter_valid = skm.ShuffleSplit(n_splits=1, test_size=0.25, random_state=1)\n\ninner_cv = skm.KFold(n_splits=5, shuffle=True, random_state=2)\nridgeCV = sklm.RidgeCV(alphas=lambdas, cv=inner_cv)\npipeCV = Pipeline(steps=[(\"scaler\", scaler), (\"ridge\", ridgeCV)])\n\nresults = skm.cross_validate(\n    pipeCV, X, y, cv=outer_valid, scoring=\"neg_mean_squared_error\"\n)\n\n-results[\"test_score\"]\n# array([132026.01644])\n\n\n\n\n\nHiger order polynomial regression + Ridge\n고차항을 추가한 후 Ridge regression을 수행하면,\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nridge = sklm.Ridge()\npoly = PolynomialFeatures(degree=2)  # 모든 2차항을 포함\npoly_ridge = Pipeline([('poly', poly), ('ridge', ridge)])\n\nK = 5\nkfold = skm.KFold(K, shuffle=True, random_state=0)\nlambdas = np.logspace(-2, 10, 100) / y.std()\nparam_grid = {'ridge__alpha': lambdas}\n\ngrid_poly_ridge = skm.GridSearchCV(poly_ridge, \n                        param_grid,\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\ngrid_poly_ridge.fit(X_train_scaled, y_train);\n\n\nprint_metrics(y_test, grid_poly_ridge.best_estimator_.predict(X_test_scaled))\n\nTest RMSE = 360.57\nTest R-squared = 0.21",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Regularization",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/shrinkage.html#lasso-regression",
    "href": "contents/shrinkage.html#lasso-regression",
    "title": "Shrinkage Implementations",
    "section": "Lasso Regression",
    "text": "Lasso Regression\nRidge에서와 마찬가지로, 우선 training/test set으로 나눈 후,\ntraining set에서 cross-validation을 이용해 최적의 \\(\\lambda\\)를 찾은 후,\ntest set을 이용해 test error를 구해보면,\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.5, random_state=1)\n\n# standardize X\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# create an array of alpha values\nlambdas = np.logspace(2, 6, 100) / y.std()\n\nlasso_coefs = []\nfor alpha in lambdas:\n    lasso = sklm.Lasso(alpha=alpha)\n    lasso.fit(X_train_scaled, y_train)\n    lasso_coefs.append(lasso.coef_)\nlasso_coefs = np.array(lasso_coefs)\n\n\nlasso_coefs = pd.DataFrame(lasso_coefs, columns=X.columns, index=lambdas)\nlasso_coefs.index.name = 'lambda'\nlasso_coefs.sample(5)\n\n                 AtBat        Hits      HmRun  Runs        RBI      Walks  \\\nlambda                                                                      \n58.877805     0.000000   28.063740   0.000000   0.0  15.507350  54.777702   \n3.964920   -282.087576  298.363238 -25.946751  -0.0  43.706836  87.276887   \n344.848531    0.000000    0.000000   0.000000   0.0   0.000000   0.000000   \n85.421614     0.000000   14.620521   0.000000   0.0  22.543629  43.842263   \n14.584482    -0.000000   55.000151  -0.000000   0.0   0.000000  71.600000   \n\n               Years  CAtBat  CHits     CHmRun     CRuns        CRBI   CWalks  \\\nlambda                                                                          \n58.877805   0.000000     0.0    0.0   0.000000  0.000000  171.424457  0.00000   \n3.964920   -1.315728    -0.0    0.0  94.297054  1.854077  136.835250 -5.38028   \n344.848531  0.000000     0.0    0.0   0.000000  0.000000    0.000000  0.00000   \n85.421614   0.000000     0.0    0.0   0.000000  0.000000  150.116450  0.00000   \n14.584482   0.000000     0.0    0.0  25.159039  0.000000  185.973787  0.00000   \n\n               PutOuts    Assists  Errors   League_N  Division_W  NewLeague_N  \nlambda                                                                         \n58.877805    94.315200   0.000000     0.0   0.000000  -13.044781          0.0  \n3.964920    124.505182  13.670514    -0.0  15.677443  -59.922602         -0.0  \n344.848531    0.000000  -0.000000     0.0  -0.000000   -0.000000         -0.0  \n85.421614    79.406766   0.000000     0.0   0.000000   -0.000000          0.0  \n14.584482   116.481406  -0.000000    -0.0   0.940461  -57.024540          0.0  \n\n\n\n\nPlot the lasso coefficients\nlasso_coefs.plot(figsize=(8, 6))\nplt.xlabel('$\\lambda$')\nplt.ylabel('Standardized coefficients')\nplt.xscale('log')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEvaluating Test Error of Cross-Validated Ridge\n5-fold cross-validation을 이용해 최적의 \\(\\lambda\\)를 찾으면,\n\n# Fit the model on the training set\nK = 5\nkfold = skm.KFold(n_splits=K, shuffle=True, random_state=1)\nlassoCV = sklm.LassoCV(cv=kfold)\nlassoCV.fit(X_train_scaled, y_train)\n\nLassoCV(cv=KFold(n_splits=5, random_state=1, shuffle=True))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LassoCV?Documentation for LassoCViFittedLassoCV(cv=KFold(n_splits=5, random_state=1, shuffle=True)) \n\n\n\nlassoCV.alpha_\n\nnp.float64(0.6353615151306538)\n\n\n\nlassoCV.coef_\n\narray([-297.37844,  254.07362,  -32.11397,  -13.73482,   85.00807,\n         89.97288,   17.75389, -969.62686,  857.24631,  154.46851,\n        179.9078 ,    0.     ,   -1.71849,  125.64956,   55.05843,\n        -28.67818,   53.39268,  -57.39778,  -30.45363])\n\n\nTest error with optimal \\(\\lambda\\)\n\n# Calculate the test RMSE and R-squared using the test set\nprint_metrics(y_test, lassoCV.predict(X_test_scaled))\n\nTest RMSE = 328.84\nTest R-squared = 0.37\n\n\n\n\nPlot the cross-validated RMSE\nlambdas = lassoCV.alphas_\nlasso_path = np.sqrt(lassoCV.mse_path_).mean(axis=1)\nlasso_path_se = np.sqrt(lassoCV.mse_path_).std(axis=1) / np.sqrt(K)\n\nplt.figure(figsize=(8, 6), dpi=70)\nplt.errorbar(lambdas, lasso_path, yerr=lasso_path_se)\nplt.axvline(lassoCV.alpha_, color=\".5\", linestyle=\":\")\nplt.xlabel(\"$\\lambda$\")\nplt.ylabel(\"Cross-validated RMSE\")\nplt.xscale(\"log\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nlasso_l20 = sklm.Lasso(alpha=20)\nlasso_l20.fit(X_train_scaled, y_train)\n\ncoefs = lasso_l20.coef_\ncoefs\n\narray([ -0.     ,  52.94574,   0.     ,   0.     ,   0.     ,  69.58717,\n         0.     ,   0.     ,   0.     ,  22.35575,   0.     , 184.34984,\n         0.     , 113.96798,  -0.     ,  -0.     ,   0.     , -51.76979,\n         0.     ])\n\n\n19개의 변수 중 다음 6개의 변수만 선택하여, 소위 variable/feature selection을 수행하게 됨.\n반면, 앞서 최적의 \\(\\lambda=0.75\\)에서는 17개의 변수가 선택되었음.\n\nX.columns[coefs.nonzero()]\n\nIndex(['Hits', 'Walks', 'CHmRun', 'CRBI', 'PutOuts', 'Division_W'], dtype='object')\n\n\nTest error with \\(\\lambda=20\\)\n\nprint_metrics(y_test, lasso_l20.predict(X_test_scaled))\n\nTest RMSE = 323.47\nTest R-squared = 0.39\n\n\n\n\n\n\n\n\nFeature selection using SelectFromModel\n\n\n\nSelectFromModel()를 이용해, Lasso regression을 통해 선택된 변수만을 사용할 수 있음\nfrom sklearn.feature_selection import SelectFromModel\n\nmodel_reduced = SelectFromModel(lasso_l20, prefit=True)\nX_new = model_reduced.transform(X)\nX_new\n# array([[  81,   39,   69,  414,  632,    1],\n#        [ 130,   76,   63,  266,  880,    1],\n#        [ 141,   37,  225,  838,  200,    0],\n#        ...,\n#        [ 126,   52,    7,   93,   37,    1],\n#        [ 144,   78,   97,  420, 1314,    0],\n#        [ 170,   31,   30,  357,  408,    1]])",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Regularization",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/shrinkage.html#comparisons",
    "href": "contents/shrinkage.html#comparisons",
    "title": "Shrinkage Implementations",
    "section": "Comparisons",
    "text": "Comparisons\n모두 함께 비교하면,\nHitters의 예는 19개의 예측변수와 N=263개의 관측값만으로 연봉을 예측하는 문제임.\n그 중 50%의 training set으로 모델을 fitting한 후, 나머지 50%의 test set으로 모델을 평가함.\n만약, 선형회귀모델을 사용한다면,\n\nlr = sklm.LinearRegression()\nlr.fit(X_train_scaled, y_train)\nprint_metrics(y_test, lr.predict(X_test_scaled))\n\nTest RMSE = 341.60\nTest R-squared = 0.32\n\n\n이전 Ridge, Ridge with a higher order polynomial, Lasso, Lasso with variable selection에 대한 결과를 비교해보면,\n\n\nRidge\nTest RMSE = 318.85\nTest R-squared = 0.41\n\n\n\n\n\nRidge with polynomial of degree 2\nTest RMSE = 304.79\nTest R-squared = 0.46\n\n\n\n\n\nLasso\nTest RMSE = 328.84\nTest R-squared = 0.37\n\n\n\n\n\nLasso with 6 predictors only (lambda = 20)\nTest RMSE = 323.47\nTest R-squared = 0.39",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Regularization",
      "Implementations"
    ]
  },
  {
    "objectID": "contents/pandas.html",
    "href": "contents/pandas.html",
    "title": "NumPy and pandas",
    "section": "",
    "text": "파이썬의 모든 것은 객체(object)이며, 특정 클래스(class)의 인스턴스(instance)임\n인스턴스는 클래스에서 정의된 속성(attribute)과 메서드(method)를 전달 받음(inherited)\n\n\nx = \"Love me tender\"  # x: string object\n\nx는 str 클래스(빵의 틀)의 인스턴스(빵)가 되면서, str 클래스의 속성과 함수를 사용할 수 있게 됨\n\nx.upper()  # upper()라는 함수를 호출\n\n'LOVE ME TENDER'\n\n\n이는 다음과 같이 원래 str 클래스의 메서드를 사용하는 것과 동일함\n\nstr.upper(x)\n\n'LOVE ME TENDER'\n\n\n마찬가지로,\n\nx.count('e')  # 함수에 인자(argument)가 포함된 경우\n\n4\n\n\n\nstr.count(x, \"e\")\n\n4\n\n\n\n이렇게, 각 인스턴스가 가지는 함수를 호출해서 적용할 수 있는데\n이 때 그 함수를 메서드(method)라고 함\n이는 각 클레스에서 고유하게 정의된 함수들을 사용할 수 있게 함\n\n이 경우 str이라는 클래스에서 정의된 함수들을 사용할 수 있음\n\n파이썬의 고유한 함수들, 예를 들어\n\ntype(x)\n\nstr\n\n\n\nlen(x)  # 문자열의 길이\n\n14\n\n\npandas 패키지의 한 클래스를 살펴보면,\n\nimport pandas as pd\npd.DataFrame?\n\n\npd.DataFrame\n\npandas.core.frame.DataFrame\n\n\nDataFrame이라는 클래스가 정의되는데, 이는 사실 다음과 같은 폴더 위치에 있는 frame.py 마듈에서 정의된 클래스임\npd.core.frame.DataFrame\n\ndf = pd.DataFrame({'mango': [1, 2, 3], 'apple': [4, 5, 6]})\ndf\n\n   mango  apple\n0      1      4\n1      2      5\n2      3      6\n\n\n\n이 때, df는 DataFrame 클래스의 인스턴스(instance)가 되면서, DataFrame 클래스(class)에서 정의된 속성(attribute)과 함수를 사용할 수 있게 됨.\n\n이 함수를 메서드(method)라고 함\n\n\ndf.columns  # columns라는 속성을 추출\n\nIndex(['mango', 'apple'], dtype='object')\n\n\n\ndf.head(2)  # head()라는 함수를 호출\n\n   mango  apple\n0      1      4\n1      2      5\n\n\n\ndf.columns.sort_values()  # df.columns는 Index object이고, 이에 대한 sort_values()라는 함수를 호출\n\nIndex(['apple', 'mango'], dtype='object')"
  },
  {
    "objectID": "contents/pandas.html#python-objects",
    "href": "contents/pandas.html#python-objects",
    "title": "NumPy and pandas",
    "section": "",
    "text": "파이썬의 모든 것은 객체(object)이며, 특정 클래스(class)의 인스턴스(instance)임\n인스턴스는 클래스에서 정의된 속성(attribute)과 메서드(method)를 전달 받음(inherited)\n\n\nx = \"Love me tender\"  # x: string object\n\nx는 str 클래스(빵의 틀)의 인스턴스(빵)가 되면서, str 클래스의 속성과 함수를 사용할 수 있게 됨\n\nx.upper()  # upper()라는 함수를 호출\n\n'LOVE ME TENDER'\n\n\n이는 다음과 같이 원래 str 클래스의 메서드를 사용하는 것과 동일함\n\nstr.upper(x)\n\n'LOVE ME TENDER'\n\n\n마찬가지로,\n\nx.count('e')  # 함수에 인자(argument)가 포함된 경우\n\n4\n\n\n\nstr.count(x, \"e\")\n\n4\n\n\n\n이렇게, 각 인스턴스가 가지는 함수를 호출해서 적용할 수 있는데\n이 때 그 함수를 메서드(method)라고 함\n이는 각 클레스에서 고유하게 정의된 함수들을 사용할 수 있게 함\n\n이 경우 str이라는 클래스에서 정의된 함수들을 사용할 수 있음\n\n파이썬의 고유한 함수들, 예를 들어\n\ntype(x)\n\nstr\n\n\n\nlen(x)  # 문자열의 길이\n\n14\n\n\npandas 패키지의 한 클래스를 살펴보면,\n\nimport pandas as pd\npd.DataFrame?\n\n\npd.DataFrame\n\npandas.core.frame.DataFrame\n\n\nDataFrame이라는 클래스가 정의되는데, 이는 사실 다음과 같은 폴더 위치에 있는 frame.py 마듈에서 정의된 클래스임\npd.core.frame.DataFrame\n\ndf = pd.DataFrame({'mango': [1, 2, 3], 'apple': [4, 5, 6]})\ndf\n\n   mango  apple\n0      1      4\n1      2      5\n2      3      6\n\n\n\n이 때, df는 DataFrame 클래스의 인스턴스(instance)가 되면서, DataFrame 클래스(class)에서 정의된 속성(attribute)과 함수를 사용할 수 있게 됨.\n\n이 함수를 메서드(method)라고 함\n\n\ndf.columns  # columns라는 속성을 추출\n\nIndex(['mango', 'apple'], dtype='object')\n\n\n\ndf.head(2)  # head()라는 함수를 호출\n\n   mango  apple\n0      1      4\n1      2      5\n\n\n\ndf.columns.sort_values()  # df.columns는 Index object이고, 이에 대한 sort_values()라는 함수를 호출\n\nIndex(['apple', 'mango'], dtype='object')"
  },
  {
    "objectID": "contents/pandas.html#numpy",
    "href": "contents/pandas.html#numpy",
    "title": "NumPy and pandas",
    "section": "NumPy",
    "text": "NumPy\n\n수학적 symbolic 연산에 대한 구현이라고 볼 수 있으며,\n행렬(matrix) 또는 벡터(vector)를 ndarray (n-dimensional array)이라는 이름으로 구현함.\n\n사실상 정수(int)나 실수(float)의 한가지 타입으로 이루어짐.\n\n고차원의 arrays 가능\n\n\n\nSource: Medium.com\n가령, 다음과 같은 행렬 연산이 있다면,\n\\(\\begin{bmatrix}1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix}2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix}0 \\\\ 2 \\\\ 4 \\end{bmatrix}\\)\n\nimport numpy as np\n\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6]]) # 3x2 matrix\nX = np.array([[2],\n              [-1]]) # 2x1 matrix\n\nA @ X  # A * X : matrix multiplication\n\narray([[0],\n       [2],\n       [4]])\n\n\n\nA.dot(X)  # A @ X와 동일\n\narray([[0],\n       [2],\n       [4]])\n\n\n\nA + A # element-wise addition\n\narray([[ 2,  4],\n       [ 6,  8],\n       [10, 12]])\n\n\n\n2 * A - 1 # braodcasting\n\narray([[ 1,  3],\n       [ 5,  7],\n       [ 9, 11]])\n\n\n\nnp.exp(A) # element-wise\n\narray([[  2.72,   7.39],\n       [ 20.09,  54.6 ],\n       [148.41, 403.43]])\n\n\n\nPython vs. NumPy\n\na = 2**31 - 1\nprint(a)\nprint(a + 1)\n\n2147483647\n2147483648\n\n\n\na = np.array([2**31 - 1], dtype='int32')\nprint(a)\nprint(a + 1)\n\n[2147483647]\n[-2147483648]\n\n\n\nSource: Ch.4 in Python for Data Analysis (3e) by Wes McKinney"
  },
  {
    "objectID": "contents/pandas.html#pandas",
    "href": "contents/pandas.html#pandas",
    "title": "NumPy and pandas",
    "section": "pandas",
    "text": "pandas\nSeries & DataFrame\n\nSeries\n1개의 칼럼으로 이루어진 데이터 포멧: 1d numpy array에 labels을 부여한 것으로 볼 수 있음.\nDataFrame의 각 칼럼들을 Series로 이해할 수 있음.\n\nSource: Practical Data Science\n\n\nDataFrame\n각 칼럼들이 한 가지 데이터 타입으로 이루어진 tabular형태 (2차원)의 데이터 포맷\n\n각 칼럼은 기본적으로 한 가지 데이터 타입인 것이 이상적이나, 다른 타입이 섞여 있을 수 있음\nNumPy의 2차원 array의 각 칼럼에 labels을 부여한 것으로 볼 수도 있으나, 여러 다른 기능들이 추가됨\nNumPy의 경우 고차원의 array를 다룰 수 있음: ndarray\n\n고차원의 DataFrame과 비슷한 것은 xarray가 존재\n\nLabels와 index를 제외한 데이터 값은 거의 NumPy ndarray로 볼 수 있음\n(pandas.array 존재)\n\n\nSource: Practical Data Science\nNumPy의 ndarray &lt;-&gt; pandas의 DataFrame 상호 변환\n\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6]]) # 3x2 matrix\n\ndf = pd.DataFrame(A, columns=[\"A1\", \"A2\"])\ndf\n\n   A1  A2\n0   1   2\n1   3   4\n2   5   6\n\n\n\n# 데이터 값들은 NumPy array\ndf.values  # 함수 호출이 아니라 속성(attribute) 접근이므로 ()가 없음\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n한 개의 Column을 추출\n보통 Series로 반환됨\n\n\ns = df[\"A1\"] # A1 칼럼 선택\ns  # DataFrame의 column 이름이 Series의 name으로 전환\n\n# 0    1\n# 1    3\n# 2    5\n# Name: A1, dtype: int64\n\n\n\ndf\n\n#    A1  A2\n# 0   1   2\n# 1   3   4\n# 2   5   6\n\n\n\ntype(s)\n\npandas.core.series.Series\n\n\n\ns.values  # 또는 s.to_numpy(); Series의 값은 NumPy 1d array\n\narray([1, 3, 5])\n\n\n\ndf2 = df[[\"A1\"]]  # list로 들어가면 DataFrame으로 반환\ndf2\n\n   A1\n0   1\n1   3\n2   5\n\n\n\ntype(df2)\n\npandas.core.frame.DataFrame\n\n\n\n\nIndex objects\nframe = pd.DataFrame(np.arange(6).reshape((2, 3)),\n                     index=pd.Index([\"Ohio\", \"Colorado\"], name=\"state\"),\n                     columns=pd.Index([\"one\", \"two\", \"three\"], name=\"number\"))\nframe\n\n\n\nnumber    one  two  three\nstate                    \nOhio        0    1      2\nColorado    3    4      5\n\n\n\n\nframe.index  # 함수 호출이 아니라 속성(attribute) 접근이므로 ()가 없음\n\nIndex(['Ohio', 'Colorado'], dtype='object', name='state')\n\n\n\nframe.columns # columns도 index object\n\nIndex(['one', 'two', 'three'], dtype='object', name='number')\n\n\nIndex는 times series에 특화\n\nbike = pd.read_csv('data/day.csv', index_col='dteday', parse_dates=True)\nbike.head(3)\n\n            instant  season  yr  mnth  holiday  weekday  workingday  \\\ndteday                                                                \n2011-01-01        1       1   0     1        0        6           0   \n2011-01-02        2       1   0     1        0        0           0   \n2011-01-03        3       1   0     1        0        1           1   \n\n            weathersit  temp  atemp  hum  windspeed  casual  registered   cnt  \ndteday                                                                         \n2011-01-01           2  0.34   0.36 0.81       0.16     331         654   985  \n2011-01-02           2  0.36   0.35 0.70       0.25     131         670   801  \n2011-01-03           1  0.20   0.19 0.44       0.25     120        1229  1349  \n\n\n\nbike.plot(kind='line', y=['casual', 'registered'], figsize=(8, 4), title='Bike Sharing')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDataFrame의 연산\nNumPy의 ndarray들이 연산되는 방식과 동일하게 series나 DataFrame들의 연산 가능함\n\ndf + 2 * df\n\n   A1  A2\n0   3   6\n1   9  12\n2  15  18\n\n\n\nnp.log(df)\n\n    A1   A2\n0 0.00 0.69\n1 1.10 1.39\n2 1.61 1.79\n\n\n사실 연산은 index를 align해서 시행됨\n\n\n\n\n\n\nnumber    one  two  three\nstate                    \nOhio        0    1      2\nColorado    3    4      5\n\n\nnumber  one  two  three\nstate                  \nOhio      0    2      4\nFloria    6    8     10\n\n\n\nframe1 + frame2\n\n\n\nnumber    one  two  three\nstate                    \nColorado  NaN  NaN    NaN\nFloria    NaN  NaN    NaN\nOhio     0.00 3.00   6.00"
  },
  {
    "objectID": "contents/pandas.html#missing",
    "href": "contents/pandas.html#missing",
    "title": "NumPy and pandas",
    "section": "Missing",
    "text": "Missing\nNaN, NA, None\n\npandas에서는 missing을 명명하는데 R의 컨벤션을 따라 NA (not available)라 부름.\n\n대부분의 경우에서 NumPy object NaN(np.nan)을 NA을 나타내는데 사용됨.\n\nnp.nan은 실제로 floating-point의 특정 값으로 float64 데이터 타입임. Integer 또는 string type에서 약간 이상하게 작동될 수 있음.\n\nPython object인 None은 pandas에서 NA로 인식함.\n\n현재 NA라는 새로운 pandas object 실험 중임\n\nNA의 handling에 대해서는 교재 참고\n.dropna(), .fillna(), .isna(), .notna()\n\nMckinney’s: 7.1 Handling Missing Data,\nWorking with missing data\n\n\ns = pd.Series([1, 2, np.nan])\ns\n\n0   1.00\n1   2.00\n2    NaN\ndtype: float64\n\n\n\n# type을 변환: float -&gt; int\ns.astype(\"Int64\")\n\n0       1\n1       2\n2    &lt;NA&gt;\ndtype: Int64\n\n\n\ns = pd.Series([\"a\", \"b\", np.nan])\ns\n\n0      a\n1      b\n2    NaN\ndtype: object\n\n\n\n# type을 변환: object -&gt; string\ns.astype(\"string\")\n\n0       a\n1       b\n2    &lt;NA&gt;\ndtype: string\n\n\n\ns = pd.Series([1, 2, np.nan, None, pd.NA])\ns\n\n0       1\n1       2\n2     NaN\n3    None\n4    &lt;NA&gt;\ndtype: object\n\n\nMissing인지를 확인: .isna(), .notna()\n\ns.isna() # or s.isnull()\n\n0    False\n1    False\n2     True\n3     True\n4     True\ndtype: bool\n\n\n\ns.notna() # or s.notnull()\n\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n\n\npandas에서는 ExtensionDtype이라는 새로운 데이터 타입이 도입되었음.\n\ns2 = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\ns2.dtype  # date type 확인\n\nInt8Dtype()\n\n\n\nimport pyarrow as pa\ns2 = pd.Series([2, pd.NA], dtype=pd.ArrowDtype(pa.uint16()))\ns2.dtype\n\nuint16[pyarrow]\n\n\npandas dtypes 참고"
  },
  {
    "objectID": "contents/mlsl-intro.html",
    "href": "contents/mlsl-intro.html",
    "title": "Machine/Statistical Learning",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n\\(f\\): \\(X_1, X_2, ..., X_p\\)가 \\(Y\\)에 관해 제공하는 systematic information; unknown function\n\\(Y = f(X_1, X_2, ..., X_p) + \\epsilon\\),   \\(Y \\perp\\!\\!\\!\\perp \\epsilon\\)\n\\(\\epsilon\\): 불확실성의 소스들; reducible error & irreducible error\n(epistemic uncertainty & aleatory uncertainty)\n앞서 선형모형은 \\(f\\)를 \\(X\\)의 선형함수인 parametric model로 가정한 후 parameter를 추정하여 문제가 단순하였으나,\n이번에는 \\(f\\)의 형태를 가정하지 않고, \\(f\\) 자체를 추정하고자 하는 매우 광범위한 문제임.\n예를 들어,",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#왜-분석하는가",
    "href": "contents/mlsl-intro.html#왜-분석하는가",
    "title": "Machine/Statistical Learning",
    "section": "왜 분석하는가?",
    "text": "왜 분석하는가?\nThe Occam’s Dilemma?\n\n예측(prediction)\n추론(inference)/해석(interpretation)\n\n정보 획득(information)의 관점에서: Leo Breiman 글 참고",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#어떻게-f를-추정하는가",
    "href": "contents/mlsl-intro.html#어떻게-f를-추정하는가",
    "title": "Machine/Statistical Learning",
    "section": "어떻게 \\(f\\)를 추정하는가?",
    "text": "어떻게 \\(f\\)를 추정하는가?\n모수적(parametric) 접근\n\n모수적 접근: \\(f\\)의 형태를 가정하고, 그 형태에 대한 parameter를 추정\n가령, 선형성을 가정한 linear model: \\(f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\)\n모델을 관찰된 데이터에 fit(적합) 또는 train(훈련) 시켜, parameter(모수) 추정\n전형적으로 (ordinary) least squares 방법을 사용\n상대적으로 적은 수의 데이터로도 추정이 가능\n해석을 통해 변수 간의 관계 추론 용이\n\\(f\\)의 형태를 잘못 가정하면, 잘못된 결과를 낼 수 있음\n\n\n\n\n\n\n\n\n\n\n\n\nSource: p.21, An Introduction to Statistical Learning with Applications in Python\n\n비모수적(non-parametric) 접근\n\n\\(f\\)의 형태를 가정하지 않음\n보통, 예측의 정확성이 높도록, 즉 데이터와 최대한 가까운 매우 복잡한 형태의 \\(\\hat f\\)를 추구\n과적합(overfit)이 되지 않도록, 새로운 데이터에도 잘 일반화되도록 sweet spot을 찾아야 함\n매우 많은 데이터가 요구됨\n해석은 어려우나, \\(f\\)로부터 다양한 정보(information)을 추출하는 방법을 고안\n모수가 없을 수도, 있을 수 있으나, 해석 가능한 모수라고 보기 어려움\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: p.22, An Introduction to Statistical Learning with Applications in Python\n\nThe tradeoff between flexibility and interpretability\n\n\nSource: p.22, An Introduction to Statistical Learning with Applications in Python\n\n어떤 기준으로 이 tradeoff의 수준을 결정할 것인가?\n\n해석에 중심을 두고, 변수 간의 관계를 추론하고자 하는 경우: 모형이 틀릴 위험이 존재 &gt;&gt; bias가 높아짐\n예측의 정확성에 중심을 두는 경우: 과적합이 될 위험이 존재 &gt;&gt; variance가 높아짐\n이 둘이 항상 대치되는 것은 아님; 단순한 모형이 새로운 데이터에서 더 나은 예측 정확도를 가질 수도 있음!",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#회귀regression-vs.-분류classification",
    "href": "contents/mlsl-intro.html#회귀regression-vs.-분류classification",
    "title": "Machine/Statistical Learning",
    "section": "회귀(regression) vs. 분류(classification)",
    "text": "회귀(regression) vs. 분류(classification)\n\n보통 \\(Y\\)가 연속형 변수인 경우, 회귀(regression) 문제로 다루고, \\(Y\\)가 범주형 변수인 경우, 분류(classification) 문제로 일컬어 짐.\n회귀모형을 확장하여 확률(연속값)을 구한 후, 이를 이용해 분류문제를 다룰 수 있음; ex. logistic regression\n예측 변수의 경우, 연속인지 범주형인지는 상관없음.",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#decision-theory",
    "href": "contents/mlsl-intro.html#decision-theory",
    "title": "Machine/Statistical Learning",
    "section": "Decision Theory",
    "text": "Decision Theory\n\nSource: Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H\n\n\\(Y = f(X) + \\epsilon\\)\n앞서 선형 회귀 모형에서 구한 것은, conditional probability \\(p(Y|X)\\)를 모델링 한 것이었음.\n이 분포를 Gaussian:\\(N(\\mu, \\sigma^2)\\)으로 가정하고, 분포의 평균과 표준편차를 likelihood가 최대가 되도록 추정하는 간단한 모델로 축소했음; predictive distribution\n\n\n분포 \\(p(Y|X)\\)를 통해 예측의 불확실성을 파악할 수 있음\n만약, 주어진 \\(X\\)에서 대해 예측값 \\(f(X)\\)을 하나 결정하는데, 실제 true 값을 \\(y\\)라고 하면,\n그 오차에 대해 발생되는 어떤 penalty 또는 cost를 정의; loss function\nloss function을 통해 “최적”의 예측값에 대한 기준을 제공\n\\(f(x) = E(Y|X=x)\\)로 예측하는 것이 많은 경우 적절하지만 일반적으로 그런 것은 아님\n\nLoss/Cost Function\nLoss function: \\(L(y, f(x))\\)\n\\(y\\): true value, \\(f\\): predicted value\n\n이 loss를 최소화하는 것이 목표\nTrue \\(y\\)를 모르기 때문에, 평균치인 expected loss를 최소화하는 것이 목표\n즉, \\(E(L) = \\displaystyle \\iint L(y, f(x))p(x, y) dx dy\\)를 최소화하도록 예측값(\\(f\\))을 결정\n\n각 \\(x\\)에 대해 \\(\\displaystyle \\int L(y, f(x))p(y|x)dy\\)를 최소화\n\n예를 들어, 다이아몬드의 무게로 가격을 예측한다면: 1 carat 다이아몬드의 true price?\n또는, 목소리(특정 frequency)로 성별을 예측한다면; 분류 문제의 경우 뒤에서 다룸\n\n\n\n\n\n\n\n\n\n\n\n Source: Language Log\n\n\n\n앞서 회귀 모형에서는 기본적으로 squared loss 사용;\n\n\\(L_2 = \\displaystyle(f(x) - y)^2\\)\n\n\\(E(L)\\)을 최소로 하는 함수: \\(f(x_0) = E(Y|X=x_0)\\): regression function \n\n\\(L_1 = \\displaystyle |~f(x) - y~|\\)\n\n\\(E(L)\\)을 최소로 하는 함수: \\(f(x_0) = median(Y|X=x_0)\\) \n\n\\(L_q = \\displaystyle |~f(x) - y~|^q\\)\n\n\n\nSource: Pattern Recognition and Machine Learning by Christopher M. Bishop\n\n예를 들어,\n\n\n\\(f(1.5) = E(Y|X=1.5)\\)인 \\(f\\)가 \\(E(L_2)\\)를 최소화하는 optimal한 함수임.\n즉, conditional mean \\(f(x) = E(Y |X = x)\\)는 \\(L_2\\) loss의 관점에서 최적의 함수이며, regression function이라고 부름.\n\n실제로는 \\(E(Y | X=x)\\)를 계산하는 것은 불가능하며, 이를 추정하기 위해 x의 근방에서 평균값을 취함.\n\n\\(\\hat f(x) = Ave(Y |X \\in N(x))\\)\n\n\n\n단, 관측치의 수에 대해 상대적으로 predictor의 갯수가 늘어남에 따라 점차 효율성이 떨어짐; the curse of dimensionality\n\n예를 들어, 다이아몬드 가격을 carat, cut, clarity의 세 변수로 예측하는 경우,\n\\(\\hat f(1.5, Fair, I1) = E(Y|carat=1.5,~ cut=Fair,~ clarity=I1)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\\(f\\)의 smoothing에 대한 여러 접근이 있음\n\nregualization, spline, kernel 등\n\nThe Palmer Archipelago penguins 데이터셋의 예로 보면,\n\n\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\n만약, 부리의 길이로 부리의 깊이를 예측한는 모형에 대해 \\(\\hat f\\)를 구한다면,\n\n\n\n\n\n\n\n\n\n\n\n\n\n앞서 선형회귀모형을 사용한다면,\n\n관계를 선형이라고 전제하고,\nError가 Gaussian 분포를 따른다고 가정하고,\nLikelihood가 최대가 되도록 파라미터(기울기와 절편)를 구한 것임; maximum likelihood estimation\nsquared error를 최소화하는 것과 동일함.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Visualizing a multivariate Gaussian\n\n\n\n\n\n\n\nExpected value\n\n\n\n\n\n\\(X: \\{a, b, a, a, b, c\\}\\)\nThe mean of \\(X=\\displaystyle \\frac{a + b + a + a + b + c}{6} = \\frac{3*a}{6} + \\frac{2*b}{6} + \\frac{1*c}{6}=a*\\frac{3}{6} + b*\\frac{2}{6} + c*\\frac{1}{6}\\)\n\\(~~~~~~~~~~~~~~~~~~~~~~~~~=a*p(X=a) + b*p(X=b) + c*(X=c)\\): weighted average\nExpectation: \\(E(X)=\\displaystyle \\sum x_i p(X=x_i)= \\sum xp(x)\\)\n연속값이라면, \\(E(X)=\\displaystyle \\sum \\Delta x * p(\\Delta x) = \\int x f(x) dx\\), where \\(f(x)\\) is the probability density function\n\\(p(\\Delta x) = f(x)\\Delta x\\)\n\n\n\n\nThe Bias–Variance Trade-off\n위의 논의는 특정 데이터셋에 의존할 수밖에 없는데, (참고: Bayesian의 경우 다른 접근)\n데이터셋마다 다른 \\(\\hat f\\)를 얻게 된다는 점을 감안했을 때, \\(\\hat f\\)의 변동성을 살펴보면,\n\\(L_2\\) loss의 경우,\n\\(\\displaystyle h(x) := E(Y|X=x)\\): optimal prediction of \\(Y\\) at any point \\(x\\)\n\\(~~~~~~~~~ = \\displaystyle \\int y \\cdot p(y|x)dy\\)\n\\(E(L_2) = \\displaystyle \\iint \\left( f(x) - y \\right)^2 p(x, y) dxdy = \\iint \\left( f(x) - h(x) + h(x) - y \\right)^2 p(x, y) dxdy\\)\n\\(~~~~~~~~~~~ = \\displaystyle \\int \\left( f(x) - h(x) \\right)^2 p(x) dx + \\iint \\left( h(x) - y) \\right)^2p(x, y) dxdy\\)\n\n두 번째 항: optimal prediction \\(h(x)\\)가 true value와 얼마나 떨어져 있는가?(분산) - irreducible error\n첫 번째 항: \\(h(x)\\)와의 차이를 최소화하도록 \\(f(x)\\)를 선택하는데 하는데, 유한한 데이터셋에서는 그 간극이 존재\n\n이제, 분포 \\(p(x, y)\\)로부터 수많은 데이터셋들 \\(D_i\\)를 얻었다고 가정했을 때,\n특정 \\(x_0\\)에 대해서\n\n\n\\(E_D[\\left( f(x_0; D_i) - h(x_0)\\right)^2]=[E_D\\left( f(x_0; D_i)\\right) - h(x_0)]^2 + E_D[\\left\\{f(x_0;D_i) - E_D\\left(f(x_0;D_i)\\right)\\right\\}^2]\\)\n\\(~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ = bias^2 + variance\\)\nBias: 최적의 예측값(\\(h\\))에 비해, (여러 데이터로부터 얻은) “평균적인 예측값”이 얼마나 차이가 나는가?\n\n즉, 최적의 예측보다 평균적으로 얼마나 틀린 예측을 하는가?\n\nVariance: 각 데이터로셋에서 얻은 예측값이 (여러 데이터셋에서 얻은) “평균적인 예측값”에서 얼마나 떨어져 있는가?(분산)\n\n즉, 데이터셋에 따라 \\(f(x_0;D_i)\\)가 얼마나 민감하게 변하는지 측정\n\n모든 \\(x\\)에 대한 것으로 비용의 총합으로 확장해서 이해하면,\n\n\n\n\n\n\nExpected Loss 분해\n\n\n\n\\(\\displaystyle E(L_2) = (bias)^2 + variance + noise\\)\n\n\nBias와 variance간에는 trade-off가 존재함\n\nFlexible한 모델의 경우 데이터에 더 잘 적합하여, variance가 높아짐. 즉 데이터셋마다 너무 다른 예측을 하게 됨\n\n데이터에 overfitting이 된다고 말할 수 있음.\n단, N이 증가하면(데이터셋 사이즈가 커지면), flexible한 모델의 variance가 줄어듦.\n여러 데이터셋으로부터 평균을 얻으면, 실제값에 가까워짐.\n\nRigid한 모델의 경우 데이터셋에 덜 적합하여, bias가 높아짐. 즉, 평균적으로 더 틀린 예측을 하게 됨\n\n한편, 데이터셋에 덜 민감하여 variance가 줄어듦\n작은 데이터셋에서 더 유리\n\n이 둘의 적절한 균형을 갖는 모델이 최적의 예측 모델임\n\n예를 들어, 함수 \\(h(x) = sin(2\\pi x)\\)로부터 생성된 데이터셋(N=10)에 대해 다항식의 차수에 따른 flexibility의 변화에 따른 OLS 모델들을 비교하면,\n\n데이터셋의 사이즈가 커지면, (M=9인 경우)\n\n함수 \\(h(x) = sin(2\\pi x)\\)로부터 생성된 100개의 데이터셋(각 N = 25)에 대해 3가지 flexibility에 대한 모델들을 비교하면,\n\n\nSource: p.10, 12, 127, Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#classification",
    "href": "contents/mlsl-intro.html#classification",
    "title": "Machine/Statistical Learning",
    "section": "Classification",
    "text": "Classification\n\\(Y\\)가 범주형 변수인 경우, 즉 분류(classification) 문제인 경우\n\nY가 k개의 범주/클래스로 나뉘는 경우: \\(Y \\in \\{C_1, C_2, ..., C_k\\}\\)\n\n두 범주의 경우, 간단히 \\(Y \\in \\{0, 1\\}\\)\n\n\\(f(x_i): P(Y = C_k|X=x_i)\\)가 최대인 클래스에 할당; \\(\\underset{k}{\\mathrm{argmax}}~ P(Y = C_k|X=x)\\)\n\nConditional class probabilities\nBayes classifier: 최대치\n\n이 \\(P(Y = C_k|X=x)\\)를 추정하기 위한 다양한 방식들이 존재\n\nK-nearest neighbors\nLogistic regression\nGeneralized additive models\nLinear/Quadratic discriminant analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n한 개의 예측변수로 예측한다면,\n가령 \\(X\\): bill_length_mm인 경우, 즉 펭귄의 부리 길이로만 두 종을 분류한다면,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuadratic discriminant analysis(QDA): 각각을 Gaussian으로 가정하고, 평균과 표준편차를 추정\n\n\n\n\n\n\n\n\n\n\nConfusion matrix with the threshold=0.5\n\n\nTruth      Adelie  Gentoo\nPredicted                \nAdelie        144       7\nGentoo          7     116\n\n\n만약, 다음과 같은 loss function (loss matrix)가 주어진다면,\n\n\nTruth      Adelie  Gentoo\nPredicted                \nAdelie          0      10\nGentoo         30       0\n\n\nExpected loss:\n\\(E(L) = \\displaystyle \\sum_{i=1} \\sum_{j=1} L_{ij}P(y\\in C_i, \\hat y\\in C_j) = \\frac{1}{274}(144*0 + 7*10 + 7*30 + 116*0)\\)\n보통의 경우, 0-1 loss를 사용. 즉, misclassified된 관측치에 대한 비율; misclassification error rate\n\\(E(L) = \\displaystyle \\frac{1}{N} \\sum_{i=1}^N I(y_i \\neq \\hat y_i)\\) where \\(I\\) is the indicator function: 0 if \\(y_i = \\hat y_i\\), 1 otherwise\n잘못된 예측에 대한 비용이 다르다면, 이에 대응하는 expected loss를 기준으로 모델을 평가\n\n만약, 와인 셀러가 와인의 품질(high:양성 vs. low:음성)을 성분들로 예측하는 모형을 만든다면,\n높은 품질의 와인을 낮은 품질로 예측하면, 수익의 악화\n\n거짓 음성을 낮춰야 하는 경우: 예를 들어, 영세한 와이너리가 수익이 중요한 경우\n\n낮은 품질의 와인을 높은 품질로 예측하면, 와인 품평가에게 신뢰를 잃을 수 있음\n\n거짓 양성을 낮춰야 하는 경우: 예를 들어, 고품질의 와인을 생산하는 것으로 유명한 와이너리; 네임밸류를 유지하기 위해. 반면, 비싼 와인이 싸게 팔리는 것은 감당할 수 있음.",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/mlsl-intro.html#model-evaluation",
    "href": "contents/mlsl-intro.html#model-evaluation",
    "title": "Machine/Statistical Learning",
    "section": "Model evaluation",
    "text": "Model evaluation\n모델의 평가는 예측함수의 오차로 인한 손실(loss)이 최소인 것으로 기준을 둘 수 있으나,\n현실적으로는 모델은 특정 데이터셋, 혹은 특정 관찰값에 의존할 수밖에 없음.\n우리가 원하는 것은 관찰값에 대해서가 아닌 일반화된(generalized) 혹은 전체(population)에 대해 손실이 최소이기를 바라는 것임.\n\n전통적 통계에서는 모집단(population)이라는 것을 상정하여, 일반화된 모형을 추론하고자 했음.\n\n이를 위해 모집단에 대한 여러 가정들이 필요했으며, 이를 통해 모집단에 대한 추론(inference)을 얻었음.\n관측치를 최대한 모두 사용하여 모형을 추정하고, 모집단에 대한 추론에 대해 수학적인 수정을 거침.\nLikelihood의 관점에서 최선의 모형을 선택하고, 모집단에 대한 가정을 기반으로 불확성을 추론함.\n\n현대적인 관점에서는 가정없이 관찰된 데이터셋만으로 일반화할 수 있는 다른 접근 방식을 택함.\n\n데이터셋을 훈련셋(training set)과 테스트셋(test set)으로 나누어, 훈련셋으로 모델을 구축하고\n모형이 훈련과정에서 보지 못한 새로운 데이터셋인 테스트셋에서 얼마나 잘 작동하는지를 평가함.\n이는 모델의 일반화(generalization)가 얼마나 잘 되는지를 평가하는 접근이라고 볼 수 있음.\n모델의 과적합을 조정하기 위해(bias-variance trade-off) 훈련셋을 다시 여러 개의 서브셋으로 나누어 교차검증(cross-validation)을 수행; 검증셋(validation set)\n데이터셋에 따라 모델의 파라미터 추정치가 어떻게 바뀌는지를 살펴보기 위해, 부트스트랩(bootstrap)을 사용\n\n\n\n\n\n\n\n\n훈련셋과 테스트셋에 대한 손실\n\n\n\n모형에 대한 평가는 테스트셋에 대한 손실을 기준으로 하고자 함므로, 두 종류의 손실에 대해 구별함.\n훈련셋에 대한 손실(오차): 모형을 세우는데 사용된 데이터셋에 대한 손실\n테스트/검증셋에 대한 손실(오차): 테스트/검증 데이터셋으로 모형을 평가 했을 때의 손실\n모형은 훈련셋에 최대한 적합되도록 세운 것이기 때문에,\n\n훈련셋의 손실(오차) &lt; 테스트/검증셋의 손실(오차)\n\n(단, 모형을 세우는 때 사용된 손실과 동일한 방식의 손실로 평가했을 때)\n\n\n\nResampling methods\n\nCross-Validation (교차검증)\n데이터셋을 훈련셋과 검증셋으로 나누어 보면; validation set approach\n예를 들어, Automobile Data(auto) (ISLP 패키지)에서 연비(mpg)를 마력(horsepower)로 예측하는 모델을 만든다면,\n\n훈련셋과 검증셋을 어떻게 선택하는지에 따라 결과가 바뀜\n훈련셋의 양을 얼마나 선택하는지에 따라 결과가 바뀜: 데이터가 작을 수록 모형의 적절성이 낮아짐\n아래 그림; 어떻게 데이터셋을 나누느냐에 따라 결과가 달라짐\n\n이를 해결하기 위해 교차검증(cross-validation)을 사용\n\n다수의 훈련셋과 검증셋을 생성하여, 평균값으로 모형의 성능을 평가\n다양한 변형이 존재\n\n\n\ncode\n# install ISLP package\n# pip install ISLP\n\nfrom ISLP import load_data\nauto = load_data(\"Auto\")\nauto.head(3)\n\n\n    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n0 18.00          8        307.00         130    3504         12.00    70   \n1 15.00          8        350.00         165    3693         11.50    70   \n2 18.00          8        318.00         150    3436         11.00    70   \n\n   origin                       name  \n0       1  chevrolet chevelle malibu  \n1       1          buick skylark 320  \n2       1         plymouth satellite  \n\n\n\nfrom sklearn.model_selection import train_test_split\nauto_train, auto_valid = train_test_split(auto, test_size=.5, random_state=0)\n\n\ncode\np = (\n    so.Plot(auto_train, x='horsepower', y='mpg')\n    .add(so.Dots(color=\".5\"))\n)\np.show()\n\nfor i in range(1, 6):\n    p.add(so.Line(), so.PolyFit(i)).label(title=f\"M = {i}\").show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft: Validation error estimates for a single split into training and validation data sets. Right: The validation method was repeated ten times, each time using a different random split of the observations into a training set and a validation set. This illustrates the variability in the estimated test MSE that results from this approach.\nSource: p. 204, An Introduction to Statistical Learning with Applications in Python\n\n교차검증의 예\n\nk-fold cross-validation: 데이터셋을 k개의 서브셋으로 나누어, k번의 모형평가를 수행\nleave-one-out cross-validation(LOOCV): k=n인 경우\n\n거의 모든 데이터로부터 훈련\nk-fold 작업에서 발생하는 무작위성이 없음\n\nShuffle-split cross-validation: 데이터셋을 무작위로 섞어서 k번의 모형평가를 수행\n\nscikit-learn: Cross-validation 문서 참고\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n주로 k=5, k=10을 사용; 경험적으로 최적임\n\nk의 선택에 따라 bias와 variance의 trade-off가 달라짐\nk가 작을수록 variance가 높아지고, bias가 낮아짐\n\n\nk개의 훈련셋에서 각각에서 얻은 모델에 대해서 검증셋으로부터 모형의 평가치를 얻음: 주로 MSE(mean squared error)\nk-fold CV estimate: \\(\\displaystyle CV_{(k)} = \\frac{1}{k} \\sum_{i=1}^k MSE_i\\)\n분류의 경우, misclassification error rate: \\(\\displaystyle CV_{(k)} = \\frac{1}{k} \\sum_{i=1}^k \\frac{1}{n_i} \\sum_{j=1}^{n_i} I(y_j \\neq \\hat y_j)\\)\n\nfrom sklearn.model_selection import cross_validate, cross_val_score, KFold, ShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom ISLP import load_data\n\nauto = load_data(\"Auto\")\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n\nmod_auto = make_pipeline(PolynomialFeatures(2), LinearRegression())  # 2차 다항함수\nmod_auto_cv = -cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # default scoring is R2\n\nprint(f\"For the quadratic polynomial:\\n10-fold Cross-validation \\nMSEs: {mod_auto_cv}, \\nAverage: {mod_auto_cv.mean()}\")\n\nFor the quadratic polynomial:\n10-fold Cross-validation \nMSEs: [15.98 15.84 18.39 25.88 18.71 23.58 23.58 23.83 11.19 14.88], \nAverage: 19.185331419375\n\n\n\n\ncode\ncv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nmod_auto = make_pipeline(PolynomialFeatures(2), LinearRegression())  # 2차 다항함수\nmod_auto_cv = -cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # default scoring is R2\n\nprint(f\"For the quadratic polynomial:\\nShuffle & Spllit Cross-validation with 20% test sets \\nMSEs: {mod_auto_cv}, \\nAverage: {mod_auto_cv.mean()}\")\n\n\nFor the quadratic polynomial:\nShuffle & Spllit Cross-validation with 20% test sets \nMSEs: [16.01 18.34 17.81 12.66 14.97 20.87 13.4  13.62 20.05 15.53], \nAverage: 16.32656735042892\n\n\n\ncode\nev_error1 = np.zeros([10, 10])\n\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n\nfor i, d in enumerate(range(1, 11)):\n    mod_auto = make_pipeline(PolynomialFeatures(d), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # cross_validate에 대한 wrapper\n    ev_error1[i, :] = -mod_auto_cv\n\n# plot\nplt.figure(figsize=(6, 4), dpi=60)\nfor i in range(10):\n    plt.plot(np.arange(1, 11), ev_error1[:, i])\n    plt.scatter(np.arange(1, 11), ev_error1[:, i], s=5)\n\nplt.xticks(np.arange(1, 11))\nplt.xlabel(\"Degree of Polynomial\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"10-fold Cross-validation \\nover 10 degrees of Polynomial\")\nplt.show()\n\n# Shuffle & Split Cross-validation\nev_error2 = np.zeros([10, 10])\ncv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nfor i, d in enumerate(range(1, 11)):\n    mod_auto = make_pipeline(PolynomialFeatures(d), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')\n    ev_error2[i, :] = -mod_auto_cv\n\n# plot\nplt.figure(figsize=(6, 4), dpi=60)\nfor i in range(10):\n    plt.plot(np.arange(1, 11), ev_error2[:, i])\n    plt.scatter(np.arange(1, 11), ev_error2[:, i], s=5)\n\nplt.xticks(np.arange(1, 11))\nplt.xlabel(\"Degree of Polynomial\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"10-split Shuffle & Split Cross-validation \\nwith 20% test sets over 10 degrees of Polynomial\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nev_error3 = np.zeros([10, 2])\n\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n\nfor i in range(10):\n    mod_auto = make_pipeline(PolynomialFeatures(i+1), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # cross_validate에 대한 wrapper\n    ev_error3[i, 0] = -mod_auto_cv.mean()\n\ncv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nfor i in range(10):\n    mod_auto = make_pipeline(PolynomialFeatures(i+1), LinearRegression())\n    mod_auto_cv = cross_val_score(mod_auto, X, y, cv=cv, scoring='neg_mean_squared_error')  # cross_validate에 대한 wrapper\n    ev_error3[i, 1] = -mod_auto_cv.mean()\n\n# plot\nplt.figure(figsize=(6, 4), dpi=60)\nlabels = [\"10-fold CV\", \"10-split Shuffle & Split CV with 20% test set\"]\nfor i in range(2):\n    plt.scatter(np.arange(1, 11), ev_error3[:, i], s=5)\n    plt.plot(np.arange(1, 11), ev_error3[:, i], label=labels[i])\n\nplt.xticks(np.arange(1, 11))\nplt.xlabel(\"Degree of Polynomial\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title(\"Average of 10-fold Cross-validation \\nover 10 degrees of Polynomial\")\nplt.ylim(15, 30)\nplt.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\nScikit-learn: Metrics and scoring\n관찰된 데이터에 가장 적합한 모형을 찾는 것이 아니고, 일반화된 모형을 찾는 것이 목표임.\n아래 그림에서 관찰된 데이터셋에 대한 예측 오류인 “test error”와 “true error”의 관계를 보여줌.\n예를 들어, 아래의 3가지 형태의 true relationship에 대해,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft: Data simulated from f, shown in black. Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel.\n\n\n\n\nSource: pp. 29-32, An Introduction to Statistical Learning with Applications in Python\n\nCross-validation의 test-error가 효과적으로 true error를 반영할 수 있는지를 살펴보면,\n\n보통 실제 test error rate보다 낮게 나오나(즉, 더 적합하게) 최적의 flexibility의 위치는 유사함\n참고로, 여기서 true error는 앞서 training/test set으로 나눈 그 test set을 의미하는 것이 아님!\n\n\n\nSource: p. 208, An Introduction to Statistical Learning with Applications in Python\n\n다음 절차를 통해, 어떤 클래스의 특정 모델을 선택 후 그 모델의 성능을 평가할 수 있음:\n\n모델 클래스를 선택: 예. linear regression\n데이터셋을 traing set과 test set으로 나눈 후,\ntraining set으로 교차검증(sub-trainging & validation sets)을 통해 모델의 flexibility를 결정: 예. 다항식의 차수, tuning parameter 등\n\n최적의 flexibility 또는 bias-variance trade off 수준를 결정\n\n최적의 flexibility를 선택한 후, 전체 training set으로 fitting한 모델을 선택 후\ntest set으로 모델의 성능을 평가\n\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nauto = load_data(\"Auto\")\nX = auto[[\"horsepower\"]]\ny = auto[\"mpg\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\n\ncv = KFold(n_splits=10, shuffle=True, random_state=0)\n## Shuffle & Split\n# cv = ShuffleSplit(n_splits=10, test_size=.2, random_state=0)\n\nmod_auto = Pipeline([(\"poly\", PolynomialFeatures()), (\"lm\", LinearRegression())])\n\n# Grid Search\nparam_grid = {\"poly__degree\": np.arange(1, 11)}  # \"지정한 estimator의 이름\"__\"parameter 이름\"\ngrid_search = GridSearchCV(mod_auto, param_grid, cv=cv, scoring='neg_mean_squared_error')\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('poly', PolynomialFeatures()),\n                                       ('lm', LinearRegression())]),\n             param_grid={'poly__degree': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('poly', PolynomialFeatures()),\n                                       ('lm', LinearRegression())]),\n             param_grid={'poly__degree': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n             scoring='neg_mean_squared_error') estimator: PipelinePipeline(steps=[('poly', PolynomialFeatures()), ('lm', LinearRegression())])  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures()  LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# Best parameter!\ngrid_search.best_params_\n\n{'poly__degree': 10}\n\n\n\n# Best estimator!\ngrid_search.best_estimator_\n\nPipeline(steps=[('poly', PolynomialFeatures(degree=10)),\n                ('lm', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('poly', PolynomialFeatures(degree=10)),\n                ('lm', LinearRegression())])  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=10)  LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\ngrid_search.cv_results_에 교차검증의 여러 지표들이 포함되어 있음\n\ngrid_search.cv_results_[\"mean_test_score\"]\n\narray([-24.82, -20.29, -20.37, -20.67, -20.2 , -20.16, -20.4 , -20.49,\n       -20.28, -19.94])\n\n\n\ngrid_search.cv_results_[\"param_poly__degree\"].data\n\narray([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=object)\n\n\n\npd.options.display.max_rows = 10\npd.DataFrame(\n    {\n        \"degree\": grid_search.cv_results_[\"param_poly__degree\"].data,\n        \"mean_test_score\": -grid_search.cv_results_[\"mean_test_score\"],\n    }\n).sort_values(\"mean_test_score\")\n\n  degree  mean_test_score\n9     10            19.94\n5      6            20.16\n4      5            20.20\n8      9            20.28\n1      2            20.29\n2      3            20.37\n6      7            20.40\n7      8            20.49\n3      4            20.67\n0      1            24.82\n\n\n2차 다항함수로 충분하다고 판단하고, 전체 training set으로 다시 fitting한 후, test set으로 성능을 평가\n\nmod_auto_best = Pipeline([(\"poly\", PolynomialFeatures(2)), (\"lm\", LinearRegression())])\nmod_auto_best.fit(X_train, y_train)\n\nPipeline(steps=[('poly', PolynomialFeatures()), ('lm', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('poly', PolynomialFeatures()), ('lm', LinearRegression())])  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures()  LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n# get root_mean_squared_error and median_absolute_error\nfrom sklearn.metrics import root_mean_squared_error, median_absolute_error\n\ny_pred = mod_auto_best.predict(X_test)\n\nrmse = root_mean_squared_error(y_test, y_pred)\nmae = median_absolute_error(y_test, y_pred)\n\nprint(f\"Root Mean Squared Error: {rmse:.3f}\")\nprint(f\"Median Absolute Error: {mae:.3f}\")\n\nRoot Mean Squared Error: 4.002\nMedian Absolute Error: 2.486\n\n\n\n\n\nThe Bootstrap\n주어진 데이터셋으로부터 같은 사이즈의 데이터셋들을 중복을 허용해서 추출.\n마치 새로운 표본들을 얻는 것과 같은 효과를 얻어, 표본들마다 모형이 어떻게 다르게 추정되는지를 파악함.\n\n\n\n\n\n\n\n예를 들어, 표본에 따라 변하는 파라미터 추정치의 분포를 얻을 수 있음; 파라미터의 불확실성(uncertainty)을 파악\n여러 표본에서 학습시킨 모형들을 평균내어 예측력을 높히는데 사용; Bagging (Bootstrap aggregating)\n평균적으로 각 표본에서 63.2%의 중복되지 않는 데이터가 선택됨\n\n\n\n\n\n\n\nSales of Child Car Seats(Carseats) 데이터셋으로 예를 들면,\n\nfrom ISLP import load_data\n\ncarseats = load_data(\"Carseats\")\ncarseats.head(3)\n\n    Sales  CompPrice  Income  Advertising  Population  Price ShelveLoc  Age  \\\n0  9.5000        138      73           11         276    120       Bad   42   \n1 11.2200        111      48           16         260     83      Good   65   \n2 10.0600        113      35           10         269     80    Medium   59   \n\n   Education Urban   US  \n0         17   Yes  Yes  \n1         10   Yes  Yes  \n2         12   Yes  Yes  \n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nX = carseats[[\"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\"]]\ny = carseats[\"Sales\"]\n\n# linear regression\nlm_carseats = LinearRegression()\nlm_carseats.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nparams = pd.DataFrame(lm_carseats.coef_, index=X.columns, columns=[\"params\"])\nparams\n\n             params\nIncome       0.0107\nAdvertising  0.1254\nPopulation  -0.0009\nPrice       -0.0574\nAge         -0.0490\n\n\n\nfrom sklearn.utils import resample\n\nnp.random.seed(0)\nparams_boot = params.copy()\n\nfor i in range(1000):\n    X_new, y_new = resample(X, y, replace=True)  # resampling with replacement\n    lm_carseats.fit(X_new, y_new)  # refit\n    \n    params_new = pd.DataFrame(lm_carseats.coef_, index=X.columns, columns=[f\"sample_{i}\"])\n    params_boot = pd.concat([params_boot, params_new], axis=1)\n\n\nparams_boot\n\n             params  sample_0  sample_1  sample_2  sample_3  ...  sample_995  \\\nIncome       0.0107    0.0037    0.0175    0.0126    0.0100  ...      0.0139   \nAdvertising  0.1254    0.1439    0.1170    0.1158    0.1232  ...      0.1392   \nPopulation  -0.0009    0.0001   -0.0019   -0.0007   -0.0012  ...      0.0008   \nPrice       -0.0574   -0.0630   -0.0462   -0.0552   -0.0595  ...     -0.0630   \nAge         -0.0490   -0.0595   -0.0404   -0.0574   -0.0465  ...     -0.0478   \n\n             sample_996  sample_997  sample_998  sample_999  \nIncome           0.0113      0.0082      0.0132      0.0063  \nAdvertising      0.0914      0.0904      0.1221      0.1253  \nPopulation      -0.0016     -0.0002     -0.0018     -0.0002  \nPrice           -0.0558     -0.0518     -0.0512     -0.0469  \nAge             -0.0486     -0.0535     -0.0394     -0.0375  \n\n[5 rows x 1001 columns]\n\n\n1000개 표본에서 파라미터 추정치의 표준편차; standard error\n\nparams_boot.std(axis=1)\n\nIncome        0.0038\nAdvertising   0.0174\nPopulation    0.0008\nPrice         0.0050\nAge           0.0069\ndtype: float64\n\n\n실제 파라미터 추정치의 분포를 보면,\nparams_boot.T.hist(bins=30, alpha=.7, figsize=(7, 3), layout=(2, 3))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n이 분포로부터 파라이터 추정치의 confidence interval (신뢰구간)을 구할 수 있음; 여러 변형이 존재함.\n보통 95% 신뢰구간을 사용\n\n\n\n\n\n\nShort version\n\n\n\n\n\nnp.random.seed(0)\n\nparams = pd.Series(lm_carseats.coef_, index=X.columns)\nerr = np.std([lm_carseats.fit(*resample(X, y)).coef_ for i in range(1000)], axis=0)\n\npd.DataFrame({'coef': params, 'error': err})\n\n#                coef  error\n# Income       0.0063 0.0038\n# Advertising  0.1253 0.0174\n# Population  -0.0002 0.0008\n# Price       -0.0469 0.0050\n# Age         -0.0375 0.0069\n\n\n\n선형회귀에서 OLS estimation 결과와 비교하면,\n\nfrom statsmodels.formula.api import ols\n\nlm_carseats2 = ols(\"Sales ~ Income + Advertising + Population + Price + Age\", data=carseats).fit()\n\nprint(lm_carseats2.summary(slim=True))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.373\nModel:                            OLS   Adj. R-squared:                  0.365\nNo. Observations:                 400   F-statistic:                     46.82\nCovariance Type:            nonrobust   Prob (F-statistic):           6.07e-38\n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      15.4227      0.808     19.098      0.000      13.835      17.010\nIncome          0.0107      0.004      2.635      0.009       0.003       0.019\nAdvertising     0.1254      0.018      7.112      0.000       0.091       0.160\nPopulation     -0.0009      0.001     -1.086      0.278      -0.002       0.001\nPrice          -0.0574      0.005    -11.962      0.000      -0.067      -0.048\nAge            -0.0490      0.007     -7.000      0.000      -0.063      -0.035\n===============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.37e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/lda.html",
    "href": "contents/lda.html",
    "title": "Generative Models",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 5, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nLogistic Regression에서는 클래스 \\(C_k\\)에 속할 확률을 다음과 같이 선형모델로 직접 얻었음.\n\\(E(Y=1|X=x)=\\)   또는\n\\(P(Y = C_1|X=x) = \\sigma (\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p)\\)   where   \\(\\displaystyle\\sigma(z) = \\frac{1}{1+e^{-z}}\\):   sigmoid/logistic function\n몇 가지 개선될 부분들",
    "crumbs": [
      "Trees",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#generative-classifier",
    "href": "contents/lda.html#generative-classifier",
    "title": "Generative Models",
    "section": "Generative Classifier",
    "text": "Generative Classifier\n관찰된 값 \\(x\\)에 대해, \\(Y\\)의 conditional probability인 \\(P(Y = C_k|X=x)\\)을 추정하는데, 각 클래스 별로 \\(X\\)의 분포를 이용해 역으로 \\(Y\\)의 분포를 계산하는 방식; Bayes theorem을 이용\n\\(\\displaystyle P(Y = C_k|X=x) = \\frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x)}\\)\n\n\\(P(Y = C_k|X=x)\\): 관측치 \\(x\\)가 클래스 \\(C_k\\)에 속할 확률; posterior probability\n\\(P(Y=C_k)\\): 임의로 선택된 관측치가 클래스 \\(C_k\\)에 속할 확률; prior probability\n\\(P(X=x|Y=C_k)\\): 클래스 \\(C_k\\)에 속하는 \\(x\\)의 probability density &gt;&gt; “generative”\n\n주어진 \\(x\\)에 대해, 각 클래스 \\(C_k\\)에 속할 확률 \\(P(Y=C_k|X=x)\\)를 계산하여, 이 중 가장 높은 확률을 가지는 클래스를 선택하는 방식으로 분류; Bayes classifier\n\n\n\n\n\n\nNote\n\n\n\nposterior probability는 prior probability의 update된 확률로 볼 수 있음.\n예를 들어, 피부암 진단을 피부 이미지의 명암(\\(X\\):0-255)으로 예측한다고 하면,\n\\(P(Y=cancer|X=240) \\propto P(X=240|Y=cancer) * P(Y=cancer)\\)\n“이미지의 명암이 240일 때 피부암일 확률” \\(\\propto\\) “피부암일 때, 그 이미지의 명암이 240일 확률” x “이미지들 중 피부암일 확률”\n이미지 검사 후의 “사후 확률”을 이미지 검사 전의 “사전 확률”의 update로 볼 수 있음.\n\\(P(Y=cancer|X=240)\\) &gt; \\(P(Y=not~cancer|X=240)\\) 이면, 이미지를 피부암으로 분류.\n\n\n분모: \\(\\displaystyle P(X=x) = P(X=x|Y=C_1)P(Y=C_1) + P(X=x|Y=C_2)P(Y=C_2)\\)\n따라서, \\(\\displaystyle P(Y = C_k|X=x) = \\frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x|Y=C_1)P(Y=C_1) + P(X=x|Y=C_2)P(Y=C_2)}\\)\n\nPrior \\(P(Y=C_k)\\)의 추정치는 각 클래스에 속하는 표본의 비율로 추정\n어려운 문제는 probability density function \\(f_k(x)=P(X=x|Y=C_k)\\)의 추정!\n\n단순화하기 위해 이 분포에 대한 가정을 부과\nLinear Discriminant Analysis (LDA): (Multivariate) Gaussian 분포, 모든 클래스에 대해 covariance matrix(공분산 행렬) 동일\nQuadratic Discriminant Analysis (QDA): (Multivariate) Gaussian 분포, 각 클래스 별로 고유한 covariance matrix(공분산 행렬)\n\n\nPalmer Penguins 데이셋의 예에서,\n1-dimensional (1 predictor)\nbill_length로 펭균 종을 구분하는 경우\n\n\n\n\n\n\n\n\n\n\n2-dimensional (2 predictors)\nbill_length과 bill_depth로 펭균 종을 구분하는 경우",
    "crumbs": [
      "Trees",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#linear-discriminant-analysis-lda",
    "href": "contents/lda.html#linear-discriminant-analysis-lda",
    "title": "Generative Models",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\n(Multivariate) Gaussian 분포 가정\n모든 클래스에 대해 covariance matrix(공분산 행렬) 동일\n\n\n1-dimensional: p = 1\n\\(\\displaystyle f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x-\\mu_k)^2}{2\\sigma^2}\\right)\\)\n\n클래스 각각에 대해서 분포의 평균 \\(\\mu_k\\)를 추정: MLE로 추정하면 클래스별 평균값\n\\(\\sigma\\)는 모든 클래스에 대해 동일하다고 가정하고 추정: MLE로 추정하면 클래스별 분산값의 weighted 평균\nMLE(Maximum Likelihood Estimation)이 이상치(outliers)에 민감함을 주의\n\n\n\n\n\n\n\n\n\n\n\n\\(\\displaystyle P(Y = C_k|X=x) = \\frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x)}\\) \\(\\displaystyle \\qquad\\qquad\\qquad\\qquad= \\frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x|Y=C_1)P(Y=C_1) + P(X=x|Y=C_2)P(Y=C_2) + P(X=x|Y=C_3)P(Y=C_3)}\\)\n\\(\\displaystyle \\qquad\\qquad\\qquad\\qquad= \\frac{f_k(x)P(Y=C_k)}{f_1(x)P(Y=C_1) + f_2(x)P(Y=C_2) + f_3(x)P(Y=C_3)}\\)   (A)\n이제 \\(P(Y=C_k)\\)만 추정하면 되는데,\n즉, 임의로 선택된 관측치가 클래스 \\(C_k\\)에 속할 확률 추정값: \\(\\displaystyle \\hat P(Y=C_k) = \\frac{n_k}{n}\\)   where   \\(n_k\\): 클래스 \\(C_k\\)에 속하는 표본의 수, \\(n\\): 전체 표본의 수\n예측값을 얻기 위한 준비 완료!\n\\(X=x\\)에 대한 예측값은 다음 세 확률 중 가장 큰 값에 해당하는 클래스에 할당; the Bayes classifier\n\n\\(P(Y = C_1|X=x)\\)\n\\(P(Y = C_2|X=x)\\)\n\\(P(Y = C_3|X=x)\\)\n\n식 A에서 분모는 모두 같기 때문에 분자만 비교하면 됨. 즉, \\(\\displaystyle f_k(x)P(Y=C_k)\\)를 비교하면 됨.\n로그를 취해 정리하면,\n\\(\\displaystyle \\delta_k(x) = \\frac{\\mu_k}{\\sigma^2}x - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(P(Y=C_k))\\)   : discriminant function\n추정치로 바꾸면, \\(\\displaystyle \\hat\\delta_k(x) = \\frac{\\hat\\mu_k}{\\hat\\sigma^2}x - \\frac{\\hat\\mu_k^2}{2\\hat\\sigma^2} + \\log(\\frac{n_k}{n})\\)   : \\(x\\)에 대한 일차함수 형태\n\n\n\n\n\n\nLogistic regression에서 log-odds의 형태로 선형모형을 세운 이유로 볼 수 있음.\n\\(\\displaystyle log~odds = log\\left(\\frac{p}{1-p}\\right) = log\\left(\\frac{P(Y=1|X=x)}{P(Y=0|X=x)}\\right) = \\beta_{0} + \\beta_{1}x\\)\n\n\n\n\\(\\mu\\), \\(\\sigma\\), prior의 추정치와 몇 개의 \\(X\\)값에 대한 예측 확률을 구해보면,\n\n\n\n\n\n\n             mu  sigma  prior\nAdelie    38.79   2.95   0.44\nChinstrap 48.83   2.95   0.20\nGentoo    47.50   2.95   0.36\n\n\nBill Length(mm)   35   42  43.35   45   50  52.05   55\nSpecies                                               \nAdelie          1.00 0.76   0.44 0.13 0.00   0.00 0.00\nChinstrap       0.00 0.04   0.12 0.22 0.42   0.50 0.61\nGentoo          0.00 0.20   0.44 0.65 0.58   0.50 0.39\n\n\n\n예를 들어, 어떤 펭균의 부리의 길이를 관찰하기 전, 그 펭균이 Adelie 펭귄일 확률은 0.44(prior)인데,\n\n만약, 부리의 길이가 45mm임을 관찰하면, 그 펭균이 Adelie 펭귄일 확률은 0.13(posterior)로 update됨.\n\n\\(\\displaystyle P(Adelie|X=45) = \\frac{f_{Adelie}(X=45)*0.44}{normalize~factor} = 0.13\\)\n\n혹은 부리의 길이가 50mm임을 관찰하면, 그 펭균이 Adelie 펭귄일 확률은 0.001(posterior)로 update됨.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n각 클래스에 속할 확률을 구하면, 이 확률을 이용해 threshold를 정해 분류할 수 있음.\n\n예를 들어, 적어도 .70의 확률/확신이 있을 때만 분류를 실행하고,\n그렇지 않다면 분류를 보류할 수 있음.\n이는 분류의 오류를 낮추는 방법 중 하나; reject option\n\n반면, 분류만이 목적이라면, threshold를 정하지 않고, 가장 높은 확률을 가지는 클래스로 분류하면 되고,\n간단히 discriminant function을 이용해 decision boundary만 찾는 것이 효율적.\nDecision boundaries: \\(\\delta_k(x)\\)가 최대가 되는 클래스가 변하는 \\(x\\)들의 위치\n\n\\(\\delta_1(x_0) = \\delta_3(x_0)\\):   \\(x_0 = 43.35\\)\n\n\\(\\delta_2(x_1) = \\delta_3(x_1)\\):   \\(x_1 = 52.05\\)\n\n\n\n\n\n\n\n\n\n\n\n\n2-dimensional: \\(\\mathbf{x_1}, \\mathbf{x_2}\\)\nMultivariate Gaussian 분포: \\(\\displaystyle f_k(\\mathbf{x}) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mu_k)^T\\Sigma^{-1}(\\mathbf{x}-\\mu_k)\\right)\\)\n\n\\(\\displaystyle \\mathbf{x}= \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\)\n\\(\\mu_k\\): 클래스 \\(C_k\\)에 대한 \\(X_1\\)과 \\(X_2\\)의 평균 벡터: \\(\\displaystyle \\begin{bmatrix} \\mu_{k1} \\\\ \\mu_{k2} \\end{bmatrix}\\)\n\\(\\Sigma\\): 클래스 \\(C_k\\)에 대한 공분산 행렬(covariance matrix): \\(\\displaystyle \\begin{bmatrix} \\sigma_{k_1}^2 & \\sigma_{k_1k_2} \\\\ \\sigma_{k_2k_1} & \\sigma_{k_2}^2 \\end{bmatrix}\\)\n\n모든 클래스 \\(C_k\\)에 대해 동일하다고 가정\n\nMLE(Maximum Likelihood Estimation)로 추정\n\n\n\nSource: p. 150, Introduction to Statistical Learning with Applications in Python by G. James, D. Witten, T. Hastie, R. Tibshirani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrimint function: 다음이 가장 큰 값을 가지는 클래스에 할당\n\\(\\displaystyle\\delta_k(\\mathbf{x}) = \\mathbf{x}^T\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\log(P(Y=C_k))\\):   \\(ax_1 + bx_2 + c\\) 형태\n    \\(\\mu_k\\)                        covariance matrix\n\n\n\n\n\n\n             Adelie  Chinstrap  Gentoo\nbill_length   38.79      48.83   47.50\nbill_depth    18.35      18.42   14.98\n\n\n \n\n\n         sigma_1  sigma_2\nsigma_1     8.68     1.74\nsigma_2     1.74     1.25\n\n\n\n\n\n       Adelie  Chinstrap  Gentoo\nprior    0.44       0.20    0.36\n\n\n예를 들어, \\(\\delta_1(x_1, x_2) = 2.1x_1 + 11.8x_2 - 149.9\\)\nDecision boundaries: \\(\\delta_k(\\mathbf{x})\\)가 최대가 되는 클래스가 변하는 \\(\\mathbf{x}\\)들의 위치.\n이는 각 클래스에 속한 데이터의 중심으로부터 멀리 떨어지도록 하는 방향으로 결정됨.\n\n\\(\\delta_1(\\mathbf{x}) = \\delta_2(\\mathbf{x})\\):   red line: Adelie vs. Chinstrap\n\\(\\delta_2(\\mathbf{x}) = \\delta_3(\\mathbf{x})\\):   purple line: Chinstrap vs. Gentoo\n\\(\\delta_3(\\mathbf{x}) = \\delta_1(\\mathbf{x})\\):   green line: Gentoo vs. Adelie\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegularization\n\n\n\nRegularization의 원리를 마찬가지로 적용하여, 파라미터 추정치를 shrink하여 overfitting을 방지할 수 있음.\n단, 이 경우 covarance matrix에 대해 shrinkage가 적용됨; scikit-learn 문서 참고\n\n\n\n\n\n\n\n\nLDA as a dimension reduction method\n\n\n\nLDA는 다음과 같이 차원 축소 방법으로도 사용될 수 있음; 클래스 간 분산을 최대화하고, 클래스 내 분산을 최소화하는 방향으로 차원 축소\n\n\nSource: p. 116, The Elements of Statistical Learning (2e) by T. Hastie, R. Tibshirani, J. Friedman",
    "crumbs": [
      "Trees",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#quadratic-discriminant-analysis-qda",
    "href": "contents/lda.html#quadratic-discriminant-analysis-qda",
    "title": "Generative Models",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\n\n(Multivariate) Gaussian 분포 가정\n각 클래스 별로 고유한 variances, covariances를 가정: covariance matrix(공분산 행렬)\n\n\\(\\displaystyle \\delta_k(\\mathbf{x}) = - \\frac{1}{2}(\\mathbf{x}-\\mu_k)^T\\Sigma_k^{-1}(\\mathbf{x}-\\mu_k) -\\frac{1}{2}\\log|\\Sigma_k| + \\log(P(Y=C_k))\\)\n\\(\\displaystyle \\qquad= - \\frac{1}{2}\\mathbf{x}^T\\Sigma_k^{-1}\\mathbf{x} + \\mathbf{x}^T\\Sigma_k^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma_k^{-1}\\mu_k -\\frac{1}{2}\\log|\\Sigma_k| + \\log(P(Y=C_k))\\)\n\\(\\displaystyle \\qquad= (ax_1^2 + bx_1x_2 + cx_2^2) + (dx_1 + ex_2) + d\\)   : 모든 2차항들을 포함 (quadratic)\n이번에는 bill_length_mm와 flipper_length_mm를 예측변수로 사용하여 펭균 종을 구분하면,\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \\(\\mu_k\\)\n\n\n\n\n\n\n                   Adelie  Chinstrap  Gentoo\nbill_length         38.79      48.83   47.50\nflipper_length_mm  189.95     195.82  217.19\n\n\n \n\n\n\n   Covariance matrices for each class\n\n\n\n\n\n\nAdelie   sigma_1  sigma_2\nsigma_1     7.09     5.67\nsigma_2     5.67    42.76\n\n\n \n\n\nChinstrap  sigma_1  sigma_2\nsigma_1      11.15    11.23\nsigma_2      11.23    50.86\n\n\n \n\n\nGentoo   sigma_1  sigma_2\nsigma_1     9.50    13.21\nsigma_2    13.21    42.05\n\n\n\n\n\n       Adelie  Chinstrap  Gentoo\nprior    0.44       0.20    0.36\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQDA vs. LDA\n\n파라미터의 수: \\(K*p\\) vs. \\(K*p(p+1)/2\\) 추가\nLDA가 훨씬 덜 flexible classifier, 따라서 lower variance\n\n데이터가 적어 variance가 문제가 된다면 LDA를 선택하는 것이 좋을 수 있음.\n\n만약, common covariance의 가정에서 많이 벗어난다면 LDA는 심각한 bias를 가질 수 있음.\n\n \n\nSource: p. 157, Introduction to Statistical Learning with Applications in Python by G. James, D. Witten, T. Hastie, R. Tibshirani",
    "crumbs": [
      "Trees",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#naive-bayes",
    "href": "contents/lda.html#naive-bayes",
    "title": "Generative Models",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\n각 클래스 내에서 예측변수들이 독립적이라고 가정: 서로 연관관계가 없다고 가정\n각 클래스 별로 고유한 분포를 가정: QDA와 유사\n\n\\(\\displaystyle P(Y = C_k|X=x) = \\frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x)}\\)\np = 2인 경우의 예에서,\n\\(P(X=(x_1, x_2)|Y=C_k) = P(X_1=x_1| Y=C_k) *  P(X_2=x_2 | Y=C_k)\\):   즉, 각 예측변수 내에서의 분포만 고려 (marginal distribution)\n따라서, \\(\\displaystyle P(Y = C_k|X=x) = \\frac{f_{k1}(x_1) * f_{k2}(x_2) * P(Y=C_k)}{P(X=x)}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n앞서 covariance matrix로 joint distribution을 추정했으나, 실질적인 joint distribution을 추정하는 것은 매우 어려움\n비현실적이지만, 예측변수들이 서로 독립이라고 가정하면, 분포를 추정하는 것이 매우 간단해지며\n속도가 매우 빠르고 조정 가능한 매개변수가 거의 없기 때문에 분류 문제에 대한 빠르고 간단한 기준 제공\n특히, 예측 변수(p) 대비 표본 수(n)가 적어 joint distribution을 추정하기 어려운 경우에 유용\n이는 bias가 높아질 수 있지만, variance가 낮아지기 때문에 그 이점이 있음.\n\n각 클래별로 (1-d density function) \\(f_{k}\\)를 추정하는데 여러 옵션을 사용할 수 있음.\n\n\n\n\n\n\n\n앞서 LDA에서 사용한 Gaussian 분포; Gaussian Naive Bayes\n적절한 binning을 통한 histogram, 또는 (smoothed) kernel density estimation\n카테고리 변수인 경우, 각 카테고리 값의 비율을 이용; Categorical Naive Bayes\n혼합되어 있는 경우; 각각 따로 확률을 구해 곱 Mixed Naive Bayes\n\n예를 들어,\n\nSource: p. 160, Introduction to Statistical Learning with Applications in Python by G. James, D. Witten, T. Hastie, R. Tibshirani\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n비슷하게, 독립성의 가정을 기반으로 예측변수들이 binominal/multinomial(이항/다항) distribution을 따르는 경우,\nMultinomial Naive Bayes로 불림\n텍스트 분류에서 사용; 스팸메일 분류 포함\n\n\n펭귄의 예로, Gaussian 분포를 사용하면",
    "crumbs": [
      "Trees",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#logistic-regression과의-관계",
    "href": "contents/lda.html#logistic-regression과의-관계",
    "title": "Generative Models",
    "section": "Logistic regression과의 관계",
    "text": "Logistic regression과의 관계\n사후 확률(posterior)의 log odds의 관점에서 보면, logistic regression은\n\\(\\displaystyle log\\left(\\frac{P(Y=1|X=x)}{P(Y=0|X=x)}\\right) = \\beta_{0} + \\beta_{1}X_1 + \\cdots + \\beta_{p}X_p\\)\nLDA에서 동일하게 posterior의 log odds를 살펴보면,\n\\(\\displaystyle log\\left(\\frac{P(Y=C_k|X=x)}{P(Y=C_K|X=x)}\\right) = \\log\\left(\\frac{f_k(x)P(Y=C_k)}{f_K(x)P(Y=C_K)}\\right) = \\log {f_k(x)P(Y=C_k)} + \\log {f_K(x)P(Y=C_K)}\\)\n\n\n\n\n\n\nNote\n\n\n\n일반적으로 K개의 클래스가 있을 때, 기준이 되는 클래스(K) 대비 특정 클래스(k)의 posterior의 log odds를 생각하면 확장될 수 있음.\n\\(\\displaystyle log\\left(\\frac{P(Y=C_k|X=x)}{P(Y=C_K|X=x)}\\right) = \\beta_{0}^k + \\beta_{1}^k X_1 + \\cdots + \\beta_{p}^k X_p\\)",
    "crumbs": [
      "Trees",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#generative-vs.-discriminative-models",
    "href": "contents/lda.html#generative-vs.-discriminative-models",
    "title": "Generative Models",
    "section": "Generative vs. Discriminative Models",
    "text": "Generative vs. Discriminative Models\nDiscriminative models: \\(P(C_k|X)\\)를 직접 추정\n\n즉, \\(P(C_k|\\mathbf{x}) = f(\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p)=f(\\mathbf{w}^T \\mathbf{x} + w_0)\\)\n\\(f\\)를 activation function이라고 부르며, 역함수를 link function이라고 부름\n\\(f\\)가 sigmoid function (\\(f^{-1}\\): logit function) 일 때 logistic regression\n이는 \\(P(\\mathbf{x}|C_k)\\)를 추정하지 않고, 직접 \\(p\\)개의 파라미터만 추정하면 됨\n또한, generative model에서 \\(P(X|C_k)\\)를 추정하는데 매우 많이 데이터와 계산이 필요한데, 종종 decision boundary를 결정하기 위한 posterior \\(P(C_k|\\mathbf{x})\\) 얻는데 \\(P(\\mathbf{x}|C_k)\\)의 정보가 다 필요하지 않을 수 있음. (아래 그림)\n이는 discriminative models이 decision boundary를 결정하는데 더 효과적일 수 있음\n또한, generative model에서 추정하는 class-conditional density, \\(f_k(\\mathbf{x})\\)가 실제 분포와 일치하지 않는다면 discriminative model이 더 정확할 수 있음\n\n   \n\n    Source: p. 145, Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H\n\n\ngenerative model의 경우 \\(P(X)\\)를 이용해 새로운 데이터에 대해 예측할 때, 그 데이터의 발생 확률을 얻을 수 있어서 이를 예측의 정확성에 대한 보완 정보로 활용할 수 있음.\n\nAnomaly detection에도 활용\n\n\n\n확률적 모형의 장점들\n\n특정 클래스에 속할 확률에 대한 정보를 얻을 수 있으며,\n이 확률과 loss function을 결합하여, 기대값(expected loss)를 계산하여 최적의 분류를 결정할 수 있음.\n\n주어진 \\(\\mathbf{x}\\)를 \\(\\displaystyle\\sum_{k} L_{kj}P(C_k|\\mathbf{x})\\)이 최소가 되는 클래스에 분류\n\n어느 클래스에도 확실히 속하지 않는 경우, 분류의 결정을 보류할 수 있음; reject option\n\n  \n\n    Source: p. 143, Deep Learning: Foundations and Concepts by Bishop, C. M. & Bishop, H\n\n\n매우 드물게 발생되는 클래스가 존재하는 경우, balanced dataset에서 학습한 후 prior를 조정\n\nposterior \\(\\sim f_k\\), prior\n\n상이한 예측변수들에 대해 독립적으로 모형을 만든 후 결합할 수 있음\n\n예를 들어, 피부암의 진단에서 사진 판독 & 혈액검사 결과를 독립적으로 모형을 만든 후 결합\n두 모형에서 사용된 예측변수들은 서로 독립이라고 가정하면\n\n\\(P(\\mathbf{x_A, x_B}|C_k) = P(\\mathbf{x_A}|C_k)P(\\mathbf{x_B}|C_k)\\)\n\n\n\n\n\n\n\n\n\n한편, 확률적 모델을 사용하지 않고 discriminant function을 구해 class label을 직접 예측하는 방식도 있음\n\ndecision tree\nsupport vector machine",
    "crumbs": [
      "Trees",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/lda.html#python-implementation",
    "href": "contents/lda.html#python-implementation",
    "title": "Generative Models",
    "section": "Python Implementation",
    "text": "Python Implementation\n\nfrom sklearn import datasets\n\nwine = datasets.load_wine(as_frame=True)\nwine_df = wine['frame']\nwine_df['target'] = wine_df['target'].map({0: 'C0', 1: 'C1', 2: 'C2'})\nwine_df.head(3)\n\n   alcohol  malic_acid  ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0    14.23        1.71 2.43              15.60     127.00           2.80   \n1    13.20        1.78 2.14              11.20     100.00           2.65   \n2    13.16        2.36 2.67              18.60     101.00           2.80   \n\n   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity  hue  \\\n0        3.06                  0.28             2.29             5.64 1.04   \n1        2.76                  0.26             1.28             4.38 1.05   \n2        3.24                  0.30             2.81             5.68 1.03   \n\n   od280/od315_of_diluted_wines  proline target  \n0                          3.92  1065.00     C0  \n1                          3.40  1050.00     C0  \n2                          3.17  1185.00     C0  \n\n\nDecision boundary를 시각화하기 위해, 2차원 예측변수를 사용하여 LDA, QDA, Naive Bayes를 적용\n\nfrom sklearn.model_selection import train_test_split\n\nX = wine_df[['color_intensity', 'alcohol']]\ny = wine_df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nlda = LDA(store_covariance=True).fit(X_train, y_train)\n\n\n\n\n\n\n\nShrinkage\n\n\n\nCovariance matrix에 shrinkage를 적용하려면 가령,\nsklearn 문서 참고\nLDA(store_covariance=True, shrinkage=\"auto\", solver=\"lsqr\")\n\n\n각 클래스별 평균, 공분산 행렬 추정\n\nMaximum Likelihood Estimation (MLE) 사용\n과적합을 방지하기 위해 shrinkage를 사용한 추정치도 사용 가능; sklearn 문서 참조\n\n\n# 각 클래스의 분포 평균 추정치\ndf_mean = pd.DataFrame(lda.means_.T, index=X.columns, columns=lda.classes_)\ndf_mean\n\n                   C0    C1    C2\ncolor_intensity  5.28  3.12  7.14\nalcohol         13.62 12.31 13.16\n\n\n\n# 각 클래스의 공분산 행렬 추정치\ndf_cov = pd.DataFrame(lda.covariance_, columns=[\"sigma_1\", \"sigma_2\"], index=pd.Index([\"sigma_1\", \"sigma_2\"]))\ndf_cov\n\n         sigma_1  sigma_2\nsigma_1     2.56     0.29\nsigma_2     0.29     0.29\n\n\n\n# 각 클래스의 사전 확률 추정치\ndf_priors = pd.DataFrame(lda.priors_, index=lda.classes_, columns=[\"prior\"])\ndf_priors\n\n    prior\nC0   0.29\nC1   0.42\nC2   0.29\n\n\n다음은 decision boundary를 시각화하기 위한 코드\n\n# grid data\nx1 = np.linspace(X_train['color_intensity'].min(), X_train['color_intensity'].max(), 100)\nx2 = np.linspace(X_train['alcohol'].min(), X_train['alcohol'].max(), 100)\n\nfrom itertools import product\nX_grid = pd.DataFrame(\n    list(product(x1, x2)),\n    columns=[\"color_intensity\", \"alcohol\"],\n)\ny_grid = lda.predict(X_grid)\nX_grid[\"target\"] = y_grid\n\nplt.figure(figsize=(6, 5))\nsns.scatterplot(X_grid, x=\"color_intensity\", y=\"alcohol\", hue=\"target\", s=10, alpha=.3, palette=[\"0\", \".3\", \".6\"], legend=False)\nsns.scatterplot(x=X_test[\"color_intensity\"], y=X_test[\"alcohol\"], hue=y_test)\n\nplt.xlabel('color intensity')\nplt.ylabel('alcohol')\nplt.title(\"LDA Decision Boundary\")\n\nplt.show()\n\n\n\n\n\n\n\n각 클래스별 예측된 확률과 예측된 클래스 (데이터프레임으로 표시)\n\npred_class = pd.DataFrame(lda.predict(X_test), columns=[\"pred\"])\npred_prob = pd.DataFrame(lda.predict_proba(X_test), columns=lda.classes_)\n\nresults_df = pd.concat([X_test.reset_index(drop=True), pred_class, pred_prob], axis=1)\nresults_df\n\n    color_intensity  alcohol pred   C0   C1   C2\n0              5.88    13.69   C0 0.72 0.03 0.25\n1              2.06    12.42   C1 0.03 0.96 0.01\n2              5.10    13.64   C0 0.80 0.05 0.14\n..              ...      ...  ...  ...  ...  ...\n69             4.60    13.28   C0 0.62 0.22 0.17\n70             9.90    12.77   C2 0.01 0.00 0.99\n71            10.68    13.45   C2 0.01 0.00 0.99\n\n[72 rows x 6 columns]\n\n\nConfusion matrix\n\nfrom ISLP import confusion_table\nconfusion_table(lda.predict(X_test), y_test)\n\nTruth      C0  C1  C2\nPredicted            \nC0         26   1   3\nC1          0  26   2\nC2          2   0  12\n\n\nClassification report: precision, recall은 각 클래스에 대한 나머지 클래스들에 대한 관계를 의미\n\nfrom sklearn.metrics import classification_report\ny_pred = lda.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n          C0       0.87      0.93      0.90        28\n          C1       0.93      0.96      0.95        27\n          C2       0.86      0.71      0.77        17\n\n    accuracy                           0.89        72\n   macro avg       0.88      0.87      0.87        72\nweighted avg       0.89      0.89      0.89        72\n\n\n\nQDA와 Naive Bayes도 동일한 방식으로 사용 가능\n각각, 다음과 같이 QuadraticDiscriminantAnalysis와 GaussianNB를 사용해 estimator를 생성\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.naive_bayes import GaussianNB\n\nqda = QDA(store_covariance=True).fit(X_train, y_train)\nnb = GaussianNB().fit(X_train, y_train)",
    "crumbs": [
      "Trees",
      "Classification",
      "Generative Models"
    ]
  },
  {
    "objectID": "contents/inspection.html",
    "href": "contents/inspection.html",
    "title": "Inspecting data",
    "section": "",
    "text": "# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")"
  },
  {
    "objectID": "contents/inspection.html#useful-method",
    "href": "contents/inspection.html#useful-method",
    "title": "Inspecting data",
    "section": "Useful method",
    "text": "Useful method\n.head(), .tail(), .sample()\n.info(), .describe(),\n.value_counts(),\n.sort_values(), .nlargest(), .nsmallest()\nData: Tips\n일정기간 한 웨이터가 얻은 팁에 대한 데이터\n\n# load a dataset\ntips = sns.load_dataset(\"tips\")\ntips\n\n     total_bill  tip     sex smoker   day    time  size\n0         16.99 1.01  Female     No   Sun  Dinner     2\n1         10.34 1.66    Male     No   Sun  Dinner     3\n2         21.01 3.50    Male     No   Sun  Dinner     3\n..          ...  ...     ...    ...   ...     ...   ...\n241       22.67 2.00    Male    Yes   Sat  Dinner     2\n242       17.82 1.75    Male     No   Sat  Dinner     2\n243       18.78 3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]\n\n\n\ntips.head(3)  # 앞 n개 나열, 기본값은 5\n\n   total_bill  tip     sex smoker  day    time  size\n0       16.99 1.01  Female     No  Sun  Dinner     2\n1       10.34 1.66    Male     No  Sun  Dinner     3\n2       21.01 3.50    Male     No  Sun  Dinner     3\n\n\n\ntips.sample(5)  # 무작위로 n개 표본 추출, 기본값은 1\n\n     total_bill  tip     sex smoker   day    time  size\n129       22.82 2.18    Male     No  Thur   Lunch     3\n30         9.55 1.45    Male     No   Sat  Dinner     2\n234       15.53 3.00    Male    Yes   Sat  Dinner     2\n215       12.90 1.10  Female    Yes   Sat  Dinner     2\n146       18.64 1.36  Female     No  Thur   Lunch     3\n\n\n\ntips.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  244 non-null    float64 \n 1   tip         244 non-null    float64 \n 2   sex         244 non-null    category\n 3   smoker      244 non-null    category\n 4   day         244 non-null    category\n 5   time        244 non-null    category\n 6   size        244 non-null    int64   \ndtypes: category(4), float64(2), int64(1)\nmemory usage: 7.4 KB\n\n\n\ntips.describe()  # numerical type만 나열\n\n       total_bill    tip   size\ncount      244.00 244.00 244.00\nmean        19.79   3.00   2.57\nstd          8.90   1.38   0.95\n...           ...    ...    ...\n50%         17.80   2.90   2.00\n75%         24.13   3.56   3.00\nmax         50.81  10.00   6.00\n\n[8 rows x 3 columns]\n\n\n\ntips.describe(include=\"all\")  # all types 나열\n\n        total_bill    tip   sex smoker  day    time   size\ncount       244.00 244.00   244    244  244     244 244.00\nunique         NaN    NaN     2      2    4       2    NaN\ntop            NaN    NaN  Male     No  Sat  Dinner    NaN\n...            ...    ...   ...    ...  ...     ...    ...\n50%          17.80   2.90   NaN    NaN  NaN     NaN   2.00\n75%          24.13   3.56   NaN    NaN  NaN     NaN   3.00\nmax          50.81  10.00   NaN    NaN  NaN     NaN   6.00\n\n[11 rows x 7 columns]\n\n\n\ntips.describe(include=\"category\")\n\n         sex smoker  day    time\ncount    244    244  244     244\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq     157    151   87     176\n\n\n\ns1 = tips.value_counts(\"day\") # \"day\" 칼럼에 대한 각 카테고리별 counts\ns2 = tips.value_counts(\"day\", sort=False) # default: sort is true\ns3 = tips.value_counts(\"day\", ascending=True) # default: ascending is False\ns4 = tips.value_counts(\"day\", normalize=True) # 카테고리별 비율\ns5 = tips.value_counts([\"sex\", \"smoker\"]) # \"sex\", \"smoker\" 칼럼에 대한 유니크한 카테고리별 counts\n\n\n\n\n\n\n\n\n\nday\nSat     87\nSun     76\nThur    62\nFri     19\nName: count, dtype: int64\n\n\n(a) s1\n\n\n\n\n\n\n\n\nday\nThur    62\nFri     19\nSat     87\nSun     76\nName: count, dtype: int64\n\n\n(b) s2\n\n\n\n\n\n\n\n\n\n\nday\nFri     19\nThur    62\nSun     76\nSat     87\nName: count, dtype: int64\n\n\n(c) s3\n\n\n\n\n\n\n\n\nday\nSat    0.36\nSun    0.31\nThur   0.25\nFri    0.08\nName: proportion, dtype: float64\n\n\n(d) s4\n\n\n\n\n\n\n\n\n\n\nsex     smoker\nMale    No        97\n        Yes       60\nFemale  No        54\n        Yes       33\nName: count, dtype: int64\n\n\n(e) s5\n\n\n\n\n\n\n\nFigure 1: value_count()의 arguments\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n.value_count()의 결과는 Series이며 그 이름은 ‘count’ 또는 ’proportion’임 (pandas 2.0)\nMissing(NA)을 count하지 않으나 dropna=False을 이용해 나타낼 수 있음\ntips.value_counts(\"day\", dropna=False)\nSeries에 대해서도 적용되며, DataFrame으로 컬럼을 선택해 적용할 수 있음\ntips[\"day\"].value_counts()  # tips[\"day\"]: Series object\ntips[[\"sex\", \"smoker\"]].value_counts()\n\n\n\nData: palmerpenguins\n\n# load a dataset\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen           39.10          18.70             181.00   \n1  Adelie  Torgersen           39.50          17.40             186.00   \n2  Adelie  Torgersen           40.30          18.00             195.00   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen           36.70          19.30             193.00   \n\n   body_mass_g     sex  \n0      3750.00    Male  \n1      3800.00  Female  \n2      3250.00  Female  \n3          NaN     NaN  \n4      3450.00  Female  \n\n\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\npenguins.describe(include=\"object\")\n\n\n\n       species  island   sex\ncount      344     344   333\nunique       3       3     2\ntop     Adelie  Biscoe  Male\nfreq       152     168   168\n\n\n\n\npenguins.value_counts([\"island\", \"species\"])\n\nisland     species  \nBiscoe     Gentoo       124\nDream      Chinstrap     68\n           Adelie        56\nTorgersen  Adelie        52\nBiscoe     Adelie        44\nName: count, dtype: int64\n\n\n\npenguins.value_counts([\"sex\", \"species\"], dropna=False) # NA은 기본적으로 생략\n\nsex     species  \nFemale  Adelie       73\nMale    Adelie       73\n        Gentoo       61\n                     ..\n        Chinstrap    34\nNaN     Adelie        6\n        Gentoo        5\nName: count, Length: 8, dtype: int64\n\n\n\ntips.sort_values(\"tip\", ascending=False)\n\n     total_bill   tip     sex smoker  day    time  size\n170       50.81 10.00    Male    Yes  Sat  Dinner     3\n212       48.33  9.00    Male     No  Sat  Dinner     4\n23        39.42  7.58    Male     No  Sat  Dinner     4\n..          ...   ...     ...    ...  ...     ...   ...\n111        7.25  1.00  Female     No  Sat  Dinner     1\n67         3.07  1.00  Female    Yes  Sat  Dinner     1\n92         5.75  1.00  Female    Yes  Fri  Dinner     2\n\n[244 rows x 7 columns]\n\n\n\ntips.sort_values([\"size\", \"tip\"], ascending=[False, True])\n\n     total_bill  tip     sex smoker   day    time  size\n125       29.80 4.20  Female     No  Thur   Lunch     6\n143       27.05 5.00  Female     No  Thur   Lunch     6\n156       48.17 5.00    Male     No   Sun  Dinner     6\n..          ...  ...     ...    ...   ...     ...   ...\n111        7.25 1.00  Female     No   Sat  Dinner     1\n82        10.07 1.83  Female     No  Thur   Lunch     1\n222        8.58 1.92    Male    Yes   Fri   Lunch     1\n\n[244 rows x 7 columns]\n\n\n\ntips.nlargest(3, \"tip\")  # 다수의 동등 순위가 있을 때 처리: keep=\"first\", \"last\", \"all\"\n\n     total_bill   tip   sex smoker  day    time  size\n170       50.81 10.00  Male    Yes  Sat  Dinner     3\n212       48.33  9.00  Male     No  Sat  Dinner     4\n23        39.42  7.58  Male     No  Sat  Dinner     4"
  },
  {
    "objectID": "contents/eda.html",
    "href": "contents/eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n다음 네 가지의 시각화 패키지를 사용해서 그 차이를 확인해 볼 것임.\nMatplotlib 방식\n두 가지 interface를 제공하는데, 혼동을 야기함\npandas/seaborn 방식",
    "crumbs": [
      "Trees",
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/eda.html#위도-경도-값의-활용",
    "href": "contents/eda.html#위도-경도-값의-활용",
    "title": "Exploratory Data Analysis",
    "section": "위도, 경도 값의 활용",
    "text": "위도, 경도 값의 활용\n\n\n\n\n\n\nShow Matplotlib styles\n\n\n\nplt.style.available\n\n\n\n# set the style\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\nlat, lon = housing['latitude'], housing['longitude']\n\n## MATLAB 스타일\n# figure() 함수를 직접 호출\nplt.figure(figsize=(7, 5)) # create a plot figure, figsize는 생략가능\n\n# scatter() 함수를 직접 호출\nplt.scatter(x=lon, y=lat, label=None, edgecolors=\"w\", linewidths=.4, alpha=0.3)\n\n# set the labels\nplt.xlabel('longitude')\nplt.ylabel('latitude')\nplt.axis('equal') # set the aspect of the plot to be equal\n\nplt.show()\n\n\n\n\n\n\n\n\n\n## 객체 방식\n# figure, axes라는 객체를 생성 후 메서드를 호출\nfig, ax = plt.subplots(figsize=(7, 5)) \n\n# ax의 메서드인 .scatter로 그래프를 그림\nax.scatter(x=lon, y=lat, label=None, edgecolors=\"w\", linewidths=.4, alpha=0.3)\n\n# ax의 메서드인 .set_xlabel, .set_ylabel로 라벨을 지정\nax.set_xlabel('longitude')\nax.set_ylabel('latitude')\nax.axis('equal')  # set the aspect of the plot to be equal\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# pandas의 plot 메서드를 사용하는 방식\nhousing.plot.scatter(x=\"longitude\", y=\"latitude\", alpha=0.3)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n# 다음과 동일함\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.3)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n\n\n\npandas가 제공하는 plots\n\n‘line’ : line plot (default)\n‘bar’ : vertical bar plot\n‘barh’ : horizontal bar plot\n‘hist’ : histogram\n‘box’ : boxplot\n‘kde’ : Kernel Density Estimation plot\n‘density’ : same as ‘kde’\n‘area’ : area plot\n‘pie’ : pie plot\n‘scatter’ : scatter plot (DataFrame only)\n‘hexbin’ : hexbin plot (DataFrame only)\n\n\n# NEAR OCEAN에 해당하는 부분만 시각화\nhousing2 = housing.query('ocean_proximity == \"NEAR OCEAN\"')\n\nhousing2.plot.scatter(x=\"longitude\", y=\"latitude\", alpha=0.3, figsize=(7, 5))\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n\n\n# Seaborn을 사용하는 방식\nplt.figure(figsize=(7, 5))\nsns.scatterplot(housing, x=\"longitude\", y=\"latitude\", hue=\"ocean_proximity\", alpha=0.5)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n\n\n\n  The San Francisco Bay Area\n\n\n\n\n집값과의 관계를 보기 위해, 집값을 컬러에 매핑하면,\n\nhousing.plot.scatter(\n    x=\"longitude\",\n    y=\"latitude\",\n    s=housing[\"population\"] / 100,  # point size\n    c=\"median_house_value\",  # color\n    alpha=0.3,  # transparency\n    cmap=\"flare\",  # color map\n    figsize=(7, 5),\n)\n\nplt.axis('equal') # set the aspect of the plot to be equal\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Annotation 추가\n\n\n\n\n\n아래 코드를 추가하여 도시 이름을 표시\npath = \"https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks_v1/data/california_cities.csv\"\ncities = pd.read_csv(path)\n\npopular_cities = cities.query('population_total &gt; 400000')\nlat, lon, names = popular_cities['latd'], popular_cities['longd'], popular_cities[\"city\"]\n\nplt.scatter(lon, lat, c=\"w\", alpha=1)\nfor name, lat, lon in zip(names, lat, lon):\n    plt.annotate(name, (lon, lat), xytext=(5, 5), textcoords=\"offset points\", color=\"k\")\n\n\n\n\n\n\n\n\n\nColor 사용에 관한 체계적 가이드\n\n\n\nChoosing color pallettes from Seaborn website",
    "crumbs": [
      "Trees",
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/eda.html#데이터의-분포",
    "href": "contents/eda.html#데이터의-분포",
    "title": "Exploratory Data Analysis",
    "section": "데이터의 분포",
    "text": "데이터의 분포\nHistogram, density plot, boxplot\n\n# pandas의 DataFrame 메서드인 hist()를 사용\nhousing.hist(bins=50, figsize=(9, 6))\nplt.show()\n\n\n\n\n\n\n\n\n\n# density plot\nhousing.plot.density(bw_method=0.2, subplots=True, layout=(3, 3), sharex=False, sharey=False, figsize=(9, 6))\nplt.show()\n\n\n\n\n\n\n\n\n\n# Using matplotlib\nfig, ax = plt.subplots(3, 3, figsize=(9, 6))\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n\nfor i in range(3):\n    for j in range(3):\n        ax[i, j].hist(housing.iloc[:, i * 3 + j], bins=30)\n        ax[i, j].set_title(housing.columns[i * 3 + j])        \n\n\n\n\n\n\n\n\n\n# 한 변수의 각 레벨/카테고리별로 그리기, using pandas\nhousing.plot.hist(column=[\"median_house_value\"], by=\"ocean_proximity\", sharey=False, sharex=True, figsize=(6, 8), bins=50)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\n\n\n\n\n\n\nsource: R for Data Science\n\n\n\n\n\n# Using pandas\nhousing.plot.box(column=\"median_house_value\", by=\"ocean_proximity\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Using seaborn\nplt.figure(figsize=(9, 5))\nsns.boxplot(housing, x=\"ocean_proximity\", y=\"median_house_value\", hue=\"median_age_cat\", fill=False, gap=.2)\nplt.show()",
    "crumbs": [
      "Trees",
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/eda.html#두-연속-변수간의-관계",
    "href": "contents/eda.html#두-연속-변수간의-관계",
    "title": "Exploratory Data Analysis",
    "section": "두 연속 변수간의 관계",
    "text": "두 연속 변수간의 관계\n\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"] / housing[\"households\"]\nhousing[\"bedrooms_per_household\"] = housing[\"total_bedrooms\"] / housing[\"households\"]\nhousing[\"people_per_household\"] = housing[\"population\"] / housing[\"households\"]\n\n또는 assign()를 사용\n\nhousing.assign(\n    rooms_per_household = lambda x: x[\"total_rooms\"] / x[\"households\"],\n    bedrooms_per_household = lambda x: x[\"total_bedrooms\"] / x[\"households\"],\n    people_per_household = lambda x: x[\"population\"] / x[\"households\"]\n).head(1)\n\n   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88               41.00       880.00          129.00   \n\n   population  households  median_income  median_house_value ocean_proximity  \\\n0      322.00      126.00           8.33           452600.00        NEAR BAY   \n\n  median_age_cat  rooms_per_household  bedrooms_per_household  \\\n0          40-52                 6.98                    1.02   \n\n   people_per_household  \n0                  2.56  \n\n\n\nhousing.value_counts(\"median_house_value\").sort_index()\n\nmedian_house_value\n14999.00       4\n17500.00       1\n22500.00       4\n            ... \n499100.00      1\n500000.00     27\n500001.00    965\nName: count, Length: 3842, dtype: int64\n\n\n\n# median_house_value &lt; 500001 값으로 필터링\nhousing = housing.query('median_house_value &lt; 500001')\n\n\nxvar = \"rooms_per_household\"\nyvar = \"median_house_value\"\n\n# matplotlib의 객체 방식\nfig, ax = plt.subplots()\nhousing.plot.scatter(ax=ax, x=xvar, y=yvar, alpha=0.1, figsize=(7, 5))\n\n# fitted line of natural spline: 아래 노트 참고\nnspline_fit = nspline(housing, xvar, yvar, df_n=15).sort_values(xvar)\nnspline_fit.plot.line(ax=ax, x=xvar, y=yvar, c=\".3\", figsize=(7, 5))\n\nplt.xlim(0, 10)\nplt.ylim(0, 500000)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNatural spline fit\n\n\n\n\n\ndef nspline(df, x, y, df_n=5):\n    from statsmodels.formula.api import ols\n\n    df = df[[x, y]].dropna()\n    formula = f\"{y} ~ cr({x}, df={df_n})\"\n    df[y] = ols(formula, data=df).fit().fittedvalues\n\n    return df\n\n\n\n해변에 가까운 정도(ocean_proximity) 따라 나누어 보면,\n\n# divide plots by ocean_proximity\nfig, ax = plt.subplots(1, 4, figsize=(12, 3))\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n\ntypes = ['NEAR OCEAN', '&lt;1H OCEAN', 'NEAR BAY', 'INLAND']\nfor i, op in enumerate(types):\n\n    df = housing.query(f'ocean_proximity == \"{op}\"')\n    df.plot.scatter(ax=ax[i], x=xvar, y=yvar, alpha=0.1)\n\n    # fitted line of natural spline\n    nspline_fit = nspline(df, xvar, yvar, df_n=15).sort_values(xvar)\n    nspline_fit.plot.line(ax=ax[i], x=xvar, y=yvar, c=\".3\")\n    \n    ax[i].set_title(op)\n    ax[i].set_xlim(1, 12)\n    ax[i].set_ylim(0, 500000)\n\nplt.show()\n\n\n\n\n\n\n\n\nseaborn.object 방식\n\n(\n    so.Plot(housing, x='rooms_per_household', y='median_house_value')\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet('ocean_proximity')\n    .limit(x=(1, 12), y=(0, 500000))\n    .layout(size=(8.9, 3))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n범주형 변수의 순서 할당\n\n\n\npd.Categorical을 사용하여 범주형 변수의 순서를 지정할 수 있음.\nhousing[\"ocean_proximity\"] = pd.Categorical(\n    housing[\"ocean_proximity\"],\n    categories=[\"NEAR BAY\", \"NEAR OCEAN\", \"&lt;1H OCEAN\", \"INLAND\"],\n    ordered=True,\n)\n\n\n해변에 가까운 정도(ocean_proximity)와 집의 연령(median_age_cat)에 따라 나누어 보면,\n\n(\n    so.Plot(\n        housing.query('ocean_proximity != \"ISLAND\"'),\n        x=\"rooms_per_household\",\n        y=\"median_house_value\",\n    )\n    .add(so.Dots(alpha=0.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet(col=\"ocean_proximity\", row=\"median_age_cat\")\n    .limit(x=(1, 12), y=(0, 500000))\n    .layout(size=(8, 8))\n)\n\n\n\n\n\n\n\n\n해안에 가까운 정도(ocean_proximity)가 고정되어 있을 때, 그 안에서 여전히\n경도(longitude)가 작을수록, 집값(median_house_value)이 변화하는지 살펴보면,\n\n(\n    so.Plot(\n        housing.query('ocean_proximity != \"ISLAND\"'),\n        x='longitude',\n        y='median_house_value')\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet(\"ocean_proximity\")\n    .layout(size=(8.9, 3))\n    .share(x=False)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nPopulation과의 관계가 있을까?\n\n\nShow the code\nhousing2 = housing.copy()\nhousing2[\"ocean_proximity\"] = (\n    housing\n    .query('ocean_proximity not in [\"ISLAND\", \"INLAND\"]')[\"ocean_proximity\"]\n    .cat.remove_unused_categories()\n)\n\n(\n    so.Plot(housing2, x='longitude', y='population')\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".3\"), so.PolyFit(5))  # polynomial fit of degree 5\n    .facet(\"ocean_proximity\")\n    .layout(size=(8.9, 3))\n    .share(x=False)\n    .limit(y=(0, 3000))\n)\n\n\n\n\n\n\n\n\n\nPanelized spline fit: pyGAM 참고",
    "crumbs": [
      "Trees",
      "Exploratory Data Analysis",
      "Explore"
    ]
  },
  {
    "objectID": "contents/knn.html",
    "href": "contents/knn.html",
    "title": "K-Nearest Neighbors",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "contents/knn.html#k-nearest-neighbors-in-classification",
    "href": "contents/knn.html#k-nearest-neighbors-in-classification",
    "title": "K-Nearest Neighbors",
    "section": "K-Nearest Neighbors in Classification",
    "text": "K-Nearest Neighbors in Classification\n앞서 분류(classification) 문제의 경우에 Bayes classifier를 정의했는데,\n\\(f(x): P(Y = C_k|X=x)\\)가 최대인 클래스에 할당; \\(\\underset{k}{\\mathrm{argmax}}~ P(Y = C_k|X=x)\\)\n간단히 말해, 어떤 관측치 \\(x\\)에 대해서, 포함될 가능성이 가장 높은 클래스에 할당하는 것임\n(조건부 확률분포 \\(P(Y|X=x)\\)에서 가장 높은 확률을 가지는 클래스에 할당)\n이를 위해서 \\(P(Y|X)\\)를 추정하는 가장 단순하며 직관적인 방법으로 K-Nearest Neighbors(K-최근접 이웃)가 있음\n대표적인 non-parametric 방법론임\n\n주어진 (test) 관측치 \\(x\\)에 대해서 가장 가까운 K개의 (training) 값을 찾음\n이 K개의 값들이 속한 클래스을 파악하면,\n각 클래스에 속할 비율, 즉 확률을 추정할 수 있음\n\n클래스 \\(C_j\\)에 대해서,\n\\(\\displaystyle P(Y = C_j|X=x) = \\frac{1}{K} \\sum_{i \\in N_k} I(y_i = C_j)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n오렌지 배경: 오렌지 클래스에 할당된 평면 안의 값들,\n\n파란 배경: 파란 클래스에 할당된 평면 안의 값들\n\n\n\n\n다음과 같이 각 클래스에 속할 확률을 설정하여, 클래스 별로 100개씩의 데이터를 추출한 예를 보면 (simulated data),\nBayes classifier가 되는 즉, \\(P(Y=orange|X) = P(Y=blue|X)\\)인 \\(X\\)을 찾을 수 있음 (=\\(\\frac{1}{2}\\))\n이를 Bayes decision boundary라고 함. (보라색 점선)\n\n이 때, KNN은 Bayes decision boundary를 추정하는데 사용될 수 있음.\n\nK가 증가함에 따라 flexibiliy가 떨어지고 선형에 가까운 결정 경계를 생성\nK = 100의 경우 variance는 낮지만 bias가 높은 classifier에 해당\nK = 1, K = 100에 대한 test error rate은 각각 0.1695, 0.1925\n\n\nK의 값이 bias-variance trade-off를 결정함으로, 과적합이 되지 않도록 cross-validation과 같은 방법을 통해 최적의 K를 찾아야 함\n\n\nSource: pp. 38-39, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "contents/knn.html#k-nearest-neighbors-regression",
    "href": "contents/knn.html#k-nearest-neighbors-regression",
    "title": "K-Nearest Neighbors",
    "section": "K-Nearest Neighbors regression",
    "text": "K-Nearest Neighbors regression\nKNN의 추정 방식은 자연스럽게 회귀(regression) 문제에 적용할 수 있음\n\n주어진 (test) 관측치 \\(x\\)에 대해서 가장 가까운 K개의 (training) 값을 찾음; 집합 \\(N\\)으로 표현\n이 K개의 값들의 평균을 계산하여, 예측값을 구함\n\n\\(f(x) = E(Y|X=x)\\)의 추정치로서 \\(E(Y|x_i \\in N)\\)를 사용\n\n\n식으로 표현하면,\n\\(\\displaystyle \\hat f(x) = \\frac{1}{K} \\sum_{x_i \\in N} y_i\\)\n\n\nSource: p. 112, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\nLinear regression과의 비교\nX와 Y가 선형관계를 가질 때 (N = 50),\n\n\n\nSource: p. 113, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\nX와 Y가 선형관계에서 벗어날 때 (N = 50),\n\n\n\nSource: p. 114, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n\n실제 X, Y의 관계가 비선형적일 수록 KNN이 더 나은 예측을 할 수 있음\n하지만, 변수의 수가 증가하면 테스트 관측치 \\(x_0\\)에서 가까운 관측치 수가 기하급수적으로 줄어\n\\(x_0\\)에 가장 가까운 K 관측치는 매우 멀리 떨어져 있어, 그 평균값들인 예측값의 정확도가 떨어질 수 있음; the curse of dimensionality\n\n\n\nSource: p. 25, The Elements of Statistical Learning (2e) by Hastie, T., Tibshirani, R., & Friedman, J.\n\n특히, Y와 관련이 적은 예측변수들이 많을 경우에는 더욱 그러함\n아래 그림은 Y와 관련이 별로 없는 변수들이 늘어날 때, KNN과 선형회귀의 성능의 변화를 보여줌 (N = 50);\nKNN의 성능이 빠르게 떨어짐\n\n\nSource: p. 115, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n\n일반적으로, parametric 방법이 예측변수 개수 대비 관측치가 적을 때 non-parametric 보다 성능이 우수함.\nKNN의 경우 해석가능하지 않기 때문에, 예측변수가 적을 때에도 선형회귀를 사용하는 것이 더 나을 수 있음",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "contents/knn.html#regression",
    "href": "contents/knn.html#regression",
    "title": "K-Nearest Neighbors",
    "section": "Regression",
    "text": "Regression\nMajor League Baseball Data from the 1986 and 1987 seasons\n\nfrom ISLP import load_data\n\nHitters = load_data('Hitters') # from ISLP\nHitters = Hitters.dropna()\nHitters.head(3)\n\n   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n1    315    81      7    24   38     39     14    3449    835      69    321   \n2    479   130     18    66   72     76      3    1624    457      63    224   \n3    496   141     20    65   78     37     11    5628   1575     225    828   \n\n   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n1   414     375      N        W      632       43      10  475.00         N  \n2   266     263      A        W      880       82      14  480.00         A  \n3   838     354      N        E      200       11       3  500.00         N  \n\n\n\n# suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nX = Hitters.drop('Salary', axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = Hitters['Salary']\n\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, \ntest_size=0.5, random_state=1)\n\n# 모든 변수들로부터 거리를 적절히 계산하려면 단위를 일치해야 함\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nK=3일 때의 KNN regression\n\nfrom sklearn.neighbors import KNeighborsRegressor\nK = 3\nknn = KNeighborsRegressor(n_neighbors=K)\nknn_pred = knn.fit(X_train_scaled, y_train).predict(X_test_scaled)\n\nprint(\n    f\"RMSE: {root_mean_squared_error(y_test, knn_pred):.2f}\\n\"\n    f\"R2: {r2_score(y_test, knn_pred):.2f}\"\n)\n\nRMSE: 315.50\nR2: 0.42\n\n\n여러 K값에 대한 grid search를 통해 최적의 K를 찾음\n10-fold cross-validation을 통해 test error rate을 계산\n\nkfold = skm.KFold(n_splits=10, shuffle=True, random_state=1)\nknn = KNeighborsRegressor()\n\nK = np.arange(1, 51)\nparam_grid = {\"n_neighbors\": K}\n\ngrid = skm.GridSearchCV(knn, param_grid, cv=kfold, scoring=\"neg_mean_squared_error\")\ngrid.fit(X_train_scaled, y_train)\n\nGridSearchCV(cv=KFold(n_splits=10, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=10, random_state=1, shuffle=True),\n             estimator=KNeighborsRegressor(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='neg_mean_squared_error') best_estimator_: KNeighborsRegressorKNeighborsRegressor(n_neighbors=np.int64(8))  KNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor(n_neighbors=np.int64(8)) \n\n\n\ngrid.best_params_\n\n{'n_neighbors': np.int64(8)}\n\n\n각 K에 대한 test error rate을 시각화\n\nplt.figure(figsize=(6, 4.5), dpi=70)\nplt.errorbar(\n    1/K,\n    -grid.cv_results_[\"mean_test_score\"],\n    yerr=grid.cv_results_[\"std_test_score\"] / np.sqrt(10),\n)\nplt.axvline(1/grid.best_params_['n_neighbors'], color=\".5\", linestyle=\":\")\nplt.xlabel(\"1/K\")\nplt.ylabel(\"Cross-validated MSE\")\nplt.show()\n\n\n\n\n\n\n\n\nK=8에 대한 test error\n\nK = grid.best_params_['n_neighbors']\nknn = KNeighborsRegressor(n_neighbors=K)\nknn_pred = knn.fit(X_train_scaled, y_train).predict(X_test_scaled)\n\nprint(\n    f\"RMSE: {root_mean_squared_error(y_test, knn_pred):.2f}\\n\"\n    f\"R2: {r2_score(y_test, knn_pred):.2f}\"\n)\n\nRMSE: 318.17\nR2: 0.41\n\n\n\n\n\n\n\n\nPlot validation curves\n\n\n\n\n\nvalidation_curve()를 사용하여, traing score와 validation score를 간단히 그릴 수 있음\ntrain_scores, val_scores = skm.validation_curve(knn, X_train_scaled, y_train, param_name='n_neighbors', param_range=K, cv=kfold, scoring='neg_mean_squared_error')  # default: r2\n\nplt.plot(K, -train_scores.mean(1), label='training score')\nplt.plot(K, -val_scores.mean(1), label='validation score')\nplt.legend(frameon=False)\nplt.xlabel('K')\nplt.ylabel('Cross-validated MSE')",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "contents/knn.html#classification",
    "href": "contents/knn.html#classification",
    "title": "K-Nearest Neighbors",
    "section": "Classification",
    "text": "Classification\n\nPalmer Archipelago penguin data\n앞서 살펴봤던 Palmer Penguins의 데이터를 이용\n부리의 길이(bill_length)와 깊이(bill_depth)로부터 종(species)을 분류하는 문제\n\npenguins = sns.load_dataset('penguins')\npenguins.head(3)\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen           39.10          18.70             181.00   \n1  Adelie  Torgersen           39.50          17.40             186.00   \n2  Adelie  Torgersen           40.30          18.00             195.00   \n\n   body_mass_g     sex  \n0      3750.00    Male  \n1      3800.00  Female  \n2      3250.00  Female  \n\n\n\nShow the code\npenguins = sns.load_dataset('penguins')\n\nsns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"body_mass_g\", hue=\"species\",\n    height=4, aspect=1.2, palette=colors[:3]\n)\n\nsns.displot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"body_mass_g\", hue=\"species\",\n    kind=\"kde\", height=4, aspect=1.2, palette=colors[:3]\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\npenguins = sns.load_dataset('penguins')\npenguins = penguins[['species', 'bill_length_mm', 'body_mass_g']].dropna()\n\nX = penguins[['bill_length_mm', 'body_mass_g']]\ny = penguins['species']\n\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.25, random_state=1)\n\n# 모든 변수들로부터 거리를 적절히 계산하려면 단위를 일치해야 함\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn3 = KNeighborsClassifier(n_neighbors=3)\nknn3.fit(X_train_scaled, y_train)\nknn3_pred = knn3.predict(X_test_scaled)\n\nprint(\n    f\"Accuracy: {knn3.score(X_test_scaled, y_test):.2f}\\n\"\n)\n\nAccuracy: 0.94\n\n\n\n\nfrom ISLP import confusion_table\nconfusion_table(knn3_pred, y_test)\n\nTruth      Adelie  Chinstrap  Gentoo\nPredicted                           \nAdelie         35          0       0\nChinstrap       0         11       0\nGentoo          3          2      35\n\n\n\nkfold = skm.KFold(n_splits=10, shuffle=True, random_state=1)\nknn = KNeighborsClassifier()\n\nK = np.arange(1, 51)\nparam_grid = {\"n_neighbors\": K}\n\ngrid = skm.GridSearchCV(knn, param_grid, cv=kfold, scoring=\"accuracy\")\ngrid.fit(X_train_scaled, y_train)\n\nGridSearchCV(cv=KFold(n_splits=10, random_state=1, shuffle=True),\n             estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=10, random_state=1, shuffle=True),\n             estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='accuracy') best_estimator_: KNeighborsClassifierKNeighborsClassifier(n_neighbors=np.int64(15))  KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_neighbors=np.int64(15)) \n\n\n\ngrid.best_params_\n\n{'n_neighbors': np.int64(15)}\n\n\n\ngrid.best_estimator_.fit(X_train_scaled, y_train)\nknn_pred = grid.best_estimator_.predict(X_test_scaled)\n\nconfusion_table(knn_pred, y_test)\n\nTruth      Adelie  Chinstrap  Gentoo\nPredicted                           \nAdelie         36          1       0\nChinstrap       0         10       0\nGentoo          2          2      35\n\n\nK = np.arange(1, 51)\n\ntrain_scores, val_scores = skm.validation_curve(knn, X_train_scaled, y_train, param_name='n_neighbors', param_range=K, cv=kfold, scoring='accuracy')\n\nplt.plot(1/K, train_scores.mean(1), label='training score')\nplt.plot(1/K, val_scores.mean(1), label='validation score')\nplt.legend(frameon=False)\nplt.xlabel('1/K')\nplt.ylabel('Cross-validated accuracy')\nplt.show()\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, knn_pred))\n\n              precision    recall  f1-score   support\n\n      Adelie       0.97      0.95      0.96        38\n   Chinstrap       1.00      0.77      0.87        13\n      Gentoo       0.90      1.00      0.95        35\n\n    accuracy                           0.94        86\n   macro avg       0.96      0.91      0.93        86\nweighted avg       0.95      0.94      0.94        86\n\n\n\n\n\nWine recognition dataset\n\nfrom sklearn import datasets\nwine = datasets.load_wine(as_frame=True)\nprint(wine.DESCR)\n\n\nwine_df = wine['frame']\nwine_df['target'] = wine_df['target'].map({0: 'C0', 1: 'C1', 2: 'C3'})\nwine_df.head(3)\n\n   alcohol  malic_acid  ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0    14.23        1.71 2.43              15.60     127.00           2.80   \n1    13.20        1.78 2.14              11.20     100.00           2.65   \n2    13.16        2.36 2.67              18.60     101.00           2.80   \n\n   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity  hue  \\\n0        3.06                  0.28             2.29             5.64 1.04   \n1        2.76                  0.26             1.28             4.38 1.05   \n2        3.24                  0.30             2.81             5.68 1.03   \n\n   od280/od315_of_diluted_wines  proline target  \n0                          3.92  1065.00     C0  \n1                          3.40  1050.00     C0  \n2                          3.17  1185.00     C0  \n\n\n\nShow the code\nsns.relplot(\n    data=wine_df,\n    x=\"color_intensity\", y=\"alcohol\", hue=\"target\",\n    height=4, aspect=1.2, palette=colors[:3]\n)\n\nsns.displot(\n    data=wine_df,\n    x=\"color_intensity\", y=\"alcohol\", hue=\"target\",\n    kind=\"kde\", height=4, aspect=1.2, palette=colors[:3]\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nX = wine_df[['color_intensity', 'alcohol']]\ny = wine_df['target']\n\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.25, random_state=1)\n\n# 모든 변수들로부터 거리를 적절히 계산하려면 단위를 일치해야 함\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn3 = KNeighborsClassifier(n_neighbors=3)\nknn3.fit(X_train_scaled, y_train)\nknn3_pred = knn3.predict(X_test_scaled)\n\nprint(\n    f\"Accuracy: {knn3.score(X_test_scaled, y_test):.2f}\\n\"\n)\n\nAccuracy: 0.87\n\n\n\n\nfrom ISLP import confusion_table\nconfusion_table(knn3_pred, y_test)\n\nTruth      C0  C1  C3\nPredicted            \nC0         17   1   4\nC1          0  16   0\nC3          1   0   6\n\n\n\nkfold = skm.KFold(n_splits=10, shuffle=True, random_state=0)\nknn = KNeighborsClassifier()\n\nK = np.arange(1, 51)\nparam_grid = {\"n_neighbors\": K}\n\ngrid = skm.GridSearchCV(knn, param_grid, cv=kfold, scoring=\"accuracy\")\ngrid.fit(X_train_scaled, y_train)\n\nGridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])},\n             scoring='accuracy') best_estimator_: KNeighborsClassifierKNeighborsClassifier(n_neighbors=np.int64(11))  KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_neighbors=np.int64(11)) \n\n\n\ngrid.best_params_\n\n{'n_neighbors': np.int64(11)}\n\n\n\ngrid.best_estimator_.fit(X_train_scaled, y_train)\nknn_pred = grid.best_estimator_.predict(X_test_scaled)\n\nconfusion_table(knn_pred, y_test)\n\nTruth      C0  C1  C3\nPredicted            \nC0         18   2   4\nC1          0  15   0\nC3          0   0   6\n\n\nK = np.arange(1, 51)\n\ntrain_scores, val_scores = skm.validation_curve(knn, X_train_scaled, y_train, param_name='n_neighbors', param_range=K, cv=kfold, scoring='accuracy')\n\nplt.plot(1/K, train_scores.mean(1), label='training score')\nplt.plot(1/K, val_scores.mean(1), label='validation score')\nplt.legend(frameon=False)\nplt.xlabel('1/K')\nplt.ylabel('Cross-validated accuracy')\nplt.show()\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, knn_pred))\n\n              precision    recall  f1-score   support\n\n          C0       0.75      1.00      0.86        18\n          C1       1.00      0.88      0.94        17\n          C3       1.00      0.60      0.75        10\n\n    accuracy                           0.87        45\n   macro avg       0.92      0.83      0.85        45\nweighted avg       0.90      0.87      0.86        45",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "contents/logistic.html",
    "href": "contents/logistic.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Trees",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#emperical-probability-odds",
    "href": "contents/logistic.html#emperical-probability-odds",
    "title": "Logistic Regression",
    "section": "Emperical probability/ Odds",
    "text": "Emperical probability/ Odds\n이제 y를 직접 예측하기보다, 확률을 예측하는 방식을 취하면,\n\n각 두께를 가진 나무들의 개수 (m)와\n그 중에 태풍으로 죽은 나무의 수 (died)를 고려하면,\n나무의 두께에 따라 죽은 나무 수의 비율 (p= died/m)을 계산할 수 있음. 이를 emperical probability라고 함.\n사실, 이 p는 binary response (0, 1)의 conditional mean(평균)인데,\n통계적으로 표현하면 \\(E(Y|d=d_i)\\)이며 선형모형의 mean function를 제공.\n\n\nblowbs_bn = blowbs.groupby(\"d\")[\"y\"].agg([(\"died\", \"sum\"), (\"m\", \"count\"), (\"p\", \"mean\")]).reset_index()\nblowbs_bn\n\n       d  died   m    p\n0   5.00     7  90 0.08\n1   6.00     7  92 0.08\n2   7.00    18  91 0.20\n..   ...   ...  ..  ...\n22 28.00     2   2 1.00\n23 29.00     1   2 0.50\n24 32.00     1   1 1.00\n\n[25 rows x 4 columns]\n\n\n\ncode\n(\n    so.Plot(blowbs_bn, x='d', y='p')\n    .add(so.Dot(), pointsize='m')\n    .add(so.Dots(alpha=.3, color=\".6\"), so.Jitter(y=.1), x=blowbs.d, y=blowbs.y)\n    # .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(color=\"#87bc45\"), so.PolyFit(1))\n    .limit(y=(-0.2, 1.2))\n    .layout(size=(8, 5))\n    .label(title='Emperical Probability', y = 'Proportion of Died', x='Diameter (cm)')\n)\n\n\n\n\n\n\n\n\n\nOLS estimate으로도 충분한가?\n1차보다는 고차 다항함수로 fit한다면?\n\n우선 d를 log2 변환해서 살펴보면,\n\ncode\nblowbs_bn[\"log2d\"] = np.log2(blowbs_bn[\"d\"])\n(\n    so.Plot(blowbs_bn, x='log2d', y='p')\n    .add(so.Dot(), pointsize='m')\n    .add(so.Dots(alpha=.3, color=\".6\"), so.Jitter(y=.1), x=blowbs.log2d, y=blowbs.y)\n    .add(so.Line(color=\"#87bc45\"), so.PolyFit(1))\n    .limit(y=(-0.2, 1.2))\n    .layout(size=(8, 5))\n    .label(title='Emperical Probability', y = 'Proportion of Died', x='Log of Diameter (cm)')\n)\n\n\n\n\n\n\n\n\n위에서 살펴본 OLS의 문제들 즉,\n\n잔차에 패턴이 보인다는 것은 충분히 좋은 모형이 아니라는 것을 의미하고,\n예측값이 확률을 의미하지 못할 수 있음.\n잔차의 분산이 x값에 따라 달라져 모집단에 대한 추론을 어렵게 함.\n\n이런 문제들을 해결하고 예측값이 분명한 “확률”의 의미를 품도록 여러 방식이 제시되는데 주로 사용되는 것이 logistic regression임.\n\n\n\n\n\n\nImportant\n\n\n\nBinary outcome을 예측하는 모형은 binary 값을 예측하는 것이 아니고, 확률 값을 예측하는 것임.\n이후에 이를 이용해 binary outcome을 예측.\n\n예를 들어, 두께가 5cm인 (특정 종의) 나무가 태풍에 쓰러질 확률(true probability)을 파악하고자 함.\n이 때, 관측값은 5cm인 나무 중 쓰러진 나무의 “비율”이고, 이 관측치들로부터 true probability를 추정하고자 함.\n\nOdds의 정의: 실패할 확률 대비 성공할 확률의 비율\n\\(\\displaystyle odds = \\frac{p}{1-p}\\)\n예를 들어, 5cm 두께의 나무는 90그루 중 7그루가 죽었으므로 83그루는 살았음.\n즉, 죽음:생존 = 7:83 \\(\\approx\\) 1:12 이고 odds = 7/83 = 0.084; 생존할 가능성 대비 죽을 가능성이 8.4%임.\n확률로 표현하면, \\(odds = \\frac{\\frac{7}{90}}{1 - \\frac{7}{90}} = \\frac{7}{90-7} = \\frac{7}{83}\\)\n확률과 odds, logit(log odds)의 관계\n\n\n\n\nblowbs_bn = blowbs_bn.assign(odds = lambda x: x.p / (1 - x.p))\n# p = 1인 경우 odds가 무한대가 되므로 편의상 inf 값을 50으로 대체\nblowbs_bn[\"odds\"] = blowbs_bn[\"odds\"].apply(lambda x: 50 if x == np.inf else x)\nblowbs_bn\n\n       d  died   m    p  log2d  odds\n0   5.00     7  90 0.08   2.32  0.08\n1   6.00     7  92 0.08   2.58  0.08\n2   7.00    18  91 0.20   2.81  0.25\n..   ...   ...  ..  ...    ...   ...\n22 28.00     2   2 1.00   4.81 50.00\n23 29.00     1   2 0.50   4.86  1.00\n24 32.00     1   1 1.00   5.00 50.00\n\n[25 rows x 6 columns]\n\n\n이 odds를 선형모형으로 나무두께로 예측하려고 하는 것인데, 예를 들어,\n\\(\\widehat{odds} = b_0 + b_1 \\cdot log_{2}(d)\\)\nodds 값의 범위는 (0, \\(\\infty\\))이므로, 선형모형으로 fit하는 것이 적절하지 않음.\n한편, odds를 log 변환하면, 그 범위는 (\\(-\\infty\\), \\(\\infty\\))가 되어 선형모형으로 fit하는 것이 적절해짐.\n이 때, 예측모형은 log odds가 \\(x\\)에 대해 선형적으로 연결된다고 가정하는 것임. 즉,\n\\(\\displaystyle log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right) = b_0 + b_1 \\cdot log_{2}(d)\\)\n이를 logit 함수로 간단히 표현하면; 전통적 통계에서 선호\n\\(\\displaystyle logit(\\hat p) = b_0 + b_1 \\cdot log_{2}(d)\\),   \\(\\displaystyle logit(x) := log\\left(\\frac{x}{1-x}\\right)\\)\nlogit의 역함수인 sigmoid 함수로 표현하면; machine learning에서 선호\n\\(\\displaystyle \\hat p = \\sigma(b_0 + b_1 \\cdot log_{2}(d))\\),   \\(\\displaystyle \\sigma(x) := \\frac{1}{1 + e^{-x}}\\)\n이를 확률로 표현하면,\n\\(\\displaystyle P(Y=1|X=x) = E(Y|X=x) = \\sigma(b_0 + b_1 \\cdot log_{2}(d))\\)\n즉, logit은 예측변수 \\(x\\) 와 선형적으로 연결되는데 반해, 확률 \\(p\\) 는 예측변수 \\(x\\) 와 비선형적 관계를 맺음\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMean function \\(E(Y|X)\\)을 변형하는 방식으로 표현할 때,\n\\(f(E(Y|X))=\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_n X_n\\)\n\n이 때, \\(f\\)를 link function이라고 하고,\n여기서는 logit 함수를 link function으로 사용함.\n\nMachine learning에서는, 그 역함수를 이용해 선형함수 쪽을 변형해 다음과 같이 표현하는데,\n\\(E(Y|X)=f(\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_n X_n)\\)\n\n이 때, \\(f\\)를 activation function이라고 부름.\n\n여기서는 sigmoid 함수를 activation function으로 사용함.\n\nsigmoid 함수는 logisitic 함수라고도 부름.\n\n\n한편, 각 클래스 내에서 \\(X\\)가 multivariate Gaussian 분포를 따르고, 클래스 간에 covariance matrix가 동일하다고 가정하면,\nlog odds가 \\(X\\)에 대해 선형적으로 연결된다는 것을 보일 수 있음.\n\n여기서는 쓰러진 나무와 산 나무의 두께가 Gaussian 분포를 따르고,\n그 두 분포의 분산이 동일하다면, log odds가 \\(X\\)에 대해 선형적 연결될 수 있음.\n나무의 두께(d)를 log2 변환한 이유임 (오른쪽 그림)\n\n\ncode\nblowbs[\"label\"] = blowbs[\"y\"].map({0: \"alive\", 1: \"died\"})\n(\n    so.Plot(blowbs, x='d', color='label')\n    .add(so.Area(), so.KDE(common_norm=False))\n    #.scale(color=so.Nominal())\n    .scale(color=['#70e20c', '#7d2be2'])\n    .layout(size=(8, 5))\n    .label(x=\"Diameter (cm)\", y=\"Density\", color=\"Label\")\n).show()\n\n(\n    so.Plot(blowbs, x='log2d', color='label')\n    .add(so.Area(), so.KDE(common_norm=False))\n    .scale(color=['#70e20c', '#7d2be2'])\n    .layout(size=(8, 5))\n    .label(x=\"Log of Diameter (cm)\", y=\"Density\", color=\"Label\")\n).show()\n\n\n\n\n\n\n\n\n\n\n\n이제, log odds을 구해, \\(X\\)에 대해 선형적인 패턴이 있는지 확인해보면,\n\nblowbs_bn = blowbs_bn.assign(log_odds = lambda x: np.log(x.odds))\nblowbs_bn   \n\n       d  died   m    p  log2d  odds  log_odds\n0   5.00     7  90 0.08   2.32  0.08     -2.47\n1   6.00     7  92 0.08   2.58  0.08     -2.50\n2   7.00    18  91 0.20   2.81  0.25     -1.40\n..   ...   ...  ..  ...    ...   ...       ...\n22 28.00     2   2 1.00   4.81 50.00      3.91\n23 29.00     1   2 0.50   4.86  1.00      0.00\n24 32.00     1   1 1.00   5.00 50.00      3.91\n\n[25 rows x 7 columns]\n\n\n관측값들과 emperical probability와 log odds을 함께 살펴보면,\n\ncode\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.p, size=blowbs_bn.m, c=\"#008fd5\", sizes=(20, 200), ax=ax)\n\ndef jitter(values, j):\n    return values + np.random.normal(0, j, values.shape)\n\nsns.scatterplot(x=blowbs.log2d, y=jitter(blowbs.y, 0.02), alpha=.3, c=\".6\", ax=ax)\n\nfor i, row in blowbs_bn.iterrows():\n    ax.annotate(f\"{row.died:n}/{row.m:n}\", xy=(row.log2d, row.p), xytext=(row.log2d, row.p-0.05), size=9)\n\nax.set_xticks(blowbs_bn.log2d.unique())\nax.set_xticklabels(blowbs_bn.log2d.unique().round(2))\nax.tick_params(axis='x', rotation=45)\nax.set_title(\"Emperical Probability\")\nax.legend_.remove()\n\n\n\n\n\n\n\n\nLogit 값(분홍색)을 추가해서 그리면,\n\ncode\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.p, size=blowbs_bn.m, sizes=(20, 200), c=\"#008fd5\", ax=ax, legend=False)\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.log_odds, size=blowbs_bn.m, sizes=(20, 200), color=\"#f46a9b\", ax=ax)\ndef jitter(values, j):\n    return values + np.random.normal(0, j, values.shape)\n\nsns.scatterplot(x=blowbs.log2d, y=jitter(blowbs.y, 0.02), alpha=.3, c=\".6\", ax=ax)\n\nfor i, row in blowbs_bn.iterrows():\n    ax.annotate(f\"logit{row.died:n}/{row.m:n}\", xy=(row.log2d, row.log_odds), xytext=(row.log2d, row.log_odds-0.4), size=9)\n\nax.set_xticks(blowbs_bn.log2d.unique())\nax.set_xticklabels(blowbs_bn.log2d.unique().round(2))\nax.tick_params(axis='x', rotation=45)\nax.set_ylabel(\"P & Logit\")\nax.set_title(\"Emperical Probability and Logit\")\nax.legend_.remove()\n\n# # polyfit 5\n# x = np.linspace(blowbs_bn.log2d.min(), blowbs_bn.log2d.max(), 100)\n# y = np.polyval(np.polyfit(blowbs_bn.log2d, blowbs_bn.log_odds, 5), x)\n# sns.lineplot(x=x, y=y, ax=ax, color=\".6\")\n\n# # polyfit 1\n# y = np.polyval(np.polyfit(blowbs_bn.log2d, blowbs_bn.log_odds, 1), x)\n# sns.lineplot(x=x, y=y, ax=ax, color=\".6\")\n\n\n\n\n\n\n\n\n위의 logit값을 선형모형으로 예측하는 모형: \\(\\displaystyle log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right) = b_0 + b_1 \\cdot log_{2}(d)\\)\n파라미터 \\(b_0, b_1\\)의 추정은 잔차들의 제곱의 합을 최소로 하는 OLS 방식은 부적절하며, 대신에 Maximum Likelihood Estimation을 사용함.\n\n아이디어는 관측치가 전체적으로 관찰될 likelihood가 최대가 되도록 \\(b_0, b_1\\)을 선택하는 것임\n이를 위해서 적절한 확률모형을 결합시켜야 함.\n선택하는 확률 모형은 Bernoulli 분포임; 평균 \\(E(Y|X) = p\\)이고, \\(logit(E(Y|X)) = b_0 + b_1 \\cdot log_{2}(d)\\)\n\n발생 비율값으로 변환해 Binomial distribution (이항분포)로 전개하는 방식도 있음; binomial logit model\n\n관찰값은 Bernoulli 분포로부터 발생했다고 가정함으로써, 실제 관찰값들이 관찰될 확률/가능도를 기준으로 파라미터를 추정할 수 있음.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\nblowbs.sort_values(\"log2d\")\n\n         d    s  y           spp  log2d  label\n2102  5.00 0.45  0  black spruce   2.32  alive\n724   5.00 0.18  0  black spruce   2.32  alive\n723   5.00 0.18  1  black spruce   2.32   died\n...    ...  ... ..           ...    ...    ...\n1784 29.00 0.38  1  black spruce   4.86   died\n1079 29.00 0.25  0  black spruce   4.86  alive\n3455 32.00 0.80  1  black spruce   5.00   died\n\n[659 rows x 6 columns]\n\n\n\n각 likelihood는 관측치들이 모두 독립적으로 발생했다고 가정했을 때의 확률값이고,\n모든 데이터가 관찰될 likelihood = \\(p_1^7 (1-p_1)^{83} \\cdot p_2^7 (1-p_2)^{85} \\cdot p_3^{18} \\cdot (1-p_3)^{73} \\cdots\\)\n이 때, 모형을 예측변수 \\(X\\)의 1차 다항함수로 fit한다면,   \\(\\displaystyle log\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 \\cdot x_i\\) 인데,\n변형하면   \\(\\displaystyle p_i = \\sigma(\\beta_0 + \\beta_1 x_i) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot x_i)}}\\)\nLikelihood = \\(\\displaystyle \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.32)}}\\right)^7 \\left(1 - \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.32)}}\\right)^{83} \\cdot \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.58)}}\\right)^7 \\left(1 - \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 \\cdot 2.58)}}\\right)^{85} \\cdots\\)\n이 Likelihood가 최대가 되도록 \\(\\beta_0, \\beta_1\\)의 추정치를 찾음.\n\n\n\n\n\n\nNote\n\n\n\nLog likelihood = \\(\\displaystyle \\sum_{i=1}^{n} y_i \\cdot log(p_i) + (1-y_i) \\cdot log(1-p_i)\\)\n이는 머신러닝에서 손실함수로 사용되는 cross-entropy와 동일함.\n\\(L = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\cdot log(p_i) + (1-y_i) \\cdot log(1-p_i)\\)\n\n\n\n\n\n\n\n\nRegularization\n\n\n\n앞서 Ridge, Lasso에서와 비슷한 원리로 OLS가 아닌 ML(maximum likelihood) estimator에 penalty를 적용하는 방식임.\n즉, likeihood에 대한 계산에서 penalty term을 추가함\n에를 들어, Logistic regression with L2 regularization: 다음 손실함수를 최소화하도록 파라미터를 추정\n\\(\\displaystyle L = \\frac{1}{n} \\left[ - \\sum_{i=1}^{n} \\left(y_i \\cdot log(p_i) + (1-y_i) \\cdot log(1-p_i)\\right) + \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2 \\right]\\)\n\n\n\nimport statsmodels.formula.api as smf\n\nmod = smf.logit('y ~ log2d', data=blowbs).fit()\nprint(mod.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.499165\n         Iterations 6\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                  659\nModel:                          Logit   Df Residuals:                      657\nMethod:                           MLE   Df Model:                            1\nDate:                Sat, 28 Dec 2024   Pseudo R-squ.:                  0.2316\nTime:                        06:05:55   Log-Likelihood:                -328.95\nconverged:                       True   LL-Null:                       -428.10\nCovariance Type:            nonrobust   LLR p-value:                 4.888e-45\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -7.8162      0.628    -12.437      0.000      -9.048      -6.584\nlog2d          2.2408      0.190     11.773      0.000       1.868       2.614\n==============================================================================\n\n\n\n\n\n\n\n\nScikit-learn implementation\n\n\n\nScikit-learn의 LogisticRegression()은 디폴트로 \\(l2\\) regularization을 사용함: 참고 문서\n우선, train-test split을 통해 데이터를 나눈 후, LogisticRegression()을 적용하면,\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nX = blowbs[['log2d']]\ny = blowbs['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n\nlr = LogisticRegression(penalty=None)  # l1, l2 regularization 가능\nlr.fit(X_train, y_train)\n\nprint(lr.coef_, lr.intercept_)\n# [[2.08]] [-7.26]\n\n# As a DataFrame with column names\ncoefs = pd.DataFrame({\"coef\": lr.coef_[0], \"name\": X.columns})\ncoefs\n#    coef   name\n# 0  2.08  log2d\n\n\n위 fitted model의 예측값들\n\ncode\nfig, ax = plt.subplots(1, 1, figsize=(9, 5))\n\nsns.scatterplot(x=blowbs_bn.log2d, y=blowbs_bn.p, size=blowbs_bn.m, c=\"#008fd5\", sizes=(20, 200), ax=ax)\n\ndef jitter(values, j):\n    return values + np.random.normal(0, j, values.shape)\n\nsns.scatterplot(x=blowbs.log2d, y=jitter(blowbs.y, 0.02), alpha=.3, c=\".6\", ax=ax)\n\nfor i, row in blowbs_bn.iterrows():\n    ax.annotate(f\"{row.died:n}/{row.m:n}\", xy=(row.log2d, row.p), xytext=(row.log2d, row.p-0.05), size=9)\n\nax.set_xticks(blowbs_bn.log2d.unique())\nax.set_xticklabels(blowbs_bn.log2d.unique().round(2))\n# x-axis with 45 degree rotation\nax.tick_params(axis='x', rotation=45)\n\n# fitted line\nsns.lineplot(x=blowbs.log2d, y=mod.predict(blowbs[\"log2d\"]), ax=ax, color=\"#87bc45\")\n\nplt.show()",
    "crumbs": [
      "Trees",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#예측값",
    "href": "contents/logistic.html#예측값",
    "title": "Logistic Regression",
    "section": "예측값",
    "text": "예측값\nLogistic regression에서는 세 가지 타입의 예측값들이 있음.\n\nPredicted probability:   \\(\\displaystyle \\hat{p} = \\frac{1}{1+e^{-(b_0 + b_1 \\cdot x)}}\\)\nOdds:   \\(\\displaystyle odds = \\frac{\\hat{p}}{1 - \\hat{p}} = e^{b_0 + b_1 \\cdot x}\\)\nLog odds:   \\(\\displaystyle logit = b_0 + b_1 \\cdot x\\)\n\n이 확률값을 이용해 binary outcome인 클래스를 예측할 수 있음.\n\n# using statsmodels package, not scikit-learn\nblowbs_pred = blowbs.assign(\n    pred_prob = mod.predict(blowbs[\"log2d\"]),\n    pred_odds = lambda x: x.pred_prob / (1 - x.pred_prob),\n    pred_logit = lambda x: mod.predict(blowbs[\"log2d\"], which=\"linear\")\n)\nblowbs_pred.iloc[:, 1:]\n\n        s  y           spp  log2d  label  pred_prob  pred_odds  pred_logit\n17   0.02  0  black spruce   3.17  alive       0.33       0.49       -0.71\n24   0.03  0  black spruce   3.46  alive       0.48       0.94       -0.06\n25   0.03  0  black spruce   3.17  alive       0.33       0.49       -0.71\n...   ... ..           ...    ...    ...        ...        ...         ...\n3646 0.94  1  black spruce   3.17   died       0.33       0.49       -0.71\n3647 0.94  1  black spruce   4.09   died       0.79       3.83        1.34\n3661 0.98  1  black spruce   3.00   died       0.25       0.33       -1.09\n\n[659 rows x 8 columns]\n\n\n예를 들어, 두께(log2d)가 3인 나무의 경우,\n\nProbabiliy: 태풍에 쓰러질 확률은 25%로 예측되며,\nOdds: 태풍에 쓰러지지 않을 가능성 대비 쓰러질 가능성의 비율은 1:0.33이므로 대략 3:1로 예측됨. 즉 다시 말하면, 1 그루가 쓰러진다면 3 그루는 쓰러지지 않을 것으로 예측함.\n\nOdds가 1이면 event의 확률(p)이 0.5\nOdds가 1보다 작으면 event의 확률(p)이 0.5보다 작고,\nOdds가 1보다 크면 event의 확률(p)이 0.5보다 큼.\n\nLog odds (logit): 확률 p의 [0, 1]의 값을 무한한 값으로 늘려 linearly fit할 수 있게 함.\n\n\n\n\n\n\n\n모형의 파라미터 해석\n\n\n\n\n\nOdds의 비율 (odds ratio, OR)을 통해 해석\n\n\\(\\displaystyle odds: \\frac{\\hat{p}}{1 - \\hat{p}} = e^{b_0 + b_1 \\cdot x}=e^{b_0}\\cdot e^{b_1 \\cdot x}\\)  로부터\n\\(x\\)가 1 증가할 때 odds의 비율: \\(\\displaystyle odds ~ratio: \\frac{\\frac{\\hat{p_2}}{1 - \\hat{p_2}}}{\\frac{\\hat{p_1}}{1 - \\hat{p_1}}}  = e^{b_1 \\cdot (x+1) - b_1 \\cdot x} = e^{b_1}\\)\n즉, \\(x\\)가 1 증가하면, odds가 “몇 배”로 증가하는지를 나타냄.\n따라서, odds ratio가 1보다 크면 (\\(b_1\\)이 양수) \\(x\\)가 1 증가할 때, event의 odds가 커지며,\nodds ratio가 1보다 작으면 (\\(b_1\\)이 음수) event의 odds가 줄어듬.\n\n위의 경우, \\(\\displaystyle \\widehat{odds} = e^{-7.82 + 2.24 \\cdot log_2(d)}\\) 이므로 odds ratio = \\(e^{2.24} = 9.4\\)\n\n해석하면, 나무의 두께가 (log2 scale로) 1 늘어남 (2배 증가)에 따라 태풍에 나무가 쓰러질 odds가 9.4배 증가함\n다시 말하면, 나무의 두께가 (log2 scale로) 1 늘어남 (2배 증가)에 따라 태풍에 나무가 쓰러지지 않을 가능성 대비 쓰러질 가능성이 9.4배 증가함.\n나무의 두께 (원래 d)로 말하면, 두께가 10% 두꺼워지면, \\(e^{2.24 \\cdot log_2(1.1)}=1.36\\) 배, 즉 odds가 36% 증가함.\n\n\\(b_0\\): d = 1일 때의 odds이므로 \\(\\displaystyle e^{-7.82 + 2.34 \\cdot 0} = e^{-7.82} = 0.004\\), 즉 두께가 1cm 일 때 태풍에 나무가 쓰러지지 않을 가능성 대비 쓰러질 가능성은 0.004임.\n\n\n\n\n\n\n\n\n\nScikit-learn implementation\n\n\n\n\n\nScikit-learn의 LogisticRegression()은 디폴트로 \\(l2\\) regularization을 사용함: 참고 문서\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nX = blowbs[['log2d']]\ny = blowbs['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n\nlr = LogisticRegression(penalty=None)  # l1, l2 regularization 가능\nlr.fit(X_train, y_train)\n\n# predict_proba: 각 클래스에 대한 확률을 반환\n# predict: 예측된 클래스를 반환 (threshold=0.5)\ntest_pred = X_test.assign(\n    y=y_test,\n    pred_prob=lr.predict_proba(X_test)[:, 1],  # 확률값\n    pred_class=lr.predict(X_test),  # 클래스\n    pred_odds=lambda x: x.pred_prob / (1 - x.pred_prob),  # odds\n    pred_logit=lambda x: np.log(x.pred_odds),  # log odds\n)\ntest_pred.head(5)\n\n#       log2d  y  pred_prob  pred_class  pred_odds  pred_logit\n# 2890   3.46  1       0.48           0       0.94       -0.07\n# 574    3.81  0       0.66           1       1.93        0.66\n# 96     3.17  0       0.34           0       0.51       -0.67\n# 2773   3.46  1       0.48           0       0.94       -0.07\n# 1813   3.58  1       0.55           1       1.21        0.19",
    "crumbs": [
      "Trees",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#모형의-예측-정확성accuracy",
    "href": "contents/logistic.html#모형의-예측-정확성accuracy",
    "title": "Logistic Regression",
    "section": "모형의 예측 정확성(accuracy)",
    "text": "모형의 예측 정확성(accuracy)\n분류 문제의 경우, 보통 두 단계를 거쳐 예측\n\nInference: 클래스에 속할 확률을 추정; 확률을 추정하지 않는 알고리즘도 있음.\nDecision: 추정된 확률을 기반으로 클래스를 결정\n\n기본적으로 0.5 이상이면 1로, 그렇지 않으면 0으로 분류\nThreshold를 조정해 분류를 조정할 수 있음\n\n예를 들어, 확실한 경우(p &gt; 0.8)에만 1로 분류\n\n애매한 확률 구간의 경우 결정을 유보할 수 있음; reject option\n\n\n이 때, 모형의 예측 정확성을 평가하는 두 가지 방식이 있음.\n\n확률 대한 예측 정확성 (evaluating predicted probability)\n클래스에 대한 예측 정확성 (evaluating predicted class)\n\n\n\n\n\n\n\nEvaluation of predicted probability\n\n\n\n\n\n이제 이 모형이 좋은 모형인지 살펴보기 위해 residual, 잔차를 살펴볼 수 있는가?\nBinomial version\n\nPearson residual: \\(\\displaystyle \\frac{actual ~ count ~ - predicted ~ count}{SD ~ of ~ count} = \\frac{y_i - m_i \\hat{p}_i}{\\sqrt{m_i \\hat{p}_i (1-\\hat{p}_i)}}\\)\nDeviance residual: \\(\\displaystyle sign(y_i - m_i\\hat{p}_i) \\sqrt{-2[y_i log(\\frac{y_i}{m_i\\hat{p}_i}) + (m_i-y_i) log(\\frac{m_i - y_i}{m_i-m_i\\hat{p}_i})]}\\)\n\nBinary version\n\nPearson residual: \\(\\displaystyle \\frac{y_i - \\hat{p}_i}{\\sqrt{\\hat{p}_i (1-\\hat{p}_i)}}\\)\nDeviance residual: \\(\\displaystyle sign(y_i - \\hat{p}_i) \\sqrt{-2[y_i log(\\hat{p}_i) + (1-y_i) log(1-\\hat{p}_i)]}=sign(y_i - \\hat{p}_i)\\sqrt{-2log(likelihood)}\\)\n\nDeviance를 이용해 OLS에서의 \\(R^2\\)와 비슷한 개념을 구성: Pseudo R-squared\nModel deviance, \\(D_k = -2[log(likelihood_k) - log(likelihood_{perfect})]\\)\n\nPerfect model의 likelihood: 1\n\nDeviance for an intercept only model; null_deviance = 856.2073760911842\n\nPseudo R-squared: \\(\\displaystyle \\frac{Null~Deviance - Deviance}{Null~Deviance}\\)\nCox-Snell’s Pseudo R-squared\nNagelkerke’s Pseudo R-squared\n\n“Coefficient of discrimination” (Tjur, 2009): average \\(\\hat{p}\\) when \\(y=1\\) - average \\(\\hat{p}\\) when \\(y=0\\)\n\n\n\n\n\ncode\ntest_pred[\"y2\"] = test_pred[\"y\"].map({0: \"survived (y=0)\", 1: \"died (y=1)\"})\n(\n    so.Plot(test_pred, x='pred_prob')\n    .add(so.Bars(color=\"#ef9b20\"), so.Hist(bins=12))\n    .facet(\"y2\")\n    .label(x=\"Predicted Probability\", y=\"Count\")\n    .layout(size=(7, 4))\n)\n\n\n\n\n\n\n\n\n\n\nEvaluation of predicted class\n\n\n\n\n\n\nImportant\n\n\n\n예측된 확률을 기반으로 binary outcome/class으로 예측하여, 모형의 예측력을 평가\n\n예측된 확률값에 대해 임계치를 정하여, 예를 들어 0.5보다 크면 1, 0.5보다 작으면 0으로 분류하여, 이 binary 예측값과 실제값을 비교하여, 예측력을 평가\nConfusion matrix\nROC curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThreshold: 0.5인 경우, accuracy rate: 0.78\n\n\n\n\n\n\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\n203\n54\n\n\ndied(1)\n19\n54\n\n\n\n\n\n \n\n\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\nTrue Negative\nFalse Positive\n\n\ndied(1)\nFalse Negative\nTrue Positive\n\n\n\n\n\n\n다른 threshold를 적용하면,\n\n\n\n\n\n\nThreshold: 0.1인 경우,\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\n42\n4\n\n\ndied(1)\n180\n104\n\n\n\nAccuracy rate: 0.44\n\n\n \n\n\nThreshold: 0.9인 경우\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\n222\n104\n\n\ndied(1)\n0\n4\n\n\n\nAccuracy rate: 0.68\n\n\n \n\n\n\n\ncode\nfrom sklearn.metrics import accuracy_score\nfrom ISLP import confusion_table\n\n# cutoff = 0.5\ncm = confusion_table(test_pred[\"pred_class\"], test_pred[\"y\"])\nscore = accuracy_score(test_pred[\"y\"], test_pred[\"pred_class\"])\n\ndisplay(cm)\nprint(f\"Accuracy rate: {score:.2f}\")\n\n# cutoff = 0.1\ncm = confusion_table(test_pred[\"pred_prob\"] &gt; 0.1, test_pred[\"y\"])\nscore = accuracy_score(test_pred[\"y\"], test_pred[\"pred_prob\"] &gt; 0.1)\n\ndisplay(cm)\nprint(f\"Accuracy rate: {score:.2f}\")\n\n# cutoff = 0.9\ncm = confusion_table(test_pred[\"pred_prob\"] &gt; 0.9, test_pred[\"y\"])\nscore = accuracy_score(test_pred[\"y\"], test_pred[\"pred_prob\"] &gt; 0.9)\n\ndisplay(cm)\nprint(f\"Accuracy rate: {score:.2f}\")\n\n\n\n\n\n\n분류모형의 성능을 평가하기 위한 여러 지표들\nPrecision & Recall: 정보 검색 시스템의 탐색 능력을 평가하는데에서 유래\n\nPrecision (정밀도): \\(\\hat{y}=1\\)일 때, \\(y=1\\)일 확률\n\n시스템이 스팸으로 분류한 이메일 중 실제 스팸인 이메일의 비율\n\n높은 precision은 스팸으로 잘못 분류된 정상 이메일이 적다는 것을 의미\n하지만, 스팸 이메일이 정상 이메일함에 나타날 수 있음.\n\n정보검색에서 반환된 문서들 중 실제로 관련 있는 문서의 비율\n\n높은 precision은 사용자가 검색 결과에서 불필요한 정보를 적게 얻고, 대부분 유용한 정보를 얻는다는 것을 의미\n하지만, 관련은 있지만 놓친 정보는 많을 수 있음.\n검색 결과의 정확도를 평가\n\n검사를 통해 암으로 진단받은 사람이 실제로 암을 가지고 있을 확률\n\n높은 precision은 암 진단을 받은 사람에게 잘못된 암 진단을 내리지 않는 것을 의미\n\n\nRecall (재현율): \\(y=1\\)일 때, \\(\\hat{y}=1\\)일 확률\n\n실제 스팸 이메일 중에서 시스템이 스팸으로 올바르게 분류한 비율\n\n높은 recall은 대부분의 스팸 이메일이 정확히 스팸으로 분류된다는 것을 의미\n하지만, 정상 이메일이 스팸함으로 분류될 수 있음.\n\n정보검색에서 관련 문서들 중에서 실제로 시스템이 반환한 문서의 비율\n\n높은 recall은 사용자가 찾고자 하는 모든 관련 정보를 검색 결과에서 얻을 수 있다는 것을 의미\n하지만, 관련 없는 정보도 많이 포함될 수 있음.\n검색 시스템의 탐색 능력을 평가\n\n실제로 암을 가진 사람 중에서 검사를 통해 암으로 진단받은 사람의 비율\n\n높은 recall은 암 환자를 놓치지 않고 모두 찾아낸다는 것을 의미\n\n참 양성, true positive rate (TPR), 또는 sensitivity라고도 함\n\n\nprecision & recall trade-off: 이 둘 사이에는 종종 상충 관계가 있음. Precision을 높이기 위해 예측 확률에 대한 더 엄격한 기준을 사용하면 recall이 낮아질 수 있고, 반대로 recall을 높이기 위해 더 느슨한 기준을 사용하면 precision이 낮아질 수 있음.\n\n정보 검색의 맥락: precision을 높여 사용자가 불필요한 정보를 받지 않게 해주며, recall을 높여 필요한 정보를 놓치지 않도록 함.\n스팸 필터링의 맥락: precision을 높여 자주 스팸 메일함을 확인하지 않아도 되도록 하며, recall을 높여 대부분의 스팸 이메일이 차단되어 정상 메일함에서 보이지 않도록 쾌적한 환경을 제공할 수 있음.\n\nSensitivity & specificity: 의학 분야에서 진단 테스트의 정확성을 평가하는데에서 유래\nPrecision & recall이 주로 양성(positive) 클래스에 초점을 두는 반면, sensitivity & specificity는 양성(positive)와 음성(negative) 클래스 모두에 초점을 둠.\n특히, 이상치 탐지라든가 희귀 질병 진단 등 불균형 데이터셋에서는 precision & recall이 더 유용할 수 있음.\n\nSensitivity (민감도): \\(y=1\\)일 때, \\(\\hat{y}=1\\)일 확률\n\nRecall, true positive rate (TPR)\n높은 민감도는 질병이 있는 사람을 놓치지 않고 모두 찾아내는 것(참 양성)을 의미\n\nSpecificity (특이도): \\(y=0\\)일 때, \\(\\hat{y}=0\\)일 확률\n\nTrue negative rate (TNR)\n실제로 질병이 없는 사람을 얼마나 잘 (질병이 없다고) 식별하는지(참 음성)를 나타냄\n높은 특이도는 질병이 없는 사람에게 질병이 있다고 잘못 진단하지 않는 것을 의미함.\n\n희귀 질병을 진단하는 경우, 정상인(negative)을 올바로 식별하는 것은 매우 쉽기 때문에 질병이 없는 사람에 대한 판별력이 과대추정될 수 있음; precision이 더 유용할 수 있음.\n\n1 - FPR (False Positive Rate; false alarm)\n0, 1을 바꿨을 때의 즉, 0을 기준으로 했을 때의 recall\n특이도가 높은 테스트라면 질병이 없다는 판정은 신뢰할 만함. 즉, 0(음성)을 기준으로한 sensitivity가 높은 것이 됨.\n\n\n\n\n\n\n\n\n거짓 음성 비율: False negative rate (FNR); \\(y=1\\)일 때, \\(\\hat{y}=0\\)일 확률: 1 - sensitivity\n- 정상이라고 진단된 환자가 실제로 암인 확률\n거짓 양성 비율: False positive rate (FPR) 또는 False alarm; \\(y=0\\)일 때, \\(\\hat{y}=1\\)일 확률: 1 - specificity\n- 정상인 사람이 검사를 통해 암으로 진단받을 확률\n\n\n\n\n\n위의 confusion matrix로부터 지표들을 계산하면,\n\n\n\n\n\n\nThreshold: 0.1인 경우,\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\n42\n4\n\n\ndied(1)\n180\n104\n\n\n\n\n\n \n\n\nThreshold: 0.9인 경우\n\n\n\nPredicted\nTruth\n\n\n\n\n\n\nsurvied(0)\ndied(1)\n\n\nsurvied(0)\n222\n104\n\n\ndied(1)\n0\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\nrecall = 104 / (4 + 104) = 0.96\nprecision = 104 / (180 + 104) = 0.37\nspecificity = 42 / (42 + 180) = 0.19\naccuracy = (42 + 104) / (42 + 4 + 180 + 104) = 0.44\n\n\n\n \n\n\n\nrecall = 4 / (4 + 104) = 0.04\nprecision = 4 / (0 + 4) = 1.00\nspecificity = 222 / (222 + 0) = 1.00\naccuracy = (222 + 4) / (222 + 104 + 0 + 4) = 0.68\n\n\n\n\nReceiver operating characteristic (ROC) curve\n예측된 확률에 대한 임계치를 조정함에 따라 옳은 예측과 틀린 예측의 비율이 어떻게 달라지는지 살펴봄으로써 임계치를 설정하는데 도움을 줌\n\n잘못된 예측에 대한 비용이 다르다면, 특정 임계치를 선택하는 것을 고려해야 함\n\n\n\ncode\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(test_pred.y, test_pred.pred_prob)\nroc = pd.DataFrame(\n    {\n        \"thresholds\": thresholds.round(2),\n        \"False Pos\": fpr,\n        \"sensitivity(TPR)\": tpr,\n        \"specificity(TNR)\": 1 - fpr\n    }\n)\nroc.sort_values(\"thresholds\").head(10)\n\n## ROC curve\n# 위에서 얻은 roc 데이터을 사용하여 그리거나\n# RocCurveDisplay를 이용\nfrom sklearn.metrics import RocCurveDisplay\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n\n# scikit-learn의 visualization 문서 참조\n# https://scikit-learn.org/stable/visualizations.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\nfrom sklearn.metrics import precision_recall_curve\n\nprecision, recall, thresholds = precision_recall_curve(test_pred.y, \ntest_pred.pred_prob)\npr = pd.DataFrame(\n    {\n        \"thresholds\": np.append(thresholds.round(2), 1),\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1-score\": 2 * precision * recall / (precision + recall)\n    }\n).query(\"recall &gt; 0.1\")\npr.sort_values(\"thresholds\").head(10)\n\n# precision-recall curve\n# 위에서 얻은 pr 데이터을 사용하여 그리거나\n# PrecisionRecallDisplay를 이용\nfrom sklearn.metrics import PrecisionRecallDisplay\npr_display = PrecisionRecallDisplay(precision=precision, recall=recall).plot()\n\n# scikit-learn의 visualization 문서 참조\n# https://scikit-learn.org/stable/visualizations.html\n\n\n\n\n\n\n\n\n\n\n\n\n위에서 언급한 클래스 불균형의 예들: 이상치 탐지, 희귀 질병 진단 등에서는 precision & recall이 더 유용할 수 있음.\n\n\n\n\n\n\n\n\n\nClassifier의 전체적 성능에 대한 지표\nROC curve:\n\nAUC: Area Under the Curve = Concordance Index\n\n각 specificity값에 대한 sensitivity의 합; 모형(classifier) 대한 전반적 평가\n0.5: random guess, 1: perfect prediction\n\nConcordance Index(c-index): 모든 서로 다른 클래스의 Y쌍, 예를 들어 \\((0_i, 1_j)\\)에 대해서 해당하는 예측된 확률의 크기가 \\(p_i &lt; p_j\\) 인 비율, 즉 순서가 맞는(concordance) 비율\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n위 blowdown 데이터셋 대한 모형의 AUC, concordance index 계산\n\n\n\n\n\n\nCode\n\n\n\n\n\nfrom sklearn.metrics import roc_auc_score, auc\n\nroc_auc = roc_auc_score(test_pred.y, test_pred.pred_prob)\nprint(f\"AUC: {roc_auc:.2f}\")  # 또는 auc(fpr, tpr)\n\n\n\nAUC: 0.81\n\n\n\n\n\n\nCode\n\n\n\n\n\ndef concordance(y, pred_prob):\n    concord = 0\n    total = 0\n\n    test_pred = pd.DataFrame({\"y\": y, \"pred_prob\": pred_prob})\n    for idx, case in test_pred.iterrows():\n        other_cases = test_pred[test_pred.index != idx]\n\n        # 같은 케이스 제외\n        df = other_cases[case[\"y\"] != other_cases[\"y\"]]\n\n        concord += (\n            (case[\"y\"] - df[\"y\"] &gt; 0) == (case[\"pred_prob\"] - df[\"pred_prob\"] &gt; 0)\n        ).sum()\n        total += len(df)\n\n    return concord / total\n\nprint(f\"Concordance index: {concordance(test_pred.y, test_pred.pred_prob):.2f}\")\n\n\n\nConcordance index: 0.81\n\n\n\nPrecision-recall:\n\nAverage precision: the area under the precision-recall curve\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nfrom sklearn.metrics import average_precision_score\n\naverage_precision = average_precision_score(test_pred.y, test_pred.pred_prob)\nprint(f\"Average Precision: {average_precision:.2f}\")\n\n\n\nAverage Precision: 0.68\n\nThe classification report in scikit-learn\nThershold: 0.4인 경우,\n\n\ncode\nfrom sklearn.metrics import classification_report, recall_score\n\nthreshold = .4\npred_class = test_pred[\"pred_prob\"] &gt;= threshold\nprint(classification_report(test_pred[\"y\"], pred_class))\n\n\n              precision    recall  f1-score   support\n\n           0       0.86      0.82      0.84       222\n           1       0.66      0.73      0.69       108\n\n    accuracy                           0.79       330\n   macro avg       0.76      0.77      0.77       330\nweighted avg       0.80      0.79      0.79       330\n\n\n\n\n\n\n\n\n\nspecificity\n\n\n\n\nspecificity for positive = recall for negative: 0.82\nmacro avg of recall: sensitivity와 specificity의 평균: 0.77\n\n\n\n\nF1-score = \\(\\displaystyle \\left(\\frac{\\text{precision}^{-1} + \\text{recall}^{-1}}{2}\\right)^{-1}\\)\n\nPrecision과 recall의 조화평균\n보수적인 지표임. 즉, precision과 recall 중 하나라도 낮으면 F1-score도 낮아짐\n\n\n\n\n\n\n\n\n\n\n\n\nClassifier로서 전반적인 모형의 성능 vs. 특정 임계치에서의 모형의 성능 vs. 확률모형\n\nClassifier로서 전반적인 모형의 성능: AUC 등\n비용을 고려한 특정 임계치에서의 모형의 성능\n\n잘못된 예측에 대한 비용이 다르다면, 임계치를 조정하여, 잘못된 예측에 대한 비용을 줄일 수 있음\n만약, 농작물에 대한 피해라고 가정하면,\n\nCosts: 농작물 피해, 펜스 설치비, 노동력 등\nBenefits: 수확물의 가치\n거짓 음성을 낮춰야 하는 경우: 예를 들어, 농작물의 작은 피해도 심각한 결과를 초래하는 경우\n거짓 양성을 낮춰야 하는 경우: 예를 들어, 농작물의 피해 예방을 위한 비용이 큰 경우\n\n만약, 와인 셀러가 와인의 품질(high:양성 vs. low:음성)을 성분들로 예측하는 모형을 만든다면, (in Stefanie Molin’s book)\n\nCosts: 높은 품질의 와인을 낮은 품질로 예측하면, 와인 품평가에게 신뢰를 잃을 수 있음\nBenefits: 낮은 품질의 와인을 높은 품질로 예측하면, 낮은 품질의 와인을 높은 가격에 팔아 수익으로 이어질 수 있음\n거짓 음성을 낮춰야 하는 경우: 예를 들어, 영세한 와이너리가 수익이 중요한 경우\n거짓 양성을 낮춰야 하는 경우: 예를 들어, 고품질의 와인을 생산하는 것으로 유명한 와이너리; 네임밸류를 유지하기 위해. 반면, 비싼 와인이 싸게 팔리는 것은 감당할 수 있음.\n\n\n확률모형: 확률을 정확히 예측하는 모형의 추구;\n\n확률값 혹은 확률 분포(Bayesian 접근)로 communicate\nDecision maker는 당사자",
    "crumbs": [
      "Trees",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#decision-boundary",
    "href": "contents/logistic.html#decision-boundary",
    "title": "Logistic Regression",
    "section": "Decision boundary",
    "text": "Decision boundary\n앞선, Log odds(logit):   \\(\\displaystyle log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)  = b_0 + b_1 \\cdot X\\)\n예측변수가 2개라면,\nLog odds(logit):   \\(\\displaystyle log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)  = b_0 + b_1 \\cdot X_1 + b_2 \\cdot X_2\\)\n\n왼편의 logit 함수는 증가함수이므로, threshold를 정하면 선형 결정 경계(decision boundary)가 나타남: hyperplane\n비선형 함수로 모형을 세우면, decision boundary가 비선형으로 나타남.\n\n예를 들어, \\(b_0 + b_1 \\cdot X + b_2 \\cdot X^2\\) 등\n\n\n밑은 두 예측변수 \\(X_1= log2d\\) (diameter)와 \\(X_2=s\\) (severity)로 나무가 쓰러질지 여부를 선형함수로 만들었을 때의 decision boundary (threshold=0.5)\n즉, Log odds(logit):   \\(\\displaystyle log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)  = b_0 + b_1 \\cdot X_1 + b_2 \\cdot X_2\\)\n\n\n\n\n\n\n\n\n\n밑은 두 예측변수 \\(X_1= log2d\\) (diameter)와 \\(X_2=s\\) (severity)에 대한 모든 2, 3차항을 추가했을 때 decision boundary; 10개의 파라미터\n예를 들어, \\(X_1^2, ~ X_1 \\cdot X_2, ~X_1^3, ~X_1^2 \\cdot X_2, ~X_2^3\\)",
    "crumbs": [
      "Trees",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/logistic.html#multiclass-k-2의-경우",
    "href": "contents/logistic.html#multiclass-k-2의-경우",
    "title": "Logistic Regression",
    "section": "Multiclass (K > 2)의 경우",
    "text": "Multiclass (K &gt; 2)의 경우\nMultinomial logistic regression\n클래스 \\(C_k\\)에 속할 확률을 예측하는 모형: Base 클래스를 기준으로 나머지 클래스에 대한 확률을 예측\nBinary 경우에서의 확률 \\(\\displaystyle P(Y=1|X=x) = \\frac{1}{1+e^{-(b_0 + b_1 \\cdot x)}}=\\frac{e^{b_0 + b_1 \\cdot x}}{1 + e^{b_0 + b_1 \\cdot x}}\\)을 다음과 같이 확장할 수 있음.\n\n\\(\\displaystyle P(Y=C_k|X=x) = \\frac{e^{\\beta_{k0} + \\beta_{k1} \\cdot x}}{1 + \\sum_{j=1}^{K-1} e^{\\beta_{j0} + \\beta_{j1} \\cdot x}}, ~~ k = 1, 2, ..., K-1\\)\n\\(k=K\\) (Base 클래스)의 경우 \\(\\displaystyle P(Y=C_K|X=x) = \\frac{1}{1 + \\sum_{j=1}^{K-1} e^{\\beta_{j0} + \\beta_{j1} \\cdot x}}\\)\n\n이 때, 마지막 클래스 \\(C_K\\)에 대한 클래스 \\(C_k\\)의 비율은 다음과 같이 나타남.\n\\(\\displaystyle log\\left(\\frac{P(Y=C_k|X=x)}{P(Y=C_K|X=x)}\\right) = \\beta_{k0} + \\beta_{k1} \\cdot x\\)\n이는 binary경우 \\(log(odds)\\) 인 \\(\\displaystyle log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right) = b_0 + b_1 \\cdot x\\)의 확장으로 볼 수 있음.\n\n\n\n\n\n\nNote\n\n\n\nMachine learning에서는 주로 base 클래스 없이 모든 클래스에 대한 확률을 예측:\nSoftmax coding: \\(\\displaystyle P(Y=C_k|X=x) = \\frac{e^{\\beta_{k0} + \\beta_{k1} \\cdot x}}{\\sum_{j=1}^{K} e^{\\beta_{j0} + \\beta_{j1} \\cdot x}}\\)\n두 클래스 \\(C_k\\)와 \\(C_{k'}\\)의 사이의 확률의 비율은\n\\(\\displaystyle log\\left(\\frac{P(Y=C_k|X=x)}{P(Y=C_{k'}|X=x)}\\right) = (\\beta_{k0} - \\beta_{k'0}) + (\\beta_{k1} - \\beta_{k'1}) \\cdot x\\)\n\n\n가령, 나무의 종(9 species)을 분류하는 문제를 생각해보면,\n나무의 두께(log2d), 쓰러졌는지 여부(y), 피해 심각도(s)로 9개의 종에 대한 확률을 예측하는 모형을 만들 수 있음.\n\ncode\n# Multinomial logistic regression\nblowdown = pd.read_csv('data/blowdown2.csv')\nX = blowdown.drop(columns=[\"spp\", \"d\"])\ny = blowdown[\"spp\"]  # classify the species\nblowdown\n\n\n\n\n\n\n\n\n\n\nd\ns\ny\nspp\nlog2d\n\n\n\n\n0\n9.00\n0.02\n0\nbalsam fir\n3.17\n\n\n1\n14.00\n0.02\n0\nbalsam fir\n3.81\n\n\n2\n18.00\n0.02\n0\nbalsam fir\n4.17\n\n\n...\n...\n...\n...\n...\n...\n\n\n3663\n19.00\n0.98\n1\njackpine\n4.25\n\n\n3664\n37.00\n0.98\n1\nblack ash\n5.21\n\n\n3665\n48.00\n0.98\n1\nblack ash\n5.58\n\n\n\n\n3666 rows × 5 columns\n\n\n\n\n# split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0, stratify=y)\n\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train);\n\n\n\nPredicted probability\npd.options.display.max_columns = 15\n\n# predicted probability\ntest_pred_prob = pd.DataFrame(lr.predict_proba(X_test), columns=lr.classes_)\npd.concat(\n    [X_test.assign(target=y_test).reset_index(drop=True), test_pred_prob], axis=1\n) # 데이터셋 병합\n\n\n\n\n\n\n\n\n\ns\ny\nlog2d\ntarget\naspen\nbalsam fir\nblack ash\nblack spruce\ncedar\njackpine\npaper birch\nred maple\nred pine\n\n\n\n\n0\n0.61\n1\n4.58\npaper birch\n0.29\n0.00\n0.02\n0.04\n0.24\n0.02\n0.33\n0.01\n0.04\n\n\n1\n0.26\n0\n4.32\ncedar\n0.14\n0.02\n0.01\n0.05\n0.20\n0.26\n0.11\n0.04\n0.18\n\n\n2\n0.86\n1\n5.17\naspen\n0.42\n0.00\n0.09\n0.01\n0.10\n0.01\n0.34\n0.00\n0.02\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1830\n0.09\n0\n3.70\ncedar\n0.05\n0.05\n0.00\n0.13\n0.25\n0.22\n0.05\n0.05\n0.20\n\n\n1831\n0.41\n1\n3.32\nblack spruce\n0.05\n0.01\n0.00\n0.33\n0.44\n0.02\n0.06\n0.02\n0.06\n\n\n1832\n0.20\n0\n3.00\ncedar\n0.01\n0.05\n0.00\n0.31\n0.24\n0.12\n0.01\n0.06\n0.21\n\n\n\n\n1833 rows × 13 columns\n\n\n\n\nPredicted class\n# prediction\ntest_pred = X_test.assign(\n    target=y_test,\n    pred_class=lr.predict(X_test),\n    pred_prob=lr.predict_proba(X_test).max(axis=1),\n)\ntest_pred\n\n\n\n\n\n\n\n\n\n\ns\ny\nlog2d\ntarget\npred_class\npred_prob\n\n\n\n\n2811\n0.61\n1\n4.58\npaper birch\npaper birch\n0.33\n\n\n1105\n0.26\n0\n4.32\ncedar\njackpine\n0.26\n\n\n3566\n0.86\n1\n5.17\naspen\naspen\n0.42\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n255\n0.09\n0\n3.70\ncedar\ncedar\n0.25\n\n\n1934\n0.41\n1\n3.32\nblack spruce\ncedar\n0.44\n\n\n816\n0.20\n0\n3.00\ncedar\nblack spruce\n0.31\n\n\n\n\n1833 rows × 6 columns\n\n\n\n# confusion matrix\nconfusion_table(test_pred[\"pred_class\"], test_pred[\"y\"])\n\n\n\n\n\n\n\n\nTruth\naspen\nbalsam fir\nblack ash\nblack spruce\ncedar\njackpine\npaper birch\nred maple\nred pine\n\n\nPredicted\n\n\n\n\n\n\n\n\n\n\n\n\n\naspen\n38\n0\n6\n0\n7\n12\n30\n0\n7\n\n\nbalsam fir\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nblack ash\n2\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nblack spruce\n8\n19\n0\n189\n88\n41\n0\n13\n74\n\n\ncedar\n63\n7\n0\n111\n278\n46\n79\n25\n76\n\n\njackpine\n21\n4\n0\n8\n47\n45\n24\n11\n28\n\n\npaper birch\n80\n0\n18\n4\n27\n11\n114\n1\n11\n\n\nred maple\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nred pine\n6\n7\n0\n17\n38\n23\n4\n12\n52\n\n\n\n\n\n\n\nPrecision, Recall: one-versus-all(rest)\n해당 클래스를 positive로 두고 나머지 클래스를 negative로 두어, 각 클래스에 대한 precision과 recall을 계산\n\n# classification report: one-versus-all\nprint(classification_report(test_pred[\"y\"], test_pred[\"pred_class\"]))\n\n              precision    recall  f1-score   support\n\n       aspen       0.38      0.17      0.24       218\n  balsam fir       0.00      0.00      0.00        37\n   black ash       0.33      0.04      0.07        25\nblack spruce       0.44      0.57      0.50       329\n       cedar       0.41      0.57      0.48       485\n    jackpine       0.24      0.25      0.25       178\n paper birch       0.43      0.45      0.44       251\n   red maple       0.00      0.00      0.00        62\n    red pine       0.33      0.21      0.26       248\n\n    accuracy                           0.39      1833\n   macro avg       0.28      0.25      0.25      1833\nweighted avg       0.36      0.39      0.36      1833\n\n\n\n\n# accuracy\nprint(accuracy_score(test_pred[\"y\"], test_pred[\"pred_class\"]))\n\n0.3911620294599018",
    "crumbs": [
      "Trees",
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "contents/model-basic.html",
    "href": "contents/model-basic.html",
    "title": "Model Basics",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#a-simple-model",
    "href": "contents/model-basic.html#a-simple-model",
    "title": "Model Basics",
    "section": "A simple model",
    "text": "A simple model\nData: sim1.csv\n\nsim1 = pd.read_csv(\"data/sim1.csv\")\n\n\n\n\n\n\n\n     x     y\n0    1  4.20\n1    1  7.51\n2    1  2.13\n..  ..   ...\n27  10 24.97\n28  10 23.35\n29  10 21.98\n\n[30 rows x 2 columns]\n\n\n\n\n\n\n\n패턴: 강한 선형 관계\n선형 모델 family/class인 \\(y = \\beta_0 + \\beta_1 x\\)을 세운 후\n무수히 많은 \\(\\beta_0, \\beta_1\\)의 값들 중 위 데이터에 가장 가까운 값을 찾음\n그 예로, 임의로 250개의 선형 모델을 그려보면,\n\n\n\n\n\n\n\n\n\n\n\n이 선형모델 중 데이터에 가장 가까운 모델을 찾고자 하는데, 이를 위해서는 데이터와 모델과의 거리를 정의해야 함.\n  \\(d =|~data - model~|\\)\n예) 모델과 데이터의 수직 거리(residuals)의 총체\n\nModel 1.1: \\(y = 1.5x+7\\)의 경우, 이 모델이 예측하는 값들\n\n\narray([ 8.5,  8.5,  8.5, 10. , 10. , 10. , 11.5, 11.5, 11.5, 13. , 13. ,\n       13. , 14.5, 14.5, 14.5, 16. , 16. , 16. , 17.5, 17.5, 17.5, 19. ,\n       19. , 19. , 20.5, 20.5, 20.5, 22. , 22. , 22. ])\n\n\n이 때, 관측치(\\(Y_i\\))와 예측치(\\(\\hat{Y}_i\\))의 차이, \\(Y_i - \\hat{Y}_i\\)를 잔차(residuals) 또는 예측 오차(errors)라고 함\n\n\n     x     y  pred  resid | e\n0    1  4.20  8.50      -4.30\n1    1  7.51  8.50      -0.99\n2    1  2.13  8.50      -6.37\n..  ..   ...   ...        ...\n27  10 24.97 22.00       2.97\n28  10 23.35 22.00       1.35\n29  10 21.98 22.00      -0.02\n\n[30 rows x 4 columns]\n\n\n\n\n\n\n\n\nRMSE = \\(\\displaystyle\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{e^2}}\\) = 2.67\n\n\nMAE = \\(\\displaystyle\\frac{1}{n} \\sum_{i=1}^{n}|~e~|\\) = 1.43\n\n\n\n\n\n\n\n\n\nModel evaluation\n\n\n\nError functions\n\nRoot-mean-squared error: \\(RMSE = \\displaystyle\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{(y_i -\\hat y_i)^2}}\\)\n\nMean absolute error: \\(MAE = \\displaystyle\\frac{1}{n} \\sum_{i=1}^{n}{|~y_i -\\hat y_i~|}\\) : 극단값들에 덜 민감함\n\n\n\n즉, 데이터셋 sim1과 model 1.1 과의 거리를 RMSE로 정의하면, \\(d=|~sim1 -model1~| = 2.67\\)\n위의 250개의 모델에 대해 각각 거리를 구하면\n\n\n       b0    b1  dist\n0   21.79 -2.92 17.42\n1   -2.83 -0.57 22.83\n2   -6.39  2.16 10.26\n..    ...   ...   ...\n247  0.51  4.19 10.38\n248 27.94 -0.84 11.59\n249 27.93  2.45 25.99\n\n[250 rows x 3 columns]\n\n\n이 중 제일 좋은 모델(dist가 최소) 10개의 모델을 그리면,\n\n\n\n\n\n\n\n\n\n\n250개의 모델 중 10개의 모델을 다음과 같은 \\((\\beta_0, \\beta_1)\\) 평면으로 살펴보면, 즉, model space에서 살펴보면\n\n오렌지 색은 위에서 구한 10 best models\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Introduction to Statistical Learning by James et al.\n\n점차 촘촘한 간격으로 grid search를 하면서 거리를 최소로 하는 모델을 찾아가는 것이고, 실제로는 Newton-Raphson search를 통해 최소값을 구하는 알고리즘을 통해 구할 수 있음.\n즉, 거리를 최소로 하는 \\(\\beta_0\\), \\(\\beta_1\\)를 찾으면,\n\nfrom scipy.optimize import minimize\nminimize(measure_distance, [0, 0], args=(sim1)).x\n\narray([4.22, 2.05])\n\n\n\n\n\n\n\n\n\n\n\n이렇게 squared error가 최소가 되도록 추정하는 것을 ordinary least squares(OLS) estimattion라고 함.\n실제로는 위에서 처럼 grid search를 하지 않고, closed-form solution을 통해 바로 구할 수 있음.",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#maximum-likelihood-estimation",
    "href": "contents/model-basic.html#maximum-likelihood-estimation",
    "title": "Model Basics",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\n데이터가 발생된 것으로 가정하는 분포를 고려했을 때,\n어떨때 주어진 데이터가 관측될 확률/가능도(likelihood)가 최대가 되겠는가로 접근하는 방식으로,\nX, Y의 관계와 확률분포를 함께 고려함.\n\n선형관계라면, 즉 \\(E(Y|X=x_i) = \\beta_0 + \\beta_1x_i\\)   (\\(E\\): expected value, 기대값)\n분포가 Gaussian이라면, 즉 \\(Y|(X=x_i) \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\)   (\\(\\sigma\\): 표준편차)\n\n\nLikelihood \\(L = \\displaystyle\\prod_{i=1}^{n}{P_i}\\)   (관측치들 독립일 때, product rule에 의해)\n분포가 Gaussian이라면(평균: \\(\\mu\\), 표준편차: \\(\\sigma\\)), 즉 \\(f(t) = \\displaystyle\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(t-\\mu)^2}{2\\sigma^2}\\right)\\)라면\n\\(L = \\displaystyle\\prod_{i=1}^{n}{f(y_i, x_i)} = \\displaystyle\\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)}\\)\n이 때, 이 likelihood를 최대화하는 \\(\\beta_0, \\beta_1, \\sigma\\)를 찾는 것이 목표이며,\n이처럼 분포가 Gaussian라면, OLS estimation과 동일한 값을 얻음. (단, \\(\\sigma\\)는 bias가 존재)\n다른 분포를 가지더라도 동일하게 적용할 수 있음!\n\n즉, likelihood의 관점에서 주어진 데이터에 가장 근접하도록(likelihood가 최대가 되는) “분포의 구조”를 얻는 과정임\n\n여러 편의를 위해, log likelihood를 최대화함.\n\n\n\n\n\n\nLog likelihood\n\n\n\n\n\n다음 두가지를 고려하면,\n\\(log(x*y) = log(x) + log(y)\\)\n\\(e^x * e^y = e^{x+y}\\)\n\\(log(L) = \\displaystyle\\sum_{i=1}^{n}{log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}\\right)\\right)} = \\displaystyle\\sum_{i=1}^{n}{-log(\\sqrt{2\\pi\\sigma^2}) - \\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}}\\)\n두 번째 항이 앞서 정의한 squared error와 동일함\n\n\n\n앞서와 마찬가지로 여러 \\(\\beta_0, \\beta_1\\)값을 대입해서 log likelihood를 최대화하는 값을 찾아보면,\n\n\ncreate a grid for b0, b1\n# create a grid for b0: (-20, 40), b1:(-5, 5)\nnp.random.seed(123)\nb0 = np.linspace(-20, 40, 100)\nb1 = np.linspace(-5, 5, 100)\n\n# meshgrid\nb0, b1 = np.meshgrid(b0, b1)\n\nmodels = pd.DataFrame(dict(b0=b0.ravel(), b1=b1.ravel()))\nmodels\n\n\n         b0    b1\n0    -20.00 -5.00\n1    -19.39 -5.00\n2    -18.79 -5.00\n...     ...   ...\n9997  38.79  5.00\n9998  39.39  5.00\n9999  40.00  5.00\n\n[10000 rows x 2 columns]\n\n\n표준편차(\\(\\sigma\\))는 고정하고(2.2), - log likelihood 값을 구해 정렬하면,\n(참고: 마찬가지로 likelihood를 최대화하는 \\(\\sigma\\)값을 찾을 수 있음)\n\n\ncalculate the likelihood\nfrom scipy.stats import norm  # normal distribution\n\ndef likelihood(b, data):\n    mu = b[0] + b[1] * data[\"x\"]\n    sigma = 2.2\n    return -np.sum(np.log(norm.pdf(data[\"y\"], mu, sigma)))\n\nmodels[\"-log likelihood\"] = models.apply(lambda x: likelihood(x, sim1), axis=1)\nmodels.sort_values(\"-log likelihood\")\n\n\n         b0    b1  -log likelihood\n7040   4.24  2.07            65.32\n6941   4.85  1.97            65.53\n7139   3.64  2.17            65.65\n...     ...   ...              ...\n406  -16.36 -4.60              inf\n209  -14.55 -4.80              inf\n0    -20.00 -5.00              inf\n\n[10000 rows x 3 columns]\n\n\nscipy의 minimize 함수를 이용해서 최소값을 구해보면,\n\nfrom scipy.optimize import minimize\nminimize(likelihood, [0, 0], args=(sim1)).x\n\narray([4.22, 2.05])\n\n\n\nfrom statsmodels.formula.api import ols\n\nmod = ols('y ~ x', data=sim1).fit()\ndisplay(mod.summary().tables[0], mod.summary().tables[1])\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.885\n\n\nModel:\nOLS\nAdj. R-squared:\n0.880\n\n\nMethod:\nLeast Squares\nF-statistic:\n214.7\n\n\nDate:\nWed, 23 Oct 2024\nProb (F-statistic):\n1.17e-14\n\n\nTime:\n05:49:40\nLog-Likelihood:\n-65.226\n\n\nNo. Observations:\n30\nAIC:\n134.5\n\n\nDf Residuals:\n28\nBIC:\n137.3\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4.2208\n0.869\n4.858\n0.000\n2.441\n6.001\n\n\nx\n2.0515\n0.140\n14.651\n0.000\n1.765\n2.338\n\n\n\n\n\n\nMaximum likelihood estimation은 다양한 분포의 데이터에 대해서도 적용할 수 있음.\n예를 들어, 다음과 같은 전형적인 Gaussian이 아닌 분포에 대해서도 적용할 수 있음.\nNon-constant variance(왼쪽), Poisson distribution(오른쪽)",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#uncertainty",
    "href": "contents/model-basic.html#uncertainty",
    "title": "Model Basics",
    "section": "Uncertainty",
    "text": "Uncertainty\n관찰된 데이터(표본, sample)로부터 모집단(population)에 대한 정보를 추론할 때, 불확실성이 존재함.\n이는 새로운 데이터에 대한 예측의 불확실성 혹은 일반화(generalization)에 대한 문제와 동일함.\n예를 들어, 과거 병원 기록으로 새로운 환자에 대한 진단을 내리는 경우\n\n이 환자의 고유한 상태로부터 오는 불확실성: 측정된 부분(measured) + 측정되지 않은 부분(unmeasured)\n과거 기록을 통한 진단의 정확성/true relationship에 대한 불확실성\n\n불확실성에 대한 종류\n\n파라미터 값에 대한 불확실성: confidence interval\n\n데이터가 많을 수록\nX가 넓게 분포할 수록\n\n특정 값에 대한 불확실성\n\n평균값(\\(E(Y|X_i)\\))에 대한 불확실성: confidence interval\n예측값(\\(f(X_i)\\))에 대한 불확실성: prediciton interval\n\n\n전통적으로는 분포에 대한 가정으로부터 이론적으로 불확실성을 추론했으나,\n현대적인 접근으로 resampling 방식의 bootstrapping이나 sample을 traing/test set으로 나누는 cross-validation 등을 통해 시뮬레이션을 통해 불확실성을 추정할 수 있음\nBayesian 방식에서는 분포에 대한 가정없이, 불확실성에 대한 분포 자체(posterior predictive distribution)에 대해 엄밀히 추정하는 방식도 있음\n반대로, machine learning에서는 특정 action 혹은 decision-making을 하는 것이 중요한 경우가 많아, 불확실성에 대한 고려가 적은 경향이 있음\n\n\ncalculate confidence intervals\nsim1_new = pd.DataFrame({\"x\": [0.5, 1.5, 4.5, 7.5, 10.5]})\nmod = ols('y ~ x', data=sim1).fit()\npredictions = mod.get_prediction(sim1_new).summary_frame(.1)\n\nplt.fill_between(sim1_new[\"x\"], predictions['obs_ci_lower'], predictions['obs_ci_upper'], alpha=.1, label='90% Prediction interval')\nplt.fill_between(sim1_new[\"x\"], predictions['mean_ci_lower'], predictions['mean_ci_upper'], alpha=.5, label='90% Confidence interval')\nplt.scatter(sim1[\"x\"], sim1[\"y\"], label='Observed', marker='o', color='.6')\nplt.scatter(sim1_new[\"x\"], sim1_new[\"x\"]*0, label='New data', marker='x', color='.2')\nplt.plot(sim1_new[\"x\"], predictions['mean'], label='Regression line')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(frameon=False)\nsns.despine()\nplt.ylim(-0.5, 30)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n0.50\n5.25\n0.81\n3.87\n6.62\n1.26\n9.24\n\n\n1\n1.50\n7.30\n0.69\n6.13\n8.47\n3.37\n11.22\n\n\n2\n4.50\n13.45\n0.43\n12.73\n14.18\n9.64\n17.27\n\n\n3\n7.50\n19.61\n0.49\n18.77\n20.44\n15.77\n23.45\n\n\n4\n10.50\n25.76\n0.81\n24.39\n27.14\n21.77\n29.75\n\n\n\n\n\n\n\n\n아래는 bootstrapping을 통한 실제 표본을 재추출하는 방식이나, 이론적으로 표본 분포(sampling distribution)에 대한 이론을 통해 closed form으로 얻을 수 있음.\n\n\n\n\n\n\n\nSource: The Truthful Art by Albert Cairo.",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#predictive-accuracy",
    "href": "contents/model-basic.html#predictive-accuracy",
    "title": "Model Basics",
    "section": "Predictive Accuracy",
    "text": "Predictive Accuracy\n전통적인 모형에서는\n\n샘플에 대해서 계산된 값은 실제보다 overestimate 되는 경향이 있으므로,\n이를 보정하는 방식으로 계산: adjusted, shrunken\n\n현대적인 방식으로는\n\n샘플을 training/test set으로 나누어서, test set에 대한 예측값을 계산하고, 이를 통해 예측의 정확성을 평가함\n비슷하게, resampling 방식의 bootstrapping을 통해 해결\n\n주로 사용되는 지표들\n\n\\(RMSE = \\displaystyle\\sqrt{{\\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}}{{e^2}}}}\\)\n\\(MAE = \\displaystyle\\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}}|~e~|\\)\n\\(R^2 = 1 - \\displaystyle\\frac{\\frac{1}{n} \\sum_{i=1}^{n}{(e-0)^2}}{\\frac{1}{n} \\sum_{i=1}^{n}{(Y-\\overline{Y})^2}} = 1 - \\frac{V(e)}{V(Y)} = \\frac{V(\\widehat Y)}{V(Y)}\\),   \\(V(Y) = V(\\widehat Y) + V(e)\\)\n\n전통적으로 X가 Y를 얼마나 잘 “설명”해주는지에 대한 지표로서 \\(R^2\\)를 사용함\n\n정규분포에서 벗어난 경우, 분포를 고려한 (log)likelihood를 기반으로 한 지표들: AIC, BIC, …\n\n\\(AIC = -2log(likelihood) + 2k\\),   \\(k\\): 모델의 자유도\n\\(BIC = -2log(likelihood) + klog(n)\\),   \\(n\\): 데이터의 수\n\n\n\n\ncalculate RMSE, MAE, R2\nfrom statsmodels.tools.eval_measures import rmse, meanabs, aic, bic\nypred = mod.predict(sim1)\ny = sim1[\"y\"]\nllf = mod.llf # log likelihood\n\nprint(f\"RMSE = {rmse(y, ypred):.2f} \\nMAE = {meanabs(y, ypred):.2f} \\nR-squared = {mod.rsquared:.2f} \\nAIC = {aic(llf, 30, 2):.2f} \\nBIC = {bic(llf, 30, 2):.2f}\")\n\n\nRMSE = 2.13 \nMAE = 1.71 \nR-squared = 0.88 \nAIC = 134.45 \nBIC = 137.25",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#predictions-the-pattern-that-the-model-has-captured",
    "href": "contents/model-basic.html#predictions-the-pattern-that-the-model-has-captured",
    "title": "Model Basics",
    "section": "Predictions: the pattern that the model has captured",
    "text": "Predictions: the pattern that the model has captured\n우선, 예측 변수들의 데이터 값을 커버하는 grid를 구성\n\nsim1\n\n     x     y\n0    1  4.20\n1    1  7.51\n2    1  2.13\n..  ..   ...\n27  10 24.97\n28  10 23.35\n29  10 21.98\n\n[30 rows x 2 columns]\n\n\n\n# create a grid for the range of x sim1: new data\ngrid = pd.DataFrame(dict(x=np.linspace(sim1.x.min(), sim1.x.max(), 10)))\n\n모델에 grid를 입력하여 prediction값을 추가\n\n# a model for sim1\nfrom statsmodels.formula.api import ols\nsim1_mod = ols(\"y ~ x\", data=sim1).fit()\n\ngrid[\"pred\"] = sim1_mod.predict(grid) # column 이름이 매치되어야 함\ngrid\n\n       x  pred\n0   1.00  6.27\n1   2.00  8.32\n2   3.00 10.38\n..   ...   ...\n7   8.00 20.63\n8   9.00 22.68\n9  10.00 24.74\n\n[10 rows x 2 columns]\n\n\nprediction을 시각화\n\n\nShow the code\n(\n    so.Plot(sim1, x='x', y='y')\n    .add(so.Dot(color=\".8\"))\n    .add(so.Line(marker=\".\", pointsize=10), x=grid.x, y=grid.pred)  # prediction!\n    .layout(size=(4.5, 3.5))\n    .scale(x=so.Continuous().tick(at=grid.x))\n)",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#residuals-what-the-model-has-missed.",
    "href": "contents/model-basic.html#residuals-what-the-model-has-missed.",
    "title": "Model Basics",
    "section": "Residuals: what the model has missed.",
    "text": "Residuals: what the model has missed.\n\\(e = Y - \\hat{Y}\\) : 관측값 - 예측값\n\nsim1[\"resid\"] = sim1_mod.resid  # Y - Y_hat\n\n\nsim1\n\n     x     y  fitted  resid\n0    1  4.20    6.27  -2.07\n1    1  7.51    6.27   1.24\n2    1  2.13    6.27  -4.15\n..  ..   ...     ...    ...\n27  10 24.97   24.74   0.23\n28  10 23.35   24.74  -1.39\n29  10 21.98   24.74  -2.76\n\n[30 rows x 4 columns]\n\n\n우선, residuals의 분포를 시각화해서 살펴보면,\n\nsim1[\"resid\"].hist(bins=20)\nplt.show()\n\n\n\n\n\n\n\n\n예측 변수와 residuals의 관계를 시각화해서 보면,\n\n(\n    so.Plot(sim1, x='x', y='resid')\n    .add(so.Dot())\n    .add(so.Line(), so.PolyFit(5))\n    .layout(size=(5, 4))\n)\n\n\n\n\n\n\n\n\n위의 residuals은 특별한 패턴을 보이지 않아야 모델이 데이터의 패턴을 잘 잡아낸 것으로 판단할 수 있음.\n아래는 원래 데이터와 일차 선형 모형에 대한 예측값의 관계를 시각화한 것\n\n\n\n\n\n\n\n\n\nResiduals에 패턴이 보이는 경우",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#two-continuous",
    "href": "contents/model-basic.html#two-continuous",
    "title": "Model Basics",
    "section": "Two continuous",
    "text": "Two continuous\n두 연속변수가 서로 상호작용하는 경우: not additive, but multiplicative\n\n각각의 효과가 더해지는 것을 넘어서서 서로의 효과를 증폭시키거나 감소시키는 경우\n강수량과 풍속이 함께 항공편의 지연을 가중시키는 경우\n운동량과 식사량이 함께 체중 감량에 영향을 미치는 경우\n\n\n\nShow the code\nnp.random.seed(123)\nx1 = np.random.uniform(0, 10, 200)\nx2 = 2*x1 - 1 + np.random.normal(0, 12, 200)\ny = x1 + x2 + x1*x2 + np.random.normal(0, 50, 200)\ndf = pd.DataFrame(dict(precip=x1, wind=x2, delay=y))\ndf\n\n\n     precip   wind  delay\n0      6.96   4.04  95.70\n1      2.86   5.60  31.23\n2      2.27   8.37 -30.97\n..      ...    ...    ...\n197    7.45  16.38 186.67\n198    4.73 -18.56 -96.90\n199    1.22  -5.63 -22.95\n\n[200 rows x 3 columns]\n\n\n\n# additive model\nmod1 = ols('delay ~ precip + wind', data=df).fit()\n\n# interaction model\nmod2 = ols('delay ~ precip + wind + precip:wind', data=df).fit()\n\nmod2: y ~ x1 + x2 + x1:x2는 \\(\\hat{y} = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\) 로 변환되고,\n변형하면, \\(\\hat{y} = a_0 + a_1x_1 + (a_2 + a_3x_1)x_2\\)",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/model-basic.html#continuous-and-categorical",
    "href": "contents/model-basic.html#continuous-and-categorical",
    "title": "Model Basics",
    "section": "Continuous and Categorical",
    "text": "Continuous and Categorical\n연속변수와 범주형 변수가 서로 상호작용하는 경우\n\n운동량이 건강에 미치는 효과: 혼자 vs. 단체\n\nData: sim3.csv\n\nsim3 = pd.read_csv(\"data/sim3.csv\")\nsim3\n\n     x1 x2  rep     y  sd\n0     1  a    1 -0.57   2\n1     1  a    2  1.18   2\n2     1  a    3  2.24   2\n..   .. ..  ...   ...  ..\n117  10  d    1  6.56   2\n118  10  d    2  5.06   2\n119  10  d    3  5.14   2\n\n[120 rows x 5 columns]\n\n\n\n\nCode\n(\n    so.Plot(sim3, x='x1', y='y', color='x2')\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(), so.PolyFit(5), color=None)\n)\n\n\n\n\n\n\n\n\n\n두 가지 모델로 fit할 수 있음\n\nmod1 = ols('y ~ x1 + x2', data=sim3).fit()\nmod2 = ols('y ~ x1 * x2', data=sim3).fit() # 같은 의미 'y ~ x1 + x2 + x1:x2'\n\nformula y ~ x1 * x2는 \\(\\hat{y} = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\)로 변환됨\n\n하지만, 여기서는 x2가 범주형 변수라 dummy-coding후 적용됨.\nDesign matrix를 확인해 보면,\n\ny, X = dmatrices(\"y ~ x1 + x2\", data=sim3, return_type=\"dataframe\")\nX.iloc[:, 1:]\n\n     x2[T.b]  x2[T.c]  x2[T.d]    x1\n0       0.00     0.00     0.00  1.00\n1       0.00     0.00     0.00  1.00\n2       0.00     0.00     0.00  1.00\n..       ...      ...      ...   ...\n117     0.00     0.00     1.00 10.00\n118     0.00     0.00     1.00 10.00\n119     0.00     0.00     1.00 10.00\n\n[120 rows x 4 columns]\n\n\n\ny, X = dmatrices(\"y ~ x1 * x2\", data=sim3, return_type=\"dataframe\")\nX.iloc[:, 1:]\n\n     x2[T.b]  x2[T.c]  x2[T.d]    x1  x1:x2[T.b]  x1:x2[T.c]  x1:x2[T.d]\n0       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n1       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n2       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n..       ...      ...      ...   ...         ...         ...         ...\n117     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n118     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n119     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n\n[120 rows x 7 columns]\n\n\n\ngrid = sim3.value_counts([\"x1\", \"x2\"]).reset_index().drop(columns=\"count\")\ngrid[\"mod1\"] =  mod1.predict(grid)\ngrid[\"mod2\"] =  mod2.predict(grid)\ngrid_long = grid.melt(id_vars=[\"x1\", \"x2\"], var_name=\"model\", value_name=\"pred\")\ngrid_full = grid_long.merge(sim3[[\"x1\", \"x2\", \"y\"]])\n\n\ngrid_full\n\n     x1 x2 model  pred     y\n0     1  a  mod1  1.67 -0.57\n1     1  a  mod1  1.67  1.18\n2     1  a  mod1  1.67  2.24\n..   .. ..   ...   ...   ...\n237  10  d  mod2  3.98  6.56\n238  10  d  mod2  3.98  5.06\n239  10  d  mod2  3.98  5.14\n\n[240 rows x 5 columns]\n\n\n\n\nCode\n(\n    so.Plot(grid_full, x=\"x1\", y=\"y\", color=\"x2\")\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(), y=\"pred\")\n    .facet(\"model\")\n    .layout(size=(8, 5))\n)\n\n\n\n\n\n\n\n\n\n\ninteraction이 없는 모형 mod1의 경우, 네 범주에 대해 기울기가 동일하고 절편의 차이만 존재\ninteraction이 있는 모형 mod2의 경우, 네 범주에 대해 기울기가 다르고 절편도 다름\n\n\n\\(y = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\)에서 \\(x_1x_2\\)항이 기울기를 변할 수 있도록 해줌 \\(y = a_0 + a_2x_2 + (a_1 + a_3x_2)x_1\\)으로 변형하면, \\(x_1\\)의 기울기는 \\(a_1 + a_3 x_2\\)\n\n\n\n\n\n\n\nFitted models\n\n\n\n\n\nmod1 = ols('y ~ x1 + x2', data=sim3).fit()\nmod1.params\n# Intercept    1.87\n# x2[T.b]      2.89\n# x2[T.c]      4.81\n# x2[T.d]      2.36\n# x1          -0.20\n\nmod2 = ols('y ~ x1 * x2', data=sim3).fit() # 같은 의미 'y ~ x1 + x2 + x1:x2'\nmod2.params\n# Intercept     1.30\n# x2[T.b]       7.07\n# x2[T.c]       4.43\n# x2[T.d]       0.83\n# x1           -0.09\n# x1:x2[T.b]   -0.76\n# x1:x2[T.c]    0.07\n# x1:x2[T.d]    0.28\n\n\n\n두 모형을 비교하여 중 더 나은 모형을 선택하기 위해, residuals을 차이를 살펴보면,\n\nsim3[\"mod1\"] = mod1.resid\nsim3[\"mod2\"] = mod2.resid\n\nsim3_long = sim3.melt(\n    id_vars=[\"x1\", \"x2\"],\n    value_vars=[\"mod1\", \"mod2\"],\n    var_name=\"model\",\n    value_name=\"resid\",\n)\nsim3_long\n\n     x1 x2 model  resid\n0     1  a  mod1  -2.25\n1     1  a  mod1  -0.49\n2     1  a  mod1   0.56\n3     1  b  mod1   2.87\n..   .. ..   ...    ...\n236  10  c  mod2  -0.64\n237  10  d  mod2   2.59\n238  10  d  mod2   1.08\n239  10  d  mod2   1.16\n\n[240 rows x 4 columns]\n\n\n\n\nCode\n(\n    so.Plot(sim3_long, x=\"x1\", y=\"resid\", color=\"x2\")\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(linestyle=\":\", color=\".5\"), so.Agg(lambda x: 0))\n    .facet(\"x2\", \"model\")\n    .layout(size=(9, 6))\n    .scale(color=\"Set2\")\n)\n\n\n\n\n\n\n\n\n\n\n둘 중 어떤 모델이 더 나은지에 대한 정확한 통계적 비교가 가능하나 (잔차의 제곱의 평균인 RMSE나 잔차의 절대값의 평균인 MAE 등)\n여기서는 직관적으로 어느 모델이 데이터의 패턴을 더 잘 잡아냈는지를 평가하는 것으로 충분\n잔차를 직접 들여다봄으로써, 어느 부분에서 어떻게 예측이 잘 되었는지, 잘 안 되었는지를 면밀히 검사할 수 있음\ninteraction 항이 있는 모형이 더 나은 모형\n\nSaratogaHouses 데이터에서 가령, livingArea와 centralAir의 interaction을 살펴보면,\n\nhouses = sm.datasets.get_rdataset(\"SaratogaHouses\", \"mosaicData\").data\n(\n    so.Plot(houses, x='livingArea', y='price')\n    .add(so.Dots(color='.6'))\n    .add(so.Line(color=\"orangered\"), so.PolyFit(1))\n    .facet(\"centralAir\")\n    .label(title=\"Central Air: {}\".format)\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\nmod1 = ols('price ~ livingArea + centralAir', data=houses).fit()\nmod2 = ols('price ~ livingArea * centralAir', data=houses).fit()\n\ndisplay(mod1.params, mod2.params)\n\nIntercept           14144.05\ncentralAir[T.Yes]   28450.58\nlivingArea            106.76\ndtype: float64\n\n\nIntercept                       44977.64\ncentralAir[T.Yes]              -53225.75\nlivingArea                         87.72\nlivingArea:centralAir[T.Yes]       44.61\ndtype: float64\n\n\nR-squared 비교\n\ndisplay(mod1.rsquared, mod2.rsquared)\n\n0.5253223149339137\n\n\n0.5430362101820772",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/regularization.html",
    "href": "contents/regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n앞서 flexibility를 조정하기 위해 여러 모델을 탐색적으로 살펴보면서 교차검증을 통해 특정 모델을 선택했다면,\n이번에는 파라미터를 추정하는 과정에서 알고리즘적으로 optimal한 bias-variance trade-off를 찾아 파라미터 값을 조정하는 방식을 알아보고자 함.\n함수 \\(h(x) = sin(2\\pi x)\\)로부터 생성된 데이터셋(N=10)에 대해 다항식의 차수에 따른 flexibility의 변화에 따른 OLS 모델들을 비교하면,",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Regularization"
    ]
  },
  {
    "objectID": "contents/regularization.html#ridge-regression",
    "href": "contents/regularization.html#ridge-regression",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\n\\(\\lambda\\)가 커질수록 \\(\\beta\\)의 \\(l_2\\) norm은 항상 줄어듦\n\n\\(l_2\\) norm: \\(\\displaystyle ||\\beta||_2 = \\sqrt{\\sum_{j=1}^{p} \\beta_j^2}\\)  : 벡터 \\(\\beta = (\\beta_1, \\beta_2, ..., \\beta_p)\\)의 길이\n\n\n\nSource: p. 241, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n다음과 같이 이상적이지 못한 경우에 regularization를 통해 적절한 bias-variance trade-off를 찾을 수 있음.\n\n에를 들어, 예측변수가 데이터에 비해 매우 많은 경우 (50개의 관측치에 45개의 예측변수)\n\n즉, OLS에서 variance가 높은 경우에 특히 유용함\n심지어, p &gt; n인 경우에도 작동함\n\n상대적으로 variance의 빠른 감소 효과 덕분에 (아래 그림), ridge regression은 OLS보다 더 좋은 예측 성능을 보임\n\nbias에 대한 약간의 손해를 보지만, variance에 대해서는 큰 이익을 얻어 정확한 예측(test error를 줄임) 가능\n\n파라미터가 작아지지만, 0이 되지는 않아, 모델의 해석이 용이하지는 않음\nshrinkage penalty 부분으로 인한 estimation에 대한 계산량의 증가가 거의 없도록 알고리즘 개발; 매우 효율적\n\n\n\nSource: p. 243, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n앞서 \\(y = sin(2\\pi x)\\)로부터 생성된 데이터셋(N=10)에 대해 다항식의 차수에 따른 flexibility의 변화에 따른 OLS 모델과 Ridge regression 모델을 비교하면,\n여기서 각각 \\(\\lambda = 0.005\\), \\(\\lambda = 1\\)",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Regularization"
    ]
  },
  {
    "objectID": "contents/regularization.html#lasso-regression",
    "href": "contents/regularization.html#lasso-regression",
    "title": "Regularization",
    "section": "Lasso regression",
    "text": "Lasso regression\n\\(\\lambda\\)가 커질수록 \\(\\beta\\)의 \\(l_1\\) norm은 항상 줄어듦\n\n\\(l_1\\) norm: \\(\\displaystyle ||\\beta||_1 = \\sum_{j=1}^{p} |\\beta_j|\\)\n\nRidge와는 달리 파라미터가 0이 될 수 있음: 변수 선택(variable/feature selection)이 가능함\n이는 변수가 많은 경우 파라미터 해석을 용이하게 함\n\n\\(\\lambda\\) 값에 따라 임의의 갯수의 변수를 포함할 수 있음\n\n\n\nSource: p. 245, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\nRidge와 마찬가지로 bias에 대한 약간의 손해를 보지만, variance에 대해서는 큰 이익을 얻어 정확한 예측(test error를 줄임) 가능\n그림 6.5와 동일한 데이터셋 (p = 45, n = 50)\n\n\nSource: p. 248, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\nRidge vs. Lasso\nRidge\n\n모든 변수를 포함하기 때문에(극히 작은 파라미터값을 포함) 종종 파라미터를 해석하기 어려워짐\n모든 예측변수들이 골고루 Y와 관련이 있을 때 높은 예측성능을 보임\n\n즉, 예측변수들이 비슷한 효과를 가질 때\n그림 6.8의 오른편에서 variance가 상대적으로 낮게 나타남.\n실제로 45개의 예측변수가 모두 Y와 관련되도록 생성된 데이터셋임.\n\n모든 계수들이 비슷한 “비율”로 줄어듬\n\nLasso\n\n일부 계수가 0이 되어 변수 선택이 가능하며, 따라서 해석이 용이해짐\n예측변수들 중 일부만이 Y와 관련이 있을 때 높은 예측성능을 보임\n\n즉, 상대적으로 소수의 예측변수들만이 큰 효과를 가질 때\n그림 6.9의 오른편에서 bias가 크게 작으며, variance도 낮음; 즉 모든 면에서 더 좋은 성능을 보임\n실제로 45개의 예측변수 중 2개만이 Y와 관련되도록 생성된 데이터셋임.\n\n모든 계수들이 비슷한 “크기”로 줄어듬; 작은 계수들은 0이 될 수 있음\n\n\n\nSource: p. 249, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n현실에서는 변수들 간의 true relationship을 미리 알 수 없으므로,\n적절한 모델의 선택과 \\(\\lambda\\)를 찾기 위해 cross-validation을 사용함\n\n\n\n\n\n\n\nLinear model selection\n\n\n\n앞서 shrinkage 방식으로 variance을 감소시키는 방법을 다루었는데, variance를 줄이는 다른 접근 방식이 있음.\n\nSubset Selection: 예측변수들 중 Y와 연관되어 있다고 생각되는 변수들을 선택해 변수의 수를 줄이는 방법\n\n설명력이 높은 조합의 변수들을 걸러내는 여러 방법이 있음\n어떤 기준으로 계산하느냐에 따라 결과가 달라질 수 있음\n많은 계산량이 필요하며, 변수의 수가 많을 때는 적용하기 어려움\nBest Subset Selection, Forward Stepwise Selection, Backward Stepwise Selection\n\nDimension Reduction: 예측변수들의 선형 조합으로 새로운 변수들을 얻어(설명 변량의 큰 손실없이) 변수의 갯수를 줄이고자 함\n\nPrincipal Component Analysis (PCA); unsupervised learning\nPartial Least Squares (PLS); supervised learning\n이 외에도 다양한 방법이 있음;",
    "crumbs": [
      "Trees",
      "Machine Learning Basics",
      "Regularization"
    ]
  },
  {
    "objectID": "contents/statistics.html",
    "href": "contents/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n통계 분석은 크게 세 가지 주제로 나눌 수 있음.\n현대적 접근에서는 모호한 모집단에 대해 추론하기보다는 표본 내의 정보만으로 일반화(generalization)을 성취하고자 함.",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#simple-regressioncorrelation",
    "href": "contents/statistics.html#simple-regressioncorrelation",
    "title": "Statistics",
    "section": "Simple Regression/Correlation",
    "text": "Simple Regression/Correlation\n두 변수 간의 correlation(상관관계)는 두 변수 간의 영향관계에 대한 방향성을 전제하지 않는 반면,\n회귀분석은 한 변수가 다른 변수에 영향 미친다는 것을 전체로 하고, 그 영향의 형태와 크기를 분석.\n예측변수가 한 개인 회귀분석: Simple Regression\n\n두 변수 간의 관계(association)을 파악: \\(Y=f(X)=b_0 + b_1X\\)\n그 관계의 크기(strength)를 측정\n\n\\(f\\)에 의해 \\(X\\)로 \\(Y\\)를 얼마나 정확히 예측할 수 있는가?\n\\(f\\)에 의해 \\(X\\)의 변량이 \\(Y\\)의 변량을 얼마나 설명할 수 있는가?\n\n\n\n\n\n\n\n\n\nPearson’s correlation coefficient: \\(r\\)\nLinear relationships을 측정\n\nx와 y의 선형적 연관성: [-1, 1]\n\nx로부터 y를 얼마나 정확히 예측가능한가?\nx와 y의 정보는 얼마나 중복(redundant)되는가?\n\n\n\n\n\n\n\n\n\nMultiple correlation coefficient: \\(R\\)\nExtented correlation: 예측치와 관측치의 pearson’s correlation\n\n\\(R\\)을 제곱한 \\(R^2\\)가 설명력의 정도를 나타냄\n\n\n\n\n\n\n\\(r\\): Pearson correlation coefficient\n\n\\(r_{XY} = \\displaystyle 1 - \\frac{\\sum{(z_X - z_Y)^2}}{2n}\\)   \\(z_X, z_Y\\) : 각각 standardized \\(X, Y\\)\n\n\\(R\\): Multiple correlation coefficient\n\n\\(Y\\) 와 \\(\\widehat Y\\) 의 Pearson correlation 즉, Y와 회귀모형이 예측한 값의 (선형적) 상관 관계의 정도; 회귀모형의 예측의 정확성\n\n다시말하면, 예측변수들의 최적의 선형 조합과 Y의 상관 관계의 정도.\n\n\\(R^2\\): Coefficient of determination, 결정계수, 설명력\n\n선형모형에 의해 설명된 Y 변량의 비율:\n\n또는 예측변수들의 최적의 선형 조합에 의해 설명된 Y 변량의 비율.\n\n  즉, \\(\\displaystyle\\frac{V(\\widehat{Y})}{V(Y)}\\) 또는 \\(\\displaystyle 1 - \\frac{V(e)}{V(Y)}\\)\n\nAssociatiions과 그 strengths 비교\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n카테고리 변수에 대해서도 비슷하게 생각할 수 있음.\n이 경우, 두 그룹 간의 차이에 대한 효과의 크기를 말할 수 있고, \\(R^2\\) 이외에도 Cohen’s d로 표현할 수 있음.\n예를 들어, 결혼과 삶의 만족도 간의 관계(association)와 그 크기(strength)\n\n\n\n\n\n\n\nImportant\n\n\n\n인과 관계에 대한 섯부른 추론은 금물!\n특히, 예측력이 낮은 경우; Leo Breiman의 중요 요지 중 하나",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#multiple-regression",
    "href": "contents/statistics.html#multiple-regression",
    "title": "Statistics",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n예측변수가 2개 이상인 경우: 변수들 간의 진실한 관계를 분석\n미혼자에 대한 임금 차별이 있는가? 차별이 의미하는 바는 무엇인가?\n연령을 고려한 후에도 기혼자의 임금은 미혼자보다 높은가?\n여전히 높다면, 연령을 고려한 후 혹은 연령을 조정한 후(adjusted for age)의 차이는 얼마라고 봐야하는가?\n연령을 고려한 임금 차이를 조사하는 방법은 무엇이 있겠는가?; 연령별로 나누어 비교?\nData from the 1985 Current Population Survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n연령을 고려한 마라톤 기록?\n70세 노인과 20세 청년이 동일하게 2시간 30분의 기록을 세웠다면?\n\n“나이 차이가 큰 두 사람의 기록을 비교하는 것은 공평하지 않아”\n나이를 감안한 마라톤 실력?\n다시 말하면, 나이와는 무관한/독립적인 마라톤 능력에 대해 말하고자 함\n이는 동일한 나이의 사람들로만 제한해서 마라톤 기록을 비교하는 것이 공평한 능력의 비교라고 말하는 것과 같은 이치임\n\n\n\nSource: https://doi.org/10.1186/2052-1847-6-31",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#regression-analysis",
    "href": "contents/statistics.html#regression-analysis",
    "title": "Statistics",
    "section": "Regression analysis",
    "text": "Regression analysis\n예측 모형 vs. 인과 모형\n\n인과적 연관성을 탐구하고자 한다면 매우 신중한 접근을 요함\n\nSource: Cohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences (3rd ed.)\n교수의 연봉(salary)이 학위를 받은 후 지난 시간(time since Ph.D.)과 출판물의 수(pubs)에 의해 어떻게 영향을 받는가?\n\n\n\n\n\n\n\n\n\n\n\n\n\nData: c0301dt.csv\n\nacad0 = pd.read_csv(\"data/c0301dt.csv\")\nacad0.head(5)\n\n   time  pubs  salary\n0     3    18   51876\n1     6     3   54511\n2     3     2   53425\n3     8    17   61863\n4     9    11   52926\n\n\n\nfrom statsmodels.formula.api import ols\n\nmod1 = ols(\"salary ~ time\", data=acad0).fit()\nmod2 = ols(\"salary ~ pubs\", data=acad0).fit()\nmod3 = ols(\"salary ~ time + pubs\", data=acad0).fit()\n\n\n\n\n\n\n\nIntercept   43658.59\ntime         1224.39\ndtype: float64\n\n\nIntercept   46357.45\npubs          335.53\ndtype: float64\n\n\nIntercept   43082.39\ntime          982.87\npubs          121.80\ndtype: float64\n\n\n\n세 모형을 비교하면,\nModel 1: \\(\\widehat{salary} = \\$1,224\\:time + \\$43,659\\)\nModel 2 : \\(\\widehat{salary} = \\$336\\:pubs + \\$46,357\\)\nModel 3: \\(\\widehat{salary} = \\$983\\:time + \\$122\\:pubs + \\$43,082\\)\n\n연차(time)의 효과는 $1,224에서 $984로 낮아졌고,\n논문수(pubs)의 효과는 $336에서 $122로 낮아졌음.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n교수들의 연차와 그들이 쓴 논문 수는 깊이 연관되어 있으며 (r = 0.66), 두 변수의 redunancy가 각 변수들의 효과를 변화시킴.\n두 예측 변수의 산술적 합으로 연봉을 예측하므로 각 예측변수의 효과는 수정될 수 밖에 없음.\n수학적으로 보면, 각 예측변수의 기울기는 다른 예측변수의 값에 상관없이 일정하므로, 다른 예측변수들을 (임의의 값에) 고정시키는 효과를 가짐\n즉, 다른 변수와는 독립적인, 고유한 효과를 추정하게 됨\n\n각 회귀계수를 partial regression coefficient (부분 회귀 계수) 라고 부름.\n부분 회귀 계수의 첫번째 해석:\n\n만약 논문 수가 일정할 때, 예를 들어 10편의 논문을 쓴 경우만 봤을 때, 연차가 1년 늘 때마다 연봉은 $984 증가함; 평면의 선형모형을 가정했기에 이 관계는 논문 수에 상관없음.\n\n연차가 일정할 때, 예를 들어 연차가 12년차인 경우만 봤을 때, 논문이 1편 늘 때마다 연봉은 $122 증가함; 평면의 선형모형을 가정했기에 이 관계는 연차에 상관없음.\n\n이는 다른 변수를 고려 (통제, controlling for) 했을 때 혹은 다른 변수의 효과를 제거 (partial out) 했을 때, 각 변수의 고유한 효과를 의미함; holding constant, controlling for, partialing out, adjusted for, residualizing\n뒤집어 말하면, 연차만 고려했을때 연차가 1년 늘면 $1,224 연봉이 증가하는 효과는 연차가 늘 때 함께 늘어나는 논문 수의 효과가 함께 섞여 나온 효과라고 말할 수 있음.\n이는 인과관계에 있는 변수들의 진정한 효과를 찾는 것이 얼마나 어려운지를 보여줌\n부분 회귀 계수에 대한 두번째 해석\n\n다른 변수들이 partial out 된 후의 효과.\n\n실제로 $122는 연차로 (선형적으로) 예측/설명되지 않는 논문수(residuals)로 [연차로 예측/설명되지 않는] 연봉을 예측할 때의 기울기\n\n  \nDirect and Indirect Effects\n만약, 다음과 같은 인과모형을 세운다면,\n\n\n연차가 연봉에 미치는 효과가 두 경로로 나뉘어지고,\n연차 \\(\\rightarrow\\) 연봉: 직접효과 $983\n연차 \\(\\rightarrow\\) 논문 \\(\\rightarrow\\) 연봉: 간접효과 1.98 x $122 = $241.56\n두 효과를 더하면: $983 + $241.56 = $1224.56 = 논문수를 고려하지 않았을 때 연차의 효과\n\n즉, 연차가 1년 늘때 연봉이 $1224 증가하는 것은 연차 자체의 효과($983)와 논문의 증가에 따른 효과($241)가 합쳐져 나온 결과라고 말할 수 있음.\n\n이 때, 논문 수를 통한 효과는 연차가 연봉에 미치는 하나의 기제(mechanism)이라고 볼 수 있음.\n\nStrength of Associations\n연차와 논문 수로 연봉을 예측했을 때의 \\(R^2 = 0.53\\)\nCorrelations with salary\n\n\n\n\n\n\n\n\n\n\n\\(r\\) (simple)\n\\(pr\\) (partial)\n\\(sr\\) (semi-partial)\n\n\n\n\ntime\n0.71\n0.53\n0.43\n\n\npubs\n0.59\n0.23\n0.16\n\n\n\n\n\n \n\n\n\n\n\n\n\\(r^2\\)\n\\(pr^2\\)\n\\(sr^2\\)\n\n\n\n\ntime\n0.50\n0.28\n0.18\n\n\npubs\n0.35\n0.05\n0.03\n\n\n\n\n\n\n만약, 예를 들어 연차의 효과 $1224이 논문수를 고려했을 때 줄어든($983) 수준을 훨씬 넘어 통계적으로 유의하지 않을 정도로 0에 가까워진다면, 연차의 효과는 모두 논문의 효과를 거쳐 나타나는 것이라고 말할 수 있음. 이 때, 완전 매개 (fully mediate)한다고 표현함.\n\nSpurious Relationships\n반대로, 만약 다음과 같이 연차를 고려했을 때 논문수(pubs)의 효과가 거의 사라진다면,\n논문수(pubs)와 연봉(salary)의 관계는 spurious한 관계라고 잠정적으로 말할 수 있음.\n연차를 논문수와 연봉의 common cause 라고 말하며, confounding이 되어 논문수와 연봉의 인과관계는 실제로 없을 수 있음을 암시함.\n\n\n\n\n\n\n\nImportant\n\n\n\n요약하면,\n\n회귀분석을 통해, 변수들 간의 관계를 파악하고, 그 관계의 크기를 추정\n그 관계가 얼마나 일반화될 수 있는지를 추론\n인과 관계 추론에 대한 위험성을 인지하고, 신중한 접근이 필요함\n\n\n\n\nSaratoga Houses dataset\n\\(\\widehat{price} = 36668.9 + 125.4 \\cdot livingArea - 14196.8 \\cdot bedrooms\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n비슷한 넓이의 집들로 나누어 보면,",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/statistics.html#uncertainty",
    "href": "contents/statistics.html#uncertainty",
    "title": "Statistics",
    "section": "Uncertainty",
    "text": "Uncertainty\n관찰자가 관찰한 대상으로부터 얻은 결과를 관찰하지 않은 더 넓은 대상으로 일반화할 수 있는가?\n가령, 다음과 같이 150명에 대해 조사한 “연령이 임금에 미치는 효과”를 일반화 할 수 있는가?\n한 나라의 국민 전체?\n\nStatistical inference (통계적 추론)\n통계학의 추론(statistical inference)은 작은 샘플(sample)로부터 얻은 분석 결과를 바탕으로 모집단(population)이라고 부르는 전체에 대해 말하고자 하는 시도에서 비롯되었음\n\n농업 분야에서 시작; 비료/종자의 효과\n사람에게도 적용될 수 있는가?\n\n앞서 논의한 모든 내용은 “특정 샘플” 내에서 변수들 간의 관계에 대한 분석임.\n통계적 추론은 수많은 같은 수의 샘플들, 가령 N = 150인 즉, 150명으로 이루어진 샘플들을 반복적으로 관찰한다면 그 샘플들 간의 편차들이 어떠하겠는가에 대한 논의임.\n\n\n\n\n\n\n\nSource: The Truthful Art by Albert Cairo.\n\n\n\n\n\n\n\n샘플들로부터 나타나는 임금 차이 값의 분포 &gt;&gt; 남녀 임금 차이가 편차는 어떠한가?\n이 분포를 sampling distribution(표본 분포)이라고 부름\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n평균이 $2.27이고, 임금 차이 값들의 95%가 $1.47 ~ $3.04 범위에 있음을 알 수 있음.\n연구자가 관찰한 샘플로부터 연구자는 매우 큰 확신(95%)을 갖고 남녀의 시간당 임금의 차이는 1.47달러에서 3.04달러 사이에 있을 것이라고 말할 수 있음.\n\n\n비슷하게, 나이(age)와 시간당 임금(wage)의 true relationship에 대해서도\n샘플마다 age와 wage의 관계는 다르게 나타날 것임 (두번째 그림).\n\n예를 들어, 샘플들로부터 나타나는 기울기들의 분포를 살펴봄으로써 (세번째 그림): sampling distribution\n이 분포에 따르면 평균이 0.066이고, 기울기 값들의 95%가 0.005 ~ 0.140 범위에 있음을 알 수 있음.\n연구자가 관찰한 샘플로부터 연구자는 (age와 wage의 선형성을 가정한다면), 매우 큰 확신을 갖고 나이가 10세 늘때마다 시간당 임금의 증가율은 0.05에서 1.4달러 사이에 있을 것이라고 말할 수 있음.\n\n\n\n\n\n\n\n\nSource: The Truthful Art by Albert Cairo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis testing\nNull hypothesis(영가설)에 대한 테스트\n즉, coefficient가 0인지 아닌지에 대한 테스트\ncp3.csv\n\nfrom statsmodels.formula.api import ols\ncps = pd.read_csv('data/cps3.csv')\nmod = ols(\"wage ~ married + sex + age\", data=cps).fit()\nmod.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nwage\nR-squared:\n0.125\n\n\nModel:\nOLS\nAdj. R-squared:\n0.107\n\n\nMethod:\nLeast Squares\nF-statistic:\n6.956\n\n\nDate:\nTue, 26 Mar 2024\nProb (F-statistic):\n0.000208\n\n\nTime:\n23:01:35\nLog-Likelihood:\n-433.48\n\n\nNo. Observations:\n150\nAIC:\n875.0\n\n\nDf Residuals:\n146\nBIC:\n887.0\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4.4946\n1.349\n3.332\n0.001\n1.829\n7.161\n\n\nmarried[T.Single]\n-0.3592\n0.764\n-0.470\n0.639\n-1.870\n1.152\n\n\nsex[T.M]\n2.4337\n0.721\n3.374\n0.001\n1.008\n3.859\n\n\nage\n0.0880\n0.031\n2.834\n0.005\n0.027\n0.149\n\n\n\n\n\n\n\n\nOmnibus:\n31.384\nDurbin-Watson:\n2.127\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n45.207\n\n\nSkew:\n1.130\nProb(JB):\n1.53e-10\n\n\nKurtosis:\n4.458\nCond. No.\n153.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nt값에 대한 해석\n예측변수 \\(X_j\\) 에 대해서 모집단의 회귀계수 \\(b_j\\) 에 대한 표본 분포는 평균이 \\(b_j\\) 인 정규분포를 따르고, 표준편차, 즉 standard error는 근사적으로 다음과 같음.\n\\(\\displaystyle SE^2(b_j) = \\frac{{MS}_{residual}}{N \\cdot Var(X_j) \\cdot (1 - R^2_j)}, ~(df = N-k-1)\\)\n\n표본이 클수록\n평균 잔차가 작을수록\njth 예측변수의 값이 퍼져 있을수록\n다른 예측변수들로부터 jth 예측변수가 예측되지 못할수록; 즉 다른 변수들과 correlate되지 않을수록\n\n\n\n\n\n\n\nSource: Wikipedia, Student’s t-distribution\n\n정규 분포의 확률값\n\n\nSource: The Truthful Art by Albert Cairo\n\n사람 키의 분포\n\n\nSource: Human Height",
    "crumbs": [
      "Trees",
      "Linear Models",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "contents/tree.html",
    "href": "contents/tree.html",
    "title": "Tree-based Models",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.5f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 5, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nsckit-learn packages\nimport sklearn.model_selection as skm\nfrom ISLP import load_data, confusion_table\nfrom ISLP.models import ModelSpec as MS\n\nfrom sklearn.tree import (DecisionTreeClassifier as DTC,\n                          DecisionTreeRegressor as DTR,\n                          plot_tree,\n                          export_text)\nfrom sklearn.metrics import (accuracy_score,\n                             log_loss)\nfrom sklearn.ensemble import \\\n     (RandomForestRegressor as RF,\n      GradientBoostingRegressor as GBR)\nfrom ISLP.bart import BART",
    "crumbs": [
      "Trees",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#regression",
    "href": "contents/tree.html#regression",
    "title": "Tree-based Models",
    "section": "Regression",
    "text": "Regression\nBaseball Data\nMajor League Baseball Data from the 1986 and 1987 seasons.\n\nfrom ISLP import load_data\n\nHitters = load_data('Hitters')\nHitters = Hitters.dropna()  # 결측치 제거\nHitters.head(3)\n\n   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n1    315    81      7    24   38     39     14    3449    835      69    321   \n2    479   130     18    66   72     76      3    1624    457      63    224   \n3    496   141     20    65   78     37     11    5628   1575     225    828   \n\n   CRBI  CWalks League Division  PutOuts  Assists  Errors    Salary NewLeague  \n1   414     375      N        W      632       43      10 475.00000         N  \n2   266     263      A        W      880       82      14 480.00000         A  \n3   838     354      N        E      200       11       3 500.00000         N  \n\n\n\n\ncode\n(\n    so.Plot(Hitters, x='Years', y='Hits', color='Salary')\n    .add(so.Dot())\n    .scale(color=so.Continuous('Blues', norm=(0, 2000), trans=\"sqrt\"))\n    .layout(size=(5, 4))\n)\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor as DTR, plot_tree, export_text\n\nX = Hitters[[\"Years\", \"Hits\"]]\ny = np.log(Hitters.Salary)  # 종모양의 분포로 변환\n\n# split the data\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.5, random_state=2)\n\nreg_tree = DTR(max_depth=2, random_state=0)        \nreg_tree.fit(X_train, y_train)\n\nDecisionTreeRegressor(max_depth=2, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(max_depth=2, random_state=0) \n\n\n\ncode\ndef visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n    ax = ax or plt.gca()\n    \n    # Plot the training points\n    ax.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, s=30, cmap=\"Blues\",\n               clim=(y.min(), y.max()), zorder=3)\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # fit the estimator\n    model.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n    Z = np.round(Z, 2)\n\n    # Create a color plot with the results\n    contours = ax.contour(xx, yy, Z, colors=\"#fc4f30\", linewidths=.5, zorder=1)\n    # plt.clabel(contours, inline=True, fontsize=8)\n    ax.set(xlim=xlim, ylim=ylim)\n\nfig, ax = plt.subplots(figsize=(5, 6.5))\nvisualize_classifier(reg_tree, X_train, y_train)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(6, 6))\nplot_tree(reg_tree, feature_names=reg_tree.feature_names_in_.tolist(), ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nprint(export_text(reg_tree, feature_names=reg_tree.feature_names_in_.tolist(), show_weights=True))\n\n|--- Years &lt;= 4.50\n|   |--- Years &lt;= 3.50\n|   |   |--- value: [4.73]\n|   |--- Years &gt;  3.50\n|   |   |--- value: [5.55]\n|--- Years &gt;  4.50\n|   |--- Hits &lt;= 117.50\n|   |   |--- value: [5.97]\n|   |--- Hits &gt;  117.50\n|   |   |--- value: [6.78]\n\n\n\n용어들\n\nTerminal/leaf node (터미널/리프 노드): 트리의 맨 끝에 있는 노드; predictor space에서 나뉘어진 각 영역을 나타냄\nInternal node (내부 노드): 트리의 중간에 있는 노드; predictor space에서 나뉘어진 지점\nBranch (가지): 노드와 노드를 연결하는 선\n\n\n\n\n\n\n\nInteractive Plots\n\n\n\n\n\nInteractive plot for decision boundary\nfrom ipywidgets import interact, fixed\n\ndef interactive_boundary(depth=1, ax=None):\n    reg_tree = DTR(max_depth=depth, random_state=0)\n    \n    fig, ax = plt.subplots(figsize=(6, 6))\n    visualize_classifier(reg_tree, X_train, y_train, ax=ax)\n\ninteract(interactive_boundary, depth=(1, 10), ax=fixed(None))\nplt.show()\n\n\n\n\n분할 알고리즘\n\n각 분할에 포함된 X값에 대해서 Y값들의 평균값을 그 예측값으로 할당\n잔차의 제곱합(RSS)을 최소화하는 분할을 찾음\n계산적으로 불가능하므로 대안적인 방법을 사용\n\nTop-down, greedy approach (recursive binary splitting)\n\n모든 예측변수들과 각 예측변수의 모든 가능한 분할점에 대해 RSS가 최소가 되는 예측변수(\\(X_j\\))와 분할점(\\(s\\))을 찾음\n즉, \\(\\displaystyle RSS = \\sum_{i: x_i \\in R_1(j, s)}(y_i - \\hat{y}_{R_1})^2 + \\sum_{i: x_i \\in R_2(j, s)}(y_i - \\hat{y}_{R_2})^2\\)을 최소화하는 \\(j\\)와 \\(s\\)를 찾음\n\n\\(\\hat{y}_{R_1}\\): \\(R_1\\)에 있는 관측치들의 \\(y\\)값들의 평균\n\\(\\hat{y}_{R_2}\\): \\(R_2\\)에 있는 관측치들의 \\(y\\)값들의 평균\n\n이를 통해 영역을 나눔: \\(R_1(j, s) = \\{X|X_j &lt; s\\}\\), \\(R_2(j, s) = \\{X|X_j \\geq s\\}\\)\n각 영역에서 반복적으로 이러한 분할을 수행\n특정 기준점에 도달할 때까지 반복;\n\n예를 들어, 모든 영역에서 5개 이상의 관측치가 남아있지 않을 때까지\n또는 트리의 깊이가 특정 수준에 도달할 때까지\n또는 터미널 노드의 수가 특정 수준에 도달할 때까지\n\n\n\n\n  Source: p. 335, An Introduction to Statistical Learning by James, G., Witten, D., Hastie, T., & Tibshirani, R.\n\n\n\nPruning (가지치기)\n\nCost-complexity pruning\n트리의 깊이를 높게 하면, 훈련셋에 대한 예측력은 높아지지만, 테스트셋에 대한 예측력은 낮아질 수 있음\n\n즉, 과적합(overfitting)이 발생할 수 있음\n\n과적합을 조절하기 위해 매우 깊은 트리(\\(T_0\\))를 우선 만들고, 이후에 가지치기(pruning)를 수행\n각 \\(\\alpha\\)에 대해 다음을 최소화하는 subtree \\(T \\subset T_0\\)가 존재함.\n\\(\\displaystyle \\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m}(y_i - \\hat{y}_{R_m})^2 + \\alpha|T|\\)\n\\(\\alpha\\)가 커질수록, 트리의 크기에 대한 페널티가 커짐\nCross-validation을 통해 최적의 \\(\\alpha\\)를 찾음\n\n\ncols = [\"AtBat\", \"Hits\", \"HmRun\", \"Runs\", \"RBI\", \"Walks\", \"Years\", \"PutOuts\", \"Assists\", \"Errors\"]\n\nX = Hitters[cols]\ny = np.log(Hitters[\"Salary\"])  # 종모양의 분포로 변환\n\n# split the data\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=.5, random_state=1)\n\nreg_tree = DTR(min_samples_leaf=5, random_state=0)        \nreg_tree.fit(X_train, y_train)\n\nDecisionTreeRegressor(min_samples_leaf=5, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(min_samples_leaf=5, random_state=0) \n\n\n\nfig, ax = plt.subplots(figsize=(15, 6))\nplot_tree(reg_tree,\n          feature_names=reg_tree.feature_names_in_.tolist(),\n          ax=ax, fontsize=7);\n\n\n\n\n\n\n\n\nCost-complexity pruning\n\nccp_path = reg_tree.cost_complexity_pruning_path(X_train, y_train)\nkfold = skm.KFold(5, shuffle=True, random_state=0)\ngrid = skm.GridSearchCV(reg_tree,\n                        {'ccp_alpha': ccp_path['ccp_alphas']},\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=DecisionTreeRegressor(min_samples_leaf=5,\n                                             random_state=0),\n             param_grid={'ccp_alpha': array([0.     , 0.00046, 0.0009 , 0.0011 , 0.0014 , 0.00277, 0.00294,\n       0.00363, 0.00505, 0.00574, 0.01015, 0.01114, 0.01298, 0.01535,\n       0.01566, 0.02022, 0.0211 , 0.02737, 0.04831, 0.10957, 0.32311])},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=DecisionTreeRegressor(min_samples_leaf=5,\n                                             random_state=0),\n             param_grid={'ccp_alpha': array([0.     , 0.00046, 0.0009 , 0.0011 , 0.0014 , 0.00277, 0.00294,\n       0.00363, 0.00505, 0.00574, 0.01015, 0.01114, 0.01298, 0.01535,\n       0.01566, 0.02022, 0.0211 , 0.02737, 0.04831, 0.10957, 0.32311])},\n             scoring='neg_mean_squared_error') estimator: DecisionTreeRegressorDecisionTreeRegressor(min_samples_leaf=5, random_state=0)  DecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(min_samples_leaf=5, random_state=0) \n\n\n\ngrid.best_params_\n\n{'ccp_alpha': 0.0153509442666242}\n\n\n\n# MSE\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, grid.best_estimator_.predict(X_test))\n\n0.4640166709041431\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_tree(grid.best_estimator_, precision=1,\n          feature_names=grid.best_estimator_.feature_names_in_.tolist(),\n          ax=ax);\n\n\n\n\n\n\n\n\n\ncode\nfrom sklearn.metrics import mean_squared_error\n\nK = 5\nresults = []\n\nfor i, alpha in enumerate(ccp_path['ccp_alphas']):\n    reg_tree = DTR(ccp_alpha=alpha, min_samples_leaf=5, random_state=0)\n\n    # training, test error\n    reg_tree.fit(X_train, y_train)\n    n_leaves = reg_tree.get_n_leaves()\n    mse_train = mean_squared_error(y_train, reg_tree.predict(X_train))\n    mse_test = mean_squared_error(y_test, reg_tree.predict(X_test))\n\n    # cross-validation error\n    kfold = skm.KFold(K, shuffle=True, random_state=0)\n    cv_results = skm.cross_validate(reg_tree, X_train, y_train, cv=kfold, return_estimator=True, scoring='neg_mean_squared_error')\n\n    results.append({\"alpha\": alpha, \"i\": i, \"n_leaves\": n_leaves, \"mse_train\": mse_train, \"mse_test\": mse_test, \"cv\": -cv_results[\"test_score\"].mean()})\n\nresults = pd.DataFrame(results)\nresults = results.query(\"n_leaves &lt; 11\")\nresults\n\n\n\ncode\n# plot the results\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.lineplot(data=results, x=\"n_leaves\", y=\"mse_train\", label=\"Training\", marker=\"o\", c=\"#fc4f30\", ax=ax)\nsns.lineplot(data=results, x=\"n_leaves\", y=\"mse_test\", label=\"Test\", marker=\"o\", c=\"#7d2be2\", ax=ax)\nsns.lineplot(data=results, x=\"n_leaves\", y=\"cv\", label=\"Cross-Validation\", marker=\"o\", c=\"#00A08A\", ax=ax)\n\nax.set(xlabel=\"Tree Size\", ylabel=\"Mean Squared Error\")\nsns.despine()\nax.get_legend().set_frame_on(False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreg_tree_x = DTR(ccp_alpha=0.012, random_state=0)\nreg_tree_x.fit(X_train, y_train)\n\nDecisionTreeRegressor(ccp_alpha=0.012, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(ccp_alpha=0.012, random_state=0) \n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_tree(reg_tree_x, feature_names=reg_tree_x.feature_names_in_.tolist(), ax=ax);",
    "crumbs": [
      "Trees",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#classification",
    "href": "contents/tree.html#classification",
    "title": "Tree-based Models",
    "section": "Classification",
    "text": "Classification\n회귀문제와 다른 점은\n\n예측값을 각 영역에서 가장 많이 나타나는 클래스에 할당.\n동시에, 각 클래스에 속한 비율(class proportion)을 얻을 수 있음.\n\n\n분할 알고리즘\n\n회귀에서 RSS를 최소화하도록 분할한 것과 비슷하게 분류에서는 다음을 기준으로 분할\n\nclassification error rate: 예측 오류률가 낮아지도록 분할\nGini index, cross-entropy: 노드의 불순도(impurity)가 낮아지도록 분할. 즉, 한 클래스에 주로 속하도록 분할\n\n동일한 클래스로 예측되는 노드라 할지라도 노드의 순도를 높이면 (해당) 클래스에 속할 확률을 높여 예측에 대한 확신을 높일 수 있음.\n\n\n새로운 노드들로부터 (가중치를 고려해) 평균적인 오류률/불순도가 줄어들 때 분할을 하며, 그 오류률/불순도가 최소가 되도록 분할\n\nClassification error rate\n\n\n\n\n\n\n\n\\(E = 1 - \\max_k(\\hat{p}_{mk})\\),   \\(\\hat{p}_{mk}\\): 영역 \\(R_m\\)에서 클래스 \\(C_k\\)에 속한 관측치의 비율\n\n동그라미: 0.7, 세모: 0.2, 네모: 0.1\nMisclassication error rate = 1 - 0.7 = 0.3\n\n예측의 정확도를 높이기 위해 선호됨\n반면, 트리의 분할에 대해 덜 민감해 큰 트리를 만들 수 없음.\n\n\n\n\n\n\n\nGini index\n\n\\(G = \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk}) = 1 - \\sum_{k=1}^K \\hat{p}_{mk}^2\\),   \\(\\hat{p}_{mk}\\): 영역 \\(R_m\\)에서 클래스 \\(C_k\\)에 속한 관측치의 비율\n\n각 클래스에 대해서 그 클래스에 속하면 1 아니면 0으로 코딩했을 때,\n해석: a measure of total variance across the K classes\n\\(G = 0.7*(1-0.7) + 0.2*(1-0.2) + 0.1*(1-0.1) = 0.46\\)\n\n노드의 불순도(impurity)를 측정; 여러 클래스로 나뉘어져 분포하는 정도\n\\(G\\)가 작을수록, 한 개의 클래스에 주로 속해있음을 의미\n\n\n\n\n\n\n\nk1 = np.array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0])\nk2 = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 0])\nk3 = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n\n# variance = p(1-p)\nprint(k1.var(), k2.var(), k3.var())\n#&gt; 0.21, 0.16, 0.09\n\n\n\n\nCross-entropy/deviance\n\n\\(D = -\\sum_{k=1}^K \\hat{p}_{mk} \\log(\\hat{p}_{mk})\\)\nGini index와 유사하며, 노드의 불순도를 측정\n\nGini index와 cross-entropy가 노드 순수도에 더 민감하게 반응하며 더 큰 트리를 만들 수 있음.\n\n예를 들어, 다음 두 가지 분할에 대해 오류율은 동일한 반면,\nGini index는 차이를 보이고, 오른쪽 분할이 더 순수한 분할로 간주됨.\n\n\n왼쪽\n\n분할 전: 오류율, Gini index = 0.5\n분할 후:\n\n오류율 = \\(\\frac{4}{8} \\cdot \\frac{1}{4} + \\frac{4}{8} \\cdot \\frac{1}{4}=0.25\\)\nGini index = \\(\\frac{4}{8} \\cdot \\left(\\frac{3}{4}\\cdot \\frac{1}{4} + \\frac{1}{4}\\cdot \\frac{3}{4} \\right) + \\frac{4}{8} \\cdot \\left(\\frac{1}{4}\\cdot \\frac{3}{4} + \\frac{3}{4}\\cdot \\frac{1}{4} \\right)=0.375\\)\n\n\n오른쪽\n\n분할 후:\n\n오류율 = \\(\\frac{6}{8} \\cdot \\frac{2}{6} + \\frac{2}{8} \\cdot \\frac{0}{2}=0.25\\)\nGini index = \\(\\frac{6}{8} \\cdot \\left(\\frac{2}{6}\\cdot \\frac{4}{6} + \\frac{4}{6}\\cdot \\frac{2}{6} \\right) + \\frac{2}{8} \\cdot \\left(\\frac{2}{2}\\cdot \\frac{0}{2} + \\frac{0}{2}\\cdot \\frac{2}{2} \\right)=0.333...\\)\n\n\n세 지표들의 비교\n\n\n\n\n\n\n\n\n\n\n\npenguins = sns.load_dataset(\"penguins\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier as DTC\n\nX = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\ny = penguins[\"species\"].map({\"Adelie\": 0, \"Chinstrap\": 1, \"Gentoo\": 2})\n\nclf_tree_2 = DTC(max_depth=2, criterion=\"gini\", random_state=0)\nclf_tree_10 = DTC(max_depth=10, criterion=\"gini\", random_state=0)\n\n\ncode\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(colors[:3])\n\ndef visualize_classifier2(model, X, y, ax=None, cmap='rainbow'):\n    ax = ax or plt.gca()\n    \n    # Plot the training points\n    ax.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, s=50, edgecolors=\"w\", linewidths=.5, cmap=cmap, clim=(y.min(), y.max()), zorder=3)\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # fit the estimator\n    model.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n    # Create a color plot with the results\n    n_classes = len(np.unique(y))\n    contours = ax.contourf(xx, yy, Z, alpha=0.15,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap=cmap, zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)\n\nfig, ax = plt.subplots(figsize=(5.5, 5))\nvisualize_classifier2(clf_tree_2, X, y, cmap=custom_cmap)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(5.5, 5))\nvisualize_classifier2(clf_tree_10, X, y, cmap=custom_cmap)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_tree(clf_tree_2, feature_names=clf_tree_2.feature_names_in_.tolist(), ax=ax);\n\n\n\n\n\n\n\n\nconfusion_table(clf_tree_2.predict(X), y)\n\nTruth        0   1    2\nPredicted              \n0          139   1    0\n1           12  67    9\n2            1   0  115",
    "crumbs": [
      "Trees",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#의사-결정-트리의-장단점",
    "href": "contents/tree.html#의사-결정-트리의-장단점",
    "title": "Tree-based Models",
    "section": "의사 결정 트리의 장단점",
    "text": "의사 결정 트리의 장단점\n장점\n\n사람들에게 설명하기 매우 쉽우며, 사실 선형 회귀보다 설명하기가 더 쉬음.\n의사 결정 트리가 다른 접근법보다 인간의 의사 결정을 더 잘 반영한다고 보기도 함.\n다이어그램으로 표시할 수 있으며, 비전문가도 쉽게 해석할 수 있음.(특히 크기가 작은 경우).\n더미 변수를 만들 필요 없이 자연스럽게 범주형 변수를 처리할 수 있음. (단, scikit-learn에서는 아직…)\n\n단점\n\n일반적으로 다른 회귀 및 분류 접근 방식보다 예측 정확도가 낮음.\nNon-robust: 데이터의 작은 변화에 민감하게 트리에 큰 변화가 생길 수 있음.",
    "crumbs": [
      "Trees",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#sales-of-child-car-seats-예제",
    "href": "contents/tree.html#sales-of-child-car-seats-예제",
    "title": "Tree-based Models",
    "section": "Sales of Child Car Seats 예제",
    "text": "Sales of Child Car Seats 예제\nCarseats 설명\n\ncarseats = load_data('Carseats')\ncarseats.head(3)\n\n     Sales  CompPrice  Income  Advertising  Population  Price ShelveLoc  Age  \\\n0  9.50000        138      73           11         276    120       Bad   42   \n1 11.22000        111      48           16         260     83      Good   65   \n2 10.06000        113      35           10         269     80    Medium   59   \n\n   Education Urban   US  \n0         17   Yes  Yes  \n1         10   Yes  Yes  \n2         12   Yes  Yes  \n\n\n\n\n# 카테고리 변수를 더미 변수로 변환\nX = pd.get_dummies(carseats.drop(columns=\"Sales\"), drop_first=True)\n\n# 분류 문제로 변환\ny = np.where(carseats[\"Sales\"] &gt; 8, \"high\", \"low\")\n\n\n# fit the model\nclf_tree = DTC(criterion='entropy', max_depth=3, random_state=0)\nclf_tree.fit(X, y)\n\nDecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0) \n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_tree(clf_tree, feature_names=clf_tree.feature_names_in_.tolist(), ax=ax);\n\n\n\n\n\n\n\n\n\nprint(export_text(clf_tree, feature_names=clf_tree.feature_names_in_.tolist(), show_weights=True))\n\n|--- ShelveLoc_Good &lt;= 0.50\n|   |--- Price &lt;= 92.50\n|   |   |--- Income &lt;= 57.00\n|   |   |   |--- weights: [3.00, 7.00] class: low\n|   |   |--- Income &gt;  57.00\n|   |   |   |--- weights: [29.00, 7.00] class: high\n|   |--- Price &gt;  92.50\n|   |   |--- Advertising &lt;= 13.50\n|   |   |   |--- weights: [41.00, 183.00] class: low\n|   |   |--- Advertising &gt;  13.50\n|   |   |   |--- weights: [25.00, 20.00] class: high\n|--- ShelveLoc_Good &gt;  0.50\n|   |--- Price &lt;= 135.00\n|   |   |--- US_Yes &lt;= 0.50\n|   |   |   |--- weights: [11.00, 6.00] class: high\n|   |   |--- US_Yes &gt;  0.50\n|   |   |   |--- weights: [49.00, 2.00] class: high\n|   |--- Price &gt;  135.00\n|   |   |--- Income &lt;= 46.00\n|   |   |   |--- weights: [0.00, 6.00] class: low\n|   |   |--- Income &gt;  46.00\n|   |   |   |--- weights: [6.00, 5.00] class: high\n\n\n\nTraining error에 대해 살펴보면,\n\nconfusion_table(clf_tree.predict(X), y)\n\nTruth      high  low\nPredicted           \nhigh        120   40\nlow          44  196\n\n\n\nfrom sklearn.metrics import accuracy_score, log_loss\n\naccuracy_score(y, clf_tree.predict(X))\n\n0.79\n\n\n좀 더 적절하게 cross-validation을 이용해 test error를 구하면,\n\nkfold = skm.KFold(n_splits=5, shuffle=True, random_state=0)\nresults = skm.cross_validate(clf_tree, X, y, scoring='accuracy', cv=kfold)\nresults['test_score']\n\narray([0.7125, 0.7625, 0.7625, 0.6125, 0.7125])\n\n\nTree-pruning에 의해 더 나은 트리를 얻을 수 있는지 확인\n우선 큰 트리를 만들기 위해, max_depth를 설정하지 않고 트리를 만든 후, 적절한 pruning을 얻기 위해 cross-validation을 이용함.\n\n# train-test split\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.5, random_state=0)\n\n\nclf_tree = DTC(criterion='entropy', random_state=0)\nclf_tree.fit(X_train, y_train)\n\nDecisionTreeClassifier(criterion='entropy', random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(criterion='entropy', random_state=0) \n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\nplot_tree(clf_tree, feature_names=clf_tree.feature_names_in_.tolist(), ax=ax);\n\n\n\n\n\n\n\n\n\nclf_tree.tree_.n_leaves\n\n35\n\n\n\naccuracy_score(y_test, clf_tree.predict(X_test))\n\n0.705\n\n\nCost-complexity pruning\n\nccp_path = clf_tree.cost_complexity_pruning_path(X_train, y_train)\nkfold = skm.KFold(10, random_state=1, shuffle=True)\n\ngrid = skm.GridSearchCV(clf_tree,\n                        {'ccp_alpha': ccp_path.ccp_alphas},\n                        refit=True,\n                        cv=kfold,\n                        scoring='accuracy')\ngrid.fit(X_train, y_train)\ngrid.best_score_\n\n0.6849999999999999\n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nbest_ = grid.best_estimator_\nplot_tree(best_, feature_names=best_.feature_names_in_.tolist(), fontsize=6, ax=ax);\n\n\n\n\n\n\n\n\n\nbest_.tree_.n_leaves\n\n10\n\n\n\naccuracy_score(y_test, best_.predict(X_test))\n\n0.68\n\n\n\nconfusion_table(best_.predict(X_test), y_test)\n\nTruth      high  low\nPredicted           \nhigh         38   20\nlow          44   98\n\n\npruning을 통해 accuracy에서 작은 손해를 보았으나; 0.705에서 0.68로 감소\n35개의 터미널 노드 가진 full tree에서 10개의 터미널 노드로 줄였음.\n\n\n\n\n\n\nDecision tree에서 확률의 의미\n\n\n\n\n\n리프 노드에서 각 클래스에 속하는 관측치들의 비율\nbest_.predict_proba(X_test)[:3]\n# array([[0.30303, 0.69697],\n#        [0.775  , 0.225  ],\n#        [0.775  , 0.225  ]])\n\nX_test[:3]\n#     CompPrice  Income  Advertising  Population  Price  Age  Education  \n# 132       125      87            9         232    136   72        10   \n# 309       131     111           13          33     80   68        18   \n# 341        98     120            0         268     93   72        10   \n\n#      ShelveLoc_Good  ShelveLoc_Medium  Urban_Yes  US_Yes  \n# 132            True             False       True    True  \n# 309           False             False       True    True  \n# 341           False              True      False   False",
    "crumbs": [
      "Trees",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#bagging",
    "href": "contents/tree.html#bagging",
    "title": "Tree-based Models",
    "section": "Bagging",
    "text": "Bagging\nBootstrap aggregation\n\n부트스트랩으로 추출되는 여러 데이터셋으로 모형을 학습\n특히, decision tree에서 흔히 사용되는데, 이는 decision tree가 variance가 매우 높은 모형이기 때문\n\n여러 샘플들로부터 얻은 통계치들을 평균내면 그 분포의 분산이 줄어드는 현상이 있는데, 실제로 여러 샘플을 얻을 수 없기 때문에 부트스트랩으로 가상의 여러 샘플을 얻어 이를 이용.\n\n\n\n\n\n\nthe sampling distribution of the mean\n\n\n\n\n\n평균이 \\(\\mu\\), 분산이 \\(\\sigma^2\\) 인 모집단으로부터 추출된 표본 사이즈가 \\(n\\)인 표본들에 대해서\n각 표본의 평균들 \\(\\bar{X}\\) 의 분포를 평균의 표본 분포, the sampling distribution of the mean이라고 하고, 이 분포는 the central limit theorem에 의해\n\n평균:  \\(\\displaystyle E(\\bar{X})=\\frac{m_1+m_2+m_3+\\cdots+m_w}{w}\\Bigg|_{w\\rightarrow\\infty}\\rightarrow ~~~\\mu\\)\n분산:  \\(\\displaystyle V(\\bar{X})\\Bigg|_{w\\rightarrow\\infty}\\rightarrow ~~~\\frac{\\sigma^2}{n},\\)   표준 편차 \\(\\displaystyle\\frac{\\sigma}{\\sqrt{n}}\\) 를 standard error of estimate (SE)라고 함.\n\n\n\n\n\nB개의 부트스트랩 샘플을 이용해 B개의 모형 \\(\\hat{f}^{b}\\)를 만들고, 이들의 평균을 이용해 예측\n\\(\\displaystyle \\hat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^B \\hat{f}^{b}(x)\\)\n\ndecision tree에서 특히 효과적임\n각 트리들은 매우 깊게 만들어 variance를 높게 만든 후 (bias는 낮아짐)\n이들의 평균을 이용해 variance를 줄임\n수백, 수천개의 트리를 결합해 매우 높은 정확도를 얻을 수 있음\n트리를 키워 variance가 충분히 낮아지도록; 트리의 수가 중요한 요소는 아님. 많은 트리가 overfitting을 만들지 않음\n한편, 여러 트리들의 평균값이므로 앞서 단일 트리를 통해 다이어그램을 그리거나 해석하기 어려움.\n\n\n\nSource: p. 344, An Introduction to Statistical Learning with Applications in Python\n\n\n\n\n\n\n\nOut-of-bag error\n\n\n\n\n\n\n부트스트랩 샘플들은 평균적으로 원래 데이터의 2/3의 관측치만를 포함함\n학습에 사용되지 않은 나머지 약 1/3의 관측치들을 out-of-bag(OOB) 관측치라고 함\n모형에 대한 test error를 cross-validation 대신에 학습에 사용되지 않은 OOB 관측치를 이용해 효과적으로 추정할 수 있음\n즉, 각 관측치 x에 대해 x가 포함되지 않은 부트스트랩 샘플들로 학습된 트리들로 예측한 값들(약 1/3개의 트리)의 평균을 이용해 test error를 추정할 수 있음\n\n\n\n\n분류의 문제의 경우\n\n각 트리들이 예측한 클래스들 중 가장 많이 나온 클래스를 예측값으로 사용: majority/hard voting\n각 트리들이 예측한 클래스들의 확률을 평균내어 가장 높은 확률을 가진 클래스를 예측값으로 사용: soft voting\n\n예측변수의 상대적 중요도\n\n예측변수에 대한 해석은 어려운 대신, 예측변수의 중요도에 대한 정보를 얻을 수 있음\n회귀문제: 주어진 예측변수로 분할될 때, RSS가 줄어든 총량을 모든 트리에 대해 평균\n분류문제: 주어진 예측변수로 분할될 때, 불순도(gini index)가 줄어든 총량을 모든 트리에 대해 평균\n\n\n\n\n\n\n\nVariable importance\n\n\n\n모든 상황에 적용할 수 있는 하나의 방법은 없으며, 연구 내용과 측정 변수들의 특성에 따라 적절한 방법을 선택.\n선형모형에서는\n\n표준화 회귀계수 (standardized regression coefficient)\nSemi-partial correlation coefficient\nDominance analysis\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier as RFC\n\nbag_carseats = RFC(max_features=X_train.shape[1], n_estimators=500, random_state=0)\nbag_carseats.fit(X_train, y_train)\n\nRandomForestClassifier(max_features=11, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(max_features=11, random_state=0) \n\n\n\naccuracy_score(y_test, bag_carseats.predict(X_test))\n\n0.77\n\n\n\nfeature_imp = pd.DataFrame(\n    {\"importance\": bag_carseats.feature_importances_}, index=X.columns\n).sort_values(\"importance\", ascending=False)\nfeature_imp\n\n                  importance\nPrice                0.31588\nCompPrice            0.13542\nAge                  0.11689\n...                      ...\nShelveLoc_Medium     0.01439\nUrban_Yes            0.00840\nUS_Yes               0.00600\n\n[11 rows x 1 columns]\n\n\n\n\ncode\n(\n    so.Plot(feature_imp.reset_index(names=\"predictor\"), x='importance', y=\"predictor\")\n    .add(so.Bar(alpha=1))\n    .label(x=\"Relative Importance\", y=\"Predictor\")\n)",
    "crumbs": [
      "Trees",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#random-forest",
    "href": "contents/tree.html#random-forest",
    "title": "Tree-based Models",
    "section": "Random Forest",
    "text": "Random Forest\n\nBaggging의 일종: 부트스트랩 샘플을 이용해 여러 트리를 생성\n각 트리를 만들 때, 랜덤하게 선택된 변수들로만 분할을 수행; 대략 \\(\\sqrt{p}\\)개의 변수를 선택\n총 예측변수보다 훨씬 적은 수의 변수를 선택함으로써, 트리 간의 상관관계를 줄임; decorrelating\n\n서로 관련이 없는 것들의 평균이 상대적으로 더 variance를 줄이는데 효과적\n상관이 높은 예측변수들이 많다면 특히, 적은 변수를 선택하는 것이 효과적\n\n중요도가 낮은 변수들도 트리에서 데이터를 설명할 수 있는 기회를 얻게 되는데, 이는 데이터 내의 다양한 정보를 캐낼 수 있는 것으로도 볼 수 있음\n항상 모든 변수를 선택한다면, Bagging과 같음\n\n\nfrom sklearn.ensemble import RandomForestClassifier as RFC\n\nrf_carseats = RFC(max_features='sqrt', n_estimators=500, random_state=0)\nrf_carseats.fit(X_train, y_train)\n\nRandomForestClassifier(n_estimators=500, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(n_estimators=500, random_state=0) \n\n\n\naccuracy_score(y_test, rf_carseats.predict(X_test))\n\n0.805\n\n\n선택되는 변수의 수와 트리의 개수에 따른 예측력 변화",
    "crumbs": [
      "Trees",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/tree.html#boosting",
    "href": "contents/tree.html#boosting",
    "title": "Tree-based Models",
    "section": "Boosting",
    "text": "Boosting\n\nBagging과 마찬가지로 weak learner들을 결합하는 방법이기는 하나\nBoosting은 처음 세운 모형을 점진적으로 향상시키면서 새 모형들을 만들어내는데, 이전에 잡아내지 못한 시그널을 보완하면서 모형을 업데이트하는 방식\nBagging과는 본질적으로 큰 차이가 있음\n다양한 알고리즘들이 개발되어 왔음; AdaBoost, Gradient Boosting, XGBoost 등\n\n여기서는 decision tree에 적용해 살펴봄\n\n\nSource: p. 349, An Introduction to Statistical Learning with Applications in Python\n\nTurning parmaters\n\n트리 수 B: Bagging과는 다르게 B가 너무 크면 과적합이 발생할 있어 교차 검증을 통해 B를 선택\nShrinkage parameter λ: Boosting이 학습하는 속도를 제어.\n\n일반적으로 0.01이나 0.001\nλ가 매우 작으면 좋은 성능을 얻기 위해 매우 큰 값의 B가 필요\n\n각 트리의 분할 수 d: 부스트된 앙상블의 복잡도를 제어\n\n종종 d = 1을 선택; 단일 분할로 구성된 stump: 한 변수만을 포함\nd는 각 단계에서 선택되는 변수의 개수를 제어하게 되는데, 이는 여러 변수들이 골고루 데이터의 시그널을 잡아낼 수 있는 기회를 제공하게 됨.\n\n\n\n# Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier as GBC\n\nboost_carseats = GBC(n_estimators=5000, learning_rate=0.01, max_depth=2, random_state=0)\nboost_carseats.fit(X_train, y_train)\n\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=5000,\n                           random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=5000,\n                           random_state=0) \n\n\n\naccuracy_score(y_test, boost_carseats.predict(X_test))\n\n0.855\n\n\n선택되는 변수의 수와 트리의 개수에 따른 예측력 변화\n\n\n\n\n\n\n\n\n\n\n\nboost_carseats = GBC(n_estimators=5000, learning_rate=0.01, max_depth=1, random_state=0)\nboost_carseats.fit(X_train, y_train)\n\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=1, n_estimators=5000,\n                           random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(learning_rate=0.01, max_depth=1, n_estimators=5000,\n                           random_state=0) \n\n\n\naccuracy_score(y_test, boost_carseats.predict(X_test))\n\n0.89\n\n\nFeature importance\n\n\ncode\nfeature_imp2 = pd.DataFrame(\n    {\"importance\": boot_carseats.feature_importances_}, index=X.columns\n).sort_values(\"importance\", ascending=False)\n\n(\n    so.Plot(feature_imp2.reset_index(names=\"predictor\"), x='importance', y=\"predictor\")\n    .add(so.Bar(alpha=1))\n    .label(x=\"Relative Importance\", y=\"Predictor\")\n)",
    "crumbs": [
      "Trees",
      "Tree-based Models"
    ]
  },
  {
    "objectID": "contents/wrangling.html",
    "href": "contents/wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Trees",
      "Exploratory Data Analysis",
      "Wrangling"
    ]
  },
  {
    "objectID": "contents/wrangling.html#nyc-taxi-and-limousine-commission-record-data",
    "href": "contents/wrangling.html#nyc-taxi-and-limousine-commission-record-data",
    "title": "Wrangling",
    "section": "NYC Taxi and Limousine Commission Record Data",
    "text": "NYC Taxi and Limousine Commission Record Data\n\ntaxi = sns.load_dataset(\"taxis\")\ntaxi\n\n                  pickup             dropoff  passengers  distance  fare  tip  \\\n0    2019-03-23 20:21:09 2019-03-23 20:27:24           1      1.60  7.00 2.15   \n1    2019-03-04 16:11:55 2019-03-04 16:19:00           1      0.79  5.00 0.00   \n2    2019-03-27 17:53:01 2019-03-27 18:00:25           1      1.37  7.50 2.36   \n...                  ...                 ...         ...       ...   ...  ...   \n6430 2019-03-23 22:55:18 2019-03-23 23:14:25           1      4.14 16.00 0.00   \n6431 2019-03-04 10:09:25 2019-03-04 10:14:29           1      1.12  6.00 0.00   \n6432 2019-03-13 19:31:22 2019-03-13 19:48:02           1      3.85 15.00 3.36   \n\n      tolls  total   color      payment            pickup_zone  \\\n0      0.00  12.95  yellow  credit card        Lenox Hill West   \n1      0.00   9.30  yellow         cash  Upper West Side South   \n2      0.00  14.16  yellow  credit card          Alphabet City   \n...     ...    ...     ...          ...                    ...   \n6430   0.00  17.30   green         cash    Crown Heights North   \n6431   0.00   6.80   green  credit card          East New York   \n6432   0.00  20.16   green  credit card            Boerum Hill   \n\n                      dropoff_zone pickup_borough dropoff_borough  \n0              UN/Turtle Bay South      Manhattan       Manhattan  \n1            Upper West Side South      Manhattan       Manhattan  \n2                     West Village      Manhattan       Manhattan  \n...                            ...            ...             ...  \n6430                Bushwick North       Brooklyn        Brooklyn  \n6431  East Flatbush/Remsen Village       Brooklyn        Brooklyn  \n6432               Windsor Terrace       Brooklyn        Brooklyn  \n\n[6433 rows x 14 columns]\n\n\n\ntaxi.describe(include=\"object\")\n\n         color      payment     pickup_zone           dropoff_zone  \\\ncount     6433         6389            6407                   6388   \nunique       2            2             194                    203   \ntop     yellow  credit card  Midtown Center  Upper East Side North   \nfreq      5451         4577             230                    245   \n\n       pickup_borough dropoff_borough  \ncount            6407            6388  \nunique              4               5  \ntop         Manhattan       Manhattan  \nfreq             5268            5206  \n\n\n\n열의 이름 수정\nrename()\n\ntaxi.rename(columns={\"passengers\": \"persons\", \"payment\": \"pay\"}, inplace=True)\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00 0.00   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color          pay            pickup_zone  \\\n0   0.00  12.95  yellow  credit card        Lenox Hill West   \n1   0.00   9.30  yellow         cash  Upper West Side South   \n2   0.00  14.16  yellow  credit card          Alphabet City   \n\n            dropoff_zone pickup_borough dropoff_borough  \n0    UN/Turtle Bay South      Manhattan       Manhattan  \n1  Upper West Side South      Manhattan       Manhattan  \n2           West Village      Manhattan       Manhattan  \n\n\n\n\n값의 대체\n\nidx_cash = (taxi[\"pay\"] == \"cash\")  # Boolean index\n\n# 직접 값을 대입\ntaxi.loc[idx_cash, \"tip\"] = np.nan  # missing(NA) values\n\nnp.where()의 활용\n(boolean condition, value if True, value if False)\n\ntaxi[\"tip\"] = np.where(idx_cash, np.nan, taxi[\"tip\"])\n\nmap()의 활용\ndictionary로 입력\n\ntaxi[\"pay\"] = taxi[\"pay\"].map({\"credit card\": \"Card\", \"cash\": \"Cash\"})\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay            pickup_zone           dropoff_zone  \\\n0   0.00  12.95  yellow  Card        Lenox Hill West    UN/Turtle Bay South   \n1   0.00   9.30  yellow  Cash  Upper West Side South  Upper West Side South   \n2   0.00  14.16  yellow  Card          Alphabet City           West Village   \n\n  pickup_borough dropoff_borough  \n0      Manhattan       Manhattan  \n1      Manhattan       Manhattan  \n2      Manhattan       Manhattan  \n\n\n\n\n새로운 변수 생성\n기본적인 Series들의 연산\n\ntaxi[\"total2\"] = taxi[\"total\"] - taxi[\"tip\"]\n\nassign()의 활용\n\ntaxi = taxi.assign(\n    tip_ratio = lambda x: x[\"tip\"] / x[\"total2\"],\n    tip_ratio_per = lambda x: x.tip_ratio / x.persons  # dot notation\n)\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay            pickup_zone           dropoff_zone  \\\n0   0.00  12.95  yellow  Card        Lenox Hill West    UN/Turtle Bay South   \n1   0.00   9.30  yellow  Cash  Upper West Side South  Upper West Side South   \n2   0.00  14.16  yellow  Card          Alphabet City           West Village   \n\n  pickup_borough dropoff_borough  total2  tip_ratio  tip_ratio_per  \n0      Manhattan       Manhattan   10.80       0.20           0.20  \n1      Manhattan       Manhattan     NaN        NaN            NaN  \n2      Manhattan       Manhattan   11.80       0.20           0.20  \n\n\n\n# total2의 NA값 원래대로 복구\ntaxi[\"total2\"] = np.where(idx_cash, taxi[\"total\"], taxi[\"total2\"])\n\n잠시, 택시요금과 팁 간의 관계를 살펴보면,\n\ntaxi.plot.scatter(x=\"total2\", y=\"tip\", alpha=.2, figsize=(5, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\ntaxi.plot.scatter(x=\"total2\", y=\"tip_ratio\", alpha=.2, figsize=(5, 3))\nplt.show()\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(taxi, x='total2', y='tip_ratio')\n    .add(so.Dot(alpha=.5, pointsize=2))\n    .add(so.Line(color=\".2\"), so.PolyFit(5))\n    .facet(\"persons\", wrap=4)\n    .layout(size=(8, 5))\n    .label(title=\"{} passengers\".format)\n)\n\n\n\n\n\n\n\n\n\n\n필터링\nquery()의 활용\n\nConditional operators\n&gt;, &gt;=, &lt;, &lt;=,\n== (equal to), != (not equal to)\nand, & (and)\nor, | (or)\nnot, ~ (not)\nin (includes), not in (not included)\n\n\n(\n    taxi.query('color == \"green\" & pickup_borough == \"Manhattan\"')\n    .head(3)\n)\n\n                  pickup             dropoff  persons  distance  fare  tip  \\\n5453 2019-03-29 18:12:27 2019-03-29 18:20:40        1      1.51  7.50 1.20   \n5454 2019-03-06 11:11:33 2019-03-06 11:15:15        1      0.45  4.50  NaN   \n5465 2019-03-27 20:55:35 2019-03-27 21:01:55        1      1.43  7.00 3.00   \n\n      tolls  total  color   pay        pickup_zone           dropoff_zone  \\\n5453   0.00  10.50  green  Card  East Harlem North      East Harlem South   \n5454   0.00   5.30  green  Cash  East Harlem North      East Harlem South   \n5465   0.00  11.30  green  Card     Central Harlem  Upper West Side North   \n\n     pickup_borough dropoff_borough  total2  tip_ratio  tip_ratio_per  \n5453      Manhattan       Manhattan    9.30       0.13           0.13  \n5454      Manhattan       Manhattan    5.30        NaN            NaN  \n5465      Manhattan       Manhattan    8.30       0.36           0.36  \n\n\n\n\n정렬\n\n(\n    taxi.query('persons &gt; 1')\n    .sort_values(\"tip_ratio\", ascending=False)\n)\n\n                  pickup             dropoff  persons  distance  fare  tip  \\\n2923 2019-03-06 23:17:06 2019-03-06 23:24:12        2      1.60  7.50 7.00   \n3272 2019-03-28 11:15:04 2019-03-28 11:21:01        3      0.62  5.50 5.00   \n4912 2019-03-27 18:04:45 2019-03-27 18:11:07        2      0.55  5.50 5.00   \n...                  ...                 ...      ...       ...   ...  ...   \n6405 2019-03-21 18:28:55 2019-03-21 18:51:08        2      3.88 16.50  NaN   \n6412 2019-03-20 17:33:25 2019-03-20 17:42:48        5      1.40  8.00  NaN   \n6420 2019-03-16 15:39:23 2019-03-16 15:46:18        2      1.20  7.00  NaN   \n\n      tolls  total   color   pay                   pickup_zone  \\\n2923   0.00  18.30  yellow  Card  Penn Station/Madison Sq West   \n3272   0.00  13.80  yellow  Card      Financial District North   \n4912   0.00  14.80  yellow  Card                   Murray Hill   \n...     ...    ...     ...   ...                           ...   \n6405   0.00  18.30   green  Cash                   Boerum Hill   \n6412   0.00   9.80   green  Cash              Brooklyn Heights   \n6420   0.00   7.80   green  Cash                Central Harlem   \n\n               dropoff_zone pickup_borough dropoff_borough  total2  tip_ratio  \\\n2923              Hudson Sq      Manhattan       Manhattan   11.30       0.62   \n3272           Battery Park      Manhattan       Manhattan    8.80       0.57   \n4912    UN/Turtle Bay South      Manhattan       Manhattan    9.80       0.51   \n...                     ...            ...             ...     ...        ...   \n6405             Ocean Hill       Brooklyn        Brooklyn   18.30        NaN   \n6412            Fort Greene       Brooklyn        Brooklyn    9.80        NaN   \n6420  Upper West Side North      Manhattan       Manhattan    7.80        NaN   \n\n      tip_ratio_per  \n2923           0.31  \n3272           0.19  \n4912           0.26  \n...             ...  \n6405            NaN  \n6412            NaN  \n6420            NaN  \n\n[1659 rows x 17 columns]\n\n\n\n\nGrouping\n\ngroupby()로 데이터를 의미있는 그룹으로 나눈 후, 다음과 같은 통계치를 계산\nsize(), count(), sum(), mean(), min(), max()\n\n  \n\nSource: Ch.10 in Python for Data Analysis (3e) by Wes McKinney\n\n\ntaxi.groupby(\"color\").size()  # size(): 열의 개수\n\ncolor\ngreen      982\nyellow    5451\ndtype: int64\n\n\n\ntaxi.groupby([\"color\", \"pay\"]).size()  # size(): 열의 개수\n\ncolor   pay \ngreen   Card     577\n        Cash     400\nyellow  Card    4000\n        Cash    1412\ndtype: int64\n\n\n\ntaxi.groupby([\"color\", \"pay\"])[\"total2\"].mean()\n\ncolor   pay \ngreen   Card   18.76\n        Cash   11.40\nyellow  Card   17.08\n        Cash   15.61\nName: total2, dtype: float64\n\n\n\ndf =(\n    taxi.groupby([\"color\", \"pay\"])[\"total2\"].mean()\n    .unstack()  # wide format으로 변환\n)\ndf\n\npay     Card  Cash\ncolor             \ngreen  18.76 11.40\nyellow 17.08 15.61\n\n\n\ndf[\"total\"] = df.sum(axis=1)\ndf\n\npay     Card  Cash  total\ncolor                    \ngreen  18.76 11.40  30.15\nyellow 17.08 15.61  32.68\n\n\n\ndf.assign(\n    Card_pct = lambda x: x.Card / x.total * 100,\n    Cash_pct = lambda x: x.Cash / x.total * 100\n)\n\npay     Card  Cash  total  Card_pct  Cash_pct\ncolor                                        \ngreen  18.76 11.40  30.15     62.21     37.79\nyellow 17.08 15.61  32.68     52.25     47.75\n\n\n여러 함수를 동시에 적용: agg()\n\ntaxi.groupby([\"color\", \"pay\"])[\"total2\"].agg([\"mean\", \"std\", \"size\"])\n\n             mean   std  size\ncolor  pay                   \ngreen  Card 18.76 14.73   577\n       Cash 11.40 10.41   400\nyellow Card 17.08 12.00  4000\n       Cash 15.61 12.38  1412\n\n\n가장 일반적인 방식으로 appy()를 사용하여 사용자 정의 함수를 적용\n\n# standardize\ndef standardize(x):\n    return (x - x.mean()) / x.std()\n\ntaxi.groupby(\"color\", group_keys=True)[\"tip_ratio\"].apply(standardize)\n\ncolor       \ngreen   5451     NaN\n        5452   -0.95\n        5453    0.23\n                ... \nyellow  5448    0.33\n        5449    0.33\n        5450   -1.12\nName: tip_ratio, Length: 6433, dtype: float64\n\n\n\n\n시간 데이터의 처리\n시간을 표시하는 datetime64 타입을 이용해 시간 데이터를 처리하며,\ndt accessor를 사용\n\n# pickup 시간으로부터 요일을 추출\ntaxi[\"day\"] = taxi[\"pickup\"].dt.day_name().str[:3]  # 요일의 앞 3글자\n\n\n# pickup 시간으로부터 시간대를 추출\ntaxi[\"hour\"] = taxi[\"pickup\"].dt.hour\n\n\n# 택시를 탄 시간(분)을 계산\ntaxi[\"duration\"] = (taxi[\"dropoff\"] - taxi[\"pickup\"]).dt.total_seconds() / 60\n\n\ntaxi.head(3)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay            pickup_zone           dropoff_zone  \\\n0   0.00  12.95  yellow  Card        Lenox Hill West    UN/Turtle Bay South   \n1   0.00   9.30  yellow  Cash  Upper West Side South  Upper West Side South   \n2   0.00  14.16  yellow  Card          Alphabet City           West Village   \n\n  pickup_borough dropoff_borough  total2  tip_ratio  tip_ratio_per  day  hour  \\\n0      Manhattan       Manhattan   10.80       0.20           0.20  Sat    20   \n1      Manhattan       Manhattan    9.30        NaN            NaN  Mon    16   \n2      Manhattan       Manhattan   11.80       0.20           0.20  Wed    17   \n\n   duration  \n0      6.25  \n1      7.08  \n2      7.40  \n\n\n요일과 시간대별로 팁의 비율은 다른가?\n\ntaxi.groupby([\"day\", \"hour\"])[\"tip_ratio\"].mean()\n\nday  hour\nFri  0      0.17\n     1      0.19\n     2      0.12\n            ... \nWed  21     0.18\n     22     0.19\n     23     0.23\nName: tip_ratio, Length: 167, dtype: float64\n\n\npivot_table()를 활용할 수도 있음\n\ntaxi.pivot_table(\"tip_ratio\", \"hour\", \"day\")\n\nday   Fri  Mon  Sat  Sun  Thu  Tue  Wed\nhour                                   \n0    0.17 0.19 0.17 0.18 0.17 0.22 0.19\n1    0.19  NaN 0.18 0.18 0.20 0.20 0.18\n2    0.12 0.18 0.18 0.19 0.22 0.20 0.17\n...   ...  ...  ...  ...  ...  ...  ...\n21   0.17 0.18 0.15 0.18 0.18 0.17 0.18\n22   0.17 0.17 0.18 0.18 0.16 0.18 0.19\n23   0.18 0.22 0.17 0.19 0.19 0.18 0.23\n\n[24 rows x 7 columns]\n\n\n\n# day를 categorical 타입으로 변환\ntaxi[\"day\"] = pd.Categorical(taxi[\"day\"], categories=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n\n\ndf = taxi.pivot_table(\"tip_ratio\", \"hour\", \"day\")\n\nfig, axes = plt.subplots(6, 4, figsize=(8, 8), sharex=True, sharey=True)\nfig.subplots_adjust(wspace=0.1, hspace=0.3)\n\nfor i, ax in enumerate(axes.flat):\n    df.iloc[i].plot.bar(ax=ax, ylim=(0.1, 0.23))\n    ax.set_title(f\"hour={i}\", fontsize=9)\n\n\n\n\n\n\n\n\n\ntaxi_day_hour = (\n    taxi.groupby([\"day\", \"hour\"])[\"tip_ratio\"]\n    .agg([\"mean\", \"size\"])\n    .reset_index()\n    .rename(columns={\"mean\": \"tip_ratio\", \"size\": \"n\"})\n)\ntaxi_day_hour\n\n     day  hour  tip_ratio   n\n0    Mon     0       0.19  13\n1    Mon     1        NaN   2\n2    Mon     2       0.18   6\n..   ...   ...        ...  ..\n165  Sun    21       0.18  42\n166  Sun    22       0.18  32\n167  Sun    23       0.19  23\n\n[168 rows x 4 columns]\n\n\n\n(\n    so.Plot(taxi_day_hour, y='day', x='tip_ratio', color='n')\n    .add(so.Bar(width=.5))\n    .facet(\"hour\", wrap=6)\n    .layout(size=(8, 5.5))\n    .limit(x=(0.12, 0.23))\n    .theme({'ytick.labelsize': 9})\n)\n\n\n\n\n\n\n\n\n\n\n데이터 프레임들의 결합\n\nMerge\n\nmerge()를 사용하여 두 데이터 프레임을 join\nKey에 해당하는 변수들의 값이 매치되는 행들을 찾아 결합\n결합 방식: “inner”, “left”, “right”, “outer”\n\n\nInner join의 예:\n\nnyc_neighborhood.csv dataset\n\nnyc = pd.read_csv(\"data/nyc_neighborhood.csv\")\nnyc\n\n         borough  units  labor  carfree  density      pop\n0      Manhattan   5083   0.68     0.88    71.90  1628706\n1          Bronx   6120   0.59     0.71    33.70  1418207\n2       Brooklyn  10129   0.64     0.75    36.90  2559903\n3         Queens   6752   0.64     0.59    20.70  2253858\n4  Staten Island    582   0.62     0.34     8.30   476143\n\n\n\n(\n    taxi.merge(\n        nyc, left_on=\"pickup_borough\", right_on=\"borough\", how=\"left\"\n    ).head(3)\n)\n\n               pickup             dropoff  persons  distance  fare  tip  \\\n0 2019-03-23 20:21:09 2019-03-23 20:27:24        1      1.60  7.00 2.15   \n1 2019-03-04 16:11:55 2019-03-04 16:19:00        1      0.79  5.00  NaN   \n2 2019-03-27 17:53:01 2019-03-27 18:00:25        1      1.37  7.50 2.36   \n\n   tolls  total   color   pay  ... tip_ratio_per  day hour duration  \\\n0   0.00  12.95  yellow  Card  ...          0.20  Sat   20     6.25   \n1   0.00   9.30  yellow  Cash  ...           NaN  Mon   16     7.08   \n2   0.00  14.16  yellow  Card  ...          0.20  Wed   17     7.40   \n\n     borough   units  labor carfree  density        pop  \n0  Manhattan 5083.00   0.68    0.88    71.90 1628706.00  \n1  Manhattan 5083.00   0.68    0.88    71.90 1628706.00  \n2  Manhattan 5083.00   0.68    0.88    71.90 1628706.00  \n\n[3 rows x 26 columns]\n\n\nMerge를 데이터 필터링에 사용할 수도 있음\n\ntaxi_day_hour\n\n     day  hour  tip_ratio   n\n0    Mon     0       0.19  13\n1    Mon     1        NaN   2\n2    Mon     2       0.18   6\n..   ...   ...        ...  ..\n165  Sun    21       0.18  42\n166  Sun    22       0.18  32\n167  Sun    23       0.19  23\n\n[168 rows x 4 columns]\n\n\n\ndays_top_ratio = (\n    taxi_day_hour\n    .query('n &gt; 10')\n    .nlargest(3, \"tip_ratio\")\n)\ndays_top_ratio\n\n    day  hour  tip_ratio   n\n71  Wed    23       0.23  39\n23  Mon    23       0.22  22\n24  Tue     0       0.22  13\n\n\n팁의 비율이 가장 높았던 3개의 (요일, 시간대) 조합에 해당하는 taxi 데이터셋을 추리려면\n\ntaxi.merge(days_top_ratio, on=[\"day\", \"hour\"])\n\n                pickup             dropoff  persons  distance  fare  tip  \\\n0  2019-03-25 23:05:54 2019-03-25 23:11:13        1      0.80  5.50 2.30   \n1  2019-03-27 23:10:02 2019-03-27 23:22:11        3      2.77 11.00 3.70   \n2  2019-03-20 23:19:55 2019-03-20 23:46:00        1      4.82 19.50 4.66   \n..                 ...                 ...      ...       ...   ...  ...   \n71 2019-03-18 23:48:38 2019-03-18 23:56:36        1      2.00  8.50  NaN   \n72 2019-03-06 23:32:50 2019-03-06 23:34:31        1      0.30  3.50 2.50   \n73 2019-03-26 00:06:16 2019-03-26 00:12:46        1      0.86  6.00  NaN   \n\n    tolls  total   color   pay  ... pickup_borough dropoff_borough total2  \\\n0    0.00  11.60  yellow  Card  ...      Manhattan       Manhattan   9.30   \n1    0.00  18.50  yellow  Card  ...      Manhattan       Manhattan  14.80   \n2    0.00  27.96  yellow  Card  ...      Manhattan       Manhattan  23.30   \n..    ...    ...     ...   ...  ...            ...             ...    ...   \n71   0.00   9.80   green  Cash  ...         Queens          Queens   9.80   \n72   0.00   7.30   green  Card  ...         Queens          Queens   4.80   \n73   0.00   7.30   green  Cash  ...      Manhattan       Manhattan   7.30   \n\n   tip_ratio_x  tip_ratio_per  day  hour duration  tip_ratio_y   n  \n0         0.25           0.25  Mon    23     5.32         0.22  22  \n1         0.25           0.08  Wed    23    12.15         0.23  39  \n2         0.20           0.20  Wed    23    26.08         0.23  39  \n..         ...            ...  ...   ...      ...          ...  ..  \n71         NaN            NaN  Mon    23     7.97         0.22  22  \n72        0.52           0.52  Wed    23     1.68         0.23  39  \n73         NaN            NaN  Tue     0     6.50         0.22  13  \n\n[74 rows x 22 columns]\n\n\n\n\nConcatenate\npd.concat([df1, df2, ...], axis=)\n행과 열의 index를 매치시켜 두 DataFrame/Series를 합침\n\ndf1 = pd.DataFrame(\n    np.arange(6).reshape(3, 2), index=[\"a\", \"b\", \"c\"], columns=[\"one\", \"two\"]\n)\ndf2 = pd.DataFrame(\n    5 + np.arange(4).reshape(2, 2), index=[\"a\", \"c\"], columns=[\"three\", \"four\"]\n)\n\n\n\n\n\n\n\n   one  two\na    0    1\nb    2    3\nc    4    5\n\n\n   three  four\na      5     6\nc      7     8\n\n\n\n\npd.concat([df1, df2], axis=1)\n\n   one  two  three  four\na    0    1   5.00  6.00\nb    2    3    NaN   NaN\nc    4    5   7.00  8.00\n\n\n\npd.concat([df1, df2])  # default: axis=0\n\n   one  two  three  four\na 0.00 1.00    NaN   NaN\nb 2.00 3.00    NaN   NaN\nc 4.00 5.00    NaN   NaN\na  NaN  NaN   5.00  6.00\nc  NaN  NaN   7.00  8.00",
    "crumbs": [
      "Trees",
      "Exploratory Data Analysis",
      "Wrangling"
    ]
  },
  {
    "objectID": "contents/misc.html",
    "href": "contents/misc.html",
    "title": "MISC.",
    "section": "",
    "text": "Human-centered AI?\nSuicide rates for girls are rising. Are smartphones to blame?\n\n\nHow Opioid Overdoses Reached Crisis Levels\n\nOverweight & Obesity Statistics - NIDDK\n\nLack of sex is driven maily by the young"
  },
  {
    "objectID": "contents/pollr.html",
    "href": "contents/pollr.html",
    "title": "설문",
    "section": "",
    "text": "설문 링크"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균\nemail: sk.cho@snu.ac.kr\n수업 시간: 월, 수 3:00 ~ 4:20PM\n면담 시간: 수업 후\nWebsite: dgds101.modellings.art\n과제: Notice, eclass\n질문: Communicate/Ask",
    "crumbs": [
      "Trees",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-정보",
    "href": "index.html#강의-정보",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균\nemail: sk.cho@snu.ac.kr\n수업 시간: 월, 수 3:00 ~ 4:20PM\n면담 시간: 수업 후\nWebsite: dgds101.modellings.art\n과제: Notice, eclass\n질문: Communicate/Ask",
    "crumbs": [
      "Trees",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-개요",
    "href": "index.html#강의-개요",
    "title": "Welcome",
    "section": "강의 개요",
    "text": "강의 개요\n데이터 분석은 오랜 역사를 거쳐 통계학의 영역에서 발전해왔고, 양적연구를 기반으로하는 여러 분야에서 핵심적인 역할을 한 반면, 인공지능의 하위 분야로 연구되어온 기계학습은 방대한 데이터와 더불어 최근에 그 유용성이 크게 부각되면서 이 두 분야는 데이터 사이언스라는 큰 틀에서 통합되고 있습니다. 이러한 광범위한 주제에 대해 각 기법의 핵심적 아이디어와 응용 예시에 초점을 맞추고, 더 세부적인 주제들을 탐구하기 위한 초석을 제공하고자 합니다. 또한 구체적인 예들을 직접 코딩하여 어느 정도 데이터 분석 기술의 기초를 갖추도록 과제를 통해 학습할 기회도 제공됩니다.\n\n전통적 통계와 기계 학습에서 추구하는 바를 이해하고,\n\n데이터로부터 패턴과 의미를 추론하는 방식을 이해하며,\n\n기계/통계적 학습의 응용 가능성에 대해 파악합니다.\n\n\n참고도서\n\nPython Data Science Handbook by Jake VanderPlas: code on GitHub\nAn Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani, Taylor: code on GitHub\nPython for Data Analysis (3e) by Wes McKinney: code on Github\n3판 번역서: 파이썬 라이브러리를 활용한 데이터 분석",
    "crumbs": [
      "Trees",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#수업-활동",
    "href": "index.html#수업-활동",
    "title": "Welcome",
    "section": "수업 활동",
    "text": "수업 활동\n출석 (10%), 일반과제 (20%), 중간고사 (35%), 기말고사 (35%)",
    "crumbs": [
      "Trees",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#수업-계획",
    "href": "index.html#수업-계획",
    "title": "Welcome",
    "section": "수업 계획",
    "text": "수업 계획\n1주. 데이터 사이언스 소개\n2주. 데이터 분석의 두 문화 1: 전통적 통계\n3주. 데이터 분석의 두 문화 2: 기계 학습\n4주. 파이썬 기초\n5주. 탐색적 분석 및 시각화\n6주. 선형 모형(linear model) 소개\n7주. 선형 모형의 활용: 해석과 의미의 추론\n8주. 중간고사\n9주. 기계 학습 기본 1\n10주. 기계 학습 기본 2\n11주. 분류(classification) 1: 로지스틱(logistic) 모형\n12주. 분류(classification) 2: Generative 모형\n13주. Tree-based 모형\n14주. 비지도 학습(unsupervised learning)\n15주. 기말고사",
    "crumbs": [
      "Trees",
      "Welcome"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#humility",
    "href": "contents/two-cultures.html#humility",
    "title": "Two Cultures",
    "section": "Humility",
    "text": "Humility\n\n아인슈타인은 한번 우주를 순전히 물리적 용어로 이해할 수 있을 것이라고 생각하는지 질문을 받았습니다. 그는 “음, 그럴 수도 있지만, 이것은 베토벤 교향곡을 음표(notes)의 주파수 분포(frequency distribution)로 이해하려는 것과 같을 것입니다.”\n\n\n… 나 자신에게는 해변에서 놀고 있는 한 소년에 불과했던 것 같습니다. 그저 평범한 것보다 조금 더 매끄러운 조약돌이나 더 예쁜 조개껍데기를 이따금 발견하며 즐기고 있었을 뿐, 그 동안 진리의 위대한 대양(the great ocean of truth)은 모두 미발견인 채로 내 앞에 놓여 있었습니다. (Isaac Newton)\n\n\nQuotes from Einstein\n\n우리가 경험할 수 있는 가장 아름다운 감정은 신비감(mystical)입니다. 그것은 모든 진정한 예술과 과학의 힘입니다. 낯설게 느껴지는 감정이지만, 더 이상 경이로움(wonder)을 느끼지 못하며 황홀한 경외심(awe)을 갖지 못하는 사람은 죽은 것이나 마찬가지입니다. 우리에게 불가해한(impenetrable) 것이 실제로 존재하며, 그것이 최고의 지혜와 가장 빛나는 아름다움으로 나타난다는 것을 아는 것—우리의 둔한 능력으로는 가장 원시적인 형태로만 이해할 수 있는—이 지식, 이 느낌이 진정한 종교성의 중심에 있습니다. 이런 의미에서, 그리고 오직 이런 의미에서만, 나는 독실하게 종교적인 사람들의 대열에 속합니다.\n\n\n\nEinstein characterized himself as “devoutly religious” in the following sense, “The most beautiful emotion we can experience is the mystical. It is the power of all true art and science. He to whom this emotion is a stranger, who can no longer wonder and stand rapt in awe, is as good as dead. To know that what is impenetrable to us really exists, manifesting itself as the highest wisdom and the most radiant beauty, which our dull faculties can comprehend only in their most primitive forms—this knowledge, this feeling, is at the center of true religiousness. In this sense, and in this sense only, I belong to the rank of devoutly religious men.”\nQuote: WHAT I BELIEVE, FORUM AND CENTURY 84 (OCTOBER 1930), NO. 4, 193-194; EINSTEIN 1954, 8-11\n\n\n\n과학적 연구는 일어나는 모든 일이 자연법칙에 의해 결정된다는 개념에 기초하고 있으며, 따라서 이는 사람들의 행동에도 적용됩니다. 이러한 이유로, 연구 과학자는 기도, 즉 초자연적 존재에게 전달되는 소원에 의해 사건이 영향을 받을 수 있다고 믿기 어렵습니다. 그러나 이러한 법칙에 대한 우리의 실제 지식이 불완전하고 단편적일 뿐이라는 것을 인정해야 합니다. 그래서 사실, 자연에 모든 것을 포괄하는 기본 법칙이 존재한다는 믿음도 일종의 믿음(faith)에 기초하고 있습니다. 그럼에도 불구하고 이러한 믿음은 지금까지 과학적 연구의 성공에 의해 크게 정당화되어 왔습니다. 하지만, 다른 한편으로는, 과학을 진지하게 추구하는 모든 사람은 우주의 법칙 속에 영(spirit)이 나타난다는 확신을 갖게 됩니다—인간의 영보다 훨씬 우월한 영이며, 우리는 우리의 미약한 능력으로 그 앞에서 겸손함을 느껴야 하는 영입니다. 이런 방식으로 과학의 추구는 특별한 종류의 종교적 감정으로 이어지며, 이는 실제로 더 순진한(naive) 사람의 종교성과는 상당히 다릅니다.\n\n\n\nScientific research is based on the idea that everything that takes place is determined by laws of nature, and therefore this holds for the actions of people. For this reason, a research scientist will hardly be inclined to believe that events could be influenced by a prayer, i.e. by a wish addressed to a supernatural being. However, it must be admitted that our actual knowledge of these laws is only imperfect and fragmentary, so that, actually, the belief in the existence of basic all-embracing laws in nature also rests on a sort of faith. All the same this faith has been largely justified so far by the success of scientific research. But, on the other hand, everyone who is seriously involved in the pursuit of science becomes convinced that a spirit is manifest in the laws of the universe—a spirit vastly superior to that of man, and one in the face of which we with our modest powers must feel humble. In this way the pursuit of science leads to a religious feeling of a special sort, which is indeed quite different from the religiosity of someone more naive.\nQuote: Einstein, Albert (2013) Albert Einstein, The Human Side. Princeton: Princeton University Press, pp. 32-33.",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#간략한-역사적-배경",
    "href": "contents/two-cultures.html#간략한-역사적-배경",
    "title": "Two Cultures",
    "section": "간략한 역사적 배경",
    "text": "간략한 역사적 배경\n\n\n\n\n\n\n인과에 대한 철학적 논쟁들\n\n\n\n\n정의, 인식 가능성, 필연성\n다중 원인, 상호작용  \n시간적 순서성, 인과의 피트백 \n결정론적 vs. 확률론적\n\n\n\n\nThe Grammar of Science (1892), by Karl Pearson\n\n특정 시퀀스가 과거에 발생하고 반복되었다는 것은 경험의 문제이며, 인과라는 개념 안에서 그렇게 표현합니다. 미래에도 계속 반복될 것이라는 것은 신념의 문제이며, 확률이라는 개념 안에서 그렇게 표현합니다. 과학은 어떤 경우에도 시퀀스에 내재된 필연성을 입증할 수 없으며, 시퀀스가 반드시 반복되어야 한다는 것을 절대적으로 확실하게 증명할 수도 없습니다. 과거에 대한 과학은 묘사이고 미래에 대한 과학은 믿음입니다;\n\n\n\nThat a certain sequence has occurred and recurred in the past is a matter of experience to which we give expression in the concept causation; that it will continue to recur in the future is a matter of belief to which we give expression in the concept probability. Science in no case can demon­strate any inherent necessity in a sequence, nor prove with absolute certainty that it must be repeated. Science for the past is a description, for the future a belief; (Pearson, 1892, p. 113).\n\n\n\n인과 관계라는 개념은 현상에서 개념적 과정을 통해 추출된 것으로, 논리적 필연도 아니고 실제 경험도 아닙니다…. 우주에 대한 더 넓은 관점에서보면 모든 현상은 상관관계로서 보이지만, 인과적으로는 관계하지 않는 것으로 보입니다.\n\n\n\nthe idea of causation is extracted by conceptual processes from phenomena, it is neither a logical necessity, nor an actual experience…. The wider view of the universe sees all phenomena as correlated, but not causally related. (Pearson, 1892, p. 177)\n\n\n\n\nPositivism: 우주가 인간 사고의 산물이며 과학은 그 사고에 대한 설명일 뿐이라고 주장\n인간의 뇌 밖의 세계에서 일어나는 객관적인 과정으로 해석되는 인과관계는 과학적 의미를 가질 수 없음\n오직 관찰된 패턴만을 반영할 수 있으며, 반복적인 규칙성(regularity of succession)을 통해 상관관계(correlation/association)로 설명될 수 있음\n피어슨은 인과관계를 분석의 언어에서 배제시키고, 이후 그 전통이 통계학에서 어어짐\n과학에서 인과문제는?; why의 문제를 how의 문제로 대수적 방정식을 통해 대체했음\n\nSewall Wright\n\n\n\nPath diagrams라는 데이터로부터 인과 관계에 대한 질문에 답하는 수학적 방법을 최초로 개발\n실제로 인과 분석의 툴로 이어지지는 못하였고,\nPath analysis(경로 분석)이라는 통계 기법으로 어어졌으나 뿌리 깊은 오해의 씨앗이 되었음.\n\n  \n\n기니피그의 털색에 영향을 미치는 요인을 설명하는 경로 다이어그램. D = 발달 요인(수태 후, 출생 전), E = 환경 요인(출생 후), G = 각 개별 부모의 유전적 요인, H = 부모 모두의 유전적 요인을 합친 것, O, O′ = 자손. 분석의 목적은 D, E, H(다이어그램에서는 D, E, H로 표기)의 영향력을 추정하는 것\n\n\n\n\n\n\n\nSewall Wright in 1954\nfrom Wikipedia\n\n\n\n\nPath Analysis",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#인과-추론을-향한-고군분투",
    "href": "contents/two-cultures.html#인과-추론을-향한-고군분투",
    "title": "Two Cultures",
    "section": "인과 추론을 향한 고군분투",
    "text": "인과 추론을 향한 고군분투\n담배는 폐암의 원인인가?\n\n1950년 ~ 1964년 미국에서 의사, 역학자, 통계학자 사이의 큰 논쟁\n통계적으로 입증할 수 있는가? 실험군-대조군의 실험적 방법론에 이외의 다른 방법이 있는가?\n\n교란(confounding) 변수가 전혀 존재하지 않는다는 것을 입증할 수 있는가?; 흡연 유전자, 흡연자의 라이프스타일, 흡연자의 주위 환경 등\n\nUS Surgeon General이 임명한 특별 자문위원회의 고민 (1964)\n\n\n위원회는 보고서를 위해 1년 이상 노력했고, 주요 문제 중 하나는 “원인”이라는 단어의 사용이었습니다. 위원회 위원들은 19세기의 결정론적 인과성 개념을 제쳐두어야 했고, 통계도 제쳐두어야 했습니다. (아마도 코크런이) 보고서에 적었듯이, “통계적 방법은 연관성에서 인과 관계를 증명할 수 없습니다. 연관성(association)의 인과적 유의성(causal significance)은 통계적 확률에 대한 진술을 넘어서는 판단의 문제입니다. 속성이나 요인과 질병 또는 건강에 미치는 영향 사이의 연관성의 인과적 중요성을 판단하거나 평가하려면 여러 가지 기준을 사용해야 하며, 그 중 어느 것도 판단의 충분한 근거가 될 수 없습니다.”\n\n\n\n“Statistical methods cannot establish proof of a causal relationship in an association. The causal significance of an association is a matter of judgment which goes beyond any statement of statistical probability. To judge or evaluate the causal significance of the association between the attribute or agent and the disease, or effect upon health, a number of criteria must be utilized, no one of which is an all-sufficient basis for judgment.”\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n\n임산부의 흡연은 태아의 생존에 이로운가?\nBirth-weight Paradox\n\n1960년대 중반, Jacob Yerushalmy의 논문\n\n이미 여러 연구에서 흡연자 아기가 출생 시 체중이 더 가볍다는 것이 밝혀졌음; 저체중은 영아사망율을 높임\n저체중아 중, 어머니가 흡연자인 경우 생존율이 더 높았음!\n\n논문이 발표된 지 40년이 넘은 2006년까지 만족스럽게 설명되지 않았음\n흡연자 대신 흑인으로 대체해도 같은 현상을 발견\n의학에서 비슷한 패러독스가 자주 발견됨; 예. 당뇨 환자의 경우 비만이 생존에 이득이 되는 것처럼 나타남.\n\n  \n\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n학과별로 공정했어도 대학은 차별할 수 있을까?\nUC Berkeley Admission Paradox\n\n1973년, 대학원에 진학한 남성과 여성의 합격 비율이 각각 44%, 35%였음\n입학 결정은 학과별로 독립적으로 내렸음\n학과별로는 여성의 합격 비율이 더 높았음!\nPeter Bickel(버클리대의 통계학자)와 William Kruskal(시카고대의 통계학자) 사이의 논쟁\n\n학과별로 나누어 살펴보는 것으로 충분한가?\n\n\n\n\n\n\n\n\n\n\nSource: The Book of Why: The New Science of Cause and Effect by Judea Pearl, Dana Mackenzie (2018)\n\n효과는 없었으나 성공적인 정책?\n\n1990년대 최악의 시카고 공립학교의 개혁 정책\n고1에서 보충 과목을 없애고, 대학 진학 준비 과목을 수강하도록 함.\n이 중 대수 1 과목(“Algebra for All”)의 경우 3년 간 유의한 성적 개선이 없었음.\nGuanglei Hong(시카고대 인간발달)은 정책의 직접적 효과는 존재한다고 판별했음!\n\n정책이 두 가지 방식으로 (다른 방향으로) 작용했음.\n이후 “Double-Dose Algebra”으로 개선했음.",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/two-cultures.html#structural-causal-modelscm",
    "href": "contents/two-cultures.html#structural-causal-modelscm",
    "title": "Two Cultures",
    "section": "Structural Causal Model(SCM)",
    "text": "Structural Causal Model(SCM)\n개입(intervention)의 과학을 위한 인과를 표현하기 위한 새로운 수학적 언어가 요구됨!\n\n\n\n관찰(seeing)과 개입(doing)에 대한 대수학(algebra)\n\n\n\n\n\n\n\n관찰(seeing)\n\n개입(doing)\n\n\n\n\n잔디가 젖었다는 것을 보았을 때(see) 비가 내렸을 가능성은 얼마인가?\n\n잔디를 젖게 하면(doing) 비가 내릴 가능성은 얼마인가?\n\n\n\\(P(~rain~ | ~wet~)\\)\n\n\\(P( ~rain~ | ~do(wet)~)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n인과 효과: 개입했을 때 어떤 변화가 나타나는가? (why → how)\n\n\n\n\n물리학의 법칙도 소위 “방정식”으로 표현되었음; 방정식의 대칭성\n\n\\(F = m*a\\)\n\\(a = \\frac{F}{m}\\)\n\\(m = \\frac{F}{a}\\)\n\n통계학은 확률이라는 수학적 언어로 표현되어 개입의 효과를 표현할 방법이 없음\n\n“잔디가 젖어 있으면, 비가 왔다”와 “이 스프링클러를 켜면, 잔디가 젖을 것이다”라는 정보가 주어지면,\n컴퓨터는 “이 스프링클러를 켜면, 비가 왔다”라고 결론 내릴 것임.\n\n\n\nDirected acyclic graph (DAG): 인과 관계 다이어그램\n\n\nSource: Causality: Models, Reasoning, and Inference (2000) by Judea Pearl",
    "crumbs": [
      "Trees",
      "Introduction",
      "Two Cultures"
    ]
  },
  {
    "objectID": "contents/python-basics.html",
    "href": "contents/python-basics.html",
    "title": "Python Basics",
    "section": "",
    "text": "NumPy and pandas\nData Inspection\nSubsetting",
    "crumbs": [
      "Trees",
      "Python Basics"
    ]
  }
]